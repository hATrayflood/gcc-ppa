# DP: Changes for the Linaro 4.8-2015.02 release.

LANG=C svn diff svn://gcc.gnu.org/svn/gcc/branches/gcc-4_8-branch@220524 \
    svn://gcc.gnu.org/svn/gcc/branches/linaro/gcc-4_8-branch@220724 \
 | filterdiff --remove-timestamps --addoldprefix=a/src/ --addnewprefix=b/src/

Index: b/src/libitm/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libitm/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libgomp/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libgomp/ChangeLog.linaro
@@ -0,0 +1,75 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-22  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200521.
+	2013-06-28  Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* testsuite/libgomp.fortran/strassen.f90:
+	Add dg-skip-if aarch64_tiny.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libgomp/testsuite/libgomp.fortran/strassen.f90
===================================================================
--- a/src/libgomp/testsuite/libgomp.fortran/strassen.f90
+++ b/src/libgomp/testsuite/libgomp.fortran/strassen.f90
@@ -1,4 +1,5 @@
 ! { dg-options "-O2" }
+! { dg-skip-if "AArch64 tiny code model does not support programs larger than 1MiB" {aarch64_tiny} {"*"} {""} }
 
 program strassen_matmul
   use omp_lib
Index: b/src/libquadmath/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libquadmath/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libsanitizer/sanitizer_common/sanitizer_linux.cc
===================================================================
--- a/src/libsanitizer/sanitizer_common/sanitizer_linux.cc
+++ b/src/libsanitizer/sanitizer_common/sanitizer_linux.cc
@@ -410,7 +410,9 @@ bool MemoryMappingLayout::Next(uptr *sta
   CHECK_EQ(*current_++, ' ');
   while (IsDecimal(*current_))
     current_++;
-  CHECK_EQ(*current_++, ' ');
+  // Qemu may lack the trailing space.
+  // http://code.google.com/p/address-sanitizer/issues/detail?id=160
+  // CHECK_EQ(*current_++, ' ');
   // Skip spaces.
   while (current_ < next_line && *current_ == ' ')
     current_++;
Index: b/src/libsanitizer/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libsanitizer/ChangeLog.linaro
@@ -0,0 +1,82 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-20  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198683.
+	2013-05-07  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* configure.tgt: Add ARM pattern.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-06-04  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199606.
+	2013-06-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* sanitizer_common/sanitizer_linux.cc (MemoryMappingLayout::Next):
+	Cherry pick upstream r182922.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libsanitizer/configure.tgt
===================================================================
--- a/src/libsanitizer/configure.tgt
+++ b/src/libsanitizer/configure.tgt
@@ -32,6 +32,8 @@ case "${target}" in
 	;;
   sparc*-*-linux*)
 	;;
+  arm*-*-linux*)
+	;;
   x86_64-*-darwin[1]* | i?86-*-darwin[1]*)
 	TSAN_SUPPORTED=no
 	;;
Index: b/src/zlib/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/zlib/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libstdc++-v3/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libstdc++-v3/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/configure.ac
===================================================================
--- a/src/configure.ac
+++ b/src/configure.ac
@@ -611,6 +611,8 @@ esac
 
 # Disable Java if libffi is not supported.
 case "${target}" in
+  aarch64-*-*)
+    ;;
   alpha*-*-*)
     ;;
   arm*-*-*)
Index: b/src/intl/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/intl/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/ChangeLog.linaro
@@ -0,0 +1,75 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-12-06  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r197997
+	2013-04-16 Andreas Schwab  <schwab@suse.de>
+
+	* configure.ac (aarch64-*-*): Don't disable java.
+	* configure: Regenerate.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libmudflap/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libmudflap/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/boehm-gc/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/boehm-gc/ChangeLog.linaro
@@ -0,0 +1,80 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197770.
+
+	2013-03-16  Yvan Roux <yvan.roux@linaro.org>
+
+	* include/private/gcconfig.h (AARCH64): New macro (defined only if
+	__aarch64__).
+	(mach_type_known): Update comment adding ARM AArch64 target.
+	(NOSYS, mach_type_known,CPP_WORDSZ, MACH_TYPE, ALIGNMENT, HBLKSIZE,
+	OS_TYPE, LINUX_STACKBOTTOM, USE_GENERIC_PUSH_REGS, DYNAMIC_LOADING,
+	DATASTART, DATAEND, STACKBOTTOM): Define for AArch64.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/boehm-gc/include/private/gcconfig.h
===================================================================
--- a/src/boehm-gc/include/private/gcconfig.h
+++ b/src/boehm-gc/include/private/gcconfig.h
@@ -60,6 +60,13 @@
 # endif
 
 /* Determine the machine type: */
+#if defined(__aarch64__)
+#    define AARCH64
+#    if !defined(LINUX)
+#      define NOSYS
+#      define mach_type_known
+#    endif
+# endif
 # if defined(__arm__) || defined(__thumb__)
 #    define ARM32
 #    if !defined(LINUX) && !defined(NETBSD)
@@ -239,6 +246,10 @@
 #    define IA64
 #    define mach_type_known
 # endif
+# if defined(LINUX) && defined(__aarch64__)
+#    define AARCH64
+#    define mach_type_known
+# endif
 # if defined(LINUX) && defined(__arm__)
 #    define ARM32
 #    define mach_type_known
@@ -500,6 +511,7 @@
 		    /* 			running Amdahl UTS4		*/
                     /*             S390       ==> 390-like machine      */
 		    /*                  running LINUX                   */
+                    /*             AARCH64    ==> ARM AArch64           */
 		    /* 		   ARM32      ==> Intel StrongARM	*/
 		    /* 		   IA64	      ==> Intel IPF		*/
 		    /*				  (e.g. Itanium)	*/
@@ -1841,6 +1853,32 @@
 #   define HEURISTIC1
 # endif
 
+# ifdef AARCH64
+#   define CPP_WORDSZ 64
+#   define MACH_TYPE "AARCH64"
+#   define ALIGNMENT 8
+#   ifndef HBLKSIZE
+#     define HBLKSIZE 4096
+#   endif
+#   ifdef LINUX
+#     define OS_TYPE "LINUX"
+#     define LINUX_STACKBOTTOM
+#     define USE_GENERIC_PUSH_REGS
+#     define DYNAMIC_LOADING
+      extern int __data_start[];
+#     define DATASTART ((ptr_t)__data_start)
+      extern char _end[];
+#     define DATAEND ((ptr_t)(&_end))
+#   endif
+#   ifdef NOSYS
+      /* __data_start is usually defined in the target linker script.   */
+      extern int __data_start[];
+#     define DATASTART ((ptr_t)__data_start)
+      extern void *__stack_base__;
+#     define STACKBOTTOM ((ptr_t)__stack_base__)
+#   endif
+# endif
+
 # ifdef ARM32
 #   define CPP_WORDSZ 32
 #   define MACH_TYPE "ARM32"
Index: b/src/include/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/include/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libiberty/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libiberty/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/lto-plugin/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/lto-plugin/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/contrib/regression/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/contrib/regression/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/contrib/config-list.mk
===================================================================
--- a/src/contrib/config-list.mk
+++ b/src/contrib/config-list.mk
@@ -11,7 +11,8 @@ TEST=all-gcc
 # nohup nice make -j25 -l36 -f ../gcc/contrib/config-list.mk > make.out 2>&1 &
 #
 # v850e1-elf is rejected by config.sub
-LIST = alpha-linux-gnu alpha-freebsd6 alpha-netbsd alpha-openbsd \
+LIST = aarch64-elf aarch64-linux-gnu \
+  alpha-linux-gnu alpha-freebsd6 alpha-netbsd alpha-openbsd \
   alpha64-dec-vms alpha-dec-vms am33_2.0-linux \
   arm-wrs-vxworks arm-netbsdelf \
   arm-linux-androideabi arm-uclinux_eabi arm-eabi \
Index: b/src/contrib/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/contrib/ChangeLog.linaro
@@ -0,0 +1,74 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198443.
+	2013-04-22  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config-list.mk (LIST): Add aarch64-elf and aarch64-linux-gnu.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/contrib/reghunt/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/contrib/reghunt/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libatomic/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libatomic/ChangeLog.linaro
@@ -0,0 +1,75 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-12-06  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203774
+	2013-10-17  Michael Hudson-Doyle  <michael.hudson@linaro.org>
+
+	* libatomic/configure.tgt (aarch64*): Remove code preventing
+	build.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libatomic/configure.tgt
===================================================================
--- a/src/libatomic/configure.tgt
+++ b/src/libatomic/configure.tgt
@@ -95,11 +95,6 @@ fi
 
 # Other system configury
 case "${target}" in
-  aarch64*)
-	# This is currently not supported in AArch64.
-	UNSUPPORTED=1
-	;;
-
   arm*-*-linux*)
 	# OS support for atomic primitives.
 	config_path="${config_path} linux/arm posix"
Index: b/src/config/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/config/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libbacktrace/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libbacktrace/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libjava/libltdl/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libjava/libltdl/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libjava/configure.host
===================================================================
--- a/src/libjava/configure.host
+++ b/src/libjava/configure.host
@@ -81,6 +81,11 @@ ATOMICSPEC=
 
 # This case statement supports per-CPU defaults.
 case "${host}" in
+  aarch64*-linux*)
+	libgcj_interpreter=yes
+	sysdeps_dir=aarch64
+	ATOMICSPEC=-fuse-atomic-builtins
+	;;
   arm*-elf)
 	with_libffi_default=no
 	PROCESS=Ecos
@@ -289,6 +294,12 @@ EOF
 	sysdeps_dir=i386
 	DIVIDESPEC=-f%{m32:no-}use-divide-subroutine
 	;;
+  aarch64*-linux* )
+	slow_pthread_self=no
+	can_unwind_signal=no
+	CHECKREFSPEC=-fcheck-references
+	DIVIDESPEC=-fuse-divide-subroutine
+	;;
   arm*-linux* )
 	slow_pthread_self=no
 	can_unwind_signal=no
Index: b/src/libjava/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libjava/ChangeLog.linaro
@@ -0,0 +1,75 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-12-06  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r197997
+	2013-04-16 Andreas Schwab  <schwab@suse.de>
+
+	* configure.host: Add support for aarch64.
+	* sysdep/aarch64/locks.h: New file.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libjava/classpath/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libjava/classpath/ChangeLog.linaro
@@ -0,0 +1,74 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-12-06  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r197997
+	2013-04-16 Andreas Schwab  <schwab@suse.de>
+
+	* native/fdlibm/ieeefp.h: Add support for aarch64.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libjava/classpath/native/fdlibm/ieeefp.h
===================================================================
--- a/src/libjava/classpath/native/fdlibm/ieeefp.h
+++ b/src/libjava/classpath/native/fdlibm/ieeefp.h
@@ -4,6 +4,14 @@
 #ifndef __IEEE_BIG_ENDIAN
 #ifndef __IEEE_LITTLE_ENDIAN
 
+#ifdef __aarch64__
+#ifdef __AARCH64EB__
+#define __IEEE_BIG_ENDIAN
+#else
+#define __IEEE_LITTLE_ENDIAN
+#endif
+#endif
+
 #ifdef __alpha__
 #define __IEEE_LITTLE_ENDIAN
 #endif
Index: b/src/libjava/sysdep/aarch64/locks.h
===================================================================
--- /dev/null
+++ b/src/libjava/sysdep/aarch64/locks.h
@@ -0,0 +1,57 @@
+// locks.h - Thread synchronization primitives. AArch64 implementation.
+
+#ifndef __SYSDEP_LOCKS_H__
+#define __SYSDEP_LOCKS_H__
+
+typedef size_t obj_addr_t;	/* Integer type big enough for object	*/
+				/* address.				*/
+
+// Atomically replace *addr by new_val if it was initially equal to old.
+// Return true if the comparison succeeded.
+// Assumed to have acquire semantics, i.e. later memory operations
+// cannot execute before the compare_and_swap finishes.
+inline static bool
+compare_and_swap(volatile obj_addr_t *addr,
+                 obj_addr_t old,
+                 obj_addr_t new_val)
+{
+  return __sync_bool_compare_and_swap(addr, old, new_val);
+}
+
+// Set *addr to new_val with release semantics, i.e. making sure
+// that prior loads and stores complete before this
+// assignment.
+inline static void
+release_set(volatile obj_addr_t *addr, obj_addr_t new_val)
+{
+  __sync_synchronize();
+  *addr = new_val;
+}
+
+// Compare_and_swap with release semantics instead of acquire semantics.
+// On many architecture, the operation makes both guarantees, so the
+// implementation can be the same.
+inline static bool
+compare_and_swap_release(volatile obj_addr_t *addr,
+			 obj_addr_t old,
+			 obj_addr_t new_val)
+{
+  return __sync_bool_compare_and_swap(addr, old, new_val);
+}
+
+// Ensure that subsequent instructions do not execute on stale
+// data that was loaded from memory before the barrier.
+inline static void
+read_barrier()
+{
+  __sync_synchronize();
+}
+
+// Ensure that prior stores to memory are completed with respect to other
+// processors.
+inline static void
+write_barrier()
+{
+  __sync_synchronize();
+}
+#endif
Index: b/src/gnattools/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gnattools/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/maintainer-scripts/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/maintainer-scripts/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/configure
===================================================================
--- a/src/configure
+++ b/src/configure
@@ -3272,6 +3272,8 @@ esac
 
 # Disable Java if libffi is not supported.
 case "${target}" in
+  aarch64-*-*)
+    ;;
   alpha*-*-*)
     ;;
   arm*-*-*)
Index: b/src/libgcc/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libgcc/ChangeLog.linaro
@@ -0,0 +1,77 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198090.
+	2013-04-19  Yufeng Zhang  <yufeng.zhang@arm.com>
+
+	* config/aarch64/sfp-machine.h (_FP_W_TYPE): Change to define
+	as 'unsigned long long' instead of 'unsigned long'.
+	(_FP_WS_TYPE): Change to define as 'signed long long' instead of
+	'signed long'.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libgcc/config/aarch64/sfp-machine.h
===================================================================
--- a/src/libgcc/config/aarch64/sfp-machine.h
+++ b/src/libgcc/config/aarch64/sfp-machine.h
@@ -24,8 +24,8 @@ see the files COPYING3 and COPYING.RUNTI
 <http://www.gnu.org/licenses/>.  */
 
 #define _FP_W_TYPE_SIZE		64
-#define _FP_W_TYPE		unsigned long
-#define _FP_WS_TYPE		signed long
+#define _FP_W_TYPE		unsigned long long
+#define _FP_WS_TYPE		signed long long
 #define _FP_I_TYPE		int
 
 typedef int TItype __attribute__ ((mode (TI)));
Index: b/src/libgcc/config/libbid/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libgcc/config/libbid/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libdecnumber/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libdecnumber/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/LINARO-VERSION
===================================================================
--- /dev/null
+++ b/src/gcc/LINARO-VERSION
@@ -0,0 +1 @@
+4.8-2015.02
Index: b/src/gcc/targhooks.c
===================================================================
--- a/src/gcc/targhooks.c
+++ b/src/gcc/targhooks.c
@@ -1042,20 +1042,17 @@ default_add_stmt_cost (void *data, int c
   unsigned *cost = (unsigned *) data;
   unsigned retval = 0;
 
-  if (flag_vect_cost_model)
-    {
-      tree vectype = stmt_info ? stmt_vectype (stmt_info) : NULL_TREE;
-      int stmt_cost = default_builtin_vectorization_cost (kind, vectype,
+  tree vectype = stmt_info ? stmt_vectype (stmt_info) : NULL_TREE;
+  int stmt_cost = default_builtin_vectorization_cost (kind, vectype,
 							  misalign);
-      /* Statements in an inner loop relative to the loop being
-	 vectorized are weighted more heavily.  The value here is
-	 arbitrary and could potentially be improved with analysis.  */
-      if (where == vect_body && stmt_info && stmt_in_inner_loop_p (stmt_info))
-	count *= 50;  /* FIXME.  */
+   /* Statements in an inner loop relative to the loop being
+      vectorized are weighted more heavily.  The value here is
+      arbitrary and could potentially be improved with analysis.  */
+  if (where == vect_body && stmt_info && stmt_in_inner_loop_p (stmt_info))
+    count *= 50;  /* FIXME.  */
 
-      retval = (unsigned) (count * stmt_cost);
-      cost[where] += retval;
-    }
+  retval = (unsigned) (count * stmt_cost);
+  cost[where] += retval;
 
   return retval;
 }
Index: b/src/gcc/cppbuiltin.c
===================================================================
--- a/src/gcc/cppbuiltin.c
+++ b/src/gcc/cppbuiltin.c
@@ -53,18 +53,41 @@ parse_basever (int *major, int *minor, i
     *patchlevel = s_patchlevel;
 }
 
+/* Parse a LINAROVER version string of the format "M.m-year.month[-spin][~dev]"
+   to create Linaro release number YYYYMM and spin version.  */
+static void
+parse_linarover (int *release, int *spin)
+{
+  static int s_year = -1, s_month, s_spin;
+
+  if (s_year == -1)
+    if (sscanf (LINAROVER, "%*[^-]-%d.%d-%d", &s_year, &s_month, &s_spin) != 3)
+      {
+	sscanf (LINAROVER, "%*[^-]-%d.%d", &s_year, &s_month);
+	s_spin = 0;
+      }
+
+  if (release)
+    *release = s_year * 100 + s_month;
+
+  if (spin)
+    *spin = s_spin;
+}
 
 /* Define __GNUC__, __GNUC_MINOR__, __GNUC_PATCHLEVEL__ and __VERSION__.  */
 static void
 define__GNUC__ (cpp_reader *pfile)
 {
-  int major, minor, patchlevel;
+  int major, minor, patchlevel, linaro_release, linaro_spin;
 
   parse_basever (&major, &minor, &patchlevel);
+  parse_linarover (&linaro_release, &linaro_spin);
   cpp_define_formatted (pfile, "__GNUC__=%d", major);
   cpp_define_formatted (pfile, "__GNUC_MINOR__=%d", minor);
   cpp_define_formatted (pfile, "__GNUC_PATCHLEVEL__=%d", patchlevel);
   cpp_define_formatted (pfile, "__VERSION__=\"%s\"", version_string);
+  cpp_define_formatted (pfile, "__LINARO_RELEASE__=%d", linaro_release);
+  cpp_define_formatted (pfile, "__LINARO_SPIN__=%d", linaro_spin);
   cpp_define_formatted (pfile, "__ATOMIC_RELAXED=%d", MEMMODEL_RELAXED);
   cpp_define_formatted (pfile, "__ATOMIC_SEQ_CST=%d", MEMMODEL_SEQ_CST);
   cpp_define_formatted (pfile, "__ATOMIC_ACQUIRE=%d", MEMMODEL_ACQUIRE);
Index: b/src/gcc/hooks.c
===================================================================
--- a/src/gcc/hooks.c
+++ b/src/gcc/hooks.c
@@ -147,6 +147,14 @@ hook_bool_FILEptr_rtx_false (FILE *a ATT
   return false;
 }
 
+/* Generic hook that takes (gimple_stmt_iterator *) and returns
+   false.  */
+bool
+hook_bool_gsiptr_false (gimple_stmt_iterator *a ATTRIBUTE_UNUSED)
+{
+  return false;
+}
+
 /* Used for the TARGET_ASM_CAN_OUTPUT_MI_THUNK hook.  */
 bool
 hook_bool_const_tree_hwi_hwi_const_tree_false (const_tree a ATTRIBUTE_UNUSED,
Index: b/src/gcc/hooks.h
===================================================================
--- a/src/gcc/hooks.h
+++ b/src/gcc/hooks.h
@@ -42,6 +42,7 @@ extern bool hook_bool_tree_false (tree);
 extern bool hook_bool_const_tree_false (const_tree);
 extern bool hook_bool_tree_true (tree);
 extern bool hook_bool_const_tree_true (const_tree);
+extern bool hook_bool_gsiptr_false (gimple_stmt_iterator *);
 extern bool hook_bool_const_tree_hwi_hwi_const_tree_false (const_tree,
 							   HOST_WIDE_INT,
 							   HOST_WIDE_INT,
Index: b/src/gcc/c-family/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/c-family/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/java/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/java/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/c/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/c/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/target.def
===================================================================
--- a/src/gcc/target.def
+++ b/src/gcc/target.def
@@ -1289,13 +1289,24 @@ DEFHOOK
  "",
  tree, (unsigned int /*location_t*/ loc, tree fndecl, void *arglist), NULL)
 
-/* Fold a target-specific builtin.  */
+/* Fold a target-specific builtin to a tree valid for both GIMPLE
+   and GENERIC.  */
 DEFHOOK
 (fold_builtin,
  "",
  tree, (tree fndecl, int n_args, tree *argp, bool ignore),
  hook_tree_tree_int_treep_bool_null)
 
+/* Fold a target-specific builtin to a valid GIMPLE tree.  */
+DEFHOOK
+(gimple_fold_builtin,
+ "Fold a call to a machine specific built-in function that was set up\n\
+by @samp{TARGET_INIT_BUILTINS}.  @var{gsi} points to the gimple\n\
+statement holding the function call.  Returns true if any change\n\
+was made to the GIMPLE stream.",
+ bool, (gimple_stmt_iterator *gsi),
+ hook_bool_gsiptr_false)
+
 /* Target hook is used to compare the target attributes in two functions to
    determine which function's features get higher priority.  This is used
    during function multi-versioning to figure out the order in which two
Index: b/src/gcc/incpath.c
===================================================================
--- a/src/gcc/incpath.c
+++ b/src/gcc/incpath.c
@@ -148,20 +148,22 @@ add_standard_paths (const char *sysroot,
 	      if (!filename_ncmp (p->fname, cpp_GCC_INCLUDE_DIR, len))
 		{
 		  char *str = concat (iprefix, p->fname + len, NULL);
-		  if (p->multilib == 1 && imultilib)
-		    str = reconcat (str, str, dir_separator_str,
-				    imultilib, NULL);
-		  else if (p->multilib == 2)
+		  if (p->multilib && imultilib)
+                    {
+		      str = reconcat (str, str, dir_separator_str,
+				      imultilib, NULL);
+		      add_path (str, SYSTEM, p->cxx_aware, false);
+		    }
+		  else
+		    add_path (str, SYSTEM, p->cxx_aware, false);
+
+		  if (p->multilib && imultiarch)
 		    {
-		      if (!imultiarch)
-			{
-			  free (str);
-			  continue;
-			}
+                      char *str = concat (iprefix, p->fname + len, NULL);
 		      str = reconcat (str, str, dir_separator_str,
 				      imultiarch, NULL);
+		      add_path (str, SYSTEM, p->cxx_aware, false);
 		    }
-		  add_path (str, SYSTEM, p->cxx_aware, false);
 		}
 	    }
 	}
@@ -171,7 +173,7 @@ add_standard_paths (const char *sysroot,
     {
       if (!p->cplusplus || cxx_stdinc)
 	{
-	  char *str;
+	  char *str, *str2;
 
 	  /* Should this directory start with the sysroot?  */
 	  if (sysroot && p->add_sysroot)
@@ -215,19 +217,20 @@ add_standard_paths (const char *sysroot,
 	  else
 	    str = update_path (p->fname, p->component);
 
-	  if (p->multilib == 1 && imultilib)
-	    str = reconcat (str, str, dir_separator_str, imultilib, NULL);
-	  else if (p->multilib == 2)
+	  str2 = xstrdup(str);
+	  if (p->multilib && imultilib)
 	    {
-	      if (!imultiarch)
-		{
-		  free (str);
-		  continue;
-		}
-	      str = reconcat (str, str, dir_separator_str, imultiarch, NULL);
+	      str = reconcat (str, str, dir_separator_str, imultilib, NULL);
+	      add_path (str, SYSTEM, p->cxx_aware, false);
 	    }
+	  else
+	    add_path (str, SYSTEM, p->cxx_aware, false);
 
-	  add_path (str, SYSTEM, p->cxx_aware, false);
+	  if (p->multilib && imultiarch)
+	    {
+	      str2 = reconcat (str2, str2, dir_separator_str, imultiarch, NULL);
+	      add_path (str2, SYSTEM, p->cxx_aware, false);
+	    }
 	}
     }
 }
Index: b/src/gcc/rtlanal.c
===================================================================
--- a/src/gcc/rtlanal.c
+++ b/src/gcc/rtlanal.c
@@ -1228,6 +1228,10 @@ noop_move_p (const_rtx insn)
   if (find_reg_note (insn, REG_EQUAL, NULL_RTX))
     return 0;
 
+  /* Check the code to be executed for COND_EXEC.  */
+  if (GET_CODE (pat) == COND_EXEC)
+    pat = COND_EXEC_CODE (pat);
+
   if (GET_CODE (pat) == SET && set_noop_p (pat))
     return 1;
 
Index: b/src/gcc/configure
===================================================================
--- a/src/gcc/configure
+++ b/src/gcc/configure
@@ -1667,7 +1667,8 @@ Optional Packages:
                           use sysroot as the system root during the build
   --with-sysroot[=DIR]    search for usr/lib, usr/include, et al, within DIR
   --with-specs=SPECS      add SPECS to driver command-line processing
-  --with-pkgversion=PKG   Use PKG in the version string in place of "GCC"
+  --with-pkgversion=PKG   Use PKG in the version string in place of "Linaro
+                          GCC `cat $srcdir/LINARO-VERSION`"
   --with-bugurl=URL       Direct users to URL to report a bug
   --with-multilib-list    select multilibs (SH and x86-64 only)
   --with-gnu-ld           assume the C compiler uses GNU ld default=no
@@ -7336,7 +7337,7 @@ if test "${with_pkgversion+set}" = set;
       *)   PKGVERSION="($withval) " ;;
      esac
 else
-  PKGVERSION="(GCC) "
+  PKGVERSION="(Linaro GCC `cat $srcdir/LINARO-VERSION`) "
 
 fi
 
@@ -26026,8 +26027,9 @@ esac
 # ??? Once 2.11 is released, probably need to add first known working
 # version to the per-target configury.
 case "$cpu_type" in
-  alpha | arm | avr | bfin | cris | i386 | m32c | m68k | microblaze | mips \
-  | pa | rs6000 | score | sparc | spu | tilegx | tilepro | xstormy16 | xtensa)
+  aarch64 | alpha | arm | avr | bfin | cris | i386 | m32c | m68k | microblaze \
+  | mips | pa | rs6000 | score | sparc | spu | tilegx | tilepro | xstormy16 \
+  | xtensa)
     insn="nop"
     ;;
   ia64 | s390)
Index: b/src/gcc/gcc.c
===================================================================
--- a/src/gcc/gcc.c
+++ b/src/gcc/gcc.c
@@ -2227,7 +2227,7 @@ for_each_path (const struct path_prefix
 	    }
 
 	  /* Now try the multiarch path.  */
-	  if (!skip_multi_dir
+	  if (!skip_multi_dir && !multi_dir
 	      && !pl->require_machine_suffix && multiarch_dir)
 	    {
 	      memcpy (path + len, multiarch_suffix, multiarch_len + 1);
@@ -2263,6 +2263,16 @@ for_each_path (const struct path_prefix
 	      if (ret)
 		break;
 	    }
+
+	  /* Now try the multiarch path.  */
+	  if (!skip_multi_dir
+	      && !pl->require_machine_suffix && multiarch_dir)
+	    {
+	      memcpy (path + len, multiarch_suffix, multiarch_len + 1);
+	      ret = callback (path, callback_info);
+	      if (ret)
+		break;
+	    }
 	}
       if (pl)
 	break;
@@ -7662,6 +7672,21 @@ set_multilib_dir (void)
 	    ++p;
 	}
 
+      if (first)
+	{
+	  if (this_path_len > 3 
+	      && this_path[0] == '.' 
+	      && this_path[1] == ':'
+	      && this_path[2] == ':')
+	    {
+	      char *new_multiarch_dir = XNEWVEC (char, this_path_len + 1);
+
+	      strncpy (new_multiarch_dir, this_path, this_path_len);
+	      new_multiarch_dir[this_path_len] = '\0';
+	      multiarch_dir = &new_multiarch_dir[3];
+	    }
+	}
+
       if (ok && first)
 	{
 	  if (this_path_len != 1
Index: b/src/gcc/gensupport.c
===================================================================
--- a/src/gcc/gensupport.c
+++ b/src/gcc/gensupport.c
@@ -1717,6 +1717,21 @@ process_one_cond_exec (struct queue_elem
 	  XVECEXP (insn, 1, 0) = pattern;
 	}
 
+       if (XVEC (ce_elem->data, 3) != NULL)
+	{
+	  rtvec attributes = rtvec_alloc (XVECLEN (insn, 4)
+	                                  + XVECLEN (ce_elem->data, 3));
+	  int i = 0;
+	  int j = 0;
+	  for (i = 0; i < XVECLEN (insn, 4); i++)
+	    RTVEC_ELT (attributes, i) = XVECEXP (insn, 4, i);
+
+	  for (j = 0; j < XVECLEN (ce_elem->data, 3); j++, i++)
+	    RTVEC_ELT (attributes, i) = XVECEXP (ce_elem->data, 3, j);
+
+	  XVEC (insn, 4) = attributes;
+	}
+
       XSTR (insn, 2) = alter_test_for_insn (ce_elem, insn_elem);
       XTMPL (insn, 3) = alter_output_for_insn (ce_elem, insn_elem,
 					      alternatives, max_operand);
Index: b/src/gcc/fold-const.c
===================================================================
--- a/src/gcc/fold-const.c
+++ b/src/gcc/fold-const.c
@@ -2470,9 +2470,13 @@ operand_equal_p (const_tree arg0, const_
     }
 
   if (TREE_CODE (arg0) != TREE_CODE (arg1)
-      /* This is needed for conversions and for COMPONENT_REF.
-	 Might as well play it safe and always test this.  */
-      || TREE_CODE (TREE_TYPE (arg0)) == ERROR_MARK
+      /* NOP_EXPR and CONVERT_EXPR are considered equal.  */
+      && !(CONVERT_EXPR_P (arg0) && CONVERT_EXPR_P (arg1)))
+    return 0;
+
+  /* This is needed for conversions and for COMPONENT_REF.
+     Might as well play it safe and always test this.  */
+  if (TREE_CODE (TREE_TYPE (arg0)) == ERROR_MARK
       || TREE_CODE (TREE_TYPE (arg1)) == ERROR_MARK
       || TYPE_MODE (TREE_TYPE (arg0)) != TYPE_MODE (TREE_TYPE (arg1)))
     return 0;
Index: b/src/gcc/objc/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/objc/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/tree-ssa-uncprop.c
===================================================================
--- a/src/gcc/tree-ssa-uncprop.c
+++ b/src/gcc/tree-ssa-uncprop.c
@@ -466,12 +466,11 @@ uncprop_into_successor_phis (basic_block
 	  struct equiv_hash_elt equiv_hash_elt;
 	  void **slot;
 
-	  /* If the argument is not an invariant, and refers to the same
-	     underlying variable as the PHI result, then there's no
-	     point in un-propagating the argument.  */
+	  /* If the argument is not an invariant and can be potentially
+	     coalesced with the result, then there's no point in
+	     un-propagating the argument.  */
 	  if (!is_gimple_min_invariant (arg)
-	      && (SSA_NAME_VAR (arg) == SSA_NAME_VAR (res)
-		  && TREE_TYPE (arg) == TREE_TYPE (res)))
+	      && gimple_can_coalesce_p (arg, res))
 	    continue;
 
 	  /* Lookup this argument's value in the hash table.  */
@@ -485,7 +484,7 @@ uncprop_into_successor_phis (basic_block
 	      int j;
 
 	      /* Walk every equivalence with the same value.  If we find
-		 one with the same underlying variable as the PHI result,
+		 one that can potentially coalesce with the PHI rsult,
 		 then replace the value in the argument with its equivalent
 		 SSA_NAME.  Use the most recent equivalence as hopefully
 		 that results in shortest lifetimes.  */
@@ -493,8 +492,7 @@ uncprop_into_successor_phis (basic_block
 		{
 		  tree equiv = elt->equivalences[j];
 
-		  if (SSA_NAME_VAR (equiv) == SSA_NAME_VAR (res)
-		      && TREE_TYPE (equiv) == TREE_TYPE (res))
+		  if (gimple_can_coalesce_p (equiv, res))
 		    {
 		      SET_PHI_ARG_DEF (phi, e->dest_idx, equiv);
 		      break;
Index: b/src/gcc/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/ChangeLog.linaro
@@ -0,0 +1,5152 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+        * LINARO-VERSION: Update.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+	* LINARO-VERSION: Update.
+
+2014-11-14  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r212178..
+	2014-06-30  Joseph Myers  <joseph@codesourcery.com>
+
+	* var-tracking.c (add_stores): Return instead of asserting if old
+	and new values for conditional store are the same.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	Add Linaro release macros (Linaro only patch.)
+
+	* Makefile.in (LINAROVER, LINAROVER_C, LINAROVER_S): Define.
+	(CFLAGS-cppbuiltin.o): Add LINAROVER macro definition.
+	(cppbuiltin.o): Depend on $(LINAROVER).
+	* cppbuiltin.c (parse_linarover): New.
+	(define_GNUC__): Define __LINARO_RELEASE__ and __LINARO_SPIN__ macros.
+
+2014-08-26  venkataramanan-kumar  <venkataramanan.kumar@linaro.org>
+
+	Backport from trunk r205807.
+	2013-12-09  Richard Earnshaw  <rearnsha@arm.com>
+
+	* arm.c (mem_ok_for_ldrd_strd): Rename first argument as MEM.  Do
+	more address validation checks.
+
+2014-08-15  Yvan Roux  <yvan.roux@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+	* LINARO-VERSION: Update.
+
+2014-08-11  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206529, r206530
+	2014-01-10  Richard Earnshaw  <rearnsha@arm.com>
+
+	PR target/59744
+	* aarch64-modes.def (CC_Zmode): New flags mode.
+	* aarch64.c (aarch64_select_cc_mode): Only allow NEG when the condition
+	represents an equality.
+	(aarch64_get_condition_code): Handle CC_Zmode.
+	* aarch64.md (compare_neg<mode>): Restrict to equality operations.
+
+2014-08-11  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r204251
+	2013-10-31  Richard Sandiford  <rsandifo@linux.vnet.ibm.com>
+	Yury Gribov  <y.gribov@samsung.com>
+
+	PR sanitizer/58543
+	* asan.c (asan_clear_shadow): Allocate a new vreg for temporary
+	shadow pointer to avoid clobbering the main one.
+
+2014-04-08  Yvan Roux  <yvan.roux@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+	* LINARO-VERSION: Update.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r205105
+	2013-11-20  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64.md: Remove "mode" and "mode2" attributes
+	from all insns.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r205050
+	2013-11-19  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/arm.md (zero_extend<mode>di2): Add type attribute.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r204852
+	2013-11-19  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64.md: Remove v8type from all insns.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r204852
+	2013-11-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd.md: Remove simd_type from all
+	patterns.
+	* config/aarch64/aarch64.md: Likewise, correct "type" attribute
+	where it is incorrect or missing.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r204784
+	2013-11-14  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-cores.def (example-1): Remove.
+	(example-2): Likewise.
+	* config/aarch64/aarch64-tune.md: Regenerate.
+	* config/aarch64/aarch64.md: Do not include "large.md" or "small.md".
+	(generic_sched): Remove "large", "small".
+	* config/aarch64/large.md: Delete.
+	* config/aarch64/small.md: Delete.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r204783
+	2013-11-14  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-cores.def (cortex-a57): Tune for cortexa15.
+	* config/aarch64/aarch64-tune.md: Regenerate.
+	* config/aarch64/aarch64.md: Include cortex-a15 pipeline model.
+	(generic_sched): "no" if we are	tuning for cortexa15.
+	* config/arm/cortex-a15.md: Include cortex-a15-neon.md by
+	relative path.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r204782
+	2013-11-14  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-arches.def (armv8-a): Tune for cortex-a53.
+	* config/aarch64/aarch64.md: Do not include aarch64-generic.md.
+	* config/aarch64/aarch64.c (aarch64_tune): Initialize to cortexa53.
+	(all_cores): Use cortexa53 when tuning for "generic".
+	(aarch64_override_options): Fix comment.
+	* config/aarch64/aarch64.h (TARGET_CPU_DEFAULT): Set to cortexa53.
+	* config/aarch64/aarch64-generic.md: Delete.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r204575
+	2013-11-08  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/aarch-common.c
+	(search_term): New typedef.
+	(shift_rtx_costs): New array.
+	(arm_rtx_shift_left_p): New.
+	(arm_find_sub_rtx_with_search_term): Likewise.
+	(arm_find_sub_rtx_with_code): Likewise.
+	(arm_early_load_addr_dep): Add sanity checking.
+	(arm_no_early_alu_shift_dep): Likewise.
+	(arm_no_early_alu_shift_value_dep): Likewise.
+	(arm_no_early_mul_dep): Likewise.
+	(arm_no_early_store_addr_dep): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203621
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/neon-schedgen.ml: Remove.
+	* config/arm/cortex-a9-neon.md: Remove comment regarding
+	neon-schedgen.ml.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203620
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/types: Remove old neon types.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203619
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/cortex-a7.md
+	(cortex_a7_neon_type): New.
+	(cortex_a7_neon_mul): Update for new types.
+	(cortex_a7_neon_mla): Likewise.
+	(cortex_a7_neon): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203618
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/cortex-a15-neon.md
+	(cortex_a15_neon_type): New,
+
+	(cortex_a15_neon_int_1): Remove.
+	(cortex_a15_neon_int_2): Likewise.
+	(cortex_a15_neon_int_3): Likewise.
+	(cortex_a15_neon_int_4): Likewise.
+	(cortex_a15_neon_int_5): Likewise.
+	(cortex_a15_neon_vqneg_vqabs): Likewise.
+	(cortex_a15_neon_vmov): Likewise.
+	(cortex_a15_neon_vaba): Likewise.
+	(cortex_a15_neon_vaba_qqq): Likewise.
+	(cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a15_neon_mul_qqq_8_16_32_ddd_32): Likewise.
+	(cortex_a15_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar):
+	Likewise.
+	(cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a15_neon_mla_qqq_8_16): Likewise.
+	(cortex_a15_neon_mla_ddd_32_qqd_16_ddd_32_scalar): Likewise.
+	(cortex_a15_neon_mla_qqq_32_qqd_32_scalar): Likewise.
+	(cortex_a15_neon_mul_ddd_16_scalar_32_16_long_scalar): Likewise.
+	(cortex_a15_neon_mul_qqd_32_scalar): Likewise.
+	(cortex_a15_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar): Likewise.
+	(cortex_a15_neon_shift_1): Likewise.
+	(cortex_a15_neon_shift_2): Likewise.
+	(cortex_a15_neon_shift_3): Likewise.
+	(cortex_a15_neon_vshl_ddd): Likewise.
+	(cortex_a15_neon_vqshl_vrshl_vqrshl_qqq): Likewise.
+	(cortex_a15_neon_vsra_vrsra): Likewise.
+	(cortex_a15_neon_fp_vmla_ddd_scalar): Likewise.
+	(cortex_a15_neon_fp_vmla_qqq_scalar): Likewise.
+	(cortex_a15_neon_bp_3cycle): Likewise.
+	(cortex_a15_neon_ldm_2): Likewise.
+	(cortex_a15_neon_stm_2): Likewise.
+	(cortex_a15_neon_mcr): Likewise.
+	(cortex_a15_neon_mrc): Likewise.
+	(cortex_a15_neon_fp_vadd_ddd_vabs_dd): Likewise.
+	(cortex_a15_neon_fp_vadd_qqq_vabs_qq): Likewise.
+	(cortex_a15_neon_fp_vmul_ddd): Likewise.
+	(cortex_a15_neon_fp_vmul_qqd): Likewise.
+	(cortex_a15_neon_fp_vmla_ddd): Likewise.
+	(cortex_a15_neon_fp_vmla_qqq): Likewise.
+	(cortex_a15_neon_fp_vmla_ddd_scalar): Likewise.
+	(cortex_a15_neon_fp_vmla_qqq_scalar): Likewise.
+	(cortex_a15_neon_fp_vrecps_vrsqrts_ddd): Likewise.
+	(cortex_a15_neon_fp_vrecps_vrsqrts_qqq): Likewise.
+	(cortex_a15_neon_bp_simple): Likewise.
+	(cortex_a15_neon_bp_2cycle): Likewise.
+	(cortex_a15_neon_bp_3cycle): Likewise.
+	(cortex_a15_neon_vld1_1_2_regs): Likewise.
+	(cortex_a15_neon_vld1_3_4_regs): Likewise.
+	(cortex_a15_neon_vld2_2_regs_vld1_vld2_all_lanes): Likewise.
+	(cortex_a15_neon_vld2_4_regs): Likewise.
+	(cortex_a15_neon_vld3_vld4): Likewise.
+	(cortex_a15_neon_vst1_1_2_regs_vst2_2_regs): Likewise.
+	(cortex_a15_neon_vst1_3_4_regs): Likewise.
+	(cortex_a15_neon_vst2_4_regs_vst3_vst4): Rename to...
+	(cortex_a15_neon_vst2_4_regs_vst3): ...This, update for new attributes.
+	(cortex_a15_neon_vst3_vst4): Rename to...
+	(cortex_a15_neon_vst4): This, update for new attributes.
+	(cortex_a15_neon_vld1_vld2_lane): Update for new attributes.
+	(cortex_a15_neon_vld3_vld4_lane): Likewise.
+	(cortex_a15_neon_vst1_vst2_lane): Likewise.
+	(cortex_a15_neon_vst3_vst4_lane): Likewise.
+	(cortex_a15_neon_vld3_vld4_all_lanes): Likewise.
+	(cortex_a15_neon_ldm_2): Likewise.
+	(cortex_a15_neon_stm_2): Likewise.
+	(cortex_a15_neon_mcr): Likewise.
+	(cortex_a15_neon_mcr_2_mcrr): Likewise.
+	(cortex_a15_neon_mrc): Likewise.
+	(cortex_a15_neon_mrrc): Likewise.
+
+	(cortex_a15_neon_abd): New.
+	(cortex_a15_neon_abd_q): Likewise.
+	(cortex_a15_neon_aba): Likewise.
+	(cortex_a15_neon_aba_q): Likewise.
+	(cortex_a15_neon_acc): Likewise.
+	(cortex_a15_neon_acc_q): Likewise.
+	(cortex_a15_neon_arith_basic): Likewise.
+	(cortex_a15_neon_arith_complex): Likewise.
+	(cortex_a15_neon_multiply): Likewise.
+	(cortex_a15_neon_multiply_q): Likewise.
+	(cortex_a15_neon_mla): Likewise.
+	(cortex_a15_neon_mla_q): Likewise.
+	(cortex_a15_neon_sat_mla_long): Likewise.
+	(cortex_a15_neon_shift_acc): Likewise.
+	(cortex_a15_neon_shift_imm_basic): Likewise.
+	(cortex_a15_neon_shift_imm_complex): Likewise.
+	(cortex_a15_neon_shift_reg_basic): Likewise.
+	(cortex_a15_neon_shift_reg_basic_q): Likewise.
+	(cortex_a15_neon_shift_reg_complex): Likewise.
+	(cortex_a15_neon_shift_reg_complex_q): Likewise.
+	(cortex_a15_neon_fp_negabs): Likewise
+	(cortex_a15_neon_fp_arith): Likewise
+	(cortex_a15_neon_fp_arith_q): Likewise
+	(cortex_a15_neon_fp_cvt_int): Likewise
+	(cortex_a15_neon_fp_cvt_int_q): Likewise
+	(cortex_a15_neon_fp_cvt_16): Likewise
+	(cortex_a15_neon_fp_mul): Likewise
+	(cortex_a15_neon_fp_mul_q): Likewise
+	(cortex_a15_neon_fp_mla): Likewise
+	(cortex_a15_neon_fp_mla_q): Likewise
+	(cortex_a15_neon_fp_recps_rsqrte): Likewise.
+	(cortex_a15_neon_fp_recps_rsqrte_q): Likewise.
+	(cortex_a15_neon_bitops): Likewise.
+	(cortex_a15_neon_bitops_q): Likewise.
+	(cortex_a15_neon_from_gp): Likewise.
+	(cortex_a15_neon_from_gp_q): Likewise.
+	(cortex_a15_neon_tbl3_tbl4): Likewise.
+	(cortex_a15_neon_zip_q): Likewise.
+	(cortex_a15_neon_to_gp): Likewise.
+	(cortex_a15_neon_load_a): Likewise.
+	(cortex_a15_neon_load_b): Likewise.
+	(cortex_a15_neon_load_c): Likewise.
+	(cortex_a15_neon_load_d): Likewise.
+	(cortex_a15_neon_load_e): Likewise.
+	(cortex_a15_neon_load_f): Likewise.
+	(cortex_a15_neon_store_a): Likewise.
+	(cortex_a15_neon_store_b): Likewise.
+	(cortex_a15_neon_store_c): Likewise.
+	(cortex_a15_neon_store_d): Likewise.
+	(cortex_a15_neon_store_e): Likewise.
+	(cortex_a15_neon_store_f): Likewise.
+	(cortex_a15_neon_store_g): Likewise.
+	(cortex_a15_neon_store_h): Likewise.
+	(cortex_a15_vfp_to_from_gp): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203617
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/cortex-a9-neon.md (cortex_a9_neon_type): New.
+
+	(cortex_a9_neon_vshl_ddd): Remove.
+	(cortex_a9_neon_vst3_vst4): Likewise.
+	(cortex_a9_neon_vld3_vld4_all_lanes): Likewise.
+
+	(cortex_a9_neon_bit_ops_q): New.
+
+	(cortex_a9_neon_int_1): Use cortex_a8_neon_type.
+	(cortex_a9_neon_int_2): Likewise.
+	(cortex_a9_neon_int_3): Likewise.
+	(cortex_a9_neon_int_4): Likewise.
+	(cortex_a9_neon_int_5): Likewise.
+	(cortex_a9_neon_vqneg_vqabs): Likewise.
+	(cortex_a9_neon_vmov): Likewise.
+	(cortex_a9_neon_vaba): Likewise.
+	(cortex_a9_neon_vaba_qqq): Likewise.
+	(cortex_a9_neon_shift_1): Likewise.
+	(cortex_a9_neon_shift_2): Likewise.
+	(cortex_a9_neon_shift_3): Likewise.
+	(cortex_a9_neon_vqshl_vrshl_vqrshl_qqq): Likewise.
+	(cortex_a9_neon_vsra_vrsra): Likewise.
+	(cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a9_neon_mul_qqq_8_16_32_ddd_32): Likewise.
+	(cortex_a9_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar):
+	Likewise.
+	(cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a9_neon_mla_qqq_8_16): Likewise.
+	(cortex_a9_neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long):
+	Likewise.
+	(cortex_a9_neon_mla_qqq_32_qqd_32_scalar): Likewise.
+	(cortex_a9_neon_mul_ddd_16_scalar_32_16_long_scalar): Likewise.
+	(cortex_a9_neon_mul_qqd_32_scalar): Likewise.
+	(cortex_a9_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar): Likewise.
+	(cortex_a9_neon_fp_vadd_ddd_vabs_dd): Likewise.
+	(cortex_a9_neon_fp_vadd_qqq_vabs_qq): Likewise.
+	(cortex_a9_neon_fp_vsum): Likewise.
+	(cortex_a9_neon_fp_vmul_ddd): Likewise.
+	(cortex_a9_neon_fp_vmul_qqd): Likewise.
+	(cortex_a9_neon_fp_vmla_ddd): Likewise.
+	(cortex_a9_neon_fp_vmla_qqq): Likewise.
+	(cortex_a9_neon_fp_vmla_ddd_scalar): Likewise.
+	(cortex_a9_neon_fp_vmla_qqq_scalar): Likewise.
+	(cortex_a9_neon_fp_vrecps_vrsqrts_ddd): Likewise.
+	(cortex_a9_neon_fp_vrecps_vrsqrts_qqq): Likewise.
+	(cortex_a9_neon_bp_simple): Likewise.
+	(cortex_a9_neon_bp_2cycle): Likewise.
+	(cortex_a9_neon_bp_3cycle): Likewise.
+	(cortex_a9_neon_ldr): Likewise.
+	(cortex_a9_neon_str): Likewise.
+	(cortex_a9_neon_vld1_1_2_regs): Likewise.
+	(cortex_a9_neon_vld1_3_4_regs): Likewise.
+	(cortex_a9_neon_vld2_2_regs_vld1_vld2_all_lanes): Likewise.
+	(cortex_a9_neon_vld2_4_regs): Likewise.
+	(cortex_a9_neon_vld3_vld4): Likewise.
+	(cortex_a9_neon_vld1_vld2_lane): Likewise.
+	(cortex_a9_neon_vld3_vld4_lane): Likewise.
+	(cortex_a9_neon_vld3_vld4_all_lanes): Likewise.
+	(cortex_a9_neon_vst1_1_2_regs_vst2_2_regs): Likewise.
+	(cortex_a9_neon_vst1_3_4_regs): Likewise.
+	(cortex_a9_neon_vst2_4_regs_vst3_vst4): Likewise.
+	(cortex_a9_neon_vst1_vst2_lane): Likewise.
+	(cortex_a9_neon_vst3_vst4_lane): Likewise.
+	(cortex_a9_neon_mcr): Likewise.
+	(cortex_a9_neon_mcr_2_mcrr): Likewise.
+	(cortex_a9_neon_mrc): Likewise.
+	(cortex_a9_neon_mrrc): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203616
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/cortex-a8-neon.md (cortex_a8_neon_type): New.
+
+	(cortex_a8_neon_vshl_ddd): Remove.
+	(cortex_a8_neon_vst3_vst4): Likewise.
+	(cortex_a8_neon_vld3_vld4_all_lanes): Likewise.
+
+	(cortex_a8_neon_bit_ops_q): New.
+
+	(cortex_a8_neon_int_1): Use cortex_a8_neon_type.
+	(cortex_a8_neon_int_2): Likewise..
+	(cortex_a8_neon_int_3): Likewise.
+	(cortex_a8_neon_int_5): Likewise.
+	(cortex_a8_neon_vqneg_vqabs): Likewise.
+	(cortex_a8_neon_int_4): Likewise.
+	(cortex_a8_neon_vaba): Likewise.
+	(cortex_a8_neon_vaba_qqq): Likewise.
+	(cortex_a8_neon_shift_1): Likewise.
+	(cortex_a8_neon_shift_2): Likewise.
+	(cortex_a8_neon_shift_3): Likewise.
+	(cortex_a8_neon_vqshl_vrshl_vqrshl_qqq): Likewise.
+	(cortex_a8_neon_vsra_vrsra): Likewise.
+	(cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a8_neon_mul_qqq_8_16_32_ddd_32): Likewise.
+	(cortex_a8_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar):
+	Likewise.
+	(cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a8_neon_mla_qqq_8_16): Likewise.
+	(cortex_a8_neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long):
+	Likewise.
+	(cortex_a8_neon_mla_qqq_32_qqd_32_scalar): Likewise.
+	(cortex_a8_neon_mul_ddd_16_scalar_32_16_long_scalar): Likewise.
+	(cortex_a8_neon_mul_qqd_32_scalar): Likewise.
+	(cortex_a8_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar): Likewise.
+	(cortex_a8_neon_fp_vadd_ddd_vabs_dd): Likewise.
+	(cortex_a8_neon_fp_vadd_qqq_vabs_qq): Likewise.
+	(cortex_a8_neon_fp_vsum): Likewise.
+	(cortex_a8_neon_fp_vmul_ddd): Likewise.
+	(cortex_a8_neon_fp_vmul_qqd): Likewise.
+	(cortex_a8_neon_fp_vmla_ddd): Likewise.
+	(cortex_a8_neon_fp_vmla_qqq): Likewise.
+	(cortex_a8_neon_fp_vmla_ddd_scalar): Likewise.
+	(cortex_a8_neon_fp_vmla_qqq_scalar): Likewise.
+	(cortex_a8_neon_fp_vrecps_vrsqrts_ddd): Likewise.
+	(cortex_a8_neon_fp_vrecps_vrsqrts_qqq): Likewise.
+	(cortex_a8_neon_bp_simple): Likewise.
+	(cortex_a8_neon_bp_2cycle): Likewise.
+	(cortex_a8_neon_bp_3cycle): Likewise.
+	(cortex_a8_neon_ldr): Likewise.
+	(cortex_a8_neon_str): Likewise.
+	(cortex_a8_neon_vld1_1_2_regs): Likewise.
+	(cortex_a8_neon_vld1_3_4_regs): Likewise.
+	(cortex_a8_neon_vld2_2_regs_vld1_vld2_all_lanes): Likewise.
+	(cortex_a8_neon_vld2_4_regs): Likewise.
+	(cortex_a8_neon_vld3_vld4): Likewise.
+	(cortex_a8_neon_vld1_vld2_lane): Likewise.
+	(cortex_a8_neon_vld3_vld4_lane): Likewise.
+	(cortex_a8_neon_vst1_1_2_regs_vst2_2_regs): Likewise.
+	(cortex_a8_neon_vst1_3_4_regs): Likewise.
+	(cortex_a8_neon_vst2_4_regs_vst3_vst4): Likewise.
+	(cortex_a8_neon_vst1_vst2_lane): Likewise.
+	(cortex_a8_neon_vst3_vst4_lane): Likewise.
+	(cortex_a8_neon_mcr): Likewise.
+	(cortex_a8_neon_mcr_2_mcrr): Likewise.
+	(cortex_a8_neon_mrc): Likewise.
+	(cortex_a8_neon_mrrc): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203614
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/iterators.md (Vetype): Add SF and DF modes.
+	(fp): New.
+	* config/aarch64/aarch64-simd.md (neon_type): Remove.
+	(aarch64_simd_dup<mode>): Add "type" attribute.
+	(aarch64_dup_lane<mode>): Likewise.
+	(aarch64_dup_lane_<vswap_width_name><mode>): Likewise.
+	(*aarch64_simd_mov<mode>): Likewise.
+	(aarch64_simd_mov_from_<mode>low): Likewise.
+	(aarch64_simd_mov_from_<mode>high): Likewise.
+	(orn<mode>3): Likewise.
+	(bic<mode>3): Likewise.
+	(add<mode>3): Likewise.
+	(sub<mode>3): Likewise.
+	(mul<mode>3): Likewise.
+	(*aarch64_mul3_elt<mode>): Likewise.
+	(*aarch64_mul3_elt_<vswap_width_name><mode>): Likewise.
+	(*aarch64_mul3_elt_to_128df): Likewise.
+	(*aarch64_mul3_elt_to_64v2df): Likewise.
+	(neg<mode>2): Likewise.
+	(abs<mode>2): Likewise.
+	(abd<mode>_3): Likewise.
+	(aba<mode>_3): Likewise.
+	(fabd<mode>_3): Likewise.
+	(*fabd_scalar<mode>3): Likewise.
+	(and<mode>3): Likewise.
+	(ior<mode>3): Likewise.
+	(xor<mode>3): Likewise.
+	(one_cmpl<mode>2): Likewise.
+	(aarch64_simd_vec_set<mode>): Likewise.
+	(aarch64_simd_lshr<mode>): Likewise.
+	(aarch64_simd_ashr<mode>): Likewise.
+	(aarch64_simd_imm_shl<mode>): Likewise.
+	(aarch64_simd_reg_sshl<mode): Likewise.
+	(aarch64_simd_reg_shl<mode>_unsigned): Likewise.
+	(aarch64_simd_reg_shl<mode>_signed): Likewise.
+	(aarch64_simd_vec_setv2di): Likewise.
+	(aarch64_simd_vec_set<mode>): Likewise.
+	(aarch64_mla<mode>): Likewise.
+	(*aarch64_mla_elt<mode>): Likewise.
+	(*aarch64_mla_elt_<vswap_width_name><mode>): Likewise.
+	(aarch64_mls<mode>): Likewise.
+	(*aarch64_mls_elt<mode>): Likewise.
+	(*aarch64_mls_elt_<vswap_width_name><mode>): Likewise.
+	(<su><maxmin><mode>3): Likewise.
+	(move_lo_quad_<mode>): Likewise.
+	(aarch64_simd_move_hi_quad_<mode>): Likewise.
+	(aarch64_simd_vec_pack_trunc_<mode>): Likewise.
+	(vec_pack_trunc_<mode>): Likewise.
+	(aarch64_simd_vec_unpack<su>_lo_<mode>): Likewise.
+	(aarch64_simd_vec_unpack<su>_hi_<mode>): Likewise.
+	(*aarch64_<su>mlal_lo<mode>): Likewise.
+	(*aarch64_<su>mlal_hi<mode>): Likewise.
+	(*aarch64_<su>mlsl_lo<mode>): Likewise.
+	(*aarch64_<su>mlsl_hi<mode>): Likewise.
+	(*aarch64_<su>mlal<mode>): Likewise.
+	(*aarch64_<su>mlsl<mode>): Likewise.
+	(aarch64_simd_vec_<su>mult_lo_<mode>): Likewise.
+	(aarch64_simd_vec_<su>mult_hi_<mode>): Likewise.
+	(add<mode>3): Likewise.
+	(sub<mode>3): Likewise.
+	(mul<mode>3): Likewise.
+	(div<mode>3): Likewise.
+	(neg<mode>2): Likewise.
+	(abs<mode>2): Likewise.
+	(fma<mode>4): Likewise.
+	(*aarch64_fma4_elt<mode>): Likewise.
+	(*aarch64_fma4_elt_<vswap_width_name><mode>): Likewise.
+	(*aarch64_fma4_elt_to_128df): Likewise.
+	(*aarch64_fma4_elt_to_64v2df): Likewise.
+	(fnma<mode>4): Likewise.
+	(*aarch64_fnma4_elt<mode>): Likewise.
+	(*aarch64_fnma4_elt_<vswap_width_name><mode>
+	(*aarch64_fnma4_elt_to_128df): Likewise.
+	(*aarch64_fnma4_elt_to_64v2df): Likewise.
+	(<frint_pattern><mode>2): Likewise.
+	(l<fcvt_pattern><su_optab><VDQF:mode><fcvt_target>2): Likewise.
+	(<optab><fcvt_target><VDQF:VDQF:mode>2): Likewise.
+	(vec_unpacks_lo_v4sf): Likewise.
+	(aarch64_float_extend_lo_v2df): Likewise.
+	(vec_unpacks_hi_v4sf): Likewise.
+	(aarch64_float_truncate_lo_v2sf): Likewise.
+	(aarch64_float_truncate_hi_v4sf): Likewise.
+	(aarch64_vmls<mode>): Likewise.
+	(<su><maxmin><mode>3): Likewise.
+	(<maxmin_uns><mode>3): Likewise.
+	(reduc_<sur>plus_<mode>): Likewise.
+	(reduc_<sur>plus_v2di): Likewise.
+	(reduc_<sur>plus_v2si): Likewise.
+	(reduc_<sur>plus_<mode>): Likewise.
+	(aarch64_addpv4sf): Likewise.
+	(clz<mode>2): Likewise.
+	(reduc_<maxmin_uns>_<mode>): Likewise.
+	(reduc_<maxmin_uns>_v2di): Likewise.
+	(reduc_<maxmin_uns>_v2si): Likewise.
+	(reduc_<maxmin_uns>_<mode>): Likewise.
+	(reduc_<maxmin_uns>_v4sf): Likewise.
+	(aarch64_simd_bsl<mode>_internal): Likewise.
+	(*aarch64_get_lane_extend<GPI:mode><VDQQH:mode>): Likewise.
+	(*aarch64_get_lane_zero_extendsi<mode>): Likewise.
+	(aarch64_get_lane<mode>): Likewise.
+	(*aarch64_combinez<mode>): Likewise.
+	(aarch64_combine<mode>): Likewise.
+	(aarch64_simd_combine<mode>): Likewise.
+	(aarch64_<ANY_EXTEND:su><ADDSUB:optab>l<mode>_hi_internal): Likewise.
+	(aarch64_<ANY_EXTEND:su><ADDSUB:optab>l<mode>_lo_internal): Likewise.
+	(aarch64_<ANY_EXTEND:su><ADDSUB:optab>l<mode>): Likewise.
+	(aarch64_<ANY_EXTEND:su><ADDSUB:optab>w<mode>): Likewise.
+	(aarch64_<ANY_EXTEND:su><ADDSUB:optab>w2<mode>_internal): Likewise.
+	(aarch64_<sur>h<addsub><mode>): Likewise.
+	(aarch64_<sur><addsub>hn<mode>): Likewise.
+	(aarch64_<sur><addsub>hn2<mode>): Likewise.
+	(aarch64_pmul<mode>): Likewise.
+	(aarch64_<su_optab><optab><mode>): Likewise.
+	(aarch64_<sur>qadd<mode>): Likewise.
+	(aarch64_sqmovun<mode>): Likewise.
+	(aarch64_<sur>qmovn<mode>): Likewise.
+	(aarch64_s<optab><mode>): Likewise.
+	(aarch64_sq<r>dmulh<mode>): Likewise.
+	(aarch64_sq<r>dmulh_lane<mode>): Likewise.
+	(aarch64_sq<r>dmulh_laneq<mode>): Likewise.
+	(aarch64_sq<r>dmulh_lane<mode>): Likewise.
+	(aarch64_sqdml<SBINQOPS:as>l<mode>): Likewise.
+	(aarch64_sqdml<SBINQOPS:as>l_lane<mode>_internal): Likewise.
+	(aarch64_sqdml<SBINQOPS:as>l_lane<mode>_internal): Likewise.
+	(aarch64_sqdml<SBINQOPS:as>l_n<mode>): Likewise.
+	(aarch64_sqdml<SBINQOPS:as>l2<mode>_internal): Likewise.
+	(aarch64_sqdml<SBINQOPS:as>l2_lane<mode>_internal): Likewise.
+	(aarch64_sqdml<SBINQOPS:as>l2_n<mode>_internal): Likewise.
+	(aarch64_sqdmull<mode>): Likewise.
+	(aarch64_sqdmull_lane<mode>_internal): Likewise.
+	(aarch64_sqdmull_n<mode>): Likewise.
+	(aarch64_sqdmull2<mode>_internal): Likewise.
+	(aarch64_sqdmull2_lane<mode>_internal): Likewise.
+	(aarch64_sqdmull2_n<mode>_internal): Likewise.
+	(aarch64_<sur>shl<mode>): Likewise.
+	(aarch64_<sur>q<r>shl<mode>
+	(aarch64_<sur>shll_n<mode>): Likewise.
+	(aarch64_<sur>shll2_n<mode>): Likewise.
+	(aarch64_<sur>shr_n<mode>): Likewise.
+	(aarch64_<sur>sra_n<mode>): Likewise.
+	(aarch64_<sur>s<lr>i_n<mode>): Likewise.
+	(aarch64_<sur>qshl<u>_n<mode>): Likewise.
+	(aarch64_<sur>q<r>shr<u>n_n<mode>): Likewise.
+	(aarch64_cm<optab><mode>): Likewise.
+	(aarch64_cm<optab>di): Likewise.
+	(aarch64_cm<optab><mode>): Likewise.
+	(aarch64_cm<optab>di): Likewise.
+	(aarch64_cmtst<mode>): Likewise.
+	(aarch64_cmtstdi): Likewise.
+	(aarch64_cm<optab><mode>): Likewise.
+	(*aarch64_fac<optab><mode>): Likewise.
+	(aarch64_addp<mode>): Likewise.
+	(aarch64_addpdi): Likewise.
+	(sqrt<mode>2): Likewise.
+	(vec_load_lanesoi<mode>): Likewise.
+	(vec_store_lanesoi<mode>): Likewise.
+	(vec_load_lanesci<mode>): Likewise.
+	(vec_store_lanesci<mode>): Likewise.
+	(vec_load_lanesxi<mode>): Likewise.
+	(vec_store_lanesxi<mode>): Likewise.
+	(*aarch64_mov<mode>): Likewise.
+	(aarch64_ld2<mode>_dreg): Likewise.
+	(aarch64_ld2<mode>_dreg): Likewise.
+	(aarch64_ld3<mode>_dreg): Likewise.
+	(aarch64_ld3<mode>_dreg): Likewise.
+	(aarch64_ld4<mode>_dreg): Likewise.
+	(aarch64_ld4<mode>_dreg): Likewise.
+	(aarch64_tbl1<mode>): Likewise.
+	(aarch64_tbl2v16qi): Likewise.
+	(aarch64_combinev16qi): Likewise.
+	(aarch64_<PERMUTE:perm_insn><PERMUTE:perm_hilo><mode>): Likewise.
+	(aarch64_st2<mode>_dreg): Likewise.
+	(aarch64_st2<mode>_dreg): Likewise.
+	(aarch64_st3<mode>_dreg): Likewise.
+	(aarch64_st3<mode>_dreg): Likewise.
+	(aarch64_st4<mode>_dreg): Likewise.
+	(aarch64_st4<mode>_dreg): Likewise.
+	(*aarch64_simd_ld1r<mode>): Likewise.
+	(aarch64_frecpe<mode>): Likewise.
+	(aarch64_frecp<FRECP:frecp_suffix><mode>): Likewise.
+	(aarch64_frecps<mode>): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203613
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/iterators.md (V_elem_ch): New.
+	(q): Likewise.
+	(VQH_type): Likewise.
+	* config/arm/arm.md (is_neon_type): New.
+	(conds): Use is_neon_type.
+	(anddi3_insn): Update type attribute.
+	(xordi3_insn): Likewise.
+	(one_cmpldi2): Likewise.
+	* gcc/config/arm/vfp.md (movhf_vfp_neon): Update type attribute.
+	* gcc/config/arm/neon.md (neon_mov): Update type attribute.
+	(*movmisalign<mode>_neon_store): Likewise.
+	(*movmisalign<mode>_neon_load): Likewise.
+	(vec_set<mode>_internal): Likewise.
+	(vec_set<mode>_internal): Likewise.
+	(vec_setv2di_internal): Likewise.
+	(vec_extract<mode>): Likewise.
+	(vec_extract<mode>): Likewise.
+	(vec_extractv2di): Likewise.
+	(*add<mode>3_neon): Likewise.
+	(adddi3_neon): Likewise.
+	(*sub<mode>3_neon): Likewise.
+	(subdi3_neon): Likewise.
+	(fma<VCVTF:mode>4): Likewise.
+	(fma<VCVTF:mode>4_intrinsic): Likewise.
+	(*fmsub<VCVTF:mode>4): Likewise.
+	(fmsub<VCVTF:mode>4_intrinsic): Likewise.
+	(neon_vrint<NEON_VRINT:nvrint_variant><VCVTF:mode>): Likewise.
+	(ior<mode>3): Likewise.
+	(and<mode>3): Likewise.
+	(orn<mode>3_neon): Likewise.
+	(orndi3_neon): Likewise.
+	(bic<mode>3_neon): Likewise.
+	(bicdi3_neon): Likewise.
+	(xor<mode>3): Likewise.
+	(one_cmpl<mode>2): Likewise.
+	(abs<mode>2): Likewise.
+	(neg<mode>2): Likewise.
+	(negdi2_neon): Likewise.
+	(*umin<mode>3_neon): Likewise.
+	(*umax<mode>3_neon): Likewise.
+	(*smin<mode>3_neon): Likewise.
+	(*smax<mode>3_neon): Likewise.
+	(vashl<mode>3): Likewise.
+	(vashr<mode>3_imm): Likewise.
+	(vlshr<mode>3_imm): Likewise.
+	(ashl<mode>3_signed): Likewise.
+	(ashl<mode>3_unsigned): Likewise.
+	(neon_load_count): Likewise.
+	(ashldi3_neon_noclobber): Likewise.
+	(ashldi3_neon): Likewise.
+	(signed_shift_di3_neon): Likewise.
+	(unsigned_shift_di3_neon): Likewise.
+	(ashrdi3_neon_imm_noclobber): Likewise.
+	(lshrdi3_neon_imm_noclobber): Likewise.
+	(<shift>di3_neon): Likewise.
+	(widen_ssum<mode>3): Likewise.
+	(widen_usum<mode>3): Likewise.
+	(quad_halves_<code>v4si): Likewise.
+	(quad_halves_<code>v4sf): Likewise.
+	(quad_halves_<code>v8hi): Likewise.
+	(quad_halves_<code>v16qi): Likewise.
+	(reduc_splus_v2di): Likewise.
+	(neon_vpadd_internal<mode>): Likewise.
+	(neon_vpsmin<mode>): Likewise.
+	(neon_vpsmax<mode>): Likewise.
+	(neon_vpumin<mode>): Likewise.
+	(neon_vpumax<mode>): Likewise.
+	(*ss_add<mode>_neon): Likewise.
+	(*us_add<mode>_neon): Likewise.
+	(*ss_sub<mode>_neon): Likewise.
+	(*us_sub<mode>_neon): Likewise.
+	(neon_vadd<mode>_unspec): Likewise.
+	(neon_vaddl<mode>): Likewise.
+	(neon_vaddw<mode>): Likewise.
+	(neon_vhadd<mode>): Likewise.
+	(neon_vqadd<mode>): Likewise.
+	(neon_vaddhn<mode>): Likewise.
+	(neon_vmul<mode>): Likewise.
+	(neon_vfms<VCVTF:mode>): Likewise.
+	(neon_vmlal<mode>): Likewise.
+	(neon_vmls<mode>): Likewise.
+	(neon_vmlsl<mode>): Likewise.
+	(neon_vqdmulh<mode>): Likewise.
+	(neon_vqdmlal<mode>): Likewise.
+	(neon_vqdmlsl<mode>): Likewise.
+	(neon_vmull<mode>): Likewise.
+	(neon_vqdmull<mode>): Likewise.
+	(neon_vsub<mode>_unspec): Likewise.
+	(neon_vsubl<mode>): Likewise.
+	(neon_vsubw<mode>): Likewise.
+	(neon_vqsub<mode>): Likewise.
+	(neon_vhsub<mode>): Likewise.
+	(neon_vsubhn<mode>): Likewise.
+	(neon_vceq<mode>): Likewise.
+	(neon_vcge<mode>): Likewise.
+	(neon_vcgeu<mode>): Likewise.
+	(neon_vcgt<mode>): Likewise.
+	(neon_vcgtu<mode>): Likewise.
+	(neon_vcle<mode>): Likewise.
+	(neon_vclt<mode>): Likewise.
+	(neon_vcage<mode>): Likewise.
+	(neon_vcagt<mode>): Likewise.
+	(neon_vtst<mode>): Likewise.
+	(neon_vabd<mode>): Likewise.
+	(neon_vabdl<mode>): Likewise.
+	(neon_vaba<mode>): Likewise.
+	(neon_vabal<mode>): Likewise.
+	(neon_vmax<mode>): Likewise.
+	(neon_vmin<mode>): Likewise.
+	(neon_vpaddl<mode>): Likewise.
+	(neon_vpadal<mode>): Likewise.
+	(neon_vpmax<mode>): Likewise.
+	(neon_vpmin<mode>): Likewise.
+	(neon_vrecps<mode>): Likewise.
+	(neon_vrsqrts<mode>): Likewise.
+	(neon_vqabs<mode>): Likewise.
+	(neon_vqneg<mode>): Likewise.
+	(neon_vcls<mode>): Likewise.
+	(clz<mode>2): Likewise.
+	(popcount<mode>2): Likewise.
+	(neon_vrecpe<mode>): Likewise.
+	(neon_vrsqrte<mode>): Likewise.
+	(neon_vget_lane<mode>_sext_internal): Likewise.
+	(neon_vget_lane<mode>_zext_internal): Likewise.
+	(neon_vdup_n<mode>): Likewise.
+	(neon_vdup_n<mode>): Likewise.
+	(neon_vdup_nv2di): Likewise.
+	(neon_vdup_lane<mode>_interal): Likewise.
+	(*neon_vswp<mode>): Likewise.
+	(neon_vcombine<mode>): Likewise.
+	(float<mode><V_cvtto>2): Likewise.
+	(floatuns<mode><V_cvtto>2): Likewise.
+	(fix_trunc<mode><V_cvtto>2): Likewise.
+	(fixuns_trunc<mode><V_cvtto>2
+	(neon_vcvt<mode>): Likewise.
+	(neon_vcvt<mode>): Likewise.
+	(neon_vcvtv4sfv4hf): Likewise.
+	(neon_vcvtv4hfv4sf): Likewise.
+	(neon_vcvt_n<mode>): Likewise.
+	(neon_vcvt_n<mode>): Likewise.
+	(neon_vmovn<mode>): Likewise.
+	(neon_vqmovn<mode>): Likewise.
+	(neon_vqmovun<mode>): Likewise.
+	(neon_vmovl<mode>): Likewise.
+	(neon_vmul_lane<mode>): Likewise.
+	(neon_vmul_lane<mode>): Likewise.
+	(neon_vmull_lane<mode>): Likewise.
+	(neon_vqdmull_lane<mode>): Likewise.
+	(neon_vqdmulh_lane<mode>): Likewise.
+	(neon_vqdmulh_lane<mode>): Likewise.
+	(neon_vmla_lane<mode>): Likewise.
+	(neon_vmla_lane<mode>): Likewise.
+	(neon_vmlal_lane<mode>): Likewise.
+	(neon_vqdmlal_lane<mode>): Likewise.
+	(neon_vmls_lane<mode>): Likewise.
+	(neon_vmls_lane<mode>): Likewise.
+	(neon_vmlsl_lane<mode>): Likewise.
+	(neon_vqdmlsl_lane<mode>): Likewise.
+	(neon_vext<mode>): Likewise.
+	(neon_vrev64<mode>): Likewise.
+	(neon_vrev32<mode>): Likewise.
+	(neon_vrev16<mode>): Likewise.
+	(neon_vbsl<mode>_internal): Likewise.
+	(neon_vshl<mode>): Likewise.
+	(neon_vqshl<mode>): Likewise.
+	(neon_vshr_n<mode>): Likewise.
+	(neon_vshrn_n<mode>): Likewise.
+	(neon_vqshrn_n<mode>): Likewise.
+	(neon_vqshrun_n<mode>): Likewise.
+	(neon_vshl_n<mode>): Likewise.
+	(neon_vqshl_n<mode>): Likewise.
+	(neon_vqshlu_n<mode>): Likewise.
+	(neon_vshll_n<mode>): Likewise.
+	(neon_vsra_n<mode>): Likewise.
+	(neon_vsri_n<mode>): Likewise.
+	(neon_vsli_n<mode>): Likewise.
+	(neon_vtbl1v8qi): Likewise.
+	(neon_vtbl2v8qi): Likewise.
+	(neon_vtbl3v8qi): Likewise.
+	(neon_vtbl4v8qi): Likewise.
+	(neon_vtbl1v16qi): Likewise.
+	(neon_vtbl2v16qi): Likewise.
+	(neon_vcombinev16qi): Likewise.
+	(neon_vtbx1v8qi): Likewise.
+	(neon_vtbx2v8qi): Likewise.
+	(neon_vtbx3v8qi): Likewise.
+	(neon_vtbx4v8qi): Likewise.
+	(*neon_vtrn<mode>_insn): Likewise.
+	(*neon_vzip<mode>_insn): Likewise.
+	(*neon_vuzp<mode>_insn): Likewise.
+	(neon_vld1<mode>): Likewise.
+	(neon_vld1_lane<mode>): Likewise.
+	(neon_vld1_lane<mode>): Likewise.
+	(neon_vld1_dup<mode>): Likewise.
+	(neon_vld1_dup<mode>): Likewise.
+	(neon_vld1_dupv2di): Likewise.
+	(neon_vst1<mode>): Likewise.
+	(neon_vst1_lane<mode>): Likewise.
+	(neon_vst1_lane<mode>): Likewise.
+	(neon_vld2<mode>): Likewise.
+	(neon_vld2<mode>): Likewise.
+	(neon_vld2_lane<mode>): Likewise.
+	(neon_vld2_lane<mode>): Likewise.
+	(neon_vld2_dup<mode>): Likewise.
+	(neon_vst2<mode>): Likewise.
+	(neon_vst2<mode>): Likewise.
+	(neon_vst2_lane<mode>): Likewise.
+	(neon_vst2_lane<mode>): Likewise.
+	(neon_vld3<mode>): Likewise.
+	(neon_vld3qa<mode>): Likewise.
+	(neon_vld3qb<mode>): Likewise.
+	(neon_vld3_lane<mode>): Likewise.
+	(neon_vld3_lane<mode>): Likewise.
+	(neon_vld3_dup<mode>): Likewise.
+	(neon_vst3<mode>): Likewise.
+	(neon_vst3qa<mode>): Likewise.
+	(neon_vst3qb<mode>): Likewise.
+	(neon_vst3_lane<mode>): Likewise.
+	(neon_vst3_lane<mode>): Likewise.
+	(neon_vld4<mode>): Likewise.
+	(neon_vld4qa<mode>): Likewise.
+	(neon_vld4qb<mode>): Likewise.
+	(neon_vld4_lane<mode>): Likewise.
+	(neon_vld4_lane<mode>): Likewise.
+	(neon_vld4_dup<mode>): Likewise.
+	(neon_vst4<mode>): Likewise.
+	(neon_vst4qa<mode>): Likewise.
+	(neon_vst4qb<mode>): Likewise.
+	(neon_vst4_lane<mode>): Likewise.
+	(neon_vst4_lane<mode>): Likewise.
+	(neon_vec_unpack<US>_lo_<mode>): Likewise.
+	(neon_vec_unpack<US>_hi_<mode>): Likewise.
+	(neon_vec_<US>mult_lo_<mode>): Likewise.
+	(neon_vec_<US>mult_hi_<mode>): Likewise.
+	(neon_vec_<US>shiftl_<mode>): Likewise.
+	(neon_unpack<US>_<mode>): Likewise.
+	(neon_vec_<US>mult_<mode>): Likewise.
+	(vec_pack_trunc_<mode>): Likewise.
+	(neon_vec_pack_trunc_<mode>): Likewise.
+	(neon_vabd<mode>_2): Likewise.
+	(neon_vabd<mode>_3): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203612
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64.md (movtf_aarch64): Update type attribute.
+	(load_pair): Update type attribute.
+	(store_pair): Update type attribute.
+	* config/aarch64/iterators.md (q): New.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203611
+	2013-10-15  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/types.md: Add new types for Neon insns.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r203241
+	2013-10-07  Renlin Li  <Renlin.Li@arm.com>
+
+	* config/arm/arm-cores.def (cortex-a53): Use cortex tuning.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202560
+	2013-09-13  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (arm_cmpsi_insn): Split rI alternative.
+	Set type attribute correctly. Set predicable_short_it attribute.
+	(cmpsi_shiftsi): Remove %? from output template.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202448
+	2013-09-10  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64.md (generic_sched): New.
+	* config/aarch64/aarch64-generic.md (load): Make conditional
+	on generic_sched attribute.
+	(nonload): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202334
+	2013-09-06  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64.md
+	(*movtf_aarch64): Use neon_<ls>dm_2 as type where v8type
+	is fpsimd_<load/store>2.
+	(load_pair<mode>): Likewise.
+	(store_pair<mode>): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202333
+	2013-09-06  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/types.md (type): Add "mrs" type.
+	* config/aarch64/aarch64.md
+	(aarch64_load_tp_hard): Make type "mrs".
+	* config/arm/arm.md
+	(load_tp_hard): Make type "mrs".
+	* config/arm/cortex-a15.md: Update with new attributes.
+	* config/arm/cortex-a5.md: Update with new attributes.
+	* config/arm/cortex-a53.md: Update with new attributes.
+	* config/arm/cortex-a7.md: Update with new attributes.
+	* config/arm/cortex-a8.md: Update with new attributes.
+	* config/arm/cortex-a9.md: Update with new attributes.
+	* config/arm/cortex-m4.md: Update with new attributes.
+	* config/arm/cortex-r4.md: Update with new attributes.
+	* config/arm/fa526.md: Update with new attributes.
+	* config/arm/fa606te.md: Update with new attributes.
+	* config/arm/fa626te.md: Update with new attributes.
+	* config/arm/fa726te.md: Update with new attributes.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202332
+	2013-09-06  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64.md
+	(*movti_aarch64): Use "multiple" for type where v8type is "move2".
+	(*movtf_aarch64): Likewise.
+	* config/arm/arm.md
+	(thumb1_movdi_insn): Use "multiple" for type where more than one
+	instruction is used for a move.
+	(*arm32_movhf): Likewise.
+	(*thumb_movdf_insn): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202331
+	2013-09-06  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/types.md (type): Rename fcpys to fmov.
+	* config/arm/vfp.md
+	(*arm_movsi_vfp): Rename type fcpys as fmov.
+	(*thumb2_movsi_vfp): Likewise
+	(*movhf_vfp_neon): Likewise
+	(*movhf_vfp): Likewise
+	(*movsf_vfp): Likewise
+	(*thumb2_movsf_vfp): Likewise
+	(*movsfcc_vfp): Likewise
+	(*thumb2_movsfcc_vfp): Likewise
+	* config/aarch64/aarch64-simd.md
+	(move_lo_quad_<mode>): Replace type mov_reg with fmovs.
+	* config/aarch64/aarch64.md
+	(*movsi_aarch64): Replace type mov_reg with fmovs.
+	(*movdi_aarch64): Likewise
+	(*movsf_aarch64): Likewise
+	(*movdf_aarch64): Likewise
+	* config/arm/arm.c
+	(cortexa7_older_only): Rename TYPE_FCPYS to TYPE_FMOV.
+	* config/arm/iwmmxt.md
+	(*iwmmxt_movsi_insn): Rename type fcpys as fmov.
+	* config/arm/arm1020e.md: Update with new attributes.
+	* config/arm/cortex-a15-neon.md: Update with new attributes.
+	* config/arm/cortex-a5.md: Update with new attributes.
+	* config/arm/cortex-a53.md: Update with new attributes.
+	* config/arm/cortex-a7.md: Update with new attributes.
+	* config/arm/cortex-a8-neon.md: Update with new attributes.
+	* config/arm/cortex-a9.md: Update with new attributes.
+	* config/arm/cortex-m4-fpu.md: Update with new attributes.
+	* config/arm/cortex-r4f.md: Update with new attributes.
+	* config/arm/marvell-pj4.md: Update with new attributes.
+	* config/arm/vfp11.md: Update with new attributes.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202330
+	2013-09-06  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64.md
+	(*madd<mode>): Fix type attribute.
+	(*maddsi_uxtw): Likewise.
+	(*msub<mode>): Likewise.
+	(*msubsi_uxtw): Likewise.
+	(<su_optab>maddsidi4): Likewise.
+	(<su_optab>msubsidi4): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202329
+	2013-09-06  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/types.md: Split fdiv<sd> as fsqrt<sd>, fdiv<sd>.
+	* config/arm/arm.md (core_cycles): Remove fdiv.
+	* config/arm/vfp.md:
+	(*sqrtsf2_vfp): Update for attribute changes.
+	(*sqrtdf2_vfp): Likewise.
+	* config/aarch64/aarch64.md:
+	(sqrt<mode>2): Update for attribute changes.
+	* config/arm/arm1020e.md: Update with new attributes.
+	* config/arm/cortex-a15-neon.md: Update with new attributes.
+	* config/arm/cortex-a5.md: Update with new attributes.
+	* config/arm/cortex-a53.md: Update with new attributes.
+	* config/arm/cortex-a7.md: Update with new attributes.
+	* config/arm/cortex-a8-neon.md: Update with new attributes.
+	* config/arm/cortex-a9.md: Update with new attributes.
+	* config/arm/cortex-m4-fpu.md: Update with new attributes.
+	* config/arm/cortex-r4f.md: Update with new attributes.
+	* config/arm/marvell-pj4.md: Update with new attributes.
+	* config/arm/vfp11.md: Update with new attributes.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202328
+	2013-09-06  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/types.md
+	(type): Split f_cvt as f_cvt, f_cvtf2i, f_cvti2f.
+	* config/aarch64/aarch64.md
+	(l<fcvt_pattern><su_optab><GPF:mode><GPI:mode>2): Update with
+	new attributes.
+	(fix_trunc<GPF:mode><GPI:mode>2): Likewise.
+	(fixuns_trunc<GPF:mode><GPI:mode>2): Likewise.
+	(float<GPI:mode><GPF:mode>2): Likewise.
+	* config/arm/vfp.md
+	(*truncsisf2_vfp): Update with new attributes.
+	(*truncsidf2_vfp): Likewise.
+	(fixuns_truncsfsi2): Likewise.
+	(fixuns_truncdfsi2): Likewise.
+	(*floatsisf2_vfp): Likewise.
+	(*floatsidf2_vfp): Likewise.
+	(floatunssisf2): Likewise.
+	(floatunssidf2): Likewise.
+	(*combine_vcvt_f32_<FCVTI32typename>): Likewise.
+	(*combine_vcvt_f64_<FCVTI32typename>): Likewise.
+	* config/arm/arm1020e.md: Update with new attributes.
+	* config/arm/cortex-a15-neon.md: Update with new attributes.
+	* config/arm/cortex-a5.md: Update with new attributes.
+	* config/arm/cortex-a53.md: Update with new attributes.
+	* config/arm/cortex-a7.md: Update with new attributes.
+	* config/arm/cortex-a8-neon.md: Update with new attributes.
+	* config/arm/cortex-a9.md: Update with new attributes.
+	* config/arm/cortex-m4-fpu.md: Update with new attributes.
+	* config/arm/cortex-r4f.md: Update with new attributes.
+	* config/arm/marvell-pj4.md: Update with new attributes.
+	* config/arm/vfp11.md: Update with new attributes.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202323
+	2013-09-06  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/arm/types.md: Add "no_insn", "multiple" and "untyped"
+	types.
+	* config/arm/arm-fixed.md: Add type attribute to all insn
+	patterns.
+	(add<mode>3): Add type attribute.
+	(add<mode>3): Likewise.
+	(usadd<mode>3): Likewise.
+	(ssadd<mode>3): Likewise.
+	(sub<mode>3): Likewise.
+	(sub<mode>3): Likewise.
+	(ussub<mode>3): Likewise.
+	(sssub<mode>3): Likewise.
+	(ssmulsa3): Likewise.
+	(usmulusa3): Likewise.
+	(arm_usatsihi): Likewise.
+	* config/arm/vfp.md
+	(*movdi_vfp): Add types for all instructions.
+	(*movdi_vfp_cortexa8): Likewise.
+	(*movhf_vfp_neon): Likewise.
+	(*movhf_vfp): Likewise.
+	(*movdf_vfp): Likewise.
+	(*thumb2_movdf_vfp): Likewise.
+	(*thumb2_movdfcc_vfp): Likewise.
+	* config/arm/arm.md: Add type attribute to all insn patterns.
+	(*thumb1_adddi3): Add type attribute.
+	(*arm_adddi3): Likewise.
+	(*adddi_sesidi_di): Likewise.
+	(*adddi_zesidi_di): Likewise.
+	(*thumb1_addsi3): Likewise.
+	(addsi3_compare0): Likewise.
+	(*addsi3_compare0_scratch): Likewise.
+	(*compare_negsi_si): Likewise.
+	(cmpsi2_addneg): Likewise.
+	(*addsi3_carryin_<optab>): Likewise.
+	(*addsi3_carryin_alt2_<optab>): Likewise.
+	(*addsi3_carryin_clobercc_<optab>): Likewise.
+	(*subsi3_carryin): Likewise.
+	(*subsi3_carryin_const): Likewise.
+	(*subsi3_carryin_compare): Likewise.
+	(*subsi3_carryin_compare_const): Likewise.
+	(*arm_subdi3): Likewise.
+	(*thumb_subdi3): Likewise.
+	(*subdi_di_zesidi): Likewise.
+	(*subdi_di_sesidi): Likewise.
+	(*subdi_zesidi_di): Likewise.
+	(*subdi_sesidi_di): Likewise.
+	(*subdi_zesidi_ze): Likewise.
+	(thumb1_subsi3_insn): Likewise.
+	(*arm_subsi3_insn): Likewise.
+	(*anddi3_insn): Likewise.
+	(*anddi_zesidi_di): Likewise.
+	(*anddi_sesdi_di): Likewise.
+	(*ne_zeroextracts): Likewise.
+	(*ne_zeroextracts): Likewise.
+	(*ite_ne_zeroextr): Likewise.
+	(*ite_ne_zeroextr): Likewise.
+	(*anddi_notdi_di): Likewise.
+	(*anddi_notzesidi): Likewise.
+	(*anddi_notsesidi): Likewise.
+	(andsi_notsi_si): Likewise.
+	(thumb1_bicsi3): Likewise.
+	(*iordi3_insn): Likewise.
+	(*iordi_zesidi_di): Likewise.
+	(*iordi_sesidi_di): Likewise.
+	(*thumb1_iorsi3_insn): Likewise.
+	(*xordi3_insn): Likewise.
+	(*xordi_zesidi_di): Likewise.
+	(*xordi_sesidi_di): Likewise.
+	(*arm_xorsi3): Likewise.
+	(*andsi_iorsi3_no): Likewise.
+	(*smax_0): Likewise.
+	(*smax_m1): Likewise.
+	(*arm_smax_insn): Likewise.
+	(*smin_0): Likewise.
+	(*arm_smin_insn): Likewise.
+	(*arm_umaxsi3): Likewise.
+	(*arm_uminsi3): Likewise.
+	(*minmax_arithsi): Likewise.
+	(*minmax_arithsi_): Likewise.
+	(*satsi_<SAT:code>): Likewise.
+	(arm_ashldi3_1bit): Likewise.
+	(arm_ashrdi3_1bit): Likewise.
+	(arm_lshrdi3_1bit): Likewise.
+	(*arm_negdi2): Likewise.
+	(*thumb1_negdi2): Likewise.
+	(*arm_negsi2): Likewise.
+	(*thumb1_negsi2): Likewise.
+	(*negdi_extendsid): Likewise.
+	(*negdi_zero_extend): Likewise.
+	(*arm_abssi2): Likewise.
+	(*thumb1_abssi2): Likewise.
+	(*arm_neg_abssi2): Likewise.
+	(*thumb1_neg_abss): Likewise.
+	(one_cmpldi2): Likewise.
+	(extend<mode>di2): Likewise.
+	(*compareqi_eq0): Likewise.
+	(*arm_extendhisi2addsi): Likewise.
+	(*arm_movdi): Likewise.
+	(*thumb1_movdi_insn): Likewise.
+	(*arm_movt): Likewise.
+	(*thumb1_movsi_insn): Likewise.
+	(pic_add_dot_plus_four): Likewise.
+	(pic_add_dot_plus_eight): Likewise.
+	(tls_load_dot_plus_eight): Likewise.
+	(*thumb1_movhi_insn): Likewise.
+	(*thumb1_movsf_insn): Likewise.
+	(*movdf_soft_insn): Likewise.
+	(*thumb_movdf_insn): Likewise.
+	(cbranchsi4_insn): Likewise.
+	(cbranchsi4_scratch): Likewise.
+	(*negated_cbranchsi4): Likewise.
+	(*tbit_cbranch): Likewise.
+	(*tlobits_cbranch): Likewise.
+	(*tstsi3_cbranch): Likewise.
+	(*cbranchne_decr1): Likewise.
+	(*addsi3_cbranch): Likewise.
+	(*addsi3_cbranch_scratch): Likewise.
+	(*arm_cmpdi_insn): Likewise.
+	(*arm_cmpdi_unsig): Likewise.
+	(*arm_cmpdi_zero): Likewise.
+	(*thumb_cmpdi_zero): Likewise.
+	(*deleted_compare): Likewise.
+	(*mov_scc): Likewise.
+	(*mov_negscc): Likewise.
+	(*mov_notscc): Likewise.
+	(*cstoresi_eq0_thumb1_insn): Likewise.
+	(cstoresi_nltu_thumb1): Likewise.
+	(cstoresi_ltu_thu): Likewise.
+	(thumb1_addsi3_addgeu): Likewise.
+	(*arm_jump): Likewise.
+	(*thumb_jump): Likewise.
+	(*check_arch2): Likewise.
+	(arm_casesi_internal): Likewise.
+	(thumb1_casesi_dispatch): Likewise.
+	(*arm_indirect_jump): Likewise.
+	(*thumb1_indirect_jump): Likewise.
+	(nop): Likewise.
+	(*and_scc): Likewise.
+	(*ior_scc): Likewise.
+	(*compare_scc): Likewise.
+	(*cond_move): Likewise.
+	(*cond_arith): Likewise.
+	(*cond_sub): Likewise.
+	(*cmp_ite0): Likewise.
+	(*cmp_ite1): Likewise.
+	(*cmp_and): Likewise.
+	(*cmp_ior): Likewise.
+	(*ior_scc_scc): Likewise.
+	(*ior_scc_scc_cmp): Likewise.
+	(*and_scc_scc): Likewise.
+	(*and_scc_scc_cmp): Likewise.
+	(*and_scc_scc_nod): Likewise.
+	(*negscc): Likewise.
+	(movcond_addsi): Likewise.
+	(movcond): Likewise.
+	(*ifcompare_plus_move): Likewise.
+	(*if_plus_move): Likewise.
+	(*ifcompare_move_plus): Likewise.
+	(*if_move_plus): Likewise.
+	(*ifcompare_arith_arith): Likewise.
+	(*if_arith_arith): Likewise.
+	(*ifcompare_arith_move): Likewise.
+	(*if_arith_move): Likewise.
+	(*ifcompare_move_arith): Likewise.
+	(*if_move_arith): Likewise.
+	(*ifcompare_move_not): Likewise.
+	(*if_move_not): Likewise.
+	(*ifcompare_not_move): Likewise.
+	(*if_not_move): Likewise.
+	(*ifcompare_shift_move): Likewise.
+	(*if_shift_move): Likewise.
+	(*ifcompare_move_shift): Likewise.
+	(*if_move_shift): Likewise.
+	(*ifcompare_shift_shift): Likewise.
+	(*ifcompare_not_arith): Likewise.
+	(*ifcompare_arith_not): Likewise.
+	(*if_arith_not): Likewise.
+	(*ifcompare_neg_move): Likewise.
+	(*if_neg_move): Likewise.
+	(*ifcompare_move_neg): Likewise.
+	(*if_move_neg): Likewise.
+	(prologue_thumb1_interwork): Likewise.
+	(*cond_move_not): Likewise.
+	(*sign_extract_onebit): Likewise.
+	(*not_signextract_onebit): Likewise.
+	(stack_tie): Likewise.
+	(align_4): Likewise.
+	(align_8): Likewise.
+	(consttable_end): Likewise.
+	(consttable_1): Likewise.
+	(consttable_2): Likewise.
+	(consttable_4): Likewise.
+	(consttable_8): Likewise.
+	(consttable_16): Likewise.
+	(*thumb1_tablejump): Likewise.
+	(prefetch): Likewise.
+	(force_register_use): Likewise.
+	(thumb_eh_return): Likewise.
+	(load_tp_hard): Likewise.
+	(load_tp_soft): Likewise.
+	(tlscall): Likewise.
+	(*arm_movtas_ze): Likewise.
+	(*arm_rev): Likewise.
+	(*arm_revsh): Likewise.
+	(*arm_rev16): Likewise.
+	* config/arm/thumb2.md
+	(*thumb2_smaxsi3): Likewise.
+	(*thumb2_sminsi3): Likewise.
+	(*thumb32_umaxsi3): Likewise.
+	(*thumb2_uminsi3): Likewise.
+	(*thumb2_negdi2): Likewise.
+	(*thumb2_abssi2): Likewise.
+	(*thumb2_neg_abss): Likewise.
+	(*thumb2_movsi_insn): Likewise.
+	(tls_load_dot_plus_four): Likewise.
+	(*thumb2_movhi_insn): Likewise.
+	(*thumb2_mov_scc): Likewise.
+	(*thumb2_mov_negs): Likewise.
+	(*thumb2_mov_negs): Likewise.
+	(*thumb2_mov_nots): Likewise.
+	(*thumb2_mov_nots): Likewise.
+	(*thumb2_movsicc_): Likewise.
+	(*thumb2_movsfcc_soft_insn): Likewise.
+	(*thumb2_indirect_jump): Likewise.
+	(*thumb2_and_scc): Likewise.
+	(*thumb2_ior_scc): Likewise.
+	(*thumb2_ior_scc_strict_it): Likewise.
+	(*thumb2_cond_move): Likewise.
+	(*thumb2_cond_arith): Likewise.
+	(*thumb2_cond_ari): Likewise.
+	(*thumb2_cond_sub): Likewise.
+	(*thumb2_negscc): Likewise.
+	(*thumb2_movcond): Likewise.
+	(thumb2_casesi_internal): Likewise.
+	(thumb2_casesi_internal_pic): Likewise.
+	(*thumb2_alusi3_short): Likewise.
+	(*thumb2_mov<mode>_shortim): Likewise.
+	(*thumb2_addsi_short): Likewise.
+	(*thumb2_subsi_short): Likewise.
+	(thumb2_addsi3_compare0): Likewise.
+	(*thumb2_cbz): Likewise.
+	(*thumb2_cbnz): Likewise.
+	(*thumb2_one_cmplsi2_short): Likewise.
+	(*thumb2_negsi2_short): Likewise.
+	(*orsi_notsi_si): Likewise.
+	* config/arm/arm1020e.md: Update with new attributes.
+	* config/arm/arm1026ejs.md: Update with new attributes.
+	* config/arm/arm1136jfs.md: Update with new attributes.
+	* config/arm/arm926ejs.md: Update with new attributes.
+	* config/arm/cortex-a15.md: Update with new attributes.
+	* config/arm/cortex-a5.md: Update with new attributes.
+	* config/arm/cortex-a53.md: Update with new attributes.
+	* config/arm/cortex-a7.md: Update with new attributes.
+	* config/arm/cortex-a8.md: Update with new attributes.
+	* config/arm/cortex-a9.md: Update with new attributes.
+	* config/arm/cortex-m4.md: Update with new attributes.
+	* config/arm/cortex-r4.md: Update with new attributes.
+	* config/arm/fa526.md: Update with new attributes.
+	* config/arm/fa606te.md: Update with new attributes.
+	* config/arm/fa626te.md: Update with new attributes.
+	* config/arm/fa726te.md: Update with new attributes.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202292
+	2013-09-05  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64.md
+	(type): Remove frecpe, frecps, frecpx.
+	(aarch64_frecp<FRECP:frecp_suffix><mode>): Move to aarch64-simd.md,
+	fix to be a TARGET_SIMD instruction.
+	(aarch64_frecps): Remove.
+	* config/aarch64/aarch64-simd.md
+	(aarch64_frecp<FRECP:frecp_suffix><mode>): New, moved from aarch64.md
+	(aarch64_frecps<mode>): Handle all float/vector of float modes.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202291
+	2013-09-05  James Greenhalgh  <james.greenhalgh@arm.com>
+	Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/arm/types.md (define_attr "type"):
+	Expand "arlo_imm"
+	into "adr", "alu_imm", "alus_imm", "logic_imm", "logics_imm".
+	Expand "arlo_reg"
+	into "adc_reg", "adc_imm", "adcs_reg", "adcs_imm", "alu_ext",
+	"alu_reg", "alus_ext", "alus_reg", "bfm", "csel", "logic_reg",
+	"logics_reg", "rev".
+	Expand "arlo_shift"
+	into "alu_shift_imm", "alus_shift_imm", "logic_shift_imm",
+	"logics_shift_imm".
+	Expand "arlo_shift_reg"
+	into "alu_shift_reg", "alus_shift_reg", "logic_shift_reg",
+	"logics_shift_reg".
+	Expand "clz" into "clz, "rbit".
+	Rename "shift" to "shift_imm".
+	* config/arm/arm.md (define_attr "core_cycles"): Update for attribute
+	changes.
+	Update for attribute changes all occurrences of arlo_* and
+	shift* types.
+	* config/arm/arm-fixed.md: Update for attribute changes
+	all occurrences of arlo_* types.
+	* config/arm/thumb2.md: Update for attribute changes all occurrences
+	of arlo_* types.
+	* config/arm/arm.c (xscale_sched_adjust_cost):  (rtx insn, rtx
+	(cortexa7_older_only): Likewise.
+	(cortexa7_younger):  Likewise.
+	* config/arm/arm1020e.md (1020alu_op): Update for attribute changes.
+	(1020alu_shift_op): Likewise.
+	(1020alu_shift_reg_op): Likewise.
+	* config/arm/arm1026ejs.md (alu_op): Update for attribute changes.
+	(alu_shift_op): Likewise.
+	(alu_shift_reg_op): Likewise.
+	* config/arm/arm1136jfs.md (11_alu_op): Update for
+	attribute changes.
+	(11_alu_shift_op): Likewise.
+	(11_alu_shift_reg_op): Likewise.
+	* config/arm/arm926ejs.md (9_alu_op): Update for attribute changes.
+	(9_alu_shift_reg_op): Likewise.
+	* config/arm/cortex-a15.md (cortex_a15_alu): Update for
+	attribute changes.
+	(cortex_a15_alu_shift): Likewise.
+	(cortex_a15_alu_shift_reg): Likewise.
+	* config/arm/cortex-a5.md (cortex_a5_alu): Update for
+	attribute changes.
+	(cortex_a5_alu_shift): Likewise.
+	* config/arm/cortex-a53.md
+	(cortex_a53_alu): Update for attribute changes.
+	(cortex_a53_alu_shift): Likewise.
+	* config/arm/cortex-a7.md
+	(cortex_a7_alu_imm): Update for attribute changes.
+	(cortex_a7_alu_reg): Likewise.
+	(cortex_a7_alu_shift): Likewise.
+	* config/arm/cortex-a8.md
+	(cortex_a8_alu): Update for attribute changes.
+	(cortex_a8_alu_shift): Likewise.
+	(cortex_a8_alu_shift_reg): Likewise.
+	* config/arm/cortex-a9.md
+	(cortex_a9_dp): Update for attribute changes.
+	(cortex_a9_dp_shift): Likewise.
+	* config/arm/cortex-m4.md
+	(cortex_m4_alu): Update for attribute changes.
+	* config/arm/cortex-r4.md
+	(cortex_r4_alu): Update for attribute changes.
+	(cortex_r4_mov): Likewise.
+	(cortex_r4_alu_shift_reg): Likewise.
+	* config/arm/fa526.md
+	(526_alu_op): Update for attribute changes.
+	(526_alu_shift_op): Likewise.
+	* config/arm/fa606te.md
+	(606te_alu_op): Update for attribute changes.
+	* config/arm/fa626te.md
+	(626te_alu_op): Update for attribute changes.
+	(626te_alu_shift_op): Likewise.
+	* config/arm/fa726te.md
+	(726te_alu_op): Update for attribute changes.
+	(726te_alu_shift_op): Likewise.
+	(726te_alu_shift_reg_op): Likewise.
+	* config/arm/fmp626.md (mp626_alu_op): Update for attribute changes.
+	(mp626_alu_shift_op): Likewise.
+	* config/arm/marvell-pj4.md (pj4_alu): Update for attribute changes.
+	(pj4_alu_conds): Likewise.
+	(pj4_shift): Likewise.
+	(pj4_shift_conds): Likewise.
+	(pj4_alu_shift): Likewise.
+	(pj4_alu_shift_conds): Likewise.
+	* config/aarch64/aarch64.md: Update for attribute change
+	all occurrences of arlo_* and shift* types.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202272
+	2013-08-02  James Greenhalgh  <james.greenhalgh@arm.com>
+	Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64.md
+	(*movti_aarch64): Rename r_2_f and f_2_r.
+	(*movsf_aarch64): Likewise.
+	(*movdf_aarch64): Likewise.
+	(*movtf_aarch64): Likewise.
+	(aarch64_movdi_<mode>low): Likewise.
+	(aarch64_movdi_<mode>high): Likewise.
+	(aarch64_mov<mode>high_di): Likewise.
+	(aarch64_mov<mode>low_di): Likewise.
+	(aarch64_movtilow_tilow): Likewise.
+	* config/arm/arm.md (attribute "neon_type"): Delete.  Move attribute
+	values to config/arm/types.md
+	(attribute "conds"): Update for attribute change.
+	(anddi3_insn): Likewise.
+	(iordi3_insn): Likewise.
+	(xordi3_insn): Likewise.
+	(one_cmpldi2): Likewise.
+	* config/arm/types.md (type): Add Neon types.
+	* config/arm/neon.md (neon_mov<mode>): Remove "neon_type" attribute,
+	use "type" attribute.
+	(movmisalign<mode>_neon_store): Likewise.
+	(movmisalign<mode>_neon_load): Likewise.
+	(vec_set<mode>_internal): Likewise.
+	(vec_setv2di_internal): Likewise.
+	(vec_extract<mode>): Likewise.
+	(vec_extractv2di): Likewise.
+	(add<mode>3_neon): Likewise.
+	(adddi3_neon): Likewise.
+	(sub<mode>3_neon): Likewise.
+	(subdi3_neon): Likewise.
+	(mul<mode>3_neon): Likewise.
+	(mul<mode>3add<mode>_neon): Likewise.
+	(mul<mode>3neg<mode>add<mode>_neon): Likewise.
+	(fma<VCVTF:mode>4)): Likewise.
+	(fma<VCVTF:mode>4_intrinsic): Likewise.
+	(fmsub<VCVTF:mode>4)): Likewise.
+	(fmsub<VCVTF:mode>4_intrinsic): Likewise.
+	(neon_vrint<NEON_VRINT:nvrint_variant><VCVTF:mode>): Likewise.
+	(ior<mode>3): Likewise.
+	(and<mode>3): Likewise.
+	(anddi3_neon): Likewise.
+	(orn<mode>3_neon): Likewise.
+	(orndi3_neon): Likewise.
+	(bic<mode>3_neon): Likewise.
+	(bicdi3_neon): Likewise.
+	(xor<mode>3): Likewise.
+	(one_cmpl<mode>2): Likewise.
+	(abs<mode>2): Likewise.
+	(neg<mode>2): Likewise.
+	(umin<mode>3_neon): Likewise.
+	(umax<mode>3_neon): Likewise.
+	(smin<mode>3_neon): Likewise.
+	(smax<mode>3_neon): Likewise.
+	(vashl<mode>3): Likewise.
+	(vashr<mode>3_imm): Likewise.
+	(vlshr<mode>3_imm): Likewise.
+	(ashl<mode>3_signed): Likewise.
+	(ashl<mode>3_unsigned): Likewise.
+	(neon_load_count): Likewise.
+	(ashldi3_neon_noclobber): Likewise.
+	(signed_shift_di3_neon): Likewise.
+	(unsigned_shift_di3_neon): Likewise.
+	(ashrdi3_neon_imm_noclobber): Likewise.
+	(lshrdi3_neon_imm_noclobber): Likewise.
+	(widen_ssum<mode>3): Likewise.
+	(widen_usum<mode>3): Likewise.
+	(quad_halves_<code>v4si): Likewise.
+	(quad_halves_<code>v4sf): Likewise.
+	(quad_halves_<code>v8hi): Likewise.
+	(quad_halves_<code>v16qi): Likewise.
+	(reduc_splus_v2di): Likewise.
+	(neon_vpadd_internal<mode>): Likewise.
+	(neon_vpsmin<mode>): Likewise.
+	(neon_vpsmax<mode>): Likewise.
+	(neon_vpumin<mode>): Likewise.
+	(neon_vpumax<mode>): Likewise.
+	(ss_add<mode>_neon): Likewise.
+	(us_add<mode>_neon): Likewise.
+	(ss_sub<mode>_neon): Likewise.
+	(us_sub<mode>_neon): Likewise.
+	(neon_vadd<mode>_unspec): Likewise.
+	(neon_vaddl<mode>): Likewise.
+	(neon_vaddw<mode>): Likewise.
+	(neon_vhadd<mode>): Likewise.
+	(neon_vqadd<mode>): Likewise.
+	(neon_vaddhn<mode>): Likewise.
+	(neon_vmul<mode>): Likewise.
+	(neon_vmla<mode>): Likewise.
+	(neon_vmlal<mode>): Likewise.
+	(neon_vmls<mode>): Likewise.
+	(neon_vmlsl<mode>): Likewise.
+	(neon_vqdmulh<mode>): Likewise.
+	(neon_vqdmlal<mode>): Likewise.
+	(neon_vqdmlsl<mode>): Likewise.
+	(neon_vmull<mode>): Likewise.
+	(neon_vqdmull<mode>): Likewise.
+	(neon_vsub<mode>_unspec): Likewise.
+	(neon_vsubl<mode>): Likewise.
+	(neon_vsubw<mode>): Likewise.
+	(neon_vqsub<mode>): Likewise.
+	(neon_vhsub<mode>): Likewise.
+	(neon_vsubhn<mode>): Likewise.
+	(neon_vceq<mode>): Likewise.
+	(neon_vcge<mode>): Likewise.
+	(neon_vcgeu<mode>): Likewise.
+	(neon_vcgt<mode>): Likewise.
+	(neon_vcgtu<mode>): Likewise.
+	(neon_vcle<mode>): Likewise.
+	(neon_vclt<mode>): Likewise.
+	(neon_vcage<mode>): Likewise.
+	(neon_vcagt<mode>): Likewise.
+	(neon_vtst<mode>): Likewise.
+	(neon_vabd<mode>): Likewise.
+	(neon_vabdl<mode>): Likewise.
+	(neon_vaba<mode>): Likewise.
+	(neon_vabal<mode>): Likewise.
+	(neon_vmax<mode>): Likewise.
+	(neon_vmin<mode>): Likewise.
+	(neon_vpaddl<mode>): Likewise.
+	(neon_vpadal<mode>): Likewise.
+	(neon_vpmax<mode>): Likewise.
+	(neon_vpmin<mode>): Likewise.
+	(neon_vrecps<mode>): Likewise.
+	(neon_vrsqrts<mode>): Likewise.
+	(neon_vqabs<mode>): Likewise.
+	(neon_vqneg<mode>): Likewise.
+	(neon_vcls<mode>): Likewise.
+	(clz<mode>2): Likewise.
+	(popcount<mode>2): Likewise.
+	(neon_vrecpe): Likewise.
+	(neon_vrsqrte): Likewise.
+	(neon_vget_lane<mode>_sext_internal): Likewise.
+	(neon_vget_lane<mode>_zext_internal): Likewise.
+	(neon_vdup_n<mode>): Likewise.
+	(neon_vdup_nv2di): Likewise.
+	(neon_vdpu_lane<mode>_internal): Likewise.
+	(neon_vswp<mode>): Likewise.
+	(float<mode><V_cvtto>2): Likewise.
+	(floatuns<mode><V_cvtto>2): Likewise.
+	(fix_trunc<mode><V_cvtto>)2): Likewise
+	(fixuns_trunc<mode><V_cvtto)2): Likewise.
+	(neon_vcvt<mode>): Likewise.
+	(neon_vcvtv4sfv4hf): Likewise.
+	(neon_vcvtv4hfv4sf): Likewise.
+	(neon_vcvt_n<mode>): Likewise.
+	(neon_vmovn<mode>): Likewise.
+	(neon_vqmovn<mode>): Likewise.
+	(neon_vqmovun<mode>): Likewise.
+	(neon_vmovl<mode>): Likewise.
+	(neon_vmul_lane<mode>): Likewise.
+	(neon_vmull_lane<mode>): Likewise.
+	(neon_vqdmull_lane<mode>): Likewise.
+	(neon_vqdmulh_lane<mode>): Likewise.
+	(neon_vmla_lane<mode>): Likewise.
+	(neon_vmlal_lane<mode>): Likewise.
+	(neon_vqdmlal_lane<mode>): Likewise.
+	(neon_vmls_lane<mode>): Likewise.
+	(neon_vmlsl_lane<mode>): Likewise.
+	(neon_vqdmlsl_lane<mode>): Likewise.
+	(neon_vext<mode>): Likewise.
+	(neon_vrev64<mode>): Likewise.
+	(neon_vrev32<mode>): Likewise.
+	(neon_vrev16<mode>): Likewise.
+	(neon_vbsl<mode>_internal): Likewise.
+	(neon_vshl<mode>): Likewise.
+	(neon_vqshl<mode>): Likewise.
+	(neon_vshr_n<mode>): Likewise.
+	(neon_vshrn_n<mode>): Likewise.
+	(neon_vqshrn_n<mode>): Likewise.
+	(neon_vqshrun_n<mode>): Likewise.
+	(neon_vshl_n<mode>): Likewise.
+	(neon_vqshl_n<mode>): Likewise.
+	(neon_vqshlu_n<mode>): Likewise.
+	(neon_vshll_n<mode>): Likewise.
+	(neon_vsra_n<mode>): Likewise.
+	(neon_vsri_n<mode>): Likewise.
+	(neon_vsli_n<mode>): Likewise.
+	(neon_vtbl1v8qi): Likewise.
+	(neon_vtbl2v8qi): Likewise.
+	(neon_vtbl3v8qi): Likewise.
+	(neon_vtbl4v8qi): Likewise.
+	(neon_vtbx1v8qi): Likewise.
+	(neon_vtbx2v8qi): Likewise.
+	(neon_vtbx3v8qi): Likewise.
+	(neon_vtbx4v8qi): Likewise.
+	(neon_vtrn<mode>_internal): Likewise.
+	(neon_vzip<mode>_internal): Likewise.
+	(neon_vuzp<mode>_internal): Likewise.
+	(neon_vld1<mode>): Likewise.
+	(neon_vld1_lane<mode>): Likewise.
+	(neon_vld1_dup<mode>): Likewise.
+	(neon_vld1_dupv2di): Likewise.
+	(neon_vst1<mode>): Likewise.
+	(neon_vst1_lane<mode>): Likewise.
+	(neon_vld2<mode>): Likewise.
+	(neon_vld2_lane<mode>): Likewise.
+	(neon_vld2_dup<mode>): Likewise.
+	(neon_vst2<mode>): Likewise.
+	(neon_vst2_lane<mode>): Likewise.
+	(neon_vld3<mode>): Likewise.
+	(neon_vld3qa<mode>): Likewise.
+	(neon_vld3qb<mode>): Likewise.
+	(neon_vld3_lane<mode>): Likewise.
+	(neon_vld3_dup<mode>): Likewise.
+	(neon_vst3<mode>): Likewise.
+	(neon_vst3qa<mode>): Likewise.
+	(neon_vst3qb<mode>): Likewise.
+	(neon_vst3_lane<mode>): Likewise.
+	(neon_vld4<mode>): Likewise.
+	(neon_vld4qa<mode>): Likewise.
+	(neon_vld4qb<mode>): Likewise.
+	(neon_vld4_lane<mode>): Likewise.
+	(neon_vld4_dup<mode>): Likewise.
+	(neon_vst4<mode>): Likewise.
+	(neon_vst4qa<mode>): Likewise.
+	(neon_vst4qb<mode>): Likewise.
+	(neon_vst4_lane<mode>): Likewise.
+	(neon_vec_unpack<US>_lo_<mode>): Likewise.
+	(neon_vec_unpack<US>_hi_<mode>): Likewise.
+	(neon_vec_<US>mult_lo_<mode>): Likewise.
+	(neon_vec_<US>mult_hi_<mode>): Likewise.
+	(neon_vec_<US>shiftl_<mode>): Likewise.
+	(neon_unpack<US>_<mode>): Likewise.
+	(neon_vec_<US>mult_<mode>): Likewise.
+	(vec_pack_trunc_<mode>): Likewise.
+	(neon_vec_pack_trunk_<mode>): Likewise.
+	(neon_vabd<mode>_2): Likewise.
+	(neon_vabd<mode>_3): Likewise.
+	* config/arm/vfp.md (arm_movsi_vfp): Update for attribute changes.
+	(thumb2_movsi_vfp): Likewise.
+	(movdi_vfp): Likewise.
+	(movdi_vfp_cortexa8): Likewise.
+	(movhf_vfp_neon): Likewise.
+	(movhf_vfp): Likewiwse.
+	(movsf_vfp): Likewiwse.
+	(thumb2_movsf_vfp): Likewiwse.
+	(movdf_vfp): Likewise.
+	(thumb2_movdf_vfp): Likewise.
+	(movsfcc_vfp): Likewise.
+	(thumb2_movsfcc_vfp): Likewise.
+	(movdfcc_vfp): Likewise.
+	(thumb2_movdfcc_vfp): Likewise.
+	* config/arm/arm.c (cortexa7_older_only): Update for attribute change.
+	* config/arm/arm1020e.md (v10_c2v): Update for attribute change.
+	(v10_v2c): Likewise.
+	* config/arm/cortex-a15-neon.md (cortex_a15_neon_int_1): Update for
+	attribute change.
+	(cortex_a15_neon_int_2): Likewise.
+	(cortex_a15_neon_int_3): Likewise.
+	(cortex_a15_neon_int_4): Likewise.
+	(cortex_a15_neon_int_5): Likewise.
+	(cortex_a15_neon_vqneg_vqabs): Likewise.
+	(cortex_a15_neon_vmov): Likewise.
+	(cortex_a15_neon_vaba): Likewise.
+	(cortex_a15_neon_vaba_qqq): Likewise.
+	(cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a15_neon_mul_qqq_8_16_32_ddd_32): Likewise.
+	(cortex_a15_neon_mul_qdd_64_32_long_qqd_16_ddd_32_\
+	scalar_64_32_long_scalar): Likewise.
+	(cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a15_neon_mla_qqq_8_16): Likewise.
+	(cortex_a15_neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_\
+	lotype_qdd_64_32_long): Likewise.
+	(cortex_a15_neon_mla_qqq_32_qqd_32_scalar): Likewise.
+	(cortex_a15_neon_mul_ddd_16_scalar_32_16_long_scalar): Likewise.
+	(cortex_a15_neon_mul_qqd_32_scalar): Likewise.
+	(cortex_a15_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar): Likewise.
+	(cortex_a15_neon_shift_1): Likewise.
+	(cortex_a15_neon_shift_2): Likewise.
+	(cortex_a15_neon_shift_3): Likewise.
+	(cortex_a15_neon_vshl_ddd): Likewise.
+	(cortex_a15_neon_vqshl_vrshl_vqrshl_qqq): Likewise.
+	(cortex_a15_neon_vsra_vrsra): Likewise.
+	(cortex_a15_neon_fp_vadd_ddd_vabs_dd): Likewise.
+	(cortex_a15_neon_fp_vadd_qqq_vabs_qq): Likewise.
+	(cortex_a15_neon_fp_vmul_ddd): Likewise.
+	(cortex_a15_neon_fp_vmul_qqd): Likewise.
+	(cortex_a15_neon_fp_vmla_ddd): Likewise.
+	(cortex_a15_neon_fp_vmla_qqq): Likewise.
+	(cortex_a15_neon_fp_vmla_ddd_scalar): Likewise.
+	(cortex_a15_neon_fp_vmla_qqq_scalar): Likewise.
+	(cortex_a15_neon_fp_vrecps_vrsqrts_ddd): Likewise.
+	(cortex_a15_neon_fp_vrecps_vrsqrts_qqq): Likewise.
+	(cortex_a15_neon_bp_simple): Likewise.
+	(cortex_a15_neon_bp_2cycle): Likewise.
+	(cortex_a15_neon_bp_3cycle): Likewise.
+	(cortex_a15_neon_vld1_1_2_regs): Likewise.
+	(cortex_a15_neon_vld1_3_4_regs): Likewise.
+	(cortex_a15_neon_vld2_2_regs_vld1_vld2_all_lanes): Likewise.
+	(cortex_a15_neon_vld2_4_regs): Likewise.
+	(cortex_a15_neon_vld3_vld4): Likewise.
+	(cortex_a15_neon_vst1_1_2_regs_vst2_2_regs): Likewise.
+	(cortex_a15_neon_vst1_3_4_regs): Likewise.
+	(cortex_a15_neon_vst2_4_regs_vst3_vst4): Likewise.
+	(cortex_a15_neon_vst3_vst4): Likewise.
+	(cortex_a15_neon_vld1_vld2_lane): Likewise.
+	(cortex_a15_neon_vld3_vld4_lane" 10
+	(cortex_a15_neon_vst1_vst2_lane): Likewise.
+	(cortex_a15_neon_vst3_vst4_lane): Likewise.
+	(cortex_a15_neon_vld3_vld4_all_lanes): Likewise.
+	(cortex_a15_neon_ldm_2): Likewise.0
+	(cortex_a15_neon_stm_2): Likewise.
+	(cortex_a15_neon_mcr): Likewise.
+	(cortex_a15_neon_mcr_2_mcrr): Likewise.
+	(cortex_a15_neon_mrc): Likewise.
+	(cortex_a15_neon_mrrc): Likewise.
+	* config/arm/cortex-a15.md (cortex_a15_alu): Update for attribute
+	change.
+	(cortex_a15_alu_shift): Likewise.
+	(cortex_a15_alu_shift_reg): Likewise.
+	(cortex_a15_mult32): Likewise.
+	(cortex_a15_mult64): Likewise.
+	(cortex_a15_block): Likewise.
+	(cortex_a15_branch): Likewise.
+	(cortex_a15_load1): Likewise.
+	(cortex_a15_load3): Likewise.
+	(cortex_a15_store1): Likewise.
+	(cortex_a15_store3): Likewise.
+	(cortex_a15_call): Likewise.
+	* config/arm/cortex-a5.md (cortex_a5_r2f): Update for attribute
+	change.
+	(cortex_a5_f2r): Likewise.
+	* config/arm/cortex-a53.md (cortex_a53_r2f): Update for attribute
+	change.
+	(cortex_a53_f2r): Likewise.
+	* config/arm/cortex-a7.md
+	(cortex_a7_branch): Update for attribute change.
+	(cortex_a7_call): Likewise.
+	(cortex_a7_alu_imm): Likewise.
+	(cortex_a7_alu_reg): Likewise.
+	(cortex_a7_alu_shift): Likewise.
+	(cortex_a7_mul): Likewise.
+	(cortex_a7_load1): Likewise.
+	(cortex_a7_store1): Likewise.
+	(cortex_a7_load2): Likewise.
+	(cortex_a7_store2): Likewise.
+	(cortex_a7_load3): Likewise.
+	(cortex_a7_store3): Likewise.
+	(cortex_a7_load4): Likewise.
+	(cortex_a7_store4): Likewise.
+	(cortex_a7_fpalu): Likewise.
+	(cortex_a7_fconst): Likewise.
+	(cortex_a7_fpmuls): Likewise.
+	(cortex_a7_neon_mul): Likewise.
+	(cortex_a7_fpmacs): Likewise.
+	(cortex_a7_neon_mla: Likewise.
+	(cortex_a7_fpmuld: Likewise.
+	(cortex_a7_fpmacd: Likewise.
+	(cortex_a7_fpfmad: Likewise.
+	(cortex_a7_fdivs: Likewise.
+	(cortex_a7_fdivd: Likewise.
+	(cortex_a7_r2f: Likewise.
+	(cortex_a7_f2r: Likewise.
+	(cortex_a7_f_flags: Likewise.
+	(cortex_a7_f_loads: Likewise.
+	(cortex_a7_f_loadd: Likewise.
+	(cortex_a7_f_stores: Likewise.
+	(cortex_a7_f_stored: Likewise.
+	(cortex_a7_neon): Likewise.
+	* config/arm/cortex-a8-neon.md
+	(cortex_a8_neon_mrc): Update for attribute change.
+	(cortex_a8_neon_mrrc): Likewise.
+	(cortex_a8_neon_int_1): Likewise.
+	(cortex_a8_neon_int_2): Likewise.
+	(cortex_a8_neon_int_3): Likewise.
+	(cortex_a8_neon_int_4): Likewise.
+	(cortex_a8_neon_int_5): Likewise.
+	(cortex_a8_neon_vqneg_vqabs): Likewise.
+	(cortex_a8_neon_vmov): Likewise.
+	(cortex_a8_neon_vaba): Likewise.
+	(cortex_a8_neon_vaba_qqq): Likewise.
+	(cortex_a8_neon_vsma): Likewise.
+	(cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a8_neon_mul_qqq_8_16_32_ddd_32): Likewise.
+	(cortex_a8_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar):
+	Likewise.
+	(cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a8_neon_mla_qqq_8_16): Likewise.
+	(cortex_a8_neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_\
+	long_scalar_qdd_64_32_long): Likewise.
+	(cortex_a8_neon_mla_qqq_32_qqd_32_scalar): Likewise.
+	(cortex_a8_neon_mul_ddd_16_scalar_32_16_long_scalar): Likewise.
+	(cortex_a8_neon_mul_qqd_32_scalar): Likewise.
+	(cortex_a8_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar): Likewise.
+	(cortex_a8_neon_shift_1): Likewise.
+	(cortex_a8_neon_shift_2): Likewise.
+	(cortex_a8_neon_shift_3): Likewise.
+	(cortex_a8_neon_vshl_ddd): Likewise.
+	(cortex_a8_neon_vqshl_vrshl_vqrshl_qqq): Likewise.
+	(cortex_a8_neon_vsra_vrsra): Likewise.
+	(cortex_a8_neon_fp_vadd_ddd_vabs_dd): Likewise.
+	(cortex_a8_neon_fp_vadd_qqq_vabs_qq): Likewise.
+	(cortex_a8_neon_fp_vsum): Likewise.
+	(cortex_a8_neon_fp_vmul_ddd): Likewise.
+	(cortex_a8_neon_fp_vmul_qqd): Likewise.
+	(cortex_a8_neon_fp_vmla_ddd): Likewise.
+	(cortex_a8_neon_fp_vmla_qqq): Likewise.
+	(cortex_a8_neon_fp_vmla_ddd_scalar): Likewise.
+	(cortex_a8_neon_fp_vmla_qqq_scalar): Likewise.
+	(cortex_a8_neon_fp_vrecps_vrsqrts_ddd): Likewise.
+	(cortex_a8_neon_fp_vrecps_vrsqrts_qqq): Likewise.
+	(cortex_a8_neon_bp_simple): Likewise.
+	(cortex_a8_neon_bp_2cycle): Likewise.
+	(cortex_a8_neon_bp_3cycle): Likewise.
+	(cortex_a8_neon_ldr): Likewise.
+	(cortex_a8_neon_str): Likewise.
+	(cortex_a8_neon_vld1_1_2_regs): Likewise.
+	(cortex_a8_neon_vld1_3_4_regs): Likewise.
+	(cortex_a8_neon_vld2_2_regs_vld1_vld2_all_lanes): Likewise.
+	(cortex_a8_neon_vld2_4_regs): Likewise.
+	(cortex_a8_neon_vld3_vld4): Likewise.
+	(cortex_a8_neon_vst1_1_2_regs_vst2_2_regs): Likewise.
+	(cortex_a8_neon_vst1_3_4_regs): Likewise.
+	(cortex_a8_neon_vst2_4_regs_vst3_vst4): Likewise.
+	(cortex_a8_neon_vst3_vst4): Likewise.
+	(cortex_a8_neon_vld1_vld2_lane): Likewise.
+	(cortex_a8_neon_vld3_vld4_lane): Likewise.
+	(cortex_a8_neon_vst1_vst2_lane): Likewise.
+	(cortex_a8_neon_vst3_vst4_lane): Likewise.
+	(cortex_a8_neon_vld3_vld4_all_lanes): Likewise.
+	(cortex_a8_neon_mcr): Likewise.
+	(cortex_a8_neon_mcr_2_mcrr): Likewise.
+	* config/arm/cortex-a8.md (cortex_a8_alu): Update for attribute
+	change.
+	* config/arm/cortex-a9-neon.md (ca9_neon_mrc): Update for attribute
+	change.
+	(ca9_neon_mrrc): Likewise.
+	(cortex_a9_neon_int_1): Likewise.
+	(cortex_a9_neon_int_2): Likewise.
+	(cortex_a9_neon_int_3): Likewise.
+	(cortex_a9_neon_int_4): Likewise.
+	(cortex_a9_neon_int_5): Likewise.
+	(cortex_a9_neon_vqneg_vqabs): Likewise.
+	(cortex_a9_neon_vmov): Likewise.
+	(cortex_a9_neon_vaba): Likewise.
+	(cortex_a9_neon_vaba_qqq): Likewise.
+	(cortex_a9_neon_vsma): Likewise.
+	(cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a9_neon_mul_qqq_8_16_32_ddd_32): Likewise.
+	(cortex_a9_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar):
+	Likewise.
+	(cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long): Likewise.
+	(cortex_a9_neon_mla_qqq_8_16): Likewise.
+	(cortex_a9_neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_\
+	long_scalar_qdd_64_32_long): Likewise.
+	(cortex_a9_neon_mla_qqq_32_qqd_32_scalar): Likewise.
+	(cortex_a9_neon_mul_ddd_16_scalar_32_16_long_scalar): Likewise.
+	(cortex_a9_neon_mul_qqd_32_scalar): Likewise.
+	(cortex_a9_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar): Likewise.
+	(cortex_a9_neon_shift_1): Likewise.
+	(cortex_a9_neon_shift_2): Likewise.
+	(cortex_a9_neon_shift_3): Likewise.
+	(cortex_a9_neon_vshl_ddd): Likewise.
+	(cortex_a9_neon_vqshl_vrshl_vqrshl_qqq): Likewise.
+	(cortex_a9_neon_vsra_vrsra): Likewise.
+	(cortex_a9_neon_fp_vadd_ddd_vabs_dd): Likewise.
+	(cortex_a9_neon_fp_vadd_qqq_vabs_qq): Likewise.
+	(cortex_a9_neon_fp_vsum): Likewise.
+	(cortex_a9_neon_fp_vmul_ddd): Likewise.
+	(cortex_a9_neon_fp_vmul_qqd): Likewise.
+	(cortex_a9_neon_fp_vmla_ddd): Likewise.
+	(cortex_a9_neon_fp_vmla_qqq): Likewise.
+	(cortex_a9_neon_fp_vmla_ddd_scalar): Likewise.
+	(cortex_a9_neon_fp_vmla_qqq_scalar): Likewise.
+	(cortex_a9_neon_fp_vrecps_vrsqrts_ddd): Likewise.
+	(cortex_a9_neon_fp_vrecps_vrsqrts_qqq): Likewise.
+	(cortex_a9_neon_bp_simple): Likewise.
+	(cortex_a9_neon_bp_2cycle): Likewise.
+	(cortex_a9_neon_bp_3cycle): Likewise.
+	(cortex_a9_neon_ldr): Likewise.
+	(cortex_a9_neon_str): Likewise.
+	(cortex_a9_neon_vld1_1_2_regs): Likewise.
+	(cortex_a9_neon_vld1_3_4_regs): Likewise.
+	(cortex_a9_neon_vld2_2_regs_vld1_vld2_all_lanes): Likewise.
+	(cortex_a9_neon_vld2_4_regs): Likewise.
+	(cortex_a9_neon_vld3_vld4): Likewise.
+	(cortex_a9_neon_vst1_1_2_regs_vst2_2_regs): Likewise.
+	(cortex_a9_neon_vst1_3_4_regs): Likewise.
+	(cortex_a9_neon_vst2_4_regs_vst3_vst4): Likewise.
+	(cortex_a9_neon_vst3_vst4): Likewise.
+	(cortex_a9_neon_vld1_vld2_lane): Likewise.
+	(cortex_a9_neon_vld3_vld4_lane): Likewise.
+	(cortex_a9_neon_vst1_vst2_lane): Likewise.
+	(cortex_a9_neon_vst3_vst4_lane): Likewise.
+	(cortex_a9_neon_vld3_vld4_all_lanes): Likewise.
+	(cortex_a9_neon_mcr): Likewise.
+	(cortex_a9_neon_mcr_2_mcrr): Likewise.
+	* config/arm/cortex-a9.md (cortex_a9_dp): Update for attribute change.
+	(cortex_a9_fps): Likewise.
+	* config/arm/cortex-m4-fpu.md (cortex_m4_vmov_2): Update for attribute
+	change.
+	(cortex_m4_fmuls): Likewise.
+	* config/arm/cortex-r4f.md (cortex_r4_mcr): Update for attribute
+	change.
+	(cortex_r4_mrc): Likewise.
+	* config/arm/iterators.md: Update comment referring to neon_type.
+	* config/arm/iwmmxt.md
+	(iwmmxt_arm_movdi): Update for attribute change.
+	(iwmmxt_movsi_insn): Likewise.
+	* config/arm/marvell-pj4.md
+	(pj4_vfp_to_core): Update for attribute change.
+	(pj4_core_to_vfp): Likewise.
+	* config/arm/neon-schedgen.ml (emit_insn_reservations): Update for
+	attribute change.
+	* config/arm/vfp11.md (vfp_fload): Update for attribute change.
+	(vfp_fstore): Likewise.
+	* doc/md.texi: Change references to neon_type to refer to type.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r201436
+	2013-08-02  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/arm/types.md (define_attr "type"): Add "load_acq" and "store_rel".
+	* config/arm/cortex-a53.md (cortex_a53_load1): Update for attribute
+	changes.
+	(cortex_a53_store1): Likewise.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r201400
+	2013-08-01  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config.gcc (aarch64*-*-*): Add aarch-common.o to extra_objs.  Add
+	aarch-common-protos.h to extra_headers.
+	(aarch64*-*-*): Add arm/aarch-common-protos.h to tm_p_file.
+	* config/aarch64/aarch64.md: Include "../arm/cortex-a53.md".
+	* config/aarch64/t-aarch64 (aarch-common.o): Define.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r201399
+	2013-08-01  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64.md (define_attr "type"): Delete.
+	Include "../arm/types.md".  Define "type" attribute for all patterns.
+	* config/aarch64/aarch64-simd.md (move_lo_quad_<mode>): Update for
+	attribute changes.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r201376
+	2013-07-31  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config.gcc (arm*-*-*): Add aarch-common.o to extra_objs.  Add
+	aarch-common-protos.h to extra_headers.
+	(arm*-*-*): Add arm/aarch-common-protos.h to tm_p_file.
+	* config/arm/arm.c (arm_early_load_addr_dep): Move from here to ...
+	(arm_early_store_addr_dep): Likewise.
+	(arm_no_early_alu_shift_dep: Likewise.
+	(arm_no_early_alu_shift_value_dep: Likewise.
+	(arm_no_early_mul_dep: Likewise.
+	(arm_no_early_store_addr_dep: Likewise.
+	(arm_mac_accumulator_is_mul_result: Likewise.
+	(arm_mac_accumulator_is_result: Likewise.
+	* config/arm/aarch-common.c: ... here.  New file.
+	* config/arm/arm-protos.h (arm_early_load_addr_dep): Move from here to ...
+	(arm_early_store_addr_dep): Likewise.
+	(arm_no_early_alu_shift_dep: Likewise.
+	(arm_no_early_alu_shift_value_dep: Likewise.
+	(arm_no_early_mul_dep: Likewise.
+	(arm_no_early_store_addr_dep: Likewise.
+	(arm_mac_accumulator_is_mul_result: Likewise.
+	(arm_mac_accumulator_is_result: Likewise.
+	* config/arm/aarch-common-protos.h: ... here.  New file.
+	* config/arm/t-arm (aarch-common.o): Define.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r201375
+	2013-07-31  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/arm/arm.md: Include new file "types.md".
+	(define_attr "type"): Move from here to ...
+	(define_attr "mul32"): Likewise.
+	(define_attr "mul64"): Likewise.
+	* config/arm/types.md: ... here.  New file.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202663
+	2013-09-17  Cong Hou  <congh@google.com>
+
+	* tree-vect-patterns.c (vect_recog_dot_prod_pattern): Fix a bug
+	when checking the dot production pattern. The type of rhs operand
+	of multiply is now checked correctly.
+
+2014-04-02  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	Backport from trunk r208511
+	2014-03-12  Christian Bruel  <christian.bruel@st.com>
+
+	PR target/60264
+	* config/arm/arm.c (arm_emit_vfp_multi_reg_pop): Emit a
+	REG_CFA_DEF_CFA note.
+	(arm_expand_epilogue_apcs_frame): call arm_add_cfa_adjust_cfa_note.
+	(arm_unwind_emit): Allow REG_CFA_DEF_CFA.
+
+2014-03-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+	* LINARO-VERSION: Update.
+
+2014-02-13  Yvan Roux  <yvan.roux@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+	* LINARO-VERSION: Update.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206518
+	2014-01-10 Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.c (arm_init_iwmmxt_builtins): Skip
+	non-iwmmxt builtins.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206151
+	2013-12-20  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/neon.ml (crypto_intrinsics): Add vceq_64 and vtst_p64.
+	* config/arm/arm_neon.h: Regenerate.
+	* config/arm/neon-docgen.ml: Add vceq_p64 and vtst_p64.
+	* doc/arm-neon-intrinsics.texi: Regenerate.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206149
+	2013-12-20  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm_acle.h: Add underscores before variables.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206132
+	2013-12-19  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/neon-docgen.ml: Add crypto intrinsics documentation.
+	* doc/arm-neon-intrinsics.texi: Regenerate.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206131
+	2013-12-19  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/neon-testgen.ml (effective_target): Handle "CRYPTO".
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206130
+	2013-12-19  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	 * config/arm/arm.c (enum arm_builtins): Add crypto builtins.
+	 (arm_init_neon_builtins): Handle crypto builtins.
+	 (bdesc_2arg): Likewise.
+	 (bdesc_1arg): Likewise.
+	 (bdesc_3arg): New table.
+	 (arm_expand_ternop_builtin): New function.
+	 (arm_expand_unop_builtin): Handle sha1h explicitly.
+	 (arm_expand_builtin): Handle ternary builtins.
+	 * config/arm/arm.h (TARGET_CPU_CPP_BUILTINS):
+	 Define __ARM_FEATURE_CRYPTO.
+	 * config/arm/arm.md: Include crypto.md.
+	 (is_neon_type): Add crypto types.
+	 * config/arm/arm_neon_builtins.def: Add TImode reinterprets.
+	 * config/arm/crypto.def: New.
+	 * config/arm/crypto.md: Likewise.
+	 * config/arm/iterators.md (CRYPTO_UNARY): New int iterator.
+	 (CRYPTO_BINARY): Likewise.
+	 (CRYPTO_TERNARY): Likewise.
+	 (CRYPTO_SELECTING): Likewise.
+	 (crypto_pattern): New int attribute.
+	 (crypto_size_sfx): Likewise.
+	 (crypto_mode): Likewise.
+	 (crypto_type): Likewise.
+	 * config/arm/neon-gen.ml: Handle poly64_t and poly128_t types.
+	 Handle crypto intrinsics.
+	 * config/arm/neon.ml: Add support for poly64 and polt128 types
+	 and intrinsics. Define crypto intrinsics.
+	 * config/arm/neon.md (neon_vreinterpretti<mode>): New pattern.
+	 (neon_vreinterpretv16qi<mode>): Use VQXMOV mode iterator.
+	 (neon_vreinterpretv8hi<mode>): Likewise.
+	 (neon_vreinterpretv4si<mode>): Likewise.
+	 (neon_vreinterpretv4sf<mode>): Likewise.
+	 (neon_vreinterpretv2di<mode>): Likewise.
+	 * config/arm/unspecs.md (UNSPEC_AESD, UNSPEC_AESE, UNSPEC_AESIMC,
+	 UNSPEC_AESMC, UNSPEC_SHA1C, UNSPEC_SHA1M, UNSPEC_SHA1P, UNSPEC_SHA1H,
+	 UNSPEC_SHA1SU0, UNSPEC_SHA1SU1, UNSPEC_SHA256H, UNSPEC_SHA256H2,
+	 UNSPEC_SHA256SU0, UNSPEC_SHA256SU1, VMULLP64): Define.
+	 * config/arm/arm_neon.h: Regenerate.
+
+	Modifications needed to backport into linaro-4_8-branch:
+	* config/arm/arm.md (attribute neon_type): neon_crypto_aes,
+	neon_crypto_sha1_xor, neon_crypto_sha1_fast,
+	neon_crypto_sha1_slow, neon_crypto_sha256_fast,
+	neon_crypto_sha256_slow, neon_mul_d_long: New.
+	instead of:
+	* config/arm/arm.md: Include crypto.md.
+	(is_neon_type): Add crypto types.
+
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206128
+	2013-12-19  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* Makefile.in (TEXI_GCC_FILES): Add arm-acle-intrinsics.texi.
+	* config.gcc (extra_headers): Add arm_acle.h.
+	* config/arm/arm.c (FL_CRC32): Define.
+	(arm_have_crc): Likewise.
+	(arm_option_override): Set arm_have_crc.
+	(arm_builtins): Add CRC32 builtins.
+	(bdesc_2arg): Likewise.
+	(arm_init_crc32_builtins): New function.
+	(arm_init_builtins): Initialise CRC32 builtins.
+	(arm_file_start): Handle architecture extensions.
+	* config/arm/arm.h (TARGET_CPU_CPP_BUILTINS): Define __ARM_FEATURE_CRC32.
+	Define __ARM_32BIT_STATE.
+	(TARGET_CRC32): Define.
+	* config/arm/arm-arches.def: Add armv8-a+crc.
+	* config/arm/arm-tables.opt: Regenerate.
+	* config/arm/arm.md (type): Add crc.
+	(<crc_variant>): New insn.
+	* config/arm/arm_acle.h: New file.
+	* config/arm/iterators.md (CRC): New int iterator.
+	(crc_variant, crc_mode): New int attributes.
+	* confg/arm/unspecs.md (UNSPEC_CRC32B, UNSPEC_CRC32H, UNSPEC_CRC32W,
+	UNSPEC_CRC32CB, UNSPEC_CRC32CH, UNSPEC_CRC32CW): New unspecs.
+	* doc/invoke.texi: Document -march=armv8-a+crc option.
+	* doc/extend.texi: Document ACLE intrinsics.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206120
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/aarch64-builtins.c (aarch64_init_simd_builtins):
+	Define builtin types for poly64_t poly128_t.
+	(TYPES_BINOPP, aarch64_types_binopp_qualifiers): New.
+	* aarch64/aarch64-simd-builtins.def: Update builtins table.
+	* config/aarch64/aarch64-simd.md (aarch64_crypto_pmulldi,
+	aarch64_crypto_pmullv2di): New.
+	* config/aarch64/aarch64.c (aarch64_simd_mangle_map): Update table for
+	poly64x2_t mangler.
+	* config/aarch64/arm_neon.h (poly64x2_t, poly64_t, poly128_t): Define.
+	(vmull_p64, vmull_high_p64): New.
+	* config/aarch64/iterators.md (UNSPEC_PMULL<2>): New.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206119
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/aarch64-simd-builtins.def: Update builtins table.
+	* config/aarch64/aarch64-simd.md (aarch64_crypto_sha256h<sha256_op>v4si,
+	aarch64_crypto_sha256su0v4si, aarch64_crypto_sha256su1v4si): New.
+	* config/aarch64/arm_neon.h (vsha256hq_u32, vsha256h2q_u32,
+	vsha256su0q_u32, vsha256su1q_u32): New.
+	* config/aarch64/iterators.md (UNSPEC_SHA256H<2>, UNSPEC_SHA256SU<01>):
+	New.
+	(CRYPTO_SHA256): New int iterator.
+	(sha256_op): New int attribute.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206118
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/aarch64-simd-builtins.def: Update builtins table.
+	* config/aarch64/aarch64-builtins.c (aarch64_types_ternopu_qualifiers,
+	TYPES_TERNOPU): New.
+	* config/aarch64/aarch64-simd.md (aarch64_crypto_sha1hsi,
+	aarch64_crypto_sha1su1v4si, aarch64_crypto_sha1<sha1_op>v4si,
+	aarch64_crypto_sha1su0v4si): New.
+	* config/aarch64/arm_neon.h (vsha1cq_u32, sha1mq_u32, vsha1pq_u32,
+	vsha1h_u32, vsha1su0q_u32, vsha1su1q_u32): New.
+	* config/aarch64/iterators.md (UNSPEC_SHA1<CPMH>, UNSPEC_SHA1SU<01>):
+	New.
+	(CRYPTO_SHA1): New int iterator.
+	(sha1_op): New int attribute.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206117
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/aarch64-simd-builtins.def: Update builtins table.
+	* config/aarch64/aarch64-builtins.c (aarch64_types_binopu_qualifiers,
+	TYPES_BINOPU): New.
+	* config/aarch64/aarch64-simd.md (aarch64_crypto_aes<aes_op>v16qi,
+	aarch64_crypto_aes<aesmc_op>v16qi): New.
+	* config/aarch64/arm_neon.h (vaeseq_u8, vaesdq_u8, vaesmcq_u8,
+	vaesimcq_u8): New.
+	* config/aarch64/iterators.md (UNSPEC_AESE, UNSPEC_AESD, UNSPEC_AESMC,
+	UNSPEC_AESIMC): New.
+	(CRYPTO_AES, CRYPTO_AESMC): New int iterators.
+	(aes_op, aesmc_op): New int attributes.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206115
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/arm/types.md (neon_mul_d_long, crypto_aes, crypto_sha1_xor,
+	crypto_sha1_fast, crypto_sha1_slow, crypto_sha256_fast,
+	crypto_sha256_slow): New.
+
+	Modifications needed to backport into linaro-4_8-branch:
+	* config/aarch64/aarch64-simd.md (attribute simd_type):
+	(simd_mul_d_long, simd_crypto_aes, simd_crypto_sha1_xor,
+	simd_crypto_sha1_fast, simd_crypto_sha1_slow, simd_crypto_sha256_fast,
+	simd_crypto_sha256_slow) : New.
+	instead of the above change.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206114
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/aarch64.h (TARGET_CRYPTO): New.
+	(__ARM_FEATURE_CRYPTO): Define if TARGET_CRYPTO is true.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r205384.
+	2013-11-26  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_type_qualifiers): Add qualifier_poly.
+	(aarch64_build_scalar_type): Also build Poly types.
+	(aarch64_build_vector_type): Likewise.
+	(aarch64_build_type): Likewise.
+	(aarch64_build_signed_type): New.
+	(aarch64_build_unsigned_type): Likewise.
+	(aarch64_build_poly_type): Likewise.
+	(aarch64_init_simd_builtins): Also handle Poly types.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r205383.
+	2013-11-26  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(VAR1): Use new naming scheme for aarch64_builtins.
+	(aarch64_builtin_vectorized_function): Use new
+	aarch64_builtins names.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r205092.
+	2013-11-20  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc/config/aarch64/aarch64-builtins.c
+	(aarch64_simd_itype): Remove.
+	(aarch64_simd_builtin_datum): Remove itype, add
+	qualifiers pointer.
+	(VAR1): Use qualifiers.
+	(aarch64_build_scalar_type): New.
+	(aarch64_build_vector_type): Likewise.
+	(aarch64_build_type): Likewise.
+	(aarch64_init_simd_builtins): Refactor, remove special cases,
+	consolidate main loop.
+	(aarch64_simd_expand_args): Likewise.
+
+2014-02-01  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r202875,202980.
+	2013-09-24  Xinliang David Li <davidxl@google.com>
+
+	* tree-vect-data-refs.c (vect_enhance_data_refs_alignment):
+	Check max peel iterations parameter.
+	* param.def: New parameter.
+	* doc/invoke.texi: Document New parameter.
+
+	2013-09-27  Xinliang David Li  <davidxl@google.com>
+
+	* opts.c (finish_options): Adjust parameters
+	according to vect cost model.
+	(common_handle_option): Set dynamic vect cost
+	model for FDO.
+	targhooks.c (default_add_stmt_cost): Compute stmt cost
+	unconditionally.
+	* tree-vect-loop.c (vect_estimate_min_profitable_iters):
+	Use helper function.
+	* tree-vectorizer.h (unlimited_cost_model): New function.
+	* tree-vect-slp.c (vect_slp_analyze_bb_1): Use helper function.
+	* tree-vect-data-refs.c (vect_peeling_hash_insert): Use helper
+	function.
+	(vect_enhance_data_refs_alignment): Ditto.
+	* flag-types.h: New enum.
+	* common/config/i386/i386-common.c (ix86_option_init_struct):
+	No need to initialize vect_cost_model flag.
+	* config/i386/i386.c (ix86_add_stmt_cost): Compute stmt cost
+	unconditionally.
+
+2014-01-21  Zhenqiang Chen <zhenqiang.chen@linaro.org>
+
+	Backport from trunk r200103
+	2013-06-15  Jeff Law  <law@redhat.com>
+
+	* gimple.h (gimple_can_coalesce_p): Prototype.
+	* tree-ssa-coalesce.c (gimple_can_coalesce_p): New function.
+	(create_outofssa_var_map, coalesce_partitions): Use it.
+	* tree-ssa-uncprop.c (uncprop_into_successor_phis): Similarly.
+	* tree-ssa-live.c (var_map_base_init): Use TYPE_CANONICAL
+	if it's available.
+
+2014-01-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+	* LINARO-VERSION: Update.
+
+2014-01-16  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	Linaro local patch for armv4t multilib support.
+	* gcc/config/arm/t-mlibs: New file.
+	* config.gcc: Add t-mlibs.
+	* incpath.c (add_standard_paths): Try multilib path first.
+	* gcc.c (for_each_path): Likewise.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+	* LINARO-VERSION: Update.
+
+2013-12-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r204737.
+	2013-11-13  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* config/aarch64/aarch64.h (FRAME_GROWS_DOWNWARD): Define to 1.
+	* config/aarch64/aarch64.c (aarch64_initial_elimination_offset):
+	Update offset calculations.
+
+2013-12-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r203327.
+	2013-10-09  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	* tree-ssa-phiopts.c (rhs_is_fed_for_value_replacement): New function.
+	(operand_equal_for_value_replacement): New function, extracted from
+	value_replacement and enhanced to catch more cases.
+	(value_replacement): Use operand_equal_for_value_replacement.
+
+2013-11-18  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+	* LINARO-VERSION: Update.
+
+2013-11-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Revert backport from trunk r197526.
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (negdi_extendsidi): New pattern.
+	(negdi_zero_extendsidi): Likewise.
+
+2013-11-05  Zhenqiang Chen <zhenqiang.chen@linaro.org>
+
+	Backport from trunk r203267, r203603 and r204247.
+	2013-10-08  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	PR target/58423
+	* config/arm/arm.c (arm_emit_ldrd_pop): Attach
+	RTX_FRAME_RELATED_P on INSN.
+
+	2013-10-15  Matthew Gretton-Dann  <matthew.gretton-dann@arm.com>
+	    Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	* config/arm/t-aprofile: New file.
+	* config.gcc: Handle --with-multilib-list option.
+
+	2013-10-31  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* lower-subreg.c (resolve_simple_move): Copy REG_INC note.
+
+2013-10-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r200956
+	2013-07-15  Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64-protos.h (aarch64_symbol_type):
+	Define SYMBOL_TINY_GOT, update comment.
+	* config/aarch64/aarch64.c
+	(aarch64_load_symref_appropriately): Handle SYMBOL_TINY_GOT.
+	(aarch64_expand_mov_immediate): Likewise.
+	(aarch64_print_operand): Likewise.
+	(aarch64_classify_symbol): Likewise.
+	* config/aarch64/aarch64.md (UNSPEC_GOTTINYPIC): Define.
+	(ldr_got_tiny): Define.
+
+2013-10-16  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+	* LINARO-VERSION: Update.
+
+2013-10-09  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198526,198527,200020,200595.
+	2013-05-02  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.md (*and_one_cmpl<mode>3_compare0):
+	New pattern.
+	(*and_one_cmplsi3_compare0_uxtw): Likewise.
+	(*and_one_cmpl_<SHIFT:optab><mode>3_compare0): Likewise.
+	(*and_one_cmpl_<SHIFT:optab>si3_compare0_uxtw): Likewise.
+
+	2013-05-02  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.md (movsi_aarch64): Only allow to/from
+	S reg when fp attribute set.
+	(movdi_aarch64): Only allow to/from D reg when fp attribute set.
+
+	2013-06-12  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64-simd.md (aarch64_combine<mode>): convert to split.
+	(aarch64_simd_combine<mode>): New instruction expansion.
+	* config/aarch64/aarch64-protos.h (aarch64_split_simd_combine): New
+	function prototype.
+	* config/aarch64/aarch64.c (aarch64_split_combine): New function.
+	* config/aarch64/iterators.md (Vdbl): Add entry for DF.
+
+	2013-07-02  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.md (*extr_insv_reg<mode>): New pattern.
+
+2013-10-09  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201879.
+	2013-08-20  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	* config/arm/linux-elf.h (MULTILIB_DEFAULTS): Remove definition.
+	* config/arm/t-linux-eabi (MULTILIB_OPTIONS): Document association
+	with MULTLIB_DEFAULTS.
+
+2013-10-09  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201871.
+	2013-08-20  Pavel Chupin  <pavel.v.chupin@intel.com>
+
+	Fix LIB_SPEC for systems without libpthread.
+
+	* config/gnu-user.h: Introduce GNU_USER_TARGET_NO_PTHREADS_LIB_SPEC.
+	* config/arm/linux-eabi.h: Use GNU_USER_TARGET_NO_PTHREADS_LIB_SPEC
+	for Android.
+	* config/i386/linux-common.h: Likewise.
+	* config/mips/linux-common.h: Likewise.
+
+2013-10-08  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r202702.
+	2013-09-18  Richard Earnshaw  <rearnsha@arm.com>
+
+	* arm.c (arm_get_frame_offsets): Validate architecture supports
+	LDRD/STRD before accepting the tuning preference.
+	(arm_expand_prologue): Likewise.
+	(arm_expand_epilogue): Likewise.
+
+2013-10-04  Venkataramanan.Kumar  <venkataramanan.kumar@linaro.org>
+
+	Backport from trunk r203028.
+	2013-09-30  Venkataramanan Kumar  <venkataramanan.kumar@linaro.org>
+
+	* config/aarch64/aarch64.h (MCOUNT_NAME): Define.
+	(NO_PROFILE_COUNTERS): Likewise.
+	(PROFILE_HOOK): Likewise.
+	(FUNCTION_PROFILER): Likewise.
+	* config/aarch64/aarch64.c (aarch64_function_profiler): Remove.
+
+2013-10-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201923,201927.
+	2013-08-22  Julian Brown  <julian@codesourcery.com>
+
+	* configure.ac: Add aarch64 to list of arches which use "nop" in
+	debug_line test.
+	* configure: Regenerate.
+
+	2013-08-22  Paolo Carlini  <paolo.carlini@oracle.com>
+
+	* configure.ac: Add backslashes missing from the last change.
+	* configure: Regenerate.
+
+2013-10-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r202023,202108.
+	2013-08-27  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/arm_neon.h: Replace all inline asm implementations
+	of vget_low_* with implementations in terms of other intrinsics.
+
+	2013-08-30  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/arm_neon.h (__AARCH64_UINT64_C, __AARCH64_INT64_C): New
+	arm_neon.h's internal macros to specify 64-bit constants. Avoid using
+	stdint.h's macros.
+
+2013-10-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201260,202400.
+	2013-07-26  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+	    Richard Earnshaw  <richard.earnshaw@arm.com>
+
+	* combine.c (simplify_comparison): Re-canonicalize operands
+	where appropriate.
+	* config/arm/arm.md (movcond_addsi): New splitter.
+
+	2013-09-09  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_select_cc_mode): Return CC_SWP for
+	comparison with negated operand.
+	* config/aarch64/aarch64.md (compare_neg<mode>): Match canonical
+	RTL form.
+
+2013-10-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r202164.
+	2013-09-02  Bin Cheng  <bin.cheng@arm.com>
+
+	* tree-ssa-loop-ivopts.c (set_autoinc_for_original_candidates):
+	Find auto-increment use both before and after candidate.
+
+2013-10-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r202279.
+	2013-09-05  Richard Earnshaw  <rearnsha@arm.com>
+
+	* arm.c (thumb2_emit_strd_push): Rewrite to use pre-decrement on
+	initial store.
+	* thumb2.md (thumb2_storewb_parisi): New pattern.
+
+2013-10-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r202275.
+	2013-09-05  Yufeng Zhang  <yufeng.zhang@arm.com>
+
+	* config/aarch64/aarch64-option-extensions.def: Add
+	AARCH64_OPT_EXTENSION of 'crc'.
+	* config/aarch64/aarch64.h (AARCH64_FL_CRC): New define.
+	(AARCH64_ISA_CRC): Ditto.
+	* doc/invoke.texi (-march and -mcpu feature modifiers): Add
+	description of the CRC extension.
+
+2013-10-01  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201250.
+	2013-07-25  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (arm_addsi3, addsi3_carryin_<optab>,
+	addsi3_carryin_alt2_<optab>): Correct output template.
+
+2013-10-01  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	Backport from trunk r203059,203116.
+	2013-10-01  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	PR target/58578
+	Revert
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+	* config/arm/arm.md (arm_ashldi3_1bit):  define_insn into
+	define_insn_and_split.
+	(arm_ashrdi3_1bit,arm_lshrdi3_1bit): Likewise.
+	(shiftsi3_compare): New pattern.
+	(rrx): New pattern.
+	* config/arm/unspecs.md (UNSPEC_RRX): New.
+
+2013-09-11  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+	* LINARO-VERSION: Update.
+
+2013-09-10  Venkataramanan Kumar  <venkataramanan.kumar@linaro.org>
+
+	Backport from trunk r200197, 201411.
+	2013-06-19  Richard Earnshaw  <rearnsha@arm.com>
+
+	arm.md (split for eq(reg, 0)): Add variants for ARMv5 and Thumb2.
+	(peepholes for eq(reg, not-0)): Ensure condition register is dead after
+	pattern.  Use more efficient sequences on ARMv5 and Thumb2.
+
+	2013-08-01  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (peepholes for eq (reg1) (reg2/imm)):
+	Generate canonical plus rtx with negated immediate instead of minus
+	where appropriate.
+	* config/arm/arm.c (thumb2_reorg): Handle ADCS <Rd>, <Rn> case.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r200593,201024,201025,201122,201124,201126.
+	2013-07-02  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (arm_andsi3_insn): Add alternatives for 16-bit
+	encoding.
+	(iorsi3_insn): Likewise.
+	(arm_xorsi3): Likewise.
+
+	2013-07-18  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/arm/arm.md (attribute "type"): Rename "simple_alu_imm" to
+	"arlo_imm".  Rename "alu_reg" to "arlo_reg".  Rename "simple_alu_shift" to
+	"extend".  Split "alu_shift" into "shift" and "arlo_shift".  Split
+	"alu_shift_reg" into "shift_reg" and "arlo_shift_reg".  List types
+	in alphabetical order.
+	(attribute "core_cycles"): Update for attribute changes.
+	(arm_addsi3): Likewise.
+	(addsi3_compare0): Likewise.
+	(addsi3_compare0_scratch): Likewise.
+	(addsi3_compare_op1): Likewise.
+	(addsi3_compare_op2): Likewise.
+	(compare_addsi2_op0): Likewise.
+	(compare_addsi2_op1): Likewise.
+	(addsi3_carryin_shift_<optab>): Likewise.
+	(subsi3_carryin_shift): Likewise.
+	(rsbsi3_carryin_shift): Likewise.
+	(arm_subsi3_insn): Likewise.
+	(subsi3_compare0): Likewise.
+	(subsi3_compare): Likewise.
+	(arm_andsi3_insn): Likewise.
+	(thumb1_andsi3_insn): Likewise.
+	(andsi3_compare0): Likewise.
+	(andsi3_compare0_scratch): Likewise.
+	(zeroextractsi_compare0_scratch
+	(andsi_not_shiftsi_si): Likewise.
+	(iorsi3_insn): Likewise.
+	(iorsi3_compare0): Likewise.
+	(iorsi3_compare0_scratch): Likewise.
+	(arm_xorsi3): Likewise.
+	(thumb1_xorsi3_insn): Likewise.
+	(xorsi3_compare0): Likewise.
+	(xorsi3_compare0_scratch): Likewise.
+	(satsi_<SAT:code>_shift): Likewise.
+	(rrx): Likewise.
+	(arm_shiftsi3): Likewise.
+	(shiftsi3_compare0): Likewise.
+	(not_shiftsi): Likewise.
+	(not_shiftsi_compare0): Likewise.
+	(not_shiftsi_compare0_scratch): Likewise.
+	(arm_one_cmplsi2): Likewise.
+	(thumb_one_complsi2): Likewise.
+	(notsi_compare0): Likewise.
+	(notsi_compare0_scratch): Likewise.
+	(thumb1_zero_extendhisi2): Likewise.
+	(arm_zero_extendhisi2): Likewise.
+	(arm_zero_extendhisi2_v6): Likewise.
+	(arm_zero_extendhisi2addsi): Likewise.
+	(thumb1_zero_extendqisi2): Likewise.
+	(thumb1_zero_extendqisi2_v6): Likewise.
+	(arm_zero_extendqisi2): Likewise.
+	(arm_zero_extendqisi2_v6): Likewise.
+	(arm_zero_extendqisi2addsi): Likewise.
+	(thumb1_extendhisi2): Likewise.
+	(arm_extendhisi2): Likewise.
+	(arm_extendhisi2_v6): Likewise.
+	(arm_extendqisi): Likewise.
+	(arm_extendqisi_v6): Likewise.
+	(arm_extendqisi2addsi): Likewise.
+	(thumb1_extendqisi2): Likewise.
+	(thumb1_movdi_insn): Likewise.
+	(arm_movsi_insn): Likewise.
+	(movsi_compare0): Likewise.
+	(movhi_insn_arch4): Likewise.
+	(movhi_bytes): Likewise.
+	(arm_movqi_insn): Likewise.
+	(thumb1_movqi_insn): Likewise.
+	(arm32_movhf): Likewise.
+	(thumb1_movhf): Likewise.
+	(arm_movsf_soft_insn): Likewise.
+	(thumb1_movsf_insn): Likewise.
+	(movdf_soft_insn): Likewise.
+	(thumb_movdf_insn): Likewise.
+	(arm_cmpsi_insn): Likewise.
+	(cmpsi_shiftsi): Likewise.
+	(cmpsi_shiftsi_swp): Likewise.
+	(arm_cmpsi_negshiftsi_si): Likewise.
+	(movsicc_insn): Likewise.
+	(movsfcc_soft_insn): Likewise.
+	(arith_shiftsi): Likewise.
+	(arith_shiftsi_compare0
+	(arith_shiftsi_compare0_scratch
+	(sub_shiftsi): Likewise.
+	(sub_shiftsi_compare0
+	(sub_shiftsi_compare0_scratch
+	(and_scc): Likewise.
+	(cond_move): Likewise.
+	(if_plus_move): Likewise.
+	(if_move_plus): Likewise.
+	(if_move_not): Likewise.
+	(if_not_move): Likewise.
+	(if_shift_move): Likewise.
+	(if_move_shift): Likewise.
+	(if_shift_shift): Likewise.
+	(if_not_arith): Likewise.
+	(if_arith_not): Likewise.
+	(cond_move_not): Likewise.
+	(thumb1_ashlsi3): Set type attribute.
+	(thumb1_ashrsi3): Likewise.
+	(thumb1_lshrsi3): Likewise.
+	(thumb1_rotrsi3): Likewise.
+	(shiftsi3_compare0_scratch): Likewise.
+	* config/arm/neon.md (neon_mov<mode>): Update for attribute changes.
+	(neon_mov<mode>): Likewise.
+	* config/arm/thumb2.md (thumb_andsi_not_shiftsi_si): Update for attribute
+	changes.
+	(thumb2_movsi_insn): Likewise.
+	(thumb2_cmpsi_neg_shiftsi): Likewise.
+	(thumb2_extendqisi_v6): Likewise.
+	(thumb2_zero_extendhisi2_v6): Likewise.
+	(thumb2_zero_extendqisi2_v6): Likewise.
+	(thumb2_shiftsi3_short): Likewise.
+	(thumb2_addsi3_compare0_scratch): Likewise.
+	(orsi_not_shiftsi_si): Likewise.
+	* config/arm/vfp.md (arm_movsi_vfp): Update for attribute changes.
+	* config/arm/arm-fixed.md (arm_ssatsihi_shift): Update for attribute
+	changes.
+	* config/arm/arm1020e.md (1020alu_op): Update for attribute changes.
+	(1020alu_shift_op): Likewise.
+	(1020alu_shift_reg_op): Likewise.
+	* config/arm/arm1026ejs.md (alu_op): Update for attribute changes.
+	(alu_shift_op): Likewise.
+	(alu_shift_reg_op): Likewise.
+	* config/arm/arm1136jfs.md (11_alu_op): Update for attribute changes.
+	(11_alu_shift_op): Likewise.
+	(11_alu_shift_reg_op): Likewise.
+	* config/arm/arm926ejs.md (9_alu_op): Update for attribute changes.
+	(9_alu_shift_reg_op): Likewise.
+	* config/arm/cortex-a15.md (cortex_a15_alu): Update for attribute changes.
+	(cortex_a15_alu_shift): Likewise.
+	(cortex_a15_alu_shift_reg): Likewise.
+	* config/arm/cortex-a5.md (cortex_a5_alu): Update for attribute changes.
+	(cortex_a5_alu_shift): Likewise.
+	* config/arm/cortex-a53.md (cortex_a53_alu) : Update for attribute
+	changes.
+	(cortex_a53_alu_shift): Likewise.
+	* config/arm/cortex-a7.md (cortex_a7_alu_imm): Update for attribute
+	changes.
+	(cortex_a7_alu_reg): Likewise.
+	(cortex_a7_alu_shift): Likewise.
+	* config/arm/cortex-a8.md (cortex_a8_alu): Update for attribute changes.
+	(cortex_a8_alu_shift): Likewise.
+	(cortex_a8_alu_shift_reg): Likewise.
+	(cortex_a8_mov): Likewise.
+	* config/arm/cortex-a9.md (cortex_a9_dp): Update for attribute changes.
+	(cortex_a9_dp_shift): Likewise.
+	* config/arm/cortex-m4.md (cortex_m4_alu): Update for attribute changes.
+	* config/arm/cortex-r4.md (cortex_r4_alu): Update for attribute changes.
+	(cortex_r4_mov): Likewise.
+	(cortex_r4_alu_shift): Likewise.
+	(cortex_r4_alu_shift_reg): Likewise.
+	* config/arm/fa526.md (526_alu_op): Update for attribute changes.
+	(526_alu_shift_op): Likewise.
+	* config/arm/fa606te.md (606te_alu_op): Update for attribute changes.
+	* config/arm/fa626te.md (626te_alu_op): Update for attribute changes.
+	(626te_alu_shift_op): Likewise.
+	* config/arm/fa726te.md (726te_shift_op): Update for attribute changes.
+	(726te_alu_op): Likewise.
+	(726te_alu_shift_op): Likewise.
+	(726te_alu_shift_reg_op): Likewise.
+	* config/arm/fmp626.md (mp626_alu_op): Update for attribute changes.
+	(mp626_alu_shift_op): Likewise.
+	* config/arm/marvell-pj4.md (pj4_alu_e1): Update for attribute changes.
+	(pj4_alu_e1_conds): Likewise.
+	(pj4_alu): Likewise.
+	(pj4_alu_conds): Likewise.
+	(pj4_shift): Likewise.
+	(pj4_shift_conds): Likewise.
+	(pj4_alu_shift): Likewise.
+	(pj4_alu_shift_conds): Likewise.
+	* config/arm/arm.c (xscale_sched_adjust_cost): Update for attribute changes.
+	(cortexa7_older_only): Likewise.
+	(cortexa7_younger): Likewise.
+
+	2013-07-18  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/arm/arm.md (attribute "insn"): Delete values "mrs", "msr",
+	"xtab" and "sat".  Move value "clz" from here to ...
+	(attriubte "type"): ... here.
+	(satsi_<SAT:code>): Delete "insn" attribute.
+	(satsi_<SAT:code>_shift): Likewise.
+	(arm_zero_extendqisi2addsi): Likewise.
+	(arm_extendqisi2addsi): Likewise.
+	(clzsi2): Update for attribute changes.
+	(rbitsi2): Likewise.
+	* config/arm/arm-fixed.md (arm_ssatsihi_shift): Delete "insn" attribute.
+	(arm_usatsihi): Likewise.
+	* config/arm/cortex-a8.md (cortex_a8_alu): Update for attribute change.
+
+	2013-07-22  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/predicates.md (shiftable_operator_strict_it):
+	New predicate.
+	* config/arm/thumb2.md (thumb_andsi_not_shiftsi_si):
+	Disable cond_exec version for arm_restrict_it.
+	(thumb2_smaxsi3): Convert to generate cond_exec.
+	(thumb2_sminsi3): Likewise.
+	(thumb32_umaxsi3): Likewise.
+	(thumb2_uminsi3): Likewise.
+	(thumb2_abssi2): Adjust constraints for arm_restrict_it.
+	(thumb2_neg_abssi2): Likewise.
+	(thumb2_mov_scc): Add alternative for 16-bit encoding.
+	(thumb2_movsicc_insn): Adjust alternatives.
+	(thumb2_mov_negscc): Disable for arm_restrict_it.
+	(thumb2_mov_negscc_strict_it): New pattern.
+	(thumb2_mov_notscc_strict_it): New pattern.
+	(thumb2_mov_notscc): Disable for arm_restrict_it.
+	(thumb2_ior_scc): Likewise.
+	(thumb2_ior_scc_strict_it): New pattern.
+	(thumb2_cond_move): Adjust for arm_restrict_it.
+	(thumb2_cond_arith): Disable for arm_restrict_it.
+	(thumb2_cond_arith_strict_it): New pattern.
+	(thumb2_cond_sub): Adjust for arm_restrict_it.
+	(thumb2_movcond): Likewise.
+	(thumb2_extendqisi_v6): Disable cond_exec variant for arm_restrict_it.
+	(thumb2_zero_extendhisi2_v6): Likewise.
+	(thumb2_zero_extendqisi2_v6): Likewise.
+	(orsi_notsi_si): Likewise.
+	(orsi_not_shiftsi_si): Likewise.
+
+	2013-07-22  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/arm/arm.md (attribute "insn"): Delete.
+	(attribute "type"): Add "mov_imm", "mov_reg", "mov_shift",
+	"mov_shift_reg", "mvn_imm", "mvn_reg", "mvn_shift" and "mvn_shift_reg".
+	(not_shiftsi): Update for attribute change.
+	(not_shiftsi_compare0): Likewise.
+	(not_shiftsi_compare0_scratch): Likewise.
+	(arm_one_cmplsi2): Likewise.
+	(thumb1_one_cmplsi2): Likewise.
+	(notsi_compare0): Likewise.
+	(notsi_compare0_scratch): Likewise.
+	(thumb1_movdi_insn): Likewise.
+	(arm_movsi_insn): Likewise.
+	(movhi_insn_arch4): Likewise.
+	(movhi_bytes): Likewise.
+	(arm_movqi_insn): Likewise.
+	(thumb1_movqi_insn): Likewise.
+	(arm32_movhf): Likewise.
+	(thumb1_movhf): Likewise.
+	(arm_movsf_soft_insn): Likewise.
+	(thumb1_movsf_insn): Likewise.
+	(thumb_movdf_insn): Likewise.
+	(movsicc_insn): Likewise.
+	(movsfcc_soft_insn): Likewise.
+	(and_scc): Likewise.
+	(cond_move): Likewise.
+	(if_move_not): Likewise.
+	(if_not_move): Likewise.
+	(if_shift_move): Likewise.
+	(if_move_shift): Likewise.
+	(if_shift_shift): Likewise.
+	(if_not_arith): Likewise.
+	(if_arith_not): Likewise.
+	(cond_move_not): Likewise.
+	* config/arm/neon.md (neon_mov<mode>): Update for attribute change.
+	(neon_mov<mode>): Likewise.
+	* config/arm/vfp.md (arm_movsi_vfp): Update for attribute change.
+	(thumb2_movsi_vfp): Likewise.
+	(movsf_vfp): Likewise.
+	(thumb2_movsf_vfp): Likewise.
+	* config/arm/arm.c (xscale_sched_adjust_cost): Update for attribute change.
+	(cortexa7_older_only): Likewise.
+	(cortexa7_younger): Likewise.
+	* config/arm/arm1020e.md (1020alu_op): Update for attribute change.
+	(1020alu_shift_op): Likewise.
+	(1020alu_shift_reg_op): Likewise.
+	* config/arm/arm1026ejs.md (alu_op): Update for attribute change.
+	(alu_shift_op): Likewise.
+	(alu_shift_reg_op): Likewise.
+	* config/arm/arm1136jfs.md (11_alu_op): Update for attribute change.
+	(11_alu_shift_op): Likewise.
+	(11_alu_shift_reg_op): Likewise.
+	* config/arm/arm926ejs.md (9_alu_op): Update for attribute change.
+	(9_alu_shift_reg_op): Likewise.
+	* config/arm/cortex-a15.md (cortex_a15_alu): Update for attribute change.
+	(cortex_a15_alu_shift): Likewise.
+	(cortex_a15_alu_shift_reg): Likewise.
+	* config/arm/cortex-a5.md (cortex_a5_alu): Update for attribute change.
+	(cortex_a5_alu_shift): Likewise.
+	* config/arm/cortex-a53.md (cortex_a53_alu): Update for attribute change.
+	(cortex_a53_alu_shift): Likewise.
+	* config/arm/cortex-a7.md (cortex_a7_alu_imm): Update for attribute change.
+	(cortex_a7_alu_reg): Likewise.
+	(cortex_a7_alu_shift): Likewise.
+	* config/arm/cortex-a8.md (cortex_a8_alu): Update for attribute change.
+	(cortex_a8_alu_shift): Likewise.
+	(cortex_a8_alu_shift_reg): Likewise.
+	(cortex_a8_mov): Likewise.
+	* config/arm/cortex-a9.md (cortex_a9_dp): Update for attribute change.
+	(cortex_a9_dp_shift): Likewise.
+	* config/arm/cortex-m4.md (cortex_m4_alu): Update for attribute change.
+	* config/arm/cortex-r4.md (cortex_r4_alu): Update for attribute change.
+	(cortex_r4_mov): Likewise.
+	(cortex_r4_alu_shift): Likewise.
+	(cortex_r4_alu_shift_reg): Likewise.
+	* config/arm/fa526.md (526_alu_op): Update for attribute change.
+	(526_alu_shift_op): Likewise.
+	* config/arm/fa606te.md (606te_alu_op): Update for attribute change.
+	* config/arm/fa626te.md (626te_alu_op): Update for attribute change.
+	(626te_alu_shift_op): Likewise.
+	* config/arm/fa726te.md (726te_shift_op): Update for attribute change.
+	(726te_alu_op): Likewise.
+	(726te_alu_shift_op): Likewise.
+	(726te_alu_shift_reg_op): Likewise.
+	* config/arm/fmp626.md (mp626_alu_op): Update for attribute change.
+	(mp626_alu_shift_op): Likewise.
+	* config/arm/marvell-pj4.md (pj4_alu_e1): Update for attribute change.
+	(pj4_alu_e1_conds): Likewise.
+	(pj4_alu): Likewise.
+	(pj4_alu_conds): Likewise.
+	(pj4_shift): Likewise.
+	(pj4_shift_conds): Likewise.
+	(pj4_alu_shift): Likewise.
+	(pj4_alu_shift_conds): Likewise.
+
+	2013-07-22  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/constraints.md (Pd): Allow TARGET_THUMB
+	instead of TARGET_THUMB1.
+	(Pz): New constraint.
+	* config/arm/arm.md (arm_addsi3): Add alternatives for 16-bit
+	encodings.
+	(compare_negsi_si): Likewise.
+	(compare_addsi2_op0): Likewise.
+	(compare_addsi2_op1): Likewise.
+	(addsi3_carryin_<optab>): Likewise.
+	(addsi3_carryin_alt2_<optab>): Likewise.
+	(addsi3_carryin_shift_<optab>): Disable cond_exec variant
+	for arm_restrict_it.
+	(subsi3_carryin): Likewise.
+	(arm_subsi3_insn): Add alternatives for 16-bit encoding.
+	(minmax_arithsi): Disable for arm_restrict_it.
+	(minmax_arithsi_non_canon): Adjust for arm_restrict_it.
+	(satsi_<SAT:code>): Disable cond_exec variant for arm_restrict_it.
+	(satsi_<SAT:code>_shift): Likewise.
+	(arm_shiftsi3): Add alternative for 16-bit encoding.
+	(arm32_movhf): Disable for arm_restrict_it.
+	(arm_cmpdi_unsigned): Add alternatives for 16-bit encoding.
+	(arm_movtas_ze): Disable cond_exec variant for arm_restrict_it.
+
+2013-09-09  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	Backport from trunk r201412.
+	2013-08-01  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (minmax_arithsi_non_canon): Emit canonical RTL form
+	when subtracting a constant.
+
+2013-09-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r201249.
+	2013-07-25  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm-fixed.md (ssmulsa3, usmulusa3):
+	Adjust for arm_restrict_it.
+	Remove trailing whitespace.
+
+2013-09-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r201342.
+	2013-07-30  Richard Earnshaw  <rearnsha@arm.com>
+
+	* config.gcc (arm): Require 64-bit host-wide-int for all ARM target
+	configs.
+
+2013-09-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199527,199792,199814.
+	2013-05-31  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	PR target/56315
+	* config/arm/arm.c (const_ok_for_dimode_op): Handle IOR.
+	* config/arm/arm.md (*iordi3_insn): Change to insn_and_split.
+	* config/arm/neon.md (iordi3_neon): Remove.
+	(neon_vorr<mode>): Generate iordi3 instead of iordi3_neon.
+	* config/arm/predicates.md (imm_for_neon_logic_operand):
+	Move to earlier in the file.
+	(neon_logic_op2): Likewise.
+	(arm_iordi_operand_neon): New predicate.
+
+	2013-06-07  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/constraints.md (Df): New constraint.
+	* config/arm/arm.md (iordi3_insn): Use Df constraint instead of De.
+	Correct length attribute for last two alternatives.
+
+	2013-06-07  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	PR target/56315
+	* config/arm/arm.md (*xordi3_insn): Change to insn_and_split.
+	(xordi3): Change operand 2 constraint to arm_xordi_operand.
+	* config/arm/arm.c (const_ok_for_dimode_op): Handle XOR.
+	* config/arm/constraints.md (Dg): New constraint.
+	* config/arm/neon.md (xordi3_neon): Remove.
+	(neon_veor<mode>): Generate xordi3 instead of xordi3_neon.
+	* config/arm/predicates.md (arm_xordi_operand): New predicate.
+
+2013-09-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201599.
+	2013-08-08  Richard Earnshaw  <rearnsha@arm.com>
+
+	PR target/57431
+	* arm/neon.md (neon_vld1_dupdi): New expand pattern.
+	(neon_vld1_dup<mode> VD iterator): Iterate over VD not VDX.
+
+2013-09-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201589.
+	2013-08-08  Bernd Edlinger  <bernd.edlinger@hotmail.de>
+
+	PR target/58065
+	* config/arm/arm.h (MALLOC_ABI_ALIGNMENT): Define.
+
+2013-09-03  Venkataramanan Kumar  <venkataramanan.kumar@linaro.org>
+
+	Backport from trunk
+	r201624, r201666.
+	2013-08-09  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd-builtins.def (get_lane_signed): Remove.
+	(get_lane_unsigned): Likewise.
+	(dup_lane_scalar): Likewise.
+	(get_lane): enable for VALL.
+	* config/aarch64/aarch64-simd.md
+	(aarch64_dup_lane_scalar<mode>): Remove.
+	(aarch64_get_lane_signed<mode>): Likewise.
+	(aarch64_get_lane_unsigned<mode>): Likewise.
+	(aarch64_get_lane_extend<GPI:mode><VDQQH:mode>): New.
+	(aarch64_get_lane_zero_extendsi<mode>): Likewise.
+	(aarch64_get_lane<mode>): Enable for all vector modes.
+	(aarch64_get_lanedi): Remove misleading constraints.
+	* config/aarch64/arm_neon.h
+	(__aarch64_vget_lane_any): Define.
+	(__aarch64_vget<q>_lane_<fpsu><8,16,32,64>): Likewise.
+	(vget<q>_lane_<fpsu><8,16,32,64>): Use __aarch64_vget_lane macros.
+	(vdup<bhsd>_lane_<su><8,16,32,64>): Likewise.
+	 * config/aarch64/iterators.md (VDQQH): New.
+	(VDQQHS): Likewise.
+	(vwcore): Likewise.
+
+	2013-08-12  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/arm_none.h
+	(vdup<bhsd>_lane_<su><8,16,32,64>): Fix macro call.
+
+2013-08-26  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	Backport from trunk r201341.
+	2013-07-30  Richard Earnshaw  <rearnsha@arm.com>
+
+	* arm.md (mulhi3): New expand pattern.
+
+2013-08-16  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+	* LINARO-VERSION: Update.
+
+2013-08-08  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk
+	r198489,200167,200199,200510,200513,200515,200576.
+	2013-05-01  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/thumb2.md (thumb2_smaxsi3,thumb2_sminsi3): Convert
+	define_insn to define_insn_and_split.
+	(thumb32_umaxsi3,thumb2_uminsi3): Likewise.
+	(thumb2_negdi2,thumb2_abssi2,thumb2_neg_abssi2): Likewise.
+	(thumb2_mov_scc,thumb2_mov_negscc,thumb2_mov_notscc): Likewise.
+	(thumb2_movsicc_insn,thumb2_and_scc,thumb2_ior_scc): Likewise.
+	(thumb2_negscc): Likewise.
+
+	2013-06-18  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/arm/arm.md (attribute "insn"): Move multiplication and division
+	attributes to...
+	(attribute "type"): ... here.  Remove mult.
+	(attribute "mul32"): New attribute.
+	(attribute "mul64"): Add umaal.
+	(*arm_mulsi3): Update attributes.
+	(*arm_mulsi3_v6): Likewise.
+	(*thumb_mulsi3): Likewise.
+	(*thumb_mulsi3_v6): Likewise.
+	(*mulsi3_compare0): Likewise.
+	(*mulsi3_compare0_v6): Likewise.
+	(*mulsi_compare0_scratch): Likewise.
+	(*mulsi_compare0_scratch_v6): Likewise.
+	(*mulsi3addsi): Likewise.
+	(*mulsi3addsi_v6): Likewise.
+	(*mulsi3addsi_compare0): Likewise.
+	(*mulsi3addsi_compare0_v6): Likewise.
+	(*mulsi3addsi_compare0_scratch): Likewise.
+	(*mulsi3addsi_compare0_scratch_v6): Likewise.
+	(*mulsi3subsi): Likewise.
+	(*mulsidi3adddi): Likewise.
+	(*mulsi3addsi_v6): Likewise.
+	(*mulsidi3adddi_v6): Likewise.
+	(*mulsidi3_nov6): Likewise.
+	(*mulsidi3_v6): Likewise.
+	(*umulsidi3_nov6): Likewise.
+	(*umulsidi3_v6): Likewise.
+	(*umulsidi3adddi): Likewise.
+	(*umulsidi3adddi_v6): Likewise.
+	(*smulsi3_highpart_nov6): Likewise.
+	(*smulsi3_highpart_v6): Likewise.
+	(*umulsi3_highpart_nov6): Likewise.
+	(*umulsi3_highpart_v6): Likewise.
+	(mulhisi3): Likewise.
+	(*mulhisi3tb): Likewise.
+	(*mulhisi3bt): Likewise.
+	(*mulhisi3tt): Likewise.
+	(maddhisi4): Likewise.
+	(*maddhisi4tb): Likewise.
+	(*maddhisi4tt): Likewise.
+	(maddhidi4): Likewise.
+	(*maddhidi4tb): Likewise.
+	(*maddhidi4tt): Likewise.
+	(divsi3): Likewise.
+	(udivsi3): Likewise.
+	* config/arm/thumb2.md (thumb2_mulsi_short): Update attributes.
+	(thumb2_mulsi_short_compare0): Likewise.
+	(thumb2_mulsi_short_compare0_scratch): Likewise.
+	* config/arm/arm1020e.md (1020mult1): Update attribute change.
+	(1020mult2): Likewise.
+	(1020mult3): Likewise.
+	(1020mult4): Likewise.
+	(1020mult5): Likewise.
+	(1020mult6): Likewise.
+	* config/arm/cortex-a15.md (cortex_a15_mult32): Update attribute change.
+	(cortex_a15_mult64): Likewise.
+	(cortex_a15_sdiv): Likewise.
+	(cortex_a15_udiv): Likewise.
+	* config/arm/arm1026ejs.md (mult1): Update attribute change.
+	(mult2): Likewise.
+	(mult3): Likewise.
+	(mult4): Likewise.
+	(mult5): Likewise.
+	(mult6): Likewise.
+	* config/arm/marvell-pj4.md (pj4_ir_mul): Update attribute change.
+	(pj4_ir_div): Likewise.
+	* config/arm/arm1136jfs.md (11_mult1): Update attribute change.
+	(11_mult2): Likewise.
+	(11_mult3): Likewise.
+	(11_mult4): Likewise.
+	(11_mult5): Likewise.
+	(11_mult6): Likewise.
+	(11_mult7): Likewise.
+	* config/arm/cortex-a8.md (cortex_a8_mul): Update attribute change.
+	(cortex_a8_mla): Likewise.
+	(cortex_a8_mull): Likewise.
+	(cortex_a8_smulwy): Likewise.
+	(cortex_a8_smlald): Likewise.
+	* config/arm/cortex-m4.md (cortex_m4_alu): Update attribute change.
+	* config/arm/cortex-r4.md (cortex_r4_mul_4): Update attribute change.
+	(cortex_r4_mul_3): Likewise.
+	(cortex_r4_mla_4): Likewise.
+	(cortex_r4_mla_3): Likewise.
+	(cortex_r4_smlald): Likewise.
+	(cortex_r4_mull): Likewise.
+	(cortex_r4_sdiv): Likewise.
+	(cortex_r4_udiv): Likewise.
+	* config/arm/cortex-a7.md (cortex_a7_mul): Update attribute change.
+	(cortex_a7_idiv): Likewise.
+	* config/arm/arm926ejs.md (9_mult1): Update attribute change.
+	(9_mult2): Likewise.
+	(9_mult3): Likewise.
+	(9_mult4): Likewise.
+	(9_mult5): Likewise.
+	(9_mult6): Likewise.
+	* config/arm/cortex-a53.md (cortex_a53_mul): Update attribute change.
+	(cortex_a53_sdiv): Likewise.
+	(cortex_a53_udiv): Likewise.
+	* config/arm/fa726te.md (726te_mult_op): Update attribute change.
+	* config/arm/fmp626.md (mp626_mult1): Update attribute change.
+	(mp626_mult2): Likewise.
+	(mp626_mult3): Likewise.
+	(mp626_mult4): Likewise.
+	* config/arm/fa526.md (526_mult1): Update attribute change.
+	(526_mult2): Likewise.
+	* config/arm/arm-generic.md (mult): Update attribute change.
+	(mult_ldsched_strongarm): Likewise.
+	(mult_ldsched): Likewise.
+	(multi_cycle): Likewise.
+	* config/arm/cortex-a5.md (cortex_a5_mul): Update attribute change.
+	* config/arm/fa606te.md (606te_mult1): Update attribute change.
+	(606te_mult2): Likewise.
+	(606te_mult3): Likewise.
+	(606te_mult4): Likewise.
+	* config/arm/cortex-a9.md (cortex_a9_mult16): Update attribute change.
+	(cortex_a9_mac16): Likewise.
+	(cortex_a9_multiply): Likewise.
+	(cortex_a9_mac): Likewise.
+	(cortex_a9_multiply_long): Likewise.
+	* config/arm/fa626te.md (626te_mult1): Update attribute change.
+	(626te_mult2): Likewise.
+	(626te_mult3): Likewise.
+	(626te_mult4): Likewise.
+
+	2013-06-19  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/arm/vfp.md: Move VFP instruction classification documentation
+	to ...
+	* config/arm/arm.md: ... here.  Update instruction classification
+	documentation.
+
+	2013-06-28  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/predicates.md (arm_cond_move_operator): New predicate.
+	* config/arm/arm.md (movsfcc): Use arm_cond_move_operator predicate.
+	(movdfcc): Likewise.
+	* config/arm/vfp.md (*thumb2_movsf_vfp):
+	Disable predication for arm_restrict_it.
+	(*thumb2_movsfcc_vfp): Disable for arm_restrict_it.
+	(*thumb2_movdfcc_vfp): Likewise.
+	(*abssf2_vfp, *absdf2_vfp, *negsf2_vfp, *negdf2_vfp,*addsf3_vfp,
+	*adddf3_vfp, *subsf3_vfp, *subdf3_vfpc, *divsf3_vfp,*divdf3_vfp,
+	*mulsf3_vfp, *muldf3_vfp, *mulsf3negsf_vfp, *muldf3negdf_vfp,
+	*mulsf3addsf_vfp, *muldf3adddf_vfp, *mulsf3subsf_vfp,
+	*muldf3subdf_vfp, *mulsf3negsfaddsf_vfp, *fmuldf3negdfadddf_vfp,
+	*mulsf3negsfsubsf_vfp, *muldf3negdfsubdf_vfp, *fma<SDF:mode>4,
+	*fmsub<SDF:mode>4, *fnmsub<SDF:mode>4, *fnmadd<SDF:mode>4,
+	*extendsfdf2_vfp, *truncdfsf2_vfp, *extendhfsf2, *truncsfhf2,
+	*truncsisf2_vfp, *truncsidf2_vfp, fixuns_truncsfsi2, fixuns_truncdfsi2,
+	*floatsisf2_vfp, *floatsidf2_vfp, floatunssisf2, floatunssidf2,
+	*sqrtsf2_vfp, *sqrtdf2_vfp, *cmpsf_vfp, *cmpsf_trap_vfp, *cmpdf_vfp,
+	*cmpdf_trap_vfp, <vrint_pattern><SDF:mode>2):
+	Disable predication for arm_restrict_it.
+
+	2013-06-28  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (arm_mulsi3_v6): Add alternative for 16-bit
+	encoding.
+	(mulsi3addsi_v6): Disable predicable variant for arm_restrict_it.
+	(mulsi3subsi): Likewise.
+	(mulsidi3adddi): Likewise.
+	(mulsidi3_v6): Likewise.
+	(umulsidi3_v6): Likewise.
+	(umulsidi3adddi_v6): Likewise.
+	(smulsi3_highpart_v6): Likewise.
+	(umulsi3_highpart_v6): Likewise.
+	(mulhisi3tb): Likewise.
+	(mulhisi3bt): Likewise.
+	(mulhisi3tt): Likewise.
+	(maddhisi4): Likewise.
+	(maddhisi4tb): Likewise.
+	(maddhisi4tt): Likewise.
+	(maddhidi4): Likewise.
+	(maddhidi4tb): Likewise.
+	(maddhidi4tt): Likewise.
+	(zeroextractsi_compare0_scratch): Likewise.
+	(insv_zero): Likewise.
+	(insv_t2): Likewise.
+	(anddi_notzesidi_di): Likewise.
+	(anddi_notsesidi_di): Likewise.
+	(andsi_notsi_si): Likewise.
+	(iordi_zesidi_di): Likewise.
+	(xordi_zesidi_di): Likewise.
+	(andsi_iorsi3_notsi): Likewise.
+	(smax_0): Likewise.
+	(smax_m1): Likewise.
+	(smin_0): Likewise.
+	(not_shiftsi): Likewise.
+	(unaligned_loadsi): Likewise.
+	(unaligned_loadhis): Likewise.
+	(unaligned_loadhiu): Likewise.
+	(unaligned_storesi): Likewise.
+	(unaligned_storehi): Likewise.
+	(extv_reg): Likewise.
+	(extzv_t2): Likewise.
+	(divsi3): Likewise.
+	(udivsi3): Likewise.
+	(arm_zero_extendhisi2addsi): Likewise.
+	(arm_zero_extendqisi2addsi): Likewise.
+	(compareqi_eq0): Likewise.
+	(arm_extendhisi2_v6): Likewise.
+	(arm_extendqisi2addsi): Likewise.
+	(arm_movt): Likewise.
+	(thumb2_ldrd): Likewise.
+	(thumb2_ldrd_base): Likewise.
+	(thumb2_ldrd_base_neg): Likewise.
+	(thumb2_strd): Likewise.
+	(thumb2_strd_base): Likewise.
+	(thumb2_strd_base_neg): Likewise.
+	(arm_negsi2): Add alternative for 16-bit encoding.
+	(arm_one_cmplsi2): Likewise.
+
+	2013-06-28  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/constraints.md (Ts): New constraint.
+	* config/arm/arm.md (arm_movqi_insn): Add alternatives for
+	16-bit encodings.
+	(compare_scc): Use "Ts" constraint for operand 0.
+	(ior_scc_scc): Likewise.
+	(and_scc_scc): Likewise.
+	(and_scc_scc_nodom): Likewise.
+	(ior_scc_scc_cmp): Likewise for operand 7.
+	(and_scc_scc_cmp): Likewise.
+	* config/arm/thumb2.md (thumb2_movsi_insn):
+	Add alternatives for 16-bit encodings.
+	(thumb2_movhi_insn): Likewise.
+	(thumb2_movsicc_insn): Likewise.
+	(thumb2_and_scc): Take 'and' outside cond_exec.  Use "Ts" constraint.
+	(thumb2_negscc): Use "Ts" constraint.
+	Move mvn instruction outside cond_exec block.
+	* config/arm/vfp.md (thumb2_movsi_vfp): Add alternatives
+	for 16-bit encodings.
+
+	2013-07-01  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* arm.md (attribute "wtype"): Delete.  Move attribute values from here
+	to ...
+	(attribute "type"): ... here, and prefix with "wmmx_".
+	(attribute "core_cycles"): Update for attribute changes.
+	* iwmmxt.md (tbcstv8qi): Update for attribute changes.
+	(tbcstv4hi): Likewise.
+	(tbcstv2si): Likewise.
+	(iwmmxt_iordi3): Likewise.
+	(iwmmxt_xordi3): Likewise.
+	(iwmmxt_anddi3): Likewise.
+	(iwmmxt_nanddi3): Likewise.
+	(iwmmxt_arm_movdi): Likewise.
+	(iwmmxt_movsi_insn): Likewise.
+	(mov<mode>_internal): Likewise.
+	(and<mode>3_iwmmxt): Likewise.
+	(ior<mode>3_iwmmxt): Likewise.
+	(xor<mode>3_iwmmxt): Likewise.
+	(add<mode>3_iwmmxt): Likewise.
+	(ssaddv8qi3): Likewise.
+	(ssaddv4hi3): Likewise.
+	(ssaddv2si3): Likewise.
+	(usaddv8qi3): Likewise.
+	(usaddv4hi3): Likewise.
+	(usaddv2si3): Likewise.
+	(sub<mode>3_iwmmxt): Likewise.
+	(sssubv8qi3): Likewise.
+	(sssubv4hi3): Likewise.
+	(sssubv2si3): Likewise.
+	(ussubv8qi3): Likewise.
+	(ussubv4hi3): Likewise.
+	(ussubv2si3): Likewise.
+	(mulv4hi3_iwmmxt): Likewise.
+	(smulv4hi3_highpart): Likewise.
+	(umulv4hi3_highpart): Likewise.
+	(iwmmxt_wmacs): Likewise.
+	(iwmmxt_wmacsz): Likewise.
+	(iwmmxt_wmacu): Likewise.
+	(iwmmxt_wmacuz): Likewise.
+	(iwmmxt_clrdi): Likewise.
+	(iwmmxt_clrv8qi): Likewise.
+	(iwmmxt_clr4hi): Likewise.
+	(iwmmxt_clr2si): Likewise.
+	(iwmmxt_uavgrndv8qi3): Likewise.
+	(iwmmxt_uavgrndv4hi3): Likewise.
+	(iwmmxt_uavgv8qi3): Likewise.
+	(iwmmxt_uavgv4hi3): Likewise.
+	(iwmmxt_tinsrb): Likewise.
+	(iwmmxt_tinsrh): Likewise.
+	(iwmmxt_tinsrw): Likewise.
+	(iwmmxt_textrmub): Likewise.
+	(iwmmxt_textrmsb): Likewise.
+	(iwmmxt_textrmuh): Likewise.
+	(iwmmxt_textrmsh): Likewise.
+	(iwmmxt_textrmw): Likewise.
+	(iwmxxt_wshufh): Likewise.
+	(eqv8qi3): Likewise.
+	(eqv4hi3): Likewise.
+	(eqv2si3): Likewise.
+	(gtuv8qi3): Likewise.
+	(gtuv4hi3): Likewise.
+	(gtuv2si3): Likewise.
+	(gtv8qi3): Likewise.
+	(gtv4hi3): Likewise.
+	(gtv2si3): Likewise.
+	(smax<mode>3_iwmmxt): Likewise.
+	(umax<mode>3_iwmmxt): Likewise.
+	(smin<mode>3_iwmmxt): Likewise.
+	(umin<mode>3_iwmmxt): Likewise.
+	(iwmmxt_wpackhss): Likewise.
+	(iwmmxt_wpackwss): Likewise.
+	(iwmmxt_wpackdss): Likewise.
+	(iwmmxt_wpackhus): Likewise.
+	(iwmmxt_wpackwus): Likewise.
+	(iwmmxt_wpackdus): Likewise.
+	(iwmmxt_wunpckihb): Likewise.
+	(iwmmxt_wunpckihh): Likewise.
+	(iwmmxt_wunpckihw): Likewise.
+	(iwmmxt_wunpckilb): Likewise.
+	(iwmmxt_wunpckilh): Likewise.
+	(iwmmxt_wunpckilw): Likewise.
+	(iwmmxt_wunpckehub): Likewise.
+	(iwmmxt_wunpckehuh): Likewise.
+	(iwmmxt_wunpckehuw): Likewise.
+	(iwmmxt_wunpckehsb): Likewise.
+	(iwmmxt_wunpckehsh): Likewise.
+	(iwmmxt_wunpckehsw): Likewise.
+	(iwmmxt_wunpckelub): Likewise.
+	(iwmmxt_wunpckeluh): Likewise.
+	(iwmmxt_wunpckeluw): Likewise.
+	(iwmmxt_wunpckelsb): Likewise.
+	(iwmmxt_wunpckelsh): Likewise.
+	(iwmmxt_wunpckelsw): Likewise.
+	(ror<mode>3): Likewise.
+	(ashr<mode>3_iwmmxt): Likewise.
+	(lshr<mode>3_iwmmxt): Likewise.
+	(ashl<mode>3_iwmmxt): Likewise.
+	(ror<mode>3_di): Likewise.
+	(ashr<mode>3_di): Likewise.
+	(lshr<mode>3_di): Likewise.
+	(ashl<mode>3_di): Likewise.
+	(iwmmxt_wmadds): Likewise.
+	(iwmmxt_wmaddu): Likewise.
+	(iwmmxt_tmia): Likewise.
+	(iwmmxt_tmiaph): Likewise.
+	(iwmmxt_tmiabb): Likewise.
+	(iwmmxt_tmiatb): Likewise.
+	(iwmmxt_tmiabt): Likewise.
+	(iwmmxt_tmiatt): Likewise.
+	(iwmmxt_tmovmskb): Likewise.
+	(iwmmxt_tmovmskh): Likewise.
+	(iwmmxt_tmovmskw): Likewise.
+	(iwmmxt_waccb): Likewise.
+	(iwmmxt_wacch): Likewise.
+	(iwmmxt_waccw): Likewise.
+	(iwmmxt_waligni): Likewise.
+	(iwmmxt_walignr): Likewise.
+	(iwmmxt_walignr0): Likewise.
+	(iwmmxt_walignr1): Likewise.
+	(iwmmxt_walignr2): Likewise.
+	(iwmmxt_walignr3): Likewise.
+	(iwmmxt_wsadb): Likewise.
+	(iwmmxt_wsadh): Likewise.
+	(iwmmxt_wsadbz): Likewise.
+	(iwmmxt_wsadhz): Likewise.
+	* iwmmxt2.md (iwmmxt_wabs<mode>3): Update for attribute changes.
+	(iwmmxt_wabsdiffb): Likewise.
+	(iwmmxt_wabsdiffh): Likewise.
+	(iwmmxt_wabsdiffw): Likewise.
+	(iwmmxt_waddsubhx): Likewise
+	(iwmmxt_wsubaddhx): Likewise.
+	(addc<mode>3): Likewise.
+	(iwmmxt_avg4): Likewise.
+	(iwmmxt_avg4r): Likewise.
+	(iwmmxt_wmaddsx): Likewise.
+	(iwmmxt_wmaddux): Likewise.
+	(iwmmxt_wmaddsn): Likewise.
+	(iwmmxt_wmaddun): Likewise.
+	(iwmmxt_wmulwsm): Likewise.
+	(iwmmxt_wmulwum): Likewise.
+	(iwmmxt_wmulsmr): Likewise.
+	(iwmmxt_wmulumr): Likewise.
+	(iwmmxt_wmulwsmr): Likewise.
+	(iwmmxt_wmulwumr): Likewise.
+	(iwmmxt_wmulwl): Likewise.
+	(iwmmxt_wqmulm): Likewise.
+	(iwmmxt_wqmulwm): Likewise.
+	(iwmmxt_wqmulmr): Likewise.
+	(iwmmxt_wqmulwmr): Likewise.
+	(iwmmxt_waddbhusm): Likewise.
+	(iwmmxt_waddbhusl): Likewise.
+	(iwmmxt_wqmiabb): Likewise.
+	(iwmmxt_wqmiabt): Likewise.
+	(iwmmxt_wqmiatb): Likewise.
+	(iwmmxt_wqmiatt): Likewise.
+	(iwmmxt_wqmiabbn): Likewise.
+	(iwmmxt_wqmiabtn): Likewise.
+	(iwmmxt_wqmiatbn): Likewise.
+	(iwmmxt_wqmiattn): Likewise.
+	(iwmmxt_wmiabb): Likewise.
+	(iwmmxt_wmiabt): Likewise.
+	(iwmmxt_wmiatb): Likewise.
+	(iwmmxt_wmiatt): Likewise.
+	(iwmmxt_wmiabbn): Likewise.
+	(iwmmxt_wmiabtn): Likewise.
+	(iwmmxt_wmiatbn): Likewise.
+	(iwmmxt_wmiattn): Likewise.
+	(iwmmxt_wmiawbb): Likewise.
+	(iwmmxt_wmiawbt): Likewise.
+	(iwmmxt_wmiawtb): Likewise.
+	(iwmmxt_wmiawtt): Likewise.
+	(iwmmxt_wmiawbbn): Likewise.
+	(iwmmxt_wmiawbtn): Likewise.
+	(iwmmxt_wmiawtbn): Likewise.
+	(iwmmxt_wmiawttn): Likewise.
+	(iwmmxt_wmerge): Likewise.
+	(iwmmxt_tandc<mode>3): Likewise.
+	(iwmmxt_torc<mode>3): Likewise.
+	(iwmmxt_torvsc<mode>3): Likewise.
+	(iwmmxt_textrc<mode>3): Likewise.
+	* marvell-f-iwmmxt.md (wmmxt_shift): Update for attribute changes.
+	(wmmxt_pack): Likewise.
+	(wmmxt_mult_c1): Likewise.
+	(wmmxt_mult_c2): Likewise.
+	(wmmxt_alu_c1): Likewise.
+	(wmmxt_alu_c2): Likewise.
+	(wmmxt_alu_c3): Likewise.
+	(wmmxt_transfer_c1): Likewise.
+	(wmmxt_transfer_c2): Likewise.
+	(wmmxt_transfer_c3): Likewise.
+	(marvell_f_iwmmxt_wstr): Likewise.
+	(marvell_f_iwmmxt_wldr): Likewise.
+
+2013-08-07  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201237.
+	2013-07-25  Terry Guo  <terry.guo@arm.com>
+
+	* config/arm/arm.c (thumb1_size_rtx_costs): Assign proper cost for
+	shift_add/shift_sub0/shift_sub1 RTXs.
+
+2013-08-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r200596,201067,201083.
+	2013-07-02  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64-simd.md (absdi2): Support abs for
+	DI mode.
+
+	2013-07-19  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/arm_neon.h (vabs_s64): New function
+
+	2013-07-20  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_fold_builtin): Fold abs in all modes.
+	* config/aarch64/aarch64-simd-builtins.def
+	(abs): Enable for all modes.
+	* config/aarch64/arm_neon.h
+	(vabs<q>_s<8,16,32,64): Rewrite using builtins.
+	(vabs_f64): Add missing intrinsic.
+
+2013-08-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198735,198831,199959.
+	2013-05-09  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64.md: New movtf split.
+	(*movtf_aarch64): Update.
+	(aarch64_movdi_tilow): Handle TF modes and rename to
+	aarch64_movdi_<mode>low.
+	(aarch64_movdi_tihigh): Handle TF modes and rename to
+	aarch64_movdi_<mode>high
+	(aarch64_movtihigh_di): Handle TF modes and rename to
+	aarch64_mov<mode>high_di
+	(aarch64_movtilow_di): Handle TF modes and rename to
+	aarch64_mov<mode>low_di
+	(aarch64_movtilow_tilow): Remove spurious whitespace.
+	* config/aarch64/aarch64.c (aarch64_split_128bit_move): Handle TFmode
+	splits.
+	(aarch64_print_operand): Update.
+
+	2013-05-13  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64-simd.md (aarch64_simd_mov<mode>): Group
+	similar switch cases.
+	(aarch64_simd_mov): Rename to aarch64_split_simd_mov. Update.
+	(aarch64_simd_mov_to_<mode>low): Delete.
+	(aarch64_simd_mov_to_<mode>high): Delete.
+	(move_lo_quad_<mode>): Add w<-r alternative.
+	(aarch64_simd_move_hi_quad_<mode>): Likewise.
+	(aarch64_simd_mov_from_*): Update type attribute.
+	* config/aarch64/aarch64.c (aarch64_split_simd_move): Refacror switch
+	statement.
+
+	2013-06-11  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64-simd.md (move_lo_quad_<mode>): Update.
+
+2013-08-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199438,199439,201326.
+
+	2013-05-30  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* config/arm/arm.c (arm_add_cfa_adjust_cfa_note): New added.
+	(arm_emit_multi_reg_pop): Add REG_CFA_ADJUST_CFA notes.
+	(arm_emit_vfp_multi_reg_pop): Likewise.
+	(thumb2_emit_ldrd_pop): Likewise.
+	(arm_expand_epilogue): Add misc REG_CFA notes.
+	(arm_unwind_emit): Skip REG_CFA_ADJUST_CFA and REG_CFA_RESTORE.
+
+	2013-05-30  Bernd Schmidt  <bernds@codesourcery.com>
+	    Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* config/arm/arm-protos.h: Add and update function protos.
+	* config/arm/arm.c (use_simple_return_p): New added.
+	(thumb2_expand_return): Check simple_return flag.
+	* config/arm/arm.md: Add simple_return and conditional simple_return.
+	* config/arm/iterators.md: Add iterator for return and simple_return.
+
+	2013-07-30  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	PR rtl-optimization/57637
+	* function.c (move_insn_for_shrink_wrap): Also check the
+	GEN set of the LIVE problem for the liveness analysis
+	if it exists, otherwise give up.
+
+2013-08-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198928,198973,199203,201240,201241,201307.
+	2013-05-15  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	* config/arm/predicates.md (call_insn_operand): New predicate.
+	* config/arm/constraints.md ("Cs", "Ss"):  New constraints.
+	* config/arm/arm.md (*call_insn, *call_value_insn): Match only
+	if insn is not a tail call.
+	(*sibcall_insn, *sibcall_value_insn): Adjust for tailcalling through
+	registers.
+	* config/arm/arm.h (enum reg_class): New caller save register class.
+	(REG_CLASS_NAMES): Likewise.
+	(REG_CLASS_CONTENTS): Likewise.
+	* config/arm/arm.c (arm_function_ok_for_sibcall): Allow tailcalling
+	without decls.
+
+	2013-05-16  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	* config/arm/arm.c (arm_function_ok_for_sibcall): Add check
+	for NULL decl.
+
+	2013-05-22  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	PR target/57340
+	* config/arm/arm.c (any_sibcall_uses_r3): Rename to ..
+	(any_sibcall_could_use_r3): this and handle indirect calls.
+	(arm_get_frame_offsets): Rename use of any_sibcall_uses_r3.
+
+	2013-07-25  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	PR target/57731
+	PR target/57748
+	* config/arm/arm.md ("*sibcall_value_insn): Replace use of
+	Ss with US. Adjust output for v5 and v4t.
+	(*sibcall_value_insn): Likewise and loosen predicate on
+	operand0.
+	* config/arm/constraints.md ("Ss"): Rename to US.
+
+	2013-07-25  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	* config/arm/arm.md (*sibcall_insn): Remove unnecessary space.
+
+	2013-07-29  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+	Fix incorrect changelog entry.
+
+	Replace
+	PR target/57748
+	with
+	PR target/57837
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200922.
+	2013-07-12  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/aarch64-protos.h
+	(aarch64_simd_immediate_valid_for_move): Remove.
+	* config/aarch64/aarch64.c (simd_immediate_info): New member.
+	(aarch64_simd_valid_immediate): Recognize idioms for shifting ones
+	cases.
+	(aarch64_output_simd_mov_immediate): Print the correct shift specifier.
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200670.
+	2013-07-04  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/aarch64-protos.h (cpu_vector_cost): New.
+	(tune_params): New member 'const vec_costs'.
+	* config/aarch64/aarch64.c (generic_vector_cost): New.
+	(generic_tunings): New member 'generic_vector_cost'.
+	(aarch64_builtin_vectorization_cost): New.
+	(aarch64_add_stmt_cost): New.
+	(TARGET_VECTORIZE_ADD_STMT_COST): New.
+	(TARGET_VECTORIZE_BUILTIN_VECTORIZATION_COST): New.
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200637.
+	2013-07-03  Yufeng Zhang  <yufeng.zhang@arm.com>
+
+	* config/aarch64/aarch64.h (enum arm_abi_type): Remove.
+	(ARM_ABI_AAPCS64): Ditto.
+	(arm_abi): Ditto.
+	(ARM_DEFAULT_ABI): Ditto.
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200532, r200565.
+	2013-06-28  Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_cannot_force_const_mem): Adjust
+	layout.
+
+	2013-06-29  Yufeng Zhang  <yufeng.zhang@arm.com>
+
+	* config/aarch64/aarch64.c: Remove junk from the beginning of the
+	file.
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200531.
+	2013-06-28  Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64-protos.h (aarch64_symbol_type):
+	Update comment w.r.t SYMBOL_TINY_ABSOLUTE.
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200519.
+	2013-06-28  Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64-protos.h
+	aarch64_classify_symbol_expression): Define.
+	(aarch64_symbolic_constant_p): Remove.
+	* config/aarch64/aarch64.c (aarch64_classify_symbol_expression): Remove
+	static.  Fix line length and white space.
+	(aarch64_symbolic_constant_p): Remove.
+	* config/aarch64/predicates.md (aarch64_valid_symref):
+	Use aarch64_classify_symbol_expression.
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200466, r200467.
+	2013-06-27  Yufeng Zhang  <yufeng.zhang@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_force_temporary): Add an extra
+	parameter 'mode' of type 'enum machine_mode mode'; change to pass
+	'mode' to force_reg.
+	(aarch64_add_offset): Update calls to aarch64_force_temporary.
+	(aarch64_expand_mov_immediate): Likewise.
+
+	2013-06-27  Yufeng Zhang  <yufeng.zhang@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_add_offset): Change to pass
+	'mode' to aarch64_plus_immediate and gen_rtx_PLUS.
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200419.
+	2013-06-26  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.h (MAX_CONDITIONAL_EXECUTE): Define macro.
+	* config/arm/arm-protos.h (arm_max_conditional_execute): New
+	declaration.
+	(tune_params): Update comment.
+	* config/arm/arm.c (arm_cortex_a15_tune): Set max_cond_insns to 2.
+	(arm_max_conditional_execute): New function.
+	(thumb2_final_prescan_insn): Use max_insn_skipped and
+	MAX_INSN_PER_IT_BLOCK to compute maximum instructions in a block.
+
+2013-07-24  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+	* LINARO-VERSION: Update.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r201005.
+	2013-07-17  Yvan Roux  <yvan.roux@linaro.org>
+
+	PR target/57909
+	* config/arm/arm.c (gen_movmem_ldrd_strd): Fix unaligned load/store
+	usage in HI mode.
+
+2013-07-09  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+	* LINARO-VERSION: Update.
+
+2013-07-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Revert backport from trunk r198928,198973,199203.
+	2013-05-22  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	PR target/57340
+	* config/arm/arm.c (any_sibcall_uses_r3): Rename to ..
+	(any_sibcall_could_use_r3): this and handle indirect calls.
+	(arm_get_frame_offsets): Rename use of any_sibcall_uses_r3.
+
+	2013-05-16  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	* config/arm/arm.c (arm_function_ok_for_sibcall): Add check
+	for NULL decl.
+
+	2013-05-15  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	* config/arm/predicates.md (call_insn_operand): New predicate.
+	* config/arm/constraints.md ("Cs", "Ss"):  New constraints.
+	* config/arm/arm.md (*call_insn, *call_value_insn): Match only
+	if insn is not a tail call.
+	(*sibcall_insn, *sibcall_value_insn): Adjust for tailcalling through
+	registers.
+	* config/arm/arm.h (enum reg_class): New caller save register class.
+	(REG_CLASS_NAMES): Likewise.
+	(REG_CLASS_CONTENTS): Likewise.
+	* config/arm/arm.c (arm_function_ok_for_sibcall): Allow tailcalling
+	without decls.
+
+2013-07-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Revert backport from mainline (r199438, r199439)
+	2013-05-30  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* config/arm/arm.c (arm_add_cfa_adjust_cfa_note): New added.
+	(arm_emit_multi_reg_pop): Add REG_CFA_ADJUST_CFA notes.
+	(arm_emit_vfp_multi_reg_pop): Likewise.
+	(thumb2_emit_ldrd_pop): Likewise.
+	(arm_expand_epilogue): Add misc REG_CFA notes.
+	(arm_unwind_emit): Skip REG_CFA_ADJUST_CFA and REG_CFA_RESTORE.
+
+	2013-05-30  Bernd Schmidt  <bernds@codesourcery.com>
+		    Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* config/arm/arm-protos.h: Add and update function protos.
+	* config/arm/arm.c (use_simple_return_p): New added.
+	(thumb2_expand_return): Check simple_return flag.
+	* config/arm/arm.md: Add simple_return and conditional simple_return.
+	* config/arm/iterators.md: Add iterator for return and simple_return.
+	* gcc.dg/shrink-wrap-alloca.c: New added.
+	* gcc.dg/shrink-wrap-pretend.c: New added.
+	* gcc.dg/shrink-wrap-sibcall.c: New added.
+
+2013-07-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199640, 199705, 199733, 199734, 199739.
+	2013-06-04  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* rtl.def: Add extra fourth optional field to define_cond_exec.
+	* gensupport.c (process_one_cond_exec): Process attributes from
+	define_cond_exec.
+	* doc/md.texi: Document fourth field in define_cond_exec.
+
+	2013-06-05  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (enabled_for_depr_it): New attribute.
+	(predicable_short_it): Likewise.
+	(predicated): Likewise.
+	(enabled): Handle above.
+	(define_cond_exec): Set predicated attribute to yes.
+
+	2013-06-06  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/sync.md (atomic_loaddi_1):
+	Disable predication for arm_restrict_it.
+	(arm_load_exclusive<mode>): Likewise.
+	(arm_load_exclusivesi): Likewise.
+	(arm_load_exclusivedi): Likewise.
+	(arm_load_acquire_exclusive<mode>): Likewise.
+	(arm_load_acquire_exclusivesi): Likewise.
+	(arm_load_acquire_exclusivedi): Likewise.
+	(arm_store_exclusive<mode>): Likewise.
+	(arm_store_exclusive<mode>): Likewise.
+	(arm_store_release_exclusivedi): Likewise.
+	(arm_store_release_exclusive<mode>): Likewise.
+
+	2013-06-06  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm-ldmstm.ml: Set "predicable_short_it" to "no"
+	where appropriate.
+	* config/arm/ldmstm.md: Regenerate.
+
+	2013-06-06  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm-fixed.md (add<mode>3,usadd<mode>3,ssadd<mode>3,
+	sub<mode>3, ussub<mode>3, sssub<mode>3, arm_ssatsihi_shift,
+	arm_usatsihi): Adjust alternatives for arm_restrict_it.
+
+2013-07-02  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 200096
+
+	2013-06-14  Vidya Praveen <vidyapraveen@arm.com>
+
+	* config/aarch64/aarch64-simd.md (aarch64_<su>mlal_lo<mode>):
+	New pattern.
+	(aarch64_<su>mlal_hi<mode>, aarch64_<su>mlsl_lo<mode>): Likewise.
+	(aarch64_<su>mlsl_hi<mode>, aarch64_<su>mlal<mode>): Likewise.
+	(aarch64_<su>mlsl<mode>): Likewise.
+
+2013-07-02  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 200062
+
+	2013-06-13  Bin Cheng  <bin.cheng@arm.com>
+	* fold-const.c (operand_equal_p): Consider NOP_EXPR and
+	CONVERT_EXPR as equal nodes.
+
+2013-07-02  Rob Savoye  <rob.savoye@linaro.org>
+	Backport from trunk 199810
+
+	2013-06-07  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (anddi3_insn): Remove duplicate alternatives.
+	Clean up alternatives.
+
+2013-06-20  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 200152
+	2013-06-17  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64-simd.md (aarch64_dup_lane<mode>): Add r<-w
+	alternative and update.
+	(aarch64_dup_lanedi): Delete.
+	* config/aarch64/arm_neon.h (vdup<bhsd>_lane_*): Update.
+	* config/aarch64/aarch64-simd-builtins.def: Update.
+
+2013-06-20  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 200061
+	2013-06-13  Bin Cheng  <bin.cheng@arm.com>
+
+	* rtlanal.c (noop_move_p): Check the code to be executed for
+	COND_EXEC.
+
+2013-06-20  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 199694
+	2013-06-05  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.c (MAX_INSN_PER_IT_BLOCK): New macro.
+	(arm_option_override): Override arm_restrict_it where appropriate.
+	(thumb2_final_prescan_insn): Use MAX_INSN_PER_IT_BLOCK.
+	* config/arm/arm.opt (mrestrict-it): New command-line option.
+	* doc/invoke.texi: Document -mrestrict-it.
+
+2013-06-20  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198683.
+	2013-05-07  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* config/arm/arm.c (arm_asan_shadow_offset): New function.
+	(TARGET_ASAN_SHADOW_OFFSET): Define.
+	* config/arm/linux-eabi.h (ASAN_CC1_SPEC): Define.
+	(LINUX_OR_ANDROID_CC): Add ASAN_CC1_SPEC.
+
+2013-06-18  Rob Savoye  <rob.savoye@linaro.org>
+
+	* LINARO-VERSION: Bump version.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+	* LINARO-VERSION: Update.
+
+2013-06-06  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	Backport from mainline (r199438, r199439)
+	2013-05-30  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* config/arm/arm.c (arm_add_cfa_adjust_cfa_note): New added.
+	(arm_emit_multi_reg_pop): Add REG_CFA_ADJUST_CFA notes.
+	(arm_emit_vfp_multi_reg_pop): Likewise.
+	(thumb2_emit_ldrd_pop): Likewise.
+	(arm_expand_epilogue): Add misc REG_CFA notes.
+	(arm_unwind_emit): Skip REG_CFA_ADJUST_CFA and REG_CFA_RESTORE.
+
+	2013-05-30  Bernd Schmidt  <bernds@codesourcery.com>
+		    Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* config/arm/arm-protos.h: Add and update function protos.
+	* config/arm/arm.c (use_simple_return_p): New added.
+	(thumb2_expand_return): Check simple_return flag.
+	* config/arm/arm.md: Add simple_return and conditional simple_return.
+	* config/arm/iterators.md: Add iterator for return and simple_return.
+	* gcc.dg/shrink-wrap-alloca.c: New added.
+	* gcc.dg/shrink-wrap-pretend.c: New added.
+	* gcc.dg/shrink-wrap-sibcall.c: New added.
+
+2013-06-06  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	Backport from mainline r198879:
+
+	2013-05-14  Chung-Lin Tang  <cltang@codesourcery.com>
+	PR target/42017
+	* config/arm/arm.h (EPILOGUE_USES): Only return true
+	for LR_REGNUM after epilogue_completed.
+
+2013-06-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199652,199653,199656,199657,199658.
+
+	2013-06-04  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.md (*mov<mode>_aarch64): Call
+	into function to generate MOVI instruction.
+	* config/aarch64/aarch64.c (aarch64_simd_container_mode):
+	New function.
+	(aarch64_preferred_simd_mode): Turn into wrapper.
+	(aarch64_output_scalar_simd_mov_immediate): New function.
+	* config/aarch64/aarch64-protos.h: Add prototype for above.
+
+	2013-06-04  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.c (simd_immediate_info): Remove
+	element_char member.
+	(sizetochar): Return signed char.
+	(aarch64_simd_valid_immediate): Remove elchar and other
+	unnecessary variables.
+	(aarch64_output_simd_mov_immediate): Take rtx instead of &rtx.
+	Calculate element_char as required.
+	* config/aarch64/aarch64-protos.h: Update and move prototype
+	for aarch64_output_simd_mov_immediate.
+	* config/aarch64/aarch64-simd.md (*aarch64_simd_mov<mode>):
+	Update arguments.
+
+	2013-06-04  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.c (simd_immediate_info): Struct to hold
+	information completed by aarch64_simd_valid_immediate.
+	(aarch64_legitimate_constant_p): Update arguments.
+	(aarch64_simd_valid_immediate): Work with struct rather than many
+	pointers.
+	(aarch64_simd_scalar_immediate_valid_for_move): Update arguments.
+	(aarch64_simd_make_constant): Update arguments.
+	(aarch64_output_simd_mov_immediate): Work with struct rather than
+	many pointers.  Output immediate directly rather than as operand.
+	* config/aarch64/aarch64-protos.h (aarch64_simd_valid_immediate):
+	Update prototype.
+	* config/aarch64/constraints.md (Dn): Update arguments.
+
+	2013-06-04  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_simd_valid_immediate): No
+	longer static.
+	(aarch64_simd_immediate_valid_for_move): Remove.
+	(aarch64_simd_scalar_immediate_valid_for_move): Update call.
+	(aarch64_simd_make_constant): Update call.
+	(aarch64_output_simd_mov_immediate): Update call.
+	* config/aarch64/aarch64-protos.h (aarch64_simd_valid_immediate):
+	Add prototype.
+	* config/aarch64/constraints.md (Dn): Update call.
+
+	2013-06-04  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_simd_valid_immediate): Change
+	return type to bool for prototype.
+	(aarch64_legitimate_constant_p): Check for true instead of not -1.
+	(aarch64_simd_valid_immediate): Fix up each return to return a bool.
+	(aarch64_simd_immediate_valid_for_move): Update retval for bool.
+
+2013-06-04  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199261.
+	2013-05-23  Christian Bruel  <christian.bruel@st.com>
+
+	PR debug/57351
+	* config/arm/arm.c (arm_dwarf_register_span): Do not use dbx number.
+
+2013-06-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk
+	r198890,199254,199259,199260,199293,199407,199408,199454,199544,199545.
+
+	2013-05-31  Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_load_symref_appropriately):
+	Remove un-necessary braces.
+
+	2013-05-31  Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_classify_symbol):
+	Use SYMBOL_TINY_ABSOLUTE for AARCH64_CMODEL_TINY_PIC.
+
+	2013-05-30  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.md (insv<mode>): New define_expand.
+	(*insv_reg<mode>): New define_insn.
+
+	2012-05-29  Chris Schlumberger-Socha <chris.schlumberger-socha@arm.com>
+	    Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64-protos.h (aarch64_symbol_type): Define
+	SYMBOL_TINY_ABSOLUTE.
+	* config/aarch64/aarch64.c (aarch64_load_symref_appropriately): Handle
+	SYMBOL_TINY_ABSOLUTE.
+	(aarch64_expand_mov_immediate): Likewise.
+	(aarch64_classify_symbol): Likewise.
+	(aarch64_mov_operand_p): Remove ATTRIBUTE_UNUSED.
+	Permit SYMBOL_TINY_ABSOLUTE.
+	* config/aarch64/predicates.md (aarch64_mov_operand): Permit CONST.
+
+	2013-05-29  Chris Schlumberger-Socha <chris.schlumberger-socha@arm.com>
+	    Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_classify_symbol): Remove comment.
+	Refactor if/switch.  Replace gcc_assert with if.
+
+	2013-05-24  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.c (aarch64_print_operand): Change the
+	X format specifier to only display bottom 16 bits.
+	* config/aarch64/aarch64.md (insv_imm<mode>): Allow any size of
+	immediate to match for operand 2, since it will be masked.
+
+	2013-05-23  Chris Schlumberger-Socha <chris.schlumberger-socha@arm.com>
+	    Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64.md (*movdi_aarch64): Replace Usa with S.
+	* config/aarch64/constraints.md (Usa): Remove.
+	* doc/md.texi (AArch64 Usa): Remove.
+
+	2013-05-23  Chris Schlumberger-Socha <chris.schlumberger-socha@arm.com>
+	    Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* config/aarch64/aarch64-protos.h (aarch64_mov_operand_p): Define.
+	* config/aarch64/aarch64.c (aarch64_mov_operand_p): Define.
+	* config/aarch64/predicates.md (aarch64_const_address): Remove.
+	(aarch64_mov_operand): Use aarch64_mov_operand_p.
+
+	2013-05-23  Vidya Praveen <vidyapraveen@arm.com>
+
+	* config/aarch64/aarch64-simd.md (clzv4si2): Support for CLZ
+	instruction (AdvSIMD).
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_builtin_vectorized_function): Handler for BUILT_IN_CLZ.
+	* config/aarch64/aarch-simd-builtins.def: Entry for CLZ.
+
+	2013-05-14  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd.md
+	(aarch64_vcond_internal<mode>): Rename to...
+	(aarch64_vcond_internal<mode><mode>): ...This, for integer modes.
+	(aarch64_vcond_internal<VDQF_COND:mode><VDQF:mode>): ...This for
+	float modes. Clarify all iterator modes.
+	(vcond<mode><mode>): Use new name for vcond expanders.
+	(vcond<v_cmp_result><mode>): Likewise.
+	(vcondu<mode><mode>: Likewise.
+	* config/aarch64/iterators.md (VDQF_COND): New.
+
+2013-05-29  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198928,198973,199203.
+	2013-05-22  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	PR target/57340
+	* config/arm/arm.c (any_sibcall_uses_r3): Rename to ..
+	(any_sibcall_could_use_r3): this and handle indirect calls.
+	(arm_get_frame_offsets): Rename use of any_sibcall_uses_r3.
+
+	2013-05-16  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	* config/arm/arm.c (arm_function_ok_for_sibcall): Add check
+	for NULL decl.
+
+	2013-05-15  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	* config/arm/predicates.md (call_insn_operand): New predicate.
+	* config/arm/constraints.md ("Cs", "Ss"):  New constraints.
+	* config/arm/arm.md (*call_insn, *call_value_insn): Match only
+	if insn is not a tail call.
+	(*sibcall_insn, *sibcall_value_insn): Adjust for tailcalling through
+	registers.
+	* config/arm/arm.h (enum reg_class): New caller save register class.
+	(REG_CLASS_NAMES): Likewise.
+	(REG_CLASS_CONTENTS): Likewise.
+	* config/arm/arm.c (arm_function_ok_for_sibcall): Allow tailcalling
+	without decls.
+
+2013-05-28  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198680.
+	2013-05-07  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64-simd.md (*aarch64_simd_mov<mode>): call splitter.
+	(aarch64_simd_mov<mode>): New expander.
+	(aarch64_simd_mov_to_<mode>low): New instruction pattern.
+	(aarch64_simd_mov_to_<mode>high): Likewise.
+	(aarch64_simd_mov_from_<mode>low): Likewise.
+	(aarch64_simd_mov_from_<mode>high): Likewise.
+	(aarch64_dup_lane<mode>): Update.
+	(aarch64_dup_lanedi): New instruction pattern.
+	* config/aarch64/aarch64-protos.h (aarch64_split_simd_move): New prototype.
+	* config/aarch64/aarch64.c (aarch64_split_simd_move): New function.
+
+2013-05-28  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198497-198500.
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_gimple_fold_builtin.c): Fold more modes for reduc_splus_.
+	* config/aarch64/aarch64-simd-builtins.def
+	(reduc_splus_): Add new modes.
+	(reduc_uplus_): New.
+	* config/aarch64/aarch64-simd.md (aarch64_addvv4sf): Remove.
+	(reduc_uplus_v4sf): Likewise.
+	(reduc_splus_v4sf): Likewise.
+	(aarch64_addv<mode>): Likewise.
+	(reduc_uplus_<mode>): Likewise.
+	(reduc_splus_<mode>): Likewise.
+	(aarch64_addvv2di): Likewise.
+	(reduc_uplus_v2di): Likewise.
+	(reduc_splus_v2di): Likewise.
+	(aarch64_addvv2si): Likewise.
+	(reduc_uplus_v2si): Likewise.
+	(reduc_splus_v2si): Likewise.
+	(reduc_<sur>plus_<mode>): New.
+	(reduc_<sur>plus_v2di): Likewise.
+	(reduc_<sur>plus_v2si): Likewise.
+	(reduc_<sur>plus_v4sf): Likewise.
+	(aarch64_addpv4sf): Likewise.
+	* config/aarch64/arm_neon.h
+	(vaddv<q>_<s,u,f><8, 16, 32, 64): Rewrite using builtins.
+	* config/aarch64/iterators.md (unspec): Remove UNSPEC_ADDV,
+	add UNSPEC_SADDV, UNSPEC_UADDV.
+	(SUADDV): New.
+	(sur): Add UNSPEC_SADDV, UNSPEC_UADDV.
+
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/arm_neon.h
+	(v<max,min><nm><q><v>_<sfu><8, 16, 32, 64>): Rewrite using builtins.
+
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins
+	(aarch64_gimple_fold_builtin): Fold reduc_<su><maxmin>_ builtins.
+
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd-builtins.def
+	(reduc_smax_): New.
+	(reduc_smin_): Likewise.
+	(reduc_umax_): Likewise.
+	(reduc_umin_): Likewise.
+	(reduc_smax_nan_): Likewise.
+	(reduc_smin_nan_): Likewise.
+	(fmax): Remove.
+	(fmin): Likewise.
+	(smax): Update for V2SF, V4SF and V2DF modes.
+	(smin): Likewise.
+	(smax_nan): New.
+	(smin_nan): Likewise.
+	* config/aarch64/aarch64-simd.md (<maxmin><mode>3): Rename to...
+	(<su><maxmin><mode>3): ...This, refactor.
+	(s<maxmin><mode>3): New.
+	(<maxmin_uns><mode>3): Likewise.
+	(reduc_<maxmin_uns>_<mode>): Refactor.
+	(reduc_<maxmin_uns>_v4sf): Likewise.
+	(reduc_<maxmin_uns>_v2si): Likewise.
+	(aarch64_<fmaxmin><mode>: Remove.
+	* config/aarch64/arm_neon.h (vmax<q>_f<32,64>): Rewrite to use
+	new builtin names.
+	(vmin<q>_f<32,64>): Likewise.
+	* config/iterators.md (unspec): Add UNSPEC_FMAXNMV, UNSPEC_FMINNMV.
+	(FMAXMIN): New.
+	(su): Add mappings for smax, smin, umax, umin.
+	(maxmin): New.
+	(FMAXMINV): Add UNSPEC_FMAXNMV, UNSPEC_FMINNMV.
+	(FMAXMIN): Rename as...
+	(FMAXMIN_UNS): ...This.
+	(maxminv): Remove.
+	(fmaxminv): Likewise.
+	(fmaxmin): Likewise.
+	(maxmin_uns): New.
+	(maxmin_uns_op): Likewise.
+
+2013-05-28  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199241.
+	2013-05-23  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd.md
+	(aarch64_cm<optab>di): Add clobber of CC_REGNUM to unsplit pattern.
+
+2013-05-23  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198970.
+	2013-05-16  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm-protos.h (gen_movmem_ldrd_strd): New declaration.
+	* config/arm/arm.c (next_consecutive_mem): New function.
+	(gen_movmem_ldrd_strd): Likewise.
+	* config/arm/arm.md (movmemqi): Update condition and code.
+	(unaligned_loaddi, unaligned_storedi): New patterns.
+
+2013-05-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	* LINARO-VERSION: Bump version number.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+	* LINARO-VERSION: Update.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198677.
+	2013-05-07  Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64.md
+	(cmp_swp_<optab><ALLX:mode>_shft_<GPI:mode>): Restrict the
+	shift value between 0-4.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198574-198575.
+	2013-05-03  Vidya Praveen  <vidyapraveen@arm.com>
+
+	* config/aarch64/aarch64-simd.md (simd_fabd): Correct the description.
+
+	2013-05-03  Vidya Praveen  <vidyapraveen@arm.com>
+
+	* config/aarch64/aarch64-simd.md (*fabd_scalar<mode>3): Support
+	scalar form of FABD instruction.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198490-198496
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/arm_neon.h
+	(vac<ge, gt><sd>_f<32, 64>): Rename to...
+	(vca<ge, gt><sd>_f<32, 64>): ...this, reimpliment in C.
+	(vca<ge, gt, lt, le><q>_f<32, 64>): Reimpliment in C.
+
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd.md (*aarch64_fac<optab><mode>): New.
+	* config/aarch64/iterators.md (FAC_COMPARISONS): New.
+
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd.md
+	(vcond<mode>_internal): Handle special cases for constant masks.
+	(vcond<mode><mode>): Allow nonmemory_operands for outcome vectors.
+	(vcondu<mode><mode>): Likewise.
+	(vcond<v_cmp_result><mode>): New.
+
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c (BUILTIN_VALLDI): Define.
+	(aarch64_fold_builtin): Add folding for cm<eq,ge,gt,tst>.
+	* config/aarch64/aarch64-simd-builtins.def
+	(cmeq): Update to BUILTIN_VALLDI.
+	(cmgt): Likewise.
+	(cmge): Likewise.
+	(cmle): Likewise.
+	(cmlt): Likewise.
+	* config/aarch64/arm_neon.h
+	(vc<eq, lt, le, gt, ge, tst><z><qsd>_<fpsu><8,16,32,64>): Remap
+	to builtins or C as appropriate.
+
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd-builtins.def (cmhs): Rename to...
+	(cmgeu): ...This.
+	(cmhi): Rename to...
+	(cmgtu): ...This.
+	* config/aarch64/aarch64-simd.md
+	(simd_mode): Add SF.
+	(aarch64_vcond_internal): Use new names for unsigned comparison insns.
+	(aarch64_cm<optab><mode>): Rewrite to not use UNSPECs.
+	* config/aarch64/aarch64.md (*cstore<mode>_neg): Rename to...
+	(cstore<mode>_neg): ...This.
+	* config/aarch64/iterators.md
+	(VALLF): new.
+	(unspec): Remove UNSPEC_CM<EQ, LE, LT, GE, GT, HS, HI, TST>.
+	(COMPARISONS): New.
+	(UCOMPARISONS): Likewise.
+	(optab): Add missing comparisons.
+	(n_optab): New.
+	(cmp_1): Likewise.
+	(cmp_2): Likewise.
+	(CMP): Likewise.
+	(cmp): Remove.
+	(VCMP_S): Likewise.
+	(VCMP_U): Likewise.
+	(V_cmp_result): Add DF, SF modes.
+	(v_cmp_result): Likewise.
+	(v): Likewise.
+	(vmtype): Likewise.
+	* config/aarch64/predicates.md (aarch64_reg_or_fp_zero): New.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198191.
+	2013-04-23  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64.md (*mov<mode>_aarch64): Add simd attribute.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@lianro.org>
+
+	Backport from trunk r197838.
+	2013-04-11   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64.c (aarch64_select_cc_mode): Allow NEG
+	code in CC_NZ mode.
+	* config/aarch64/aarch64.md (*neg_<shift><mode>3_compare0): New
+	pattern.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198019.
+	2013-04-16   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64.md (*adds_mul_imm_<mode>): New pattern.
+	(*subs_mul_imm_<mode>): New pattern.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198424-198425.
+	2013-04-29  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.md (movsi_aarch64): Support LDR/STR
+	from/to S register.
+	(movdi_aarch64): Support LDR/STR from/to D register.
+
+	2013-04-29  Ian Bolton  <ian.bolton@arm.com>
+
+	* common/config/aarch64/aarch64-common.c: Enable REE pass at O2
+	or higher by default.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198412.
+	2013-04-29  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (store_minmaxsi): Use only when
+	optimize_insn_for_size_p.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk 198394,198396-198400,198402-198404.
+	2013-04-29  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/arm_neon.h
+	(vcvt<sd>_f<32,64>_s<32,64>): Rewrite in C.
+	(vcvt<q>_f<32,64>_s<32,64>): Rewrite using builtins.
+	(vcvt_<high_>_f<32,64>_f<32,64>): Likewise.
+	(vcvt<qsd>_<su><32,64>_f<32,64>): Likewise.
+	(vcvta<qsd>_<su><32,64>_f<32,64>): Likewise.
+	(vcvtm<qsd>_<su><32,64>_f<32,64>): Likewise.
+	(vcvtn<qsd>_<su><32,64>_f<32,64>): Likewise.
+	(vcvtp<qsd>_<su><32,64>_f<32,64>): Likewise.
+
+	2013-04-29  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd.md
+	(<optab><VDQF:mode><fcvt_target>2): New, maps to fix, fixuns.
+	(<fix_trunc_optab><VDQF:mode><fcvt_target>2): New, maps to
+	fix_trunc, fixuns_trunc.
+	(ftrunc<VDQF:mode>2): New.
+	* config/aarch64/iterators.md (optab): Add fix, fixuns.
+	(fix_trunc_optab): New.
+
+	2013-04-29  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_builtin_vectorized_function): Vectorize over ifloorf,
+	iceilf, lround, iroundf.
+
+	2013-04-29  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd-builtins.def (vec_unpacks_hi_): New.
+	(float_truncate_hi_): Likewise.
+	(float_extend_lo_): Likewise.
+	(float_truncate_lo_): Likewise.
+	* config/aarch64/aarch64-simd.md (vec_unpacks_lo_v4sf): New.
+	(aarch64_float_extend_lo_v2df): Likewise.
+	(vec_unpacks_hi_v4sf): Likewise.
+	(aarch64_float_truncate_lo_v2sf): Likewise.
+	(aarch64_float_truncate_hi_v4sf): Likewise.
+	(vec_pack_trunc_v2df): Likewise.
+	(vec_pack_trunc_df): Likewise.
+
+	2013-04-29  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_fold_builtin): Fold float conversions.
+	* config/aarch64/aarch64-simd-builtins.def
+	(floatv2si, floatv4si, floatv2di): New.
+	(floatunsv2si, floatunsv4si, floatunsv2di): Likewise.
+	* config/aarch64/aarch64-simd.md
+	(<optab><fcvt_target><VDQF:mode>2): New, expands to float and floatuns.
+	* config/aarch64/iterators.md (FLOATUORS): New.
+	(optab): Add float, floatuns.
+	(su_optab): Likewise.
+
+	2013-04-29  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_builtin_vectorized_function): Fold to standard pattern names.
+	* config/aarch64/aarch64-simd-builtins.def (frintn): New.
+	(frintz): Rename to...
+	(btrunc): ...this.
+	(frintp): Rename to...
+	(ceil): ...this.
+	(frintm): Rename to...
+	(floor): ...this.
+	(frinti): Rename to...
+	(nearbyint): ...this.
+	(frintx): Rename to...
+	(rint): ...this.
+	(frinta): Rename to...
+	(round): ...this.
+	* config/aarch64/aarch64-simd.md
+	(aarch64_frint<frint_suffix><mode>): Delete.
+	(<frint_pattern><mode>2): Convert to insn.
+	* config/aarch64/aarch64.md (unspec): Add UNSPEC_FRINTN.
+	* config/aarch64/iterators.md (FRINT): Add UNSPEC_FRINTN.
+	(frint_pattern): Likewise.
+	(frint_suffix): Likewise.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198302-198306,198316.
+	2013-04-25  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd.md
+	(aarch64_simd_bsl<mode>_internal): Rewrite RTL to not use UNSPEC_BSL.
+	(aarch64_simd_bsl<mode>): Likewise.
+	* config/aarch64/iterators.md (unspec): Remove UNSPEC_BSL.
+
+	2013-04-25  James Greenhalgh  <jame.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-simd.md (neg<mode>2): Use VDQ iterator.
+
+	2013-04-25  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_fold_builtin): New.
+	* config/aarch64/aarch64-protos.h (aarch64_fold_builtin): New.
+	* config/aarch64/aarch64.c (TARGET_FOLD_BUILTIN): Define.
+	* config/aarch64/aarch64-simd-builtins.def (abs): New.
+	* config/aarch64/arm_neon.h
+	(vabs<q>_<f32, 64>): Implement using __builtin_aarch64_fabs.
+
+	2013-04-25  James Greenhalgh  <james.greenhalgh@arm.com>
+	    Tejas Belagod  <tejas.belagod@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_gimple_fold_builtin): New.
+	* config/aarch64/aarch64-protos.h (aarch64_gimple_fold_builtin): New.
+	* config/aarch64/aarch64-simd-builtins.def (addv): New.
+	* config/aarch64/aarch64-simd.md (addpv4sf): New.
+	(addvv4sf): Update.
+	* config/aarch64/aarch64.c (TARGET_GIMPLE_FOLD_BUILTIN): Define.
+
+	2013-04-25  Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64.md 
+	(*cmp_swp_<optab><ALLX:mode>_shft_<GPI:mode>): New pattern.
+
+	2013-04-25  Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64.md (*ngc<mode>): New pattern.
+	(*ngcsi_uxtw): New pattern.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk 198298.
+	2013-04-25  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+	    Julian Brown  <julian@codesourcery.com>
+
+	* config/arm/arm.c (neon_builtin_type_mode): Add T_V4HF.
+	(TB_DREG): Add T_V4HF.
+	(v4hf_UP): New macro.
+	(neon_itype): Add NEON_FLOAT_WIDEN, NEON_FLOAT_NARROW.
+	(arm_init_neon_builtins): Handle NEON_FLOAT_WIDEN,
+	NEON_FLOAT_NARROW.
+	Handle initialisation of V4HF. Adjust initialisation of reinterpret
+	built-ins.
+	(arm_expand_neon_builtin): Handle NEON_FLOAT_WIDEN,
+	NEON_FLOAT_NARROW.
+	(arm_vector_mode_supported_p): Handle V4HF.
+	(arm_mangle_map): Handle V4HFmode.
+	* config/arm/arm.h (VALID_NEON_DREG_MODE): Add V4HF.
+	* config/arm/arm_neon_builtins.def: Add entries for
+	vcvtv4hfv4sf, vcvtv4sfv4hf.
+	* config/arm/neon.md (neon_vcvtv4sfv4hf): New pattern.
+	(neon_vcvtv4hfv4sf): Likewise.
+	* config/arm/neon-gen.ml: Handle half-precision floating point
+	features.
+	* config/arm/neon-testgen.ml: Handle Requires_FP_bit feature.
+	* config/arm/arm_neon.h: Regenerate.
+	* config/arm/neon.ml (type elts): Add F16.
+	(type vectype): Add T_float16x4, T_floatHF.
+	(type vecmode): Add V4HF.
+	(type features): Add Requires_FP_bit feature.
+	(elt_width): Handle F16.
+	(elt_class): Likewise.
+	(elt_of_class_width): Likewise.
+	(mode_of_elt): Refactor.
+	(type_for_elt): Handle F16, fix error messages.
+	(vectype_size): Handle T_float16x4.
+	(vcvt_sh): New function.
+	(ops): Add entries for vcvt_f16_f32, vcvt_f32_f16.
+	(string_of_vectype): Handle T_floatHF, T_float16, T_float16x4.
+	(string_of_mode): Handle V4HF.
+	* doc/arm-neon-intrinsics.texi: Regenerate.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198136-198137,198142,198176.
+	2013-04-23  Andreas Schwab  <schwab@linux-m68k.org>
+
+	* coretypes.h (gimple_stmt_iterator): Add struct to make
+	compatible with C.
+
+	2013-04-22  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* coretypes.h (gimple_stmt_iterator_d): Forward declare.
+	(gimple_stmt_iterator): New typedef.
+	* gimple.h (gimple_stmt_iterator): Rename to...
+	(gimple_stmt_iterator_d): ... This.
+	* doc/tm.texi.in (TARGET_FOLD_BUILTIN): Detail restriction that
+	trees be valid for GIMPLE and GENERIC.
+	(TARGET_GIMPLE_FOLD_BUILTIN): New.
+	* gimple-fold.c (gimple_fold_call): Call target hook
+	gimple_fold_builtin.
+	* hooks.c (hook_bool_gsiptr_false): New.
+	* hooks.h (hook_bool_gsiptr_false): New.
+	* target.def (fold_stmt): New.
+	* doc/tm.texi: Regenerate.
+
+	2013-04-22  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(CF): Remove.
+	(CF0, CF1, CF2, CF3, CF4, CF10): New.
+	(VAR<1-12>): Add MAP parameter.
+	(BUILTIN_*): Likewise.
+	* config/aarch64/aarch64-simd-builtins.def: Set MAP parameter.
+	* config/aarch64/aarch64-simd.md (aarch64_sshl_n<mode>): Remove.
+	(aarch64_ushl_n<mode>): Likewise.
+	(aarch64_sshr_n<mode>): Likewise.
+	(aarch64_ushr_n<mode>): Likewise.
+	(aarch64_<maxmin><mode>): Likewise.
+	(aarch64_sqrt<mode>): Likewise.
+	* config/aarch64/arm_neon.h (vshl<q>_n_*): Use new builtin names.
+	(vshr<q>_n_*): Likewise.
+
+	2013-04-22  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* config/aarch64/aarch64-builtins.c
+	(aarch64_simd_builtin_type_mode): Handle SF types.
+	(sf_UP): Define.
+	(BUILTIN_GPF): Define.
+	(aarch64_init_simd_builtins): Handle SF types.
+	* config/aarch64/aarch64-simd-builtins.def (frecpe): Add support.
+	(frecps): Likewise.
+	(frecpx): Likewise.
+	* config/aarch64/aarch64-simd.md
+	(simd_types): Update simd_frcp<esx> to simd_frecp<esx>.
+	(aarch64_frecpe<mode>): New.
+	(aarch64_frecps<mode>): Likewise.
+	* config/aarch64/aarch64.md (unspec): Add UNSPEC_FRECP<ESX>.
+	(v8type): Add frecp<esx>.
+	(aarch64_frecp<FRECP:frecp_suffix><mode>): New.
+	(aarch64_frecps<mode>): Likewise.
+	* config/aarch64/iterators.md (FRECP): New.
+	(frecp_suffix): Likewise.
+	* config/aarch64/arm_neon.h
+	(vrecp<esx><qsd>_<fd><32, 64>): Convert to using builtins.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198030.
+	2013-04-17  Greta Yorsh  <Greta.Yorsh at arm.com>
+
+	* config/arm/arm.md (movsicc_insn): Convert define_insn into
+	define_insn_and_split.
+	(and_scc,ior_scc,negscc): Likewise.
+	(cmpsi2_addneg, subsi3_compare): Convert to named patterns.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198020.
+	2013-04-16   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64.md (*adds_<optab><mode>_multp2):
+	New pattern.
+	(*subs_<optab><mode>_multp2): New pattern.
+	(*adds_<optab><ALLX:mode>_<GPI:mode>): New pattern.
+	(*subs_<optab><ALLX:mode>_<GPI:mode>): New pattern.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198004,198029.
+	2013-04-17  Greta Yorsh  <Greta.Yorsh at arm.com>
+
+	* config/arm/arm.c (use_return_insn): Return 0 for targets that
+	can benefit from using a sequence of LDRD instructions in epilogue
+	instead of a single LDM instruction.
+
+	2013-04-16  Greta Yorsh  <Greta.Yorsh at arm.com>
+
+	* config/arm/arm.c (emit_multi_reg_push): New declaration
+	for an existing function.
+	(arm_emit_strd_push): New function.
+	(arm_expand_prologue): Used here.
+	(arm_emit_ldrd_pop): New function.
+	(arm_expand_epilogue): Used here.
+	(arm_get_frame_offsets): Update condition.
+	(arm_emit_multi_reg_pop): Add a special case for load of a single
+	register with writeback.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197965.
+	2013-04-15  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.c (const_ok_for_dimode_op): Handle AND case.
+	* config/arm/arm.md (*anddi3_insn): Change to insn_and_split.
+	* config/arm/constraints.md (De): New constraint.
+	* config/arm/neon.md (anddi3_neon): Delete.
+	(neon_vand<mode>): Expand to standard anddi3 pattern.
+	* config/arm/predicates.md (imm_for_neon_inv_logic_operand):
+	Move earlier in the file.
+	(neon_inv_logic_op2): Likewise.
+	(arm_anddi_operand_neon): New predicate.
+
+2013-05-02  Matthew Gretton-Dann <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197925.
+	2013-04-12  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (mov_scc,mov_negscc,mov_notscc): Convert
+	define_insn into define_insn_and_split and emit movsicc patterns.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197807.
+	2013-04-11   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64.h (REVERSIBLE_CC_MODE): Define.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197642.
+	2013-04-09  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (minmax_arithsi_non_canon): New pattern.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197530,197921.
+	2013-04-12  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.c (gen_operands_ldrd_strd): Initialize "base".
+
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/constraints.md (q): New constraint.
+	* config/arm/ldrdstrd.md: New file.
+	* config/arm/arm.md (ldrdstrd.md) New include.
+	(arm_movdi): Use "q" instead of "r" constraint
+	for double-word memory access.
+	(movdf_soft_insn): Likewise.
+	* config/arm/vfp.md (movdi_vfp): Likewise.
+	* config/arm/t-arm (MD_INCLUDES): Add ldrdstrd.md.
+	* config/arm/arm-protos.h (gen_operands_ldrd_strd): New declaration.
+	* config/arm/arm.c (gen_operands_ldrd_strd): New function.
+	(mem_ok_for_ldrd_strd): Likewise.
+	(output_move_double): Update assertion.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport of trunk r197518-197522,197526-197528.
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (arm_smax_insn): Convert define_insn into
+	define_insn_and_split.
+	(arm_smin_insn,arm_umaxsi3,arm_uminsi3): Likewise.
+
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (arm_ashldi3_1bit): Convert define_insn into
+	define_insn_and_split.
+	(arm_ashrdi3_1bit,arm_lshrdi3_1bit): Likewise.
+	(shiftsi3_compare): New pattern.
+	(rrx): New pattern.
+	* config/arm/unspecs.md (UNSPEC_RRX): New.
+
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (negdi_extendsidi): New pattern.
+	(negdi_zero_extendsidi): Likewise.
+
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (andsi_iorsi3_notsi): Convert define_insn into
+	define_insn_and_split.
+	(arm_negdi2,arm_abssi2,arm_neg_abssi2): Likewise.
+	(arm_cmpdi_insn,arm_cmpdi_unsigned): Likewise.
+
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (arm_subdi3): Convert define_insn into
+	define_insn_and_split.
+	(subdi_di_zesidi,subdi_di_sesidi): Likewise.
+	(subdi_zesidi_di,subdi_sesidi_di,subdi_zesidi_zesidi): Likewise.
+
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (subsi3_carryin): New pattern.
+	(subsi3_carryin_const): Likewise.
+	(subsi3_carryin_compare,subsi3_carryin_compare_const): Likewise.
+	(subsi3_carryin_shift,rsbsi3_carryin_shift): Likewise.
+
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (incscc,arm_incscc,decscc,arm_decscc): Delete.
+
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* config/arm/arm.md (addsi3_carryin_<optab>): Set attribute predicable.
+	(addsi3_carryin_alt2_<optab>,addsi3_carryin_shift_<optab>): Likewise.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport of trunk r197517.
+	2013-04-05  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.c (arm_expand_builtin): Change fcode
+	type to unsigned int.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport of trunk r197513.
+	2013-04-05  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	* doc/invoke.texi (ARM Options): Document cortex-a53 support.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport of trunk r197489-197491.
+	2013-04-04  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm-protos.h (arm_builtin_vectorized_function):
+	New function prototype.
+	* config/arm/arm.c (TARGET_VECTORIZE_BUILTINS): Define.
+	(TARGET_VECTORIZE_BUILTIN_VECTORIZED_FUNCTION): Likewise.
+	(arm_builtin_vectorized_function): New function.
+
+	2013-04-04  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm_neon_builtins.def: New file.
+	* config/arm/arm.c (neon_builtin_data): Move contents to
+	arm_neon_builtins.def.
+	(enum arm_builtins): Include neon builtin definitions.
+	(ARM_BUILTIN_NEON_BASE): Move from enum to macro.
+	* config/arm/t-arm (arm.o): Add dependency on
+	arm_neon_builtins.def.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport of trunk 196795-196797,196957
+	2013-03-19  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.md (*sub<mode>3_carryin): New pattern.
+	(*subsi3_carryin_uxtw): Likewise.
+
+	2013-03-19  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.md (*ror<mode>3_insn): New pattern.
+	(*rorsi3_insn_uxtw): Likewise.
+
+	2013-03-19  Ian Bolton  <ian.bolton@arm.com>
+
+	* config/aarch64/aarch64.md (*extr<mode>5_insn): New pattern.
+	(*extrsi5_insn_uxtw): Likewise.
+
+2013-04-10  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	* LINARO-VERSION: Bump version number.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
+	* LINARO-VERSION: New file.
+	* configure.ac: Add Linaro version string.
+	* configure: Regenerate.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport of trunk r197346.
+	2013-04-02  Ian Caulfield  <ian.caulfield@arm.com>
+	    Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	* config/arm/arm-arches.def (armv8-a): Default to cortex-a53.
+	* config/arm/t-arm (MD_INCLUDES): Depend on cortex-a53.md.
+	* config/arm/cortex-a53.md: New file.
+	* config/arm/bpabi.h (BE8_LINK_SPEC): Handle cortex-a53.
+	* config/arm/arm.md (generic_sched, generic_vfp): Handle cortex-a53.
+	* config/arm/arm.c (arm_issue_rate): Likewise.
+	* config/arm/arm-tune.md: Regenerate
+	* config/arm/arm-tables.opt: Regenerate.
+	* config/arm/arm-cores.def: Add cortex-a53.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport of trunk r197342.
+	2013-04-02  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64.md (*mov<mode>_aarch64): Add variants for
+	scalar load/store operations using B/H registers.
+	(*zero_extend<SHORT:mode><GPI:mode>2_aarch64): Likewise.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport of trunk r197341.
+	2013-04-02  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* config/aarch64/aarch64.md (*mov<mode>_aarch64): Add alternatives for
+	scalar move.
+	* config/aarch64/aarch64.c
+	(aarch64_simd_scalar_immediate_valid_for_move): New.
+	* config/aarch64/aarch64-protos.h
+	(aarch64_simd_scalar_immediate_valid_for_move): New.
+	* config/aarch64/constraints.md (Dh, Dq): New.
+	* config/aarch64/iterators.md (hq): New.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197207.
+	2013-03-28   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64.md (*and<mode>3_compare0): New pattern.
+	(*andsi3_compare0_uxtw): New pattern.
+	(*and_<SHIFT:optab><mode>3_compare0): New pattern.
+	(*and_<SHIFT:optab>si3_compare0_uxtw): New pattern.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197153.
+	2013-03-27  Terry Guo  <terry.guo@arm.com>
+
+	* config/arm/arm-cores.def: Added core cortex-r7.
+	* config/arm/arm-tune.md: Regenerated.
+	* config/arm/arm-tables.opt: Regenerated.
+	* doc/invoke.texi: Added entry for core cortex-r7.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197052.
+	2013-03-25  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.md (f_sels, f_seld): New types.
+	(*cmov<mode>): New pattern.
+	* config/arm/predicates.md (arm_vsel_comparison_operator): New
+	predicate.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197046.
+	2013-03-25  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* config/arm/arm.c (arm_emit_load_exclusive): Add acq parameter.
+	Emit load-acquire versions when acq is true.
+	(arm_emit_store_exclusive): Add rel parameter.
+	Emit store-release versions when rel is true.
+	(arm_split_compare_and_swap): Use acquire-release instructions
+	instead.
+	of barriers when appropriate.
+	(arm_split_atomic_op): Likewise.
+	* config/arm/arm.h (TARGET_HAVE_LDACQ): New macro.
+	* config/arm/unspecs.md (VUNSPEC_LAX): New unspec.
+	(VUNSPEC_SLX): Likewise.
+	(VUNSPEC_LDA): Likewise.
+	(VUNSPEC_STL): Likewise.
+	* config/arm/sync.md (atomic_load<mode>): New pattern.
+	(atomic_store<mode>): Likewise.
+	(arm_load_acquire_exclusive<mode>): Likewise.
+	(arm_load_acquire_exclusivesi): Likewise.
+	(arm_load_acquire_exclusivedi): Likewise.
+	(arm_store_release_exclusive<mode>): Likewise.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r196876.
+	2013-03-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* config/arm/arm-protos.h (tune_params): Add
+	prefer_neon_for_64bits field.
+	* config/arm/arm.c (prefer_neon_for_64bits): New variable.
+	(arm_slowmul_tune): Default prefer_neon_for_64bits to false.
+	(arm_fastmul_tune, arm_strongarm_tune, arm_xscale_tune): Ditto.
+	(arm_9e_tune, arm_v6t2_tune, arm_cortex_tune): Ditto.
+	(arm_cortex_a15_tune, arm_cortex_a5_tune): Ditto.
+	(arm_cortex_a9_tune, arm_v6m_tune, arm_fa726te_tune): Ditto.
+	(arm_option_override): Handle -mneon-for-64bits new option.
+	* config/arm/arm.h (TARGET_PREFER_NEON_64BITS): New macro.
+	(prefer_neon_for_64bits): Declare new variable.
+	* config/arm/arm.md (arch): Rename neon_onlya8 and neon_nota8 to
+	avoid_neon_for_64bits and neon_for_64bits. Remove onlya8 and
+	nota8.
+	(arch_enabled): Handle new arch types. Remove support for onlya8
+	and nota8.
+	(one_cmpldi2): Use new arch names.
+	* config/arm/arm.opt (mneon-for-64bits): Add option.
+	* config/arm/neon.md (adddi3_neon, subdi3_neon, iordi3_neon)
+	(anddi3_neon, xordi3_neon, ashldi3_neon, <shift>di3_neon): Use
+	neon_for_64bits instead of nota8 and avoid_neon_for_64bits instead
+	of onlya8.
+	* doc/invoke.texi (-mneon-for-64bits): Document.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r196858.
+	2013-03-21   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64-simd.md (simd_fabd): New Attribute.
+	(abd<mode>_3): New pattern.
+	(aba<mode>_3): New pattern.
+	(fabd<mode>_3): New pattern.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r196856.
+	2013-03-21   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* config/aarch64/aarch64-elf.h (REGISTER_PREFIX): Remove.
+	* config/aarch64/aarch64.c (aarch64_print_operand): Remove all
+	occurrence of REGISTER_PREFIX as its empty string.
Index: b/src/gcc/testsuite/gcc.target/arm/vect-rounding-floorf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vect-rounding-floorf.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_neon_ok } */
+/* { dg-options "-O2 -ffast-math -ftree-vectorize" } */
+/* { dg-add-options arm_v8_neon } */
+
+#define N 32
+
+void
+foo (float *output, float *input)
+{
+  int i = 0;
+  /* Vectorizable.  */
+  for (i = 0; i < N; i++)
+    output[i] = __builtin_floorf (input[i]);
+}
+
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 1 "vect" { target vect_call_floorf } } } */
+/* { dg-final { cleanup-tree-dump "vect" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vaesdq_u8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vaesdq_u8.c
@@ -0,0 +1,22 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint8x16_t a, b, c;
+  int i = 0;
+
+  for (i = 0; i < 16; ++i)
+    {
+      a[i] = i;
+      b[i] = 15 - i;
+    }
+  c = vaesdq_u8 (a, b);
+  return c[0];
+}
+
+/* { dg-final { scan-assembler "aesd.8\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha256su0q_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha256su0q_u32.c
@@ -0,0 +1,17 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32x4_t a = {0xd, 0xe, 0xa, 0xd};
+  uint32x4_t b = {0, 1, 2, 3};
+
+  uint32x4_t res = vsha256su0q_u32 (a, b);
+  return res[0];
+}
+
+/* { dg-final { scan-assembler "sha256su0.32\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld1p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld1p64.c
@@ -0,0 +1,19 @@
+/* Test the `vld1p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld1p64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+
+  out_poly64x1_t = vld1_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+((\\\{\[dD\]\[0-9\]+\\\})|(\[dD\]\[0-9\]+)), \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vst4p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vst4p64.c
@@ -0,0 +1,20 @@
+/* Test the `vst4p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vst4p64 (void)
+{
+  poly64_t *arg0_poly64_t;
+  poly64x1x4_t arg1_poly64x1x4_t;
+
+  vst4_p64 (arg0_poly64_t, arg1_poly64x1x4_t);
+}
+
+/* { dg-final { scan-assembler "vst1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_u8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_u8.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_u8' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_u8 (void)
+{
+  poly128_t out_poly128_t;
+  uint8x16_t arg0_uint8x16_t;
+
+  out_poly128_t = vreinterpretq_p128_u8 (arg0_uint8x16_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_s32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_s32.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_s32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_s32 (void)
+{
+  poly128_t out_poly128_t;
+  int32x4_t arg0_int32x4_t;
+
+  out_poly128_t = vreinterpretq_p128_s32 (arg0_int32x4_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_u8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_u8.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_u8' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_u8 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  uint8x16_t arg0_uint8x16_t;
+
+  out_poly64x2_t = vreinterpretq_p64_u8 (arg0_uint8x16_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp8_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp8_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp8_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp8_p64 (void)
+{
+  poly8x16_t out_poly8x16_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_poly8x16_t = vreinterpretq_p8_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_s16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_s16.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_s16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_s16 (void)
+{
+  poly128_t out_poly128_t;
+  int16x8_t arg0_int16x8_t;
+
+  out_poly128_t = vreinterpretq_p128_s16 (arg0_int16x8_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld2p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld2p64.c
@@ -0,0 +1,19 @@
+/* Test the `vld2p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld2p64 (void)
+{
+  poly64x1x2_t out_poly64x1x2_t;
+
+  out_poly64x1x2_t = vld2_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_u64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_u64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_u64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_u64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  uint64x2_t arg0_uint64x2_t;
+
+  out_poly64x2_t = vreinterpretq_p64_u64 (arg0_uint64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld4_dupp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld4_dupp64.c
@@ -0,0 +1,19 @@
+/* Test the `vld4_dupp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld4_dupp64 (void)
+{
+  poly64x1x4_t out_poly64x1x4_t;
+
+  out_poly64x1x4_t = vld4_dup_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu8_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu8_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQu8_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQu8_p64 (void)
+{
+  uint8x16_t out_uint8x16_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_uint8x16_t = vreinterpretq_u8_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_p16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_p16.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_p16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_p16 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly16x8_t arg0_poly16x8_t;
+
+  out_poly64x2_t = vreinterpretq_p64_p16 (arg0_poly16x8_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vdupQ_np64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vdupQ_np64.c
@@ -0,0 +1,19 @@
+/* Test the `vdupQ_np64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vdupQ_np64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly64_t arg0_poly64_t;
+
+  out_poly64x2_t = vdupq_n_p64 (arg0_poly64_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs32_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs32_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQs32_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQs32_p64 (void)
+{
+  int32x4_t out_int32x4_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_int32x4_t = vreinterpretq_s32_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_u32.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_u32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_u32 (void)
+{
+  poly128_t out_poly128_t;
+  uint32x4_t arg0_uint32x4_t;
+
+  out_poly128_t = vreinterpretq_p128_u32 (arg0_uint32x4_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs64_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs64_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQs64_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQs64_p64 (void)
+{
+  int64x2_t out_int64x2_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_int64x2_t = vreinterpretq_s64_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld3p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld3p64.c
@@ -0,0 +1,19 @@
+/* Test the `vld3p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld3p64 (void)
+{
+  poly64x1x3_t out_poly64x1x3_t;
+
+  out_poly64x1x3_t = vld3_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu16_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu16_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQu16_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQu16_p128 (void)
+{
+  uint16x8_t out_uint16x8_t;
+  poly128_t arg0_poly128_t;
+
+  out_uint16x8_t = vreinterpretq_u16_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_u16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_u16.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_u16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_u16 (void)
+{
+  poly128_t out_poly128_t;
+  uint16x8_t arg0_uint16x8_t;
+
+  out_poly128_t = vreinterpretq_p128_u16 (arg0_uint16x8_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vcreatep64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vcreatep64.c
@@ -0,0 +1,19 @@
+/* Test the `vcreatep64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vcreatep64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  uint64_t arg0_uint64_t;
+
+  out_poly64x1_t = vcreate_p64 (arg0_uint64_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vdupQ_lanep64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vdupQ_lanep64.c
@@ -0,0 +1,19 @@
+/* Test the `vdupQ_lanep64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vdupQ_lanep64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_poly64x2_t = vdupq_lane_p64 (arg0_poly64x1_t, 0);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_f32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_f32.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_f32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_f32 (void)
+{
+  poly128_t out_poly128_t;
+  float32x4_t arg0_float32x4_t;
+
+  out_poly128_t = vreinterpretq_p128_f32 (arg0_float32x4_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vsri_np64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vsri_np64.c
@@ -0,0 +1,21 @@
+/* Test the `vsri_np64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vsri_np64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly64x1_t arg0_poly64x1_t;
+  poly64x1_t arg1_poly64x1_t;
+
+  out_poly64x1_t = vsri_n_p64 (arg0_poly64x1_t, arg1_poly64x1_t, 1);
+}
+
+/* { dg-final { scan-assembler "vsri\.64\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld1_lanep64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld1_lanep64.c
@@ -0,0 +1,20 @@
+/* Test the `vld1_lanep64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld1_lanep64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly64x1_t arg1_poly64x1_t;
+
+  out_poly64x1_t = vld1_lane_p64 (0, arg1_poly64x1_t, 0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+((\\\{\[dD\]\[0-9\]+\\\})|(\[dD\]\[0-9\]+)), \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs8_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs8_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQs8_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQs8_p128 (void)
+{
+  int8x16_t out_int8x16_t;
+  poly128_t arg0_poly128_t;
+
+  out_int8x16_t = vreinterpretq_s8_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld4p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld4p64.c
@@ -0,0 +1,19 @@
+/* Test the `vld4p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld4p64 (void)
+{
+  poly64x1x4_t out_poly64x1x4_t;
+
+  out_poly64x1x4_t = vld4_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_s32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_s32.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_s32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_s32 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  int32x4_t arg0_int32x4_t;
+
+  out_poly64x2_t = vreinterpretq_p64_s32 (arg0_int32x4_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld1Q_lanep64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld1Q_lanep64.c
@@ -0,0 +1,20 @@
+/* Test the `vld1Q_lanep64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld1Q_lanep64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly64x2_t arg1_poly64x2_t;
+
+  out_poly64x2_t = vld1q_lane_p64 (0, arg1_poly64x2_t, 1);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+((\\\{\[dD\]\[0-9\]+\\\})|(\[dD\]\[0-9\]+)), \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_p8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_p8.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_p8' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_p8 (void)
+{
+  poly128_t out_poly128_t;
+  poly8x16_t arg0_poly8x16_t;
+
+  out_poly128_t = vreinterpretq_p128_p8 (arg0_poly8x16_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_p8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_p8.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_p8' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_p8 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly8x16_t arg0_poly8x16_t;
+
+  out_poly64x2_t = vreinterpretq_p64_p8 (arg0_poly8x16_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vget_lowp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vget_lowp64.c
@@ -0,0 +1,19 @@
+/* Test the `vget_lowp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vget_lowp64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_poly64x1_t = vget_low_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs64_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs64_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQs64_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQs64_p128 (void)
+{
+  int64x2_t out_int64x2_t;
+  poly128_t arg0_poly128_t;
+
+  out_int64x2_t = vreinterpretq_s64_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vst1_lanep64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vst1_lanep64.c
@@ -0,0 +1,20 @@
+/* Test the `vst1_lanep64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vst1_lanep64 (void)
+{
+  poly64_t *arg0_poly64_t;
+  poly64x1_t arg1_poly64x1_t;
+
+  vst1_lane_p64 (arg0_poly64_t, arg1_poly64x1_t, 0);
+}
+
+/* { dg-final { scan-assembler "vst1\.64\[ 	\]+((\\\{\[dD\]\[0-9\]+\\\})|(\[dD\]\[0-9\]+)), \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs16_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs16_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQs16_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQs16_p64 (void)
+{
+  int16x8_t out_int16x8_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_int16x8_t = vreinterpretq_s16_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_s16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_s16.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_s16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_s16 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  int16x8_t arg0_int16x8_t;
+
+  out_poly64x2_t = vreinterpretq_p64_s16 (arg0_int16x8_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp16_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp16_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp16_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp16_p64 (void)
+{
+  poly16x4_t out_poly16x4_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_poly16x4_t = vreinterpret_p16_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vst1Qp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vst1Qp64.c
@@ -0,0 +1,20 @@
+/* Test the `vst1Qp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vst1Qp64 (void)
+{
+  poly64_t *arg0_poly64_t;
+  poly64x2_t arg1_poly64x2_t;
+
+  vst1q_p64 (arg0_poly64_t, arg1_poly64x2_t);
+}
+
+/* { dg-final { scan-assembler "vst1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_s64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_s64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_s64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_s64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  int64x1_t arg0_int64x1_t;
+
+  out_poly64x1_t = vreinterpret_p64_s64 (arg0_int64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu32_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu32_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQu32_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQu32_p64 (void)
+{
+  uint32x4_t out_uint32x4_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_uint32x4_t = vreinterpretq_u32_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_u32.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_u32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_u32 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  uint32x4_t arg0_uint32x4_t;
+
+  out_poly64x2_t = vreinterpretq_p64_u32 (arg0_uint32x4_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vget_highp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vget_highp64.c
@@ -0,0 +1,19 @@
+/* Test the `vget_highp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vget_highp64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_poly64x1_t = vget_high_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu64_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu64_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQu64_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQu64_p64 (void)
+{
+  uint64x2_t out_uint64x2_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_uint64x2_t = vreinterpretq_u64_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_u16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_u16.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_u16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_u16 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  uint16x8_t arg0_uint16x8_t;
+
+  out_poly64x2_t = vreinterpretq_p64_u16 (arg0_uint16x8_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vcvtf32_f16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vcvtf32_f16.c
@@ -0,0 +1,20 @@
+/* Test the `vcvtf32_f16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_neon_fp16_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_neon_fp16 } */
+
+#include "arm_neon.h"
+
+void test_vcvtf32_f16 (void)
+{
+  float32x4_t out_float32x4_t;
+  float16x4_t arg0_float16x4_t;
+
+  out_float32x4_t = vcvt_f32_f16 (arg0_float16x4_t);
+}
+
+/* { dg-final { scan-assembler "vcvt\.f32.f16\[ 	\]+\[qQ\]\[0-9\]+, \[dD\]\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_f32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_f32.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_f32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_f32 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  float32x4_t arg0_float32x4_t;
+
+  out_poly64x2_t = vreinterpretq_p64_f32 (arg0_float32x4_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vbslp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vbslp64.c
@@ -0,0 +1,22 @@
+/* Test the `vbslp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vbslp64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  uint64x1_t arg0_uint64x1_t;
+  poly64x1_t arg1_poly64x1_t;
+  poly64x1_t arg2_poly64x1_t;
+
+  out_poly64x1_t = vbsl_p64 (arg0_uint64x1_t, arg1_poly64x1_t, arg2_poly64x1_t);
+}
+
+/* { dg-final { scan-assembler "((vbsl)|(vbit)|(vbif))\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp16_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp16_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp16_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp16_p128 (void)
+{
+  poly16x8_t out_poly16x8_t;
+  poly128_t arg0_poly128_t;
+
+  out_poly16x8_t = vreinterpretq_p16_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vsli_np64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vsli_np64.c
@@ -0,0 +1,21 @@
+/* Test the `vsli_np64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vsli_np64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly64x1_t arg0_poly64x1_t;
+  poly64x1_t arg1_poly64x1_t;
+
+  out_poly64x1_t = vsli_n_p64 (arg0_poly64x1_t, arg1_poly64x1_t, 1);
+}
+
+/* { dg-final { scan-assembler "vsli\.64\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld1Q_dupp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld1Q_dupp64.c
@@ -0,0 +1,19 @@
+/* Test the `vld1Q_dupp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld1Q_dupp64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+
+  out_poly64x2_t = vld1q_dup_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+((\\\{\[dD\]\[0-9\]+\\\})|(\[dD\]\[0-9\]+)), \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu8_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu8_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQu8_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQu8_p128 (void)
+{
+  uint8x16_t out_uint8x16_t;
+  poly128_t arg0_poly128_t;
+
+  out_uint8x16_t = vreinterpretq_u8_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQf32_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQf32_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQf32_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQf32_p64 (void)
+{
+  float32x4_t out_float32x4_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_float32x4_t = vreinterpretq_f32_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_u64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_u64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_u64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_u64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  uint64x1_t arg0_uint64x1_t;
+
+  out_poly64x1_t = vreinterpret_p64_u64 (arg0_uint64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vdup_lanep64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vdup_lanep64.c
@@ -0,0 +1,19 @@
+/* Test the `vdup_lanep64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vdup_lanep64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_poly64x1_t = vdup_lane_p64 (arg0_poly64x1_t, 0);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_p16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_p16.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_p16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_p16 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly16x4_t arg0_poly16x4_t;
+
+  out_poly64x1_t = vreinterpret_p64_p16 (arg0_poly16x4_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_p64 (void)
+{
+  poly128_t out_poly128_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_poly128_t = vreinterpretq_p128_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu16_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu16_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQu16_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQu16_p64 (void)
+{
+  uint16x8_t out_uint16x8_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_uint16x8_t = vreinterpretq_u16_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterprets32_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterprets32_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterprets32_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterprets32_p64 (void)
+{
+  int32x2_t out_int32x2_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_int32x2_t = vreinterpret_s32_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterprets8_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterprets8_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterprets8_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterprets8_p64 (void)
+{
+  int8x8_t out_int8x8_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_int8x8_t = vreinterpret_s8_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vcvtf16_f32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vcvtf16_f32.c
@@ -0,0 +1,20 @@
+/* Test the `vcvtf16_f32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_neon_fp16_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_neon_fp16 } */
+
+#include "arm_neon.h"
+
+void test_vcvtf16_f32 (void)
+{
+  float16x4_t out_float16x4_t;
+  float32x4_t arg0_float32x4_t;
+
+  out_float16x4_t = vcvt_f16_f32 (arg0_float32x4_t);
+}
+
+/* { dg-final { scan-assembler "vcvt\.f16.f32\[ 	\]+\[dD\]\[0-9\]+, \[qQ\]\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterprets64_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterprets64_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterprets64_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterprets64_p64 (void)
+{
+  int64x1_t out_int64x1_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_int64x1_t = vreinterpret_s64_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu64_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu64_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQu64_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQu64_p128 (void)
+{
+  uint64x2_t out_uint64x2_t;
+  poly128_t arg0_poly128_t;
+
+  out_uint64x2_t = vreinterpretq_u64_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_s8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_s8.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_s8' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_s8 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  int8x8_t arg0_int8x8_t;
+
+  out_poly64x1_t = vreinterpret_p64_s8 (arg0_int8x8_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld1_dupp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld1_dupp64.c
@@ -0,0 +1,19 @@
+/* Test the `vld1_dupp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld1_dupp64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+
+  out_poly64x1_t = vld1_dup_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+((\\\{\[dD\]\[0-9\]+\\\})|(\[dD\]\[0-9\]+)), \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_s32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_s32.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_s32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_s32 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  int32x2_t arg0_int32x2_t;
+
+  out_poly64x1_t = vreinterpret_p64_s32 (arg0_int32x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vsriQ_np64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vsriQ_np64.c
@@ -0,0 +1,21 @@
+/* Test the `vsriQ_np64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vsriQ_np64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly64x2_t arg0_poly64x2_t;
+  poly64x2_t arg1_poly64x2_t;
+
+  out_poly64x2_t = vsriq_n_p64 (arg0_poly64x2_t, arg1_poly64x2_t, 1);
+}
+
+/* { dg-final { scan-assembler "vsri\.64\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vbslQp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vbslQp64.c
@@ -0,0 +1,22 @@
+/* Test the `vbslQp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vbslQp64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  uint64x2_t arg0_uint64x2_t;
+  poly64x2_t arg1_poly64x2_t;
+  poly64x2_t arg2_poly64x2_t;
+
+  out_poly64x2_t = vbslq_p64 (arg0_uint64x2_t, arg1_poly64x2_t, arg2_poly64x2_t);
+}
+
+/* { dg-final { scan-assembler "((vbsl)|(vbit)|(vbif))\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs32_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs32_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQs32_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQs32_p128 (void)
+{
+  int32x4_t out_int32x4_t;
+  poly128_t arg0_poly128_t;
+
+  out_int32x4_t = vreinterpretq_s32_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_u8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_u8.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_u8' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_u8 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  uint8x8_t arg0_uint8x8_t;
+
+  out_poly64x1_t = vreinterpret_p64_u8 (arg0_uint8x8_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vst1Q_lanep64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vst1Q_lanep64.c
@@ -0,0 +1,20 @@
+/* Test the `vst1Q_lanep64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vst1Q_lanep64 (void)
+{
+  poly64_t *arg0_poly64_t;
+  poly64x2_t arg1_poly64x2_t;
+
+  vst1q_lane_p64 (arg0_poly64_t, arg1_poly64x2_t, 1);
+}
+
+/* { dg-final { scan-assembler "vst1\.64\[ 	\]+((\\\{\[dD\]\[0-9\]+\\\})|(\[dD\]\[0-9\]+)), \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_s16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_s16.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_s16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_s16 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  int16x4_t arg0_int16x4_t;
+
+  out_poly64x1_t = vreinterpret_p64_s16 (arg0_int16x4_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterprets16_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterprets16_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterprets16_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterprets16_p64 (void)
+{
+  int16x4_t out_int16x4_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_int16x4_t = vreinterpret_s16_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vcombinep64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vcombinep64.c
@@ -0,0 +1,20 @@
+/* Test the `vcombinep64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vcombinep64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly64x1_t arg0_poly64x1_t;
+  poly64x1_t arg1_poly64x1_t;
+
+  out_poly64x2_t = vcombine_p64 (arg0_poly64x1_t, arg1_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld1Qp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld1Qp64.c
@@ -0,0 +1,19 @@
+/* Test the `vld1Qp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld1Qp64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+
+  out_poly64x2_t = vld1q_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_s64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_s64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_s64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_s64 (void)
+{
+  poly128_t out_poly128_t;
+  int64x2_t arg0_int64x2_t;
+
+  out_poly128_t = vreinterpretq_p128_s64 (arg0_int64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp8_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp8_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp8_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp8_p64 (void)
+{
+  poly8x8_t out_poly8x8_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_poly8x8_t = vreinterpret_p8_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vdup_np64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vdup_np64.c
@@ -0,0 +1,19 @@
+/* Test the `vdup_np64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vdup_np64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly64_t arg0_poly64_t;
+
+  out_poly64x1_t = vdup_n_p64 (arg0_poly64_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vst1p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vst1p64.c
@@ -0,0 +1,20 @@
+/* Test the `vst1p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vst1p64 (void)
+{
+  poly64_t *arg0_poly64_t;
+  poly64x1_t arg1_poly64x1_t;
+
+  vst1_p64 (arg0_poly64_t, arg1_poly64x1_t);
+}
+
+/* { dg-final { scan-assembler "vst1\.64\[ 	\]+((\\\{\[dD\]\[0-9\]+\\\})|(\[dD\]\[0-9\]+)), \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretu8_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretu8_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretu8_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretu8_p64 (void)
+{
+  uint8x8_t out_uint8x8_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_uint8x8_t = vreinterpret_u8_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_u32.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_u32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_u32 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  uint32x2_t arg0_uint32x2_t;
+
+  out_poly64x1_t = vreinterpret_p64_u32 (arg0_uint32x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretu32_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretu32_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretu32_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretu32_p64 (void)
+{
+  uint32x2_t out_uint32x2_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_uint32x2_t = vreinterpret_u32_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld2_dupp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld2_dupp64.c
@@ -0,0 +1,19 @@
+/* Test the `vld2_dupp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld2_dupp64 (void)
+{
+  poly64x1x2_t out_poly64x1x2_t;
+
+  out_poly64x1x2_t = vld2_dup_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretu64_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretu64_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretu64_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretu64_p64 (void)
+{
+  uint64x1_t out_uint64x1_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_uint64x1_t = vreinterpret_u64_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vsliQ_np64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vsliQ_np64.c
@@ -0,0 +1,21 @@
+/* Test the `vsliQ_np64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vsliQ_np64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly64x2_t arg0_poly64x2_t;
+  poly64x2_t arg1_poly64x2_t;
+
+  out_poly64x2_t = vsliq_n_p64 (arg0_poly64x2_t, arg1_poly64x2_t, 1);
+}
+
+/* { dg-final { scan-assembler "vsli\.64\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_u16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_u16.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_u16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_u16 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  uint16x4_t arg0_uint16x4_t;
+
+  out_poly64x1_t = vreinterpret_p64_u16 (arg0_uint16x4_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_u64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_u64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_u64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_u64 (void)
+{
+  poly128_t out_poly128_t;
+  uint64x2_t arg0_uint64x2_t;
+
+  out_poly128_t = vreinterpretq_p128_u64 (arg0_uint64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vst2p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vst2p64.c
@@ -0,0 +1,20 @@
+/* Test the `vst2p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vst2p64 (void)
+{
+  poly64_t *arg0_poly64_t;
+  poly64x1x2_t arg1_poly64x1x2_t;
+
+  vst2_p64 (arg0_poly64_t, arg1_poly64x1x2_t);
+}
+
+/* { dg-final { scan-assembler "vst1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp8_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp8_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp8_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp8_p128 (void)
+{
+  poly8x16_t out_poly8x16_t;
+  poly128_t arg0_poly128_t;
+
+  out_poly8x16_t = vreinterpretq_p8_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_f32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_f32.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_f32' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_f32 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  float32x2_t arg0_float32x2_t;
+
+  out_poly64x1_t = vreinterpret_p64_f32 (arg0_float32x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQf32_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQf32_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQf32_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQf32_p128 (void)
+{
+  float32x4_t out_float32x4_t;
+  poly128_t arg0_poly128_t;
+
+  out_float32x4_t = vreinterpretq_f32_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vextQp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vextQp64.c
@@ -0,0 +1,21 @@
+/* Test the `vextQp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vextQp64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly64x2_t arg0_poly64x2_t;
+  poly64x2_t arg1_poly64x2_t;
+
+  out_poly64x2_t = vextq_p64 (arg0_poly64x2_t, arg1_poly64x2_t, 0);
+}
+
+/* { dg-final { scan-assembler "vext\.64\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_p16.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_p16.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_p16' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_p16 (void)
+{
+  poly128_t out_poly128_t;
+  poly16x8_t arg0_poly16x8_t;
+
+  out_poly128_t = vreinterpretq_p128_p16 (arg0_poly16x8_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_p128 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  poly128_t arg0_poly128_t;
+
+  out_poly64x2_t = vreinterpretq_p64_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs16_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs16_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQs16_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQs16_p128 (void)
+{
+  int16x8_t out_int16x8_t;
+  poly128_t arg0_poly128_t;
+
+  out_int16x8_t = vreinterpretq_s16_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs8_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQs8_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQs8_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQs8_p64 (void)
+{
+  int8x16_t out_int8x16_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_int8x16_t = vreinterpretq_s8_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vextp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vextp64.c
@@ -0,0 +1,21 @@
+/* Test the `vextp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vextp64 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly64x1_t arg0_poly64x1_t;
+  poly64x1_t arg1_poly64x1_t;
+
+  out_poly64x1_t = vext_p64 (arg0_poly64x1_t, arg1_poly64x1_t, 0);
+}
+
+/* { dg-final { scan-assembler "vext\.64\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #\[0-9\]+!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp16_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp16_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp16_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp16_p64 (void)
+{
+  poly16x8_t out_poly16x8_t;
+  poly64x2_t arg0_poly64x2_t;
+
+  out_poly16x8_t = vreinterpretq_p16_p64 (arg0_poly64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretf32_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretf32_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretf32_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretf32_p64 (void)
+{
+  float32x2_t out_float32x2_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_float32x2_t = vreinterpret_f32_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_s8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp128_s8.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp128_s8' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp128_s8 (void)
+{
+  poly128_t out_poly128_t;
+  int8x16_t arg0_int8x16_t;
+
+  out_poly128_t = vreinterpretq_p128_s8 (arg0_int8x16_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_s8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_s8.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_s8' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_s8 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  int8x16_t arg0_int8x16_t;
+
+  out_poly64x2_t = vreinterpretq_p64_s8 (arg0_int8x16_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vst3p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vst3p64.c
@@ -0,0 +1,20 @@
+/* Test the `vst3p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vst3p64 (void)
+{
+  poly64_t *arg0_poly64_t;
+  poly64x1x3_t arg1_poly64x1x3_t;
+
+  vst3_p64 (arg0_poly64_t, arg1_poly64x1x3_t);
+}
+
+/* { dg-final { scan-assembler "vst1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vld3_dupp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vld3_dupp64.c
@@ -0,0 +1,19 @@
+/* Test the `vld3_dupp64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vld3_dupp64 (void)
+{
+  poly64x1x3_t out_poly64x1x3_t;
+
+  out_poly64x1x3_t = vld3_dup_p64 (0);
+}
+
+/* { dg-final { scan-assembler "vld1\.64\[ 	\]+\\\{((\[dD\]\[0-9\]+-\[dD\]\[0-9\]+)|(\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+))\\\}, \\\[\[rR\]\[0-9\]+\(:\[0-9\]+\)?\\\]!?\(\[ 	\]+@\[a-zA-Z0-9 \]+\)?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretu16_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretu16_p64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretu16_p64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretu16_p64 (void)
+{
+  uint16x4_t out_uint16x4_t;
+  poly64x1_t arg0_poly64x1_t;
+
+  out_uint16x4_t = vreinterpret_u16_p64 (arg0_poly64x1_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_p8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretp64_p8.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretp64_p8' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretp64_p8 (void)
+{
+  poly64x1_t out_poly64x1_t;
+  poly8x8_t arg0_poly8x8_t;
+
+  out_poly64x1_t = vreinterpret_p64_p8 (arg0_poly8x8_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_s64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQp64_s64.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQp64_s64' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQp64_s64 (void)
+{
+  poly64x2_t out_poly64x2_t;
+  int64x2_t arg0_int64x2_t;
+
+  out_poly64x2_t = vreinterpretq_p64_s64 (arg0_int64x2_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu32_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon/vreinterpretQu32_p128.c
@@ -0,0 +1,19 @@
+/* Test the `vreinterpretQu32_p128' ARM Neon intrinsic.  */
+/* This file was autogenerated by neon-testgen.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void test_vreinterpretQu32_p128 (void)
+{
+  uint32x4_t out_uint32x4_t;
+  poly128_t arg0_poly128_t;
+
+  out_uint32x4_t = vreinterpretq_u32_p128 (arg0_poly128_t);
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/anddi3-opt.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/anddi3-opt.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-options "-O1" } */
+
+unsigned long long
+muld (unsigned long long X, unsigned long long Y)
+{
+  unsigned long long mask = 0xffffffffull;
+  return (X & mask) * (Y & mask);
+}
+
+/* { dg-final { scan-assembler-not "and\[\\t \]+.+,\[\\t \]*.+,\[\\t \]*.+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/peep-ldrd-1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/peep-ldrd-1.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_prefer_ldrd_strd } */
+/* { dg-options "-O2" }  */
+int foo(int a, int b, int* p, int *q)
+{
+  a = p[2] + p[3];
+  *q = a;
+  *p = a;
+  return a;
+}
+/* { dg-final { scan-assembler "ldrd" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselgtdf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselgtdf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+double
+foo (double x, double y)
+{
+  volatile int i = 0;
+  return i > 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselgt.f64\td\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/acle/acle.exp
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/acle/acle.exp
@@ -0,0 +1,35 @@
+# Copyright (C) 2013-2014 Free Software Foundation, Inc.
+
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3 of the License, or
+# (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with GCC; see the file COPYING3.  If not see
+# <http://www.gnu.org/licenses/>.
+
+# GCC testsuite that uses the `dg.exp' driver.
+
+# Exit immediately if this isn't an ARM target.
+if ![istarget arm*-*-*] then {
+  return
+}
+
+# Load support procs.
+load_lib gcc-dg.exp
+
+# Initialize `dg'.
+dg-init
+
+# Main loop.
+dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.\[cCS\]]] \
+	"" ""
+
+# All done.
+dg-finish
Index: b/src/gcc/testsuite/gcc.target/arm/acle/crc32b.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/acle/crc32b.c
@@ -0,0 +1,20 @@
+/* Test the crc32b ACLE intrinsic.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crc_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crc } */
+
+#include "arm_acle.h"
+
+void test_crc32b (void)
+{
+  uint32_t out_uint32_t;
+  uint32_t arg0_uint32_t;
+  uint8_t arg1_uint8_t;
+
+  out_uint32_t = __crc32b (arg0_uint32_t, arg1_uint8_t);
+}
+
+/* { dg-final { scan-assembler "crc32b\t...?, ...?, ...?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/acle/crc32d.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/acle/crc32d.c
@@ -0,0 +1,20 @@
+/* Test the crc32d ACLE intrinsic.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crc_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crc } */
+
+#include "arm_acle.h"
+
+void test_crc32d (void)
+{
+  uint32_t out_uint32_t;
+  uint32_t arg0_uint32_t;
+  uint64_t arg1_uint64_t;
+
+  out_uint32_t = __crc32d (arg0_uint32_t, arg1_uint64_t);
+}
+
+/* { dg-final { scan-assembler-times "crc32w\t...?, ...?, ...?\n" 2 } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/acle/crc32cb.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/acle/crc32cb.c
@@ -0,0 +1,20 @@
+/* Test the crc32cb ACLE intrinsic.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crc_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crc } */
+
+#include "arm_acle.h"
+
+void test_crc32cb (void)
+{
+  uint32_t out_uint32_t;
+  uint32_t arg0_uint32_t;
+  uint8_t arg1_uint8_t;
+
+  out_uint32_t = __crc32cb (arg0_uint32_t, arg1_uint8_t);
+}
+
+/* { dg-final { scan-assembler "crc32cb\t...?, ...?, ...?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/acle/crc32cd.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/acle/crc32cd.c
@@ -0,0 +1,20 @@
+/* Test the crc32cd ACLE intrinsic.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crc_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crc } */
+
+#include "arm_acle.h"
+
+void test_crc32cd (void)
+{
+  uint32_t out_uint32_t;
+  uint32_t arg0_uint32_t;
+  uint64_t arg1_uint64_t;
+
+  out_uint32_t = __crc32cd (arg0_uint32_t, arg1_uint64_t);
+}
+
+/* { dg-final { scan-assembler-times "crc32cw\t...?, ...?, ...?\n" 2 } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/acle/crc32w.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/acle/crc32w.c
@@ -0,0 +1,20 @@
+/* Test the crc32w ACLE intrinsic.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crc_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crc } */
+
+#include "arm_acle.h"
+
+void test_crc32w (void)
+{
+  uint32_t out_uint32_t;
+  uint32_t arg0_uint32_t;
+  uint32_t arg1_uint32_t;
+
+  out_uint32_t = __crc32w (arg0_uint32_t, arg1_uint32_t);
+}
+
+/* { dg-final { scan-assembler "crc32w\t...?, ...?, ...?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/acle/crc32h.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/acle/crc32h.c
@@ -0,0 +1,20 @@
+/* Test the crc32h ACLE intrinsic.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crc_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crc } */
+
+#include "arm_acle.h"
+
+void test_crc32h (void)
+{
+  uint32_t out_uint32_t;
+  uint32_t arg0_uint32_t;
+  uint16_t arg1_uint16_t;
+
+  out_uint32_t = __crc32h (arg0_uint32_t, arg1_uint16_t);
+}
+
+/* { dg-final { scan-assembler "crc32h\t...?, ...?, ...?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/acle/crc32cw.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/acle/crc32cw.c
@@ -0,0 +1,20 @@
+/* Test the crc32cw ACLE intrinsic.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crc_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crc } */
+
+#include "arm_acle.h"
+
+void test_crc32cw (void)
+{
+  uint32_t out_uint32_t;
+  uint32_t arg0_uint32_t;
+  uint32_t arg1_uint32_t;
+
+  out_uint32_t = __crc32cw (arg0_uint32_t, arg1_uint32_t);
+}
+
+/* { dg-final { scan-assembler "crc32cw\t...?, ...?, ...?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/acle/crc32ch.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/acle/crc32ch.c
@@ -0,0 +1,20 @@
+/* Test the crc32ch ACLE intrinsic.  */
+
+/* { dg-do assemble } */
+/* { dg-require-effective-target arm_crc_ok } */
+/* { dg-options "-save-temps -O0" } */
+/* { dg-add-options arm_crc } */
+
+#include "arm_acle.h"
+
+void test_crc32ch (void)
+{
+  uint32_t out_uint32_t;
+  uint32_t arg0_uint32_t;
+  uint16_t arg1_uint16_t;
+
+  out_uint32_t = __crc32ch (arg0_uint32_t, arg1_uint16_t);
+}
+
+/* { dg-final { scan-assembler "crc32ch\t...?, ...?, ...?\n" } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/arm/iordi3-opt.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/iordi3-opt.c
@@ -0,0 +1,9 @@
+/* { dg-do compile } */
+/* { dg-options "-O1" } */
+
+unsigned long long or64 (unsigned long long input)
+{
+    return input | 0x200000004ULL;
+}
+
+/* { dg-final { scan-assembler-not "mov\[\\t \]+.+,\[\\t \]*.+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1pq_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1pq_u32.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32_t hash = 0xdeadbeef;
+  uint32x4_t a = {0, 1, 2, 3};
+  uint32x4_t b = {3, 2, 1, 0};
+
+  uint32x4_t res = vsha1pq_u32 (a, hash, b);
+  return res[0];
+}
+
+/* { dg-final { scan-assembler "sha1p.32\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-op-relaxed.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-op-relaxed.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-op-relaxed.x"
+
+/* { dg-final { scan-assembler-times "ldrex\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-times "strex\t...?, r\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselgesf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselgesf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+float
+foo (float x, float y)
+{
+  volatile int i = 0;
+  return i >= 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselge.f32\ts\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/peep-strd-1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/peep-strd-1.c
@@ -0,0 +1,9 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_prefer_ldrd_strd } */
+/* { dg-options "-O2" }  */
+void foo(int a, int b, int* p)
+{
+  p[2] = a;
+  p[3] = b;
+}
+/* { dg-final { scan-assembler "strd" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1su1q_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1su1q_u32.c
@@ -0,0 +1,17 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32x4_t a = {0xd, 0xe, 0xa, 0xd};
+  uint32x4_t b = {0, 1, 2, 3};
+
+  uint32x4_t res = vsha1su1q_u32 (a, b);
+  return res[0];
+}
+
+/* { dg-final { scan-assembler "sha1su1.32\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vmullp64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vmullp64.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+poly128_t
+foo (void)
+{
+  poly64_t a = 0xdeadbeef;
+  poly64_t b = 0xadadadad;
+  return vmull_p64 (a, b);
+}
+
+/* { dg-final { scan-assembler "vmull.p64.*" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/lp1243022.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/lp1243022.c
@@ -0,0 +1,201 @@
+/* { dg-do compile { target arm_thumb2 } } */
+/* { dg-options "-O2 -fdump-rtl-subreg2" } */
+
+/* { dg-final { scan-rtl-dump "REG_INC" "subreg2" { target { ! arm_neon } } } } */
+/* { dg-final { cleanup-rtl-dump "subreg2" } } */
+struct device;
+typedef unsigned int __u32;
+typedef unsigned long long u64;
+typedef __u32 __le32;
+typedef u64 dma_addr_t;
+typedef unsigned gfp_t;
+int dev_warn (const struct device *dev, const char *fmt, ...);
+struct usb_bus
+{
+    struct device *controller;
+};
+struct usb_hcd
+{
+    struct usb_bus self;
+};
+struct xhci_generic_trb
+{
+    __le32 field[4];
+};
+union xhci_trb
+{
+    struct xhci_generic_trb generic;
+};
+struct xhci_segment
+{
+    union xhci_trb *trbs;
+    dma_addr_t dma;
+};
+struct xhci_ring
+{
+    struct xhci_segment *first_seg;
+};
+struct xhci_hcd
+{
+    struct xhci_ring *cmd_ring;
+    struct xhci_ring *event_ring;
+};
+struct usb_hcd *xhci_to_hcd (struct xhci_hcd *xhci)
+{
+}
+dma_addr_t xhci_trb_virt_to_dma (struct xhci_segment * seg,
+				 union xhci_trb * trb);
+struct xhci_segment *trb_in_td (struct xhci_segment *start_seg,
+				dma_addr_t suspect_dma);
+xhci_test_trb_in_td (struct xhci_hcd *xhci, struct xhci_segment *input_seg,
+		     union xhci_trb *start_trb, union xhci_trb *end_trb,
+		     dma_addr_t input_dma, struct xhci_segment *result_seg,
+		     char *test_name, int test_number)
+{
+    unsigned long long start_dma;
+    unsigned long long end_dma;
+    struct xhci_segment *seg;
+    start_dma = xhci_trb_virt_to_dma (input_seg, start_trb);
+    end_dma = xhci_trb_virt_to_dma (input_seg, end_trb);
+    {
+        dev_warn (xhci_to_hcd (xhci)->self.controller,
+                  "%d\n", test_number);
+        dev_warn (xhci_to_hcd (xhci)->self.controller,
+                  "Expected seg %p, got seg %p\n", result_seg, seg);
+    }
+}
+xhci_check_trb_in_td_math (struct xhci_hcd *xhci, gfp_t mem_flags)
+{
+    struct
+    {
+        dma_addr_t input_dma;
+        struct xhci_segment *result_seg;
+    }
+    simple_test_vector[] =
+        {
+            {
+                0, ((void *) 0)
+            }
+            ,
+            {
+                xhci->event_ring->first_seg->dma - 16, ((void *) 0)}
+            ,
+            {
+                xhci->event_ring->first_seg->dma - 1, ((void *) 0)}
+            ,
+            {
+                xhci->event_ring->first_seg->dma, xhci->event_ring->first_seg}
+            ,
+            {
+                xhci->event_ring->first_seg->dma + (64 - 1) * 16,
+                xhci->event_ring->first_seg
+            }
+            ,
+            {
+                xhci->event_ring->first_seg->dma + (64 - 1) * 16 + 1, ((void *) 0)}
+            ,
+            {
+                xhci->event_ring->first_seg->dma + (64) * 16, ((void *) 0)}
+            ,
+            {
+                (dma_addr_t) (~0), ((void *) 0)
+            }
+        };
+    struct
+    {
+        struct xhci_segment *input_seg;
+        union xhci_trb *start_trb;
+        union xhci_trb *end_trb;
+        dma_addr_t input_dma;
+        struct xhci_segment *result_seg;
+    }
+    complex_test_vector[] =
+        {
+            {
+                .input_seg = xhci->event_ring->first_seg,.start_trb =
+                xhci->event_ring->first_seg->trbs,.end_trb =
+                &xhci->event_ring->first_seg->trbs[64 - 1],.input_dma =
+                xhci->cmd_ring->first_seg->dma,.result_seg = ((void *) 0),
+            }
+            ,
+            {
+                .input_seg = xhci->event_ring->first_seg,.start_trb =
+                xhci->event_ring->first_seg->trbs,.end_trb =
+                &xhci->cmd_ring->first_seg->trbs[64 - 1],.input_dma =
+                xhci->cmd_ring->first_seg->dma,.result_seg = ((void *) 0),
+            }
+            ,
+            {
+                .input_seg = xhci->event_ring->first_seg,.start_trb =
+                xhci->cmd_ring->first_seg->trbs,.end_trb =
+                &xhci->cmd_ring->first_seg->trbs[64 - 1],.input_dma =
+                xhci->cmd_ring->first_seg->dma,.result_seg = ((void *) 0),
+            }
+            ,
+            {
+                .input_seg = xhci->event_ring->first_seg,.start_trb =
+                &xhci->event_ring->first_seg->trbs[0],.end_trb =
+                &xhci->event_ring->first_seg->trbs[3],.input_dma =
+                xhci->event_ring->first_seg->dma + 4 * 16,.result_seg = ((void *) 0),
+            }
+            ,
+            {
+                .input_seg = xhci->event_ring->first_seg,.start_trb =
+                &xhci->event_ring->first_seg->trbs[3],.end_trb =
+                &xhci->event_ring->first_seg->trbs[6],.input_dma =
+                xhci->event_ring->first_seg->dma + 2 * 16,.result_seg = ((void *) 0),
+            }
+            ,
+            {
+                .input_seg = xhci->event_ring->first_seg,.start_trb =
+                &xhci->event_ring->first_seg->trbs[64 - 3],.end_trb =
+                &xhci->event_ring->first_seg->trbs[1],.input_dma =
+                xhci->event_ring->first_seg->dma + 2 * 16,.result_seg = ((void *) 0),
+            }
+            ,
+            {
+                .input_seg = xhci->event_ring->first_seg,.start_trb =
+                &xhci->event_ring->first_seg->trbs[64 - 3],.end_trb =
+                &xhci->event_ring->first_seg->trbs[1],.input_dma =
+                xhci->event_ring->first_seg->dma + (64 - 4) * 16,.result_seg =
+                ((void *) 0),
+            }
+            ,
+            {
+                .input_seg = xhci->event_ring->first_seg,.start_trb =
+                &xhci->event_ring->first_seg->trbs[64 - 3],.end_trb =
+                &xhci->event_ring->first_seg->trbs[1],.input_dma =
+                xhci->cmd_ring->first_seg->dma + 2 * 16,.result_seg = ((void *) 0),
+            }
+        };
+    unsigned int num_tests;
+    int i, ret;
+    num_tests =
+        (sizeof (simple_test_vector) / sizeof ((simple_test_vector)[0]) +
+         (sizeof (struct
+             {
+         }
+             )));
+    for (i = 0; i < num_tests; i++)
+    {
+        ret =
+            xhci_test_trb_in_td (xhci, xhci->event_ring->first_seg,
+                                 xhci->event_ring->first_seg->trbs,
+                                 &xhci->event_ring->first_seg->trbs[64 - 1],
+                                 simple_test_vector[i].input_dma,
+                                 simple_test_vector[i].result_seg, "Simple", i);
+        if (ret < 0)
+            return ret;
+    }
+    for (i = 0; i < num_tests; i++)
+    {
+        ret =
+            xhci_test_trb_in_td (xhci, complex_test_vector[i].input_seg,
+                                 complex_test_vector[i].start_trb,
+                                 complex_test_vector[i].end_trb,
+                                 complex_test_vector[i].input_dma,
+                                 complex_test_vector[i].result_seg, "Complex", i);
+        if (ret < 0)
+            return ret;
+    }
+}
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-comp-swap-release-acquire.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-comp-swap-release-acquire.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-comp-swap-release-acquire.x"
+
+/* { dg-final { scan-assembler-times "ldaex\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 4 } } */
+/* { dg-final { scan-assembler-times "stlex" 4 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/pr19599.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr19599.c
@@ -0,0 +1,10 @@
+/* { dg-skip-if "need at least armv5te" { *-*-* } { "-march=armv[234]*" "-mthumb" } { "" } } */
+/* { dg-options "-O2 -march=armv5te -marm" }  */
+/* { dg-final { scan-assembler "bx" } } */
+
+int (*indirect_func)();
+
+int indirect_call()
+{
+  return indirect_func();
+}
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vstrq_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vstrq_p128.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+void
+foo (poly128_t* ptr, poly128_t val)
+{
+  vstrq_p128 (ptr, val);
+}
+
+/* { dg-final { scan-assembler "vst1.64\t{d\[0-9\]+-d\[0-9\]+}.*" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-op-seq_cst.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-op-seq_cst.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-op-seq_cst.x"
+
+/* { dg-final { scan-assembler-times "ldaex\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-times "stlex\t...?, r\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselgedf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselgedf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+double
+foo (double x, double y)
+{
+  volatile int i = 0;
+  return i >= 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselge.f64\td\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-op-consume.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-op-consume.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-op-consume.x"
+
+/* { dg-final { scan-assembler-times "ldrex\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-times "strex\t...?, r\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-op-char.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-op-char.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-op-char.x"
+
+/* { dg-final { scan-assembler-times "ldrexb\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-times "strexb\t...?, r\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/thumb-ltu.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/arm/thumb-ltu.c
+++ b/src/gcc/testsuite/gcc.target/arm/thumb-ltu.c
@@ -1,5 +1,5 @@
 /* { dg-do compile } */
-/* { dg-skip-if "incompatible options" { arm*-*-* } { "-march=*" } { "-march=armv6" "-march=armv6j" "-march=armv6z" } } */
+/* { dg-require-effective-target arm_thumb1_ok } */
 /* { dg-options "-mcpu=arm1136jf-s -mthumb -O2" } */
 
 void f(unsigned a, unsigned b, unsigned c, unsigned d)
Index: b/src/gcc/testsuite/gcc.target/arm/pr60264.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr60264.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-options "-mapcs -g" } */
+
+double bar(void);
+
+int foo(void)
+{
+  int i = bar() + bar();
+
+  return i;
+}
+
Index: b/src/gcc/testsuite/gcc.target/arm/vselnesf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselnesf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+float
+foo (float x, float y)
+{
+  volatile int i = 0;
+  return i != 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vseleq.f32\ts\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vaesmcq_u8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vaesmcq_u8.c
@@ -0,0 +1,20 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint8x16_t a, b;
+  int i = 0;
+
+  for (i = 0; i < 16; ++i)
+    a[i] = i;
+
+  b = vaesmcq_u8 (a);
+  return b[0];
+}
+
+/* { dg-final { scan-assembler "aesmc.8\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselvcsf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselvcsf.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+float
+foo (float x, float y)
+{
+  return !__builtin_isunordered (x, y) ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselvs.f32\ts\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha256hq_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha256hq_u32.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32x4_t a = {0xd, 0xe, 0xa, 0xd};
+  uint32x4_t b = {0, 1, 2, 3};
+  uint32x4_t c = {3, 2, 1, 0};
+
+  uint32x4_t res = vsha256hq_u32 (a, b, c);
+  return res[0];
+}
+
+/* { dg-final { scan-assembler "sha256h.32\tq\[0-9\]+, q\[0-9\]+, q\[0-9\]" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/minmax_minus.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/minmax_minus.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_cond_exec } */
+/* { dg-options "-O2" } */
+
+#define MAX(a, b) (a > b ? a : b)
+int
+foo (int a, int b, int c)
+{
+  return c - MAX (a, b);
+}
+
+/* { dg-final { scan-assembler-not "mov" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-op-release.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-op-release.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-op-release.x"
+
+/* { dg-final { scan-assembler-times "ldrex\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-times "stlex\t...?, r\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselvssf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselvssf.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+float
+foo (float x, float y)
+{
+  return __builtin_isunordered (x, y) ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselvs.f32\ts\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1cq_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1cq_u32.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32_t hash = 0xdeadbeef;
+  uint32x4_t a = {0, 1, 2, 3};
+  uint32x4_t b = {3, 2, 1, 0};
+
+  uint32x4_t res = vsha1cq_u32 (a, hash, b);
+  return res[0];
+}
+
+/* { dg-final { scan-assembler "sha1c.32\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vaeseq_u8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vaeseq_u8.c
@@ -0,0 +1,22 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint8x16_t a, b, c;
+  int i = 0;
+
+  for (i = 0; i < 16; ++i)
+    {
+      a[i] = i;
+      b[i] = 15 - i;
+    }
+  c = vaeseq_u8 (a, b);
+  return c[0];
+}
+
+/* { dg-final { scan-assembler "aese.8\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vect-rounding-roundf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vect-rounding-roundf.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_neon_ok } */
+/* { dg-options "-O2 -ffast-math -ftree-vectorize" } */
+/* { dg-add-options arm_v8_neon } */
+
+#define N 32
+
+void
+foo (float *output, float *input)
+{
+  int i = 0;
+  /* Vectorizable.  */
+  for (i = 0; i < N; i++)
+    output[i] = __builtin_roundf (input[i]);
+}
+
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 1 "vect" { target vect_call_roundf } } } */
+/* { dg-final { cleanup-tree-dump "vect" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon-vtst_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon-vtst_p64.c
@@ -0,0 +1,38 @@
+/* { dg-do run } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-require-effective-target arm_neon_hw } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+#include <stdio.h>
+
+extern void abort (void);
+
+int
+main (void)
+{
+  uint64_t args[] = { 0x0, 0xdeadbeef, ~0xdeadbeef, 0xffff,
+                      ~0xffff, 0xffffffff, ~0xffffffff, ~0x0 };
+  int i, j;
+
+  for (i = 0; i < sizeof (args) / sizeof (args[0]); ++i)
+    {
+       for (j = 0; j < sizeof (args) / sizeof (args[0]); ++j)
+         {
+           uint64_t a1 = args[i];
+           uint64_t a2 = args[j];
+           uint64_t res = vtst_p64 (vreinterpret_p64_u64 (a1),
+                                    vreinterpret_p64_u64 (a2));
+           uint64_t exp = (a1 & a2) ? ~0x0 : 0x0;
+
+           if (res != exp)
+             {
+               fprintf (stderr, "vtst_p64 (a1= %lx, a2= %lx)"
+                                " returned %lx, expected %lx\n",
+                                 a1, a2, res, exp);
+               abort ();
+             }
+         }
+    }
+  return 0;
+}
Index: b/src/gcc/testsuite/gcc.target/arm/neon-for-64bits-1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon-for-64bits-1.c
@@ -0,0 +1,54 @@
+/* Check that Neon is *not* used by default to handle 64-bits scalar
+   operations.  */
+
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_neon_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_neon } */
+
+typedef long long i64;
+typedef unsigned long long u64;
+typedef unsigned int u32;
+typedef int i32;
+
+/* Unary operators */
+#define UNARY_OP(name, op) \
+  void unary_##name(u64 *a, u64 *b) { *a = op (*b + 0x1234567812345678ULL) ; }
+
+/* Binary operators */
+#define BINARY_OP(name, op) \
+  void binary_##name(u64 *a, u64 *b, u64 *c) { *a = *b op *c ; }
+
+/* Unsigned shift */
+#define SHIFT_U(name, op, amount) \
+  void ushift_##name(u64 *a, u64 *b, int c) { *a = *b op amount; }
+
+/* Signed shift */
+#define SHIFT_S(name, op, amount) \
+  void sshift_##name(i64 *a, i64 *b, int c) { *a = *b op amount; }
+
+UNARY_OP(not, ~)
+
+BINARY_OP(add, +)
+BINARY_OP(sub, -)
+BINARY_OP(and, &)
+BINARY_OP(or, |)
+BINARY_OP(xor, ^)
+
+SHIFT_U(right1, >>, 1)
+SHIFT_U(right2, >>, 2)
+SHIFT_U(right5, >>, 5)
+SHIFT_U(rightn, >>, c)
+
+SHIFT_S(right1, >>, 1)
+SHIFT_S(right2, >>, 2)
+SHIFT_S(right5, >>, 5)
+SHIFT_S(rightn, >>, c)
+
+/* { dg-final {scan-assembler-times "vmvn" 0} }  */
+/* { dg-final {scan-assembler-times "vadd" 0} }  */
+/* { dg-final {scan-assembler-times "vsub" 0} }  */
+/* { dg-final {scan-assembler-times "vand" 0} }  */
+/* { dg-final {scan-assembler-times "vorr" 0} }  */
+/* { dg-final {scan-assembler-times "veor" 0} }  */
+/* { dg-final {scan-assembler-times "vshr" 0} }  */
Index: b/src/gcc/testsuite/gcc.target/arm/unaligned-memcpy-2.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/arm/unaligned-memcpy-2.c
+++ b/src/gcc/testsuite/gcc.target/arm/unaligned-memcpy-2.c
@@ -4,7 +4,7 @@
 
 #include <string.h>
 
-char dest[16];
+char dest[16] = { 0 };
 
 void aligned_dest (char *src)
 {
@@ -14,7 +14,10 @@ void aligned_dest (char *src)
 /* Expect a multi-word store for the main part of the copy, but subword
    loads/stores for the remainder.  */
 
-/* { dg-final { scan-assembler-times "stmia" 1 } } */
+/* { dg-final { scan-assembler-times "ldmia" 0 } } */
+/* { dg-final { scan-assembler-times "ldrd" 0 } } */
+/* { dg-final { scan-assembler-times "stmia" 1 { target { ! { arm_prefer_ldrd_strd } } } } } */
+/* { dg-final { scan-assembler-times "strd" 1 { target { arm_prefer_ldrd_strd } } } } */
 /* { dg-final { scan-assembler-times "ldrh" 1 } } */
 /* { dg-final { scan-assembler-times "strh" 1 } } */
 /* { dg-final { scan-assembler-times "ldrb" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1h_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1h_u32.c
@@ -0,0 +1,14 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32_t val = 0xdeadbeef;
+  return vsha1h_u32 (val);
+}
+
+/* { dg-final { scan-assembler "sha1h.32\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/xordi3-opt.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/xordi3-opt.c
@@ -0,0 +1,9 @@
+/* { dg-do compile } */
+/* { dg-options "-O1" } */
+
+unsigned long long xor64 (unsigned long long input)
+{
+    return input ^ 0x200000004ULL;
+}
+
+/* { dg-final { scan-assembler-not "mov\[\\t \]+.+,\[\\t \]*.+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha256su1q_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha256su1q_u32.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32x4_t a = {0xd, 0xe, 0xa, 0xd};
+  uint32x4_t b = {0, 1, 2, 3};
+  uint32x4_t c = {3, 2, 1, 0};
+
+  uint32x4_t res = vsha256su1q_u32 (a, b, c);
+  return res[0];
+}
+
+/* { dg-final { scan-assembler "sha256su1.32\tq\[0-9\]+, q\[0-9\]+, q\[0-9\]" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-op-acq_rel.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-op-acq_rel.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-op-acq_rel.x"
+
+/* { dg-final { scan-assembler-times "ldaex\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-times "stlex\t...?, r\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselltsf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselltsf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+float
+foo (float x, float y)
+{
+  volatile int i = 0;
+  return i < 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselge.f32\ts\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselnedf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselnedf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+double
+foo (double x, double y)
+{
+  volatile int i = 0;
+  return i != 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vseleq.f64\td\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselvcdf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselvcdf.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+double
+foo (double x, double y)
+{
+  return !__builtin_isunordered (x, y) ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselvs.f64\td\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vect-rounding-btruncf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vect-rounding-btruncf.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_neon_ok } */
+/* { dg-options "-O2 -ffast-math -ftree-vectorize" } */
+/* { dg-add-options arm_v8_neon } */
+
+#define N 32
+
+void
+foo (float *output, float *input)
+{
+  int i = 0;
+  /* Vectorizable.  */
+  for (i = 0; i < N; i++)
+    output[i] = __builtin_truncf (input[i]);
+}
+
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 1 "vect" { target vect_call_btruncf } } } */
+/* { dg-final { cleanup-tree-dump "vect" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vseleqsf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vseleqsf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+float
+foo (float x, float y)
+{
+  volatile int i = 0;
+  return i == 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vseleq.f32\ts\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/ivopts-orig_biv-inc.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/ivopts-orig_biv-inc.c
@@ -0,0 +1,19 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-tree-ivopts-details" } */
+/* { dg-skip-if "" { arm_thumb1 } } */
+
+extern char *__ctype_ptr__;
+
+unsigned char * foo(unsigned char *ReadPtr)
+{
+
+ unsigned char c;
+
+ while (!(((__ctype_ptr__+sizeof(""[*ReadPtr]))[(int)(*ReadPtr)])&04) == (!(0)))
+  ReadPtr++;
+
+ return ReadPtr;
+}
+
+/* { dg-final { scan-tree-dump-times "original biv" 2 "ivopts"} } */
+/* { dg-final { cleanup-tree-dump "ivopts" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselvsdf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselvsdf.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+double
+foo (double x, double y)
+{
+  return __builtin_isunordered (x, y) ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselvs.f64\td\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/unaligned-memcpy-3.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/arm/unaligned-memcpy-3.c
+++ b/src/gcc/testsuite/gcc.target/arm/unaligned-memcpy-3.c
@@ -4,7 +4,7 @@
 
 #include <string.h>
 
-char src[16];
+char src[16] = {0};
 
 void aligned_src (char *dest)
 {
@@ -14,8 +14,11 @@ void aligned_src (char *dest)
 /* Expect a multi-word load for the main part of the copy, but subword
    loads/stores for the remainder.  */
 
-/* { dg-final { scan-assembler-times "ldmia" 1 } } */
-/* { dg-final { scan-assembler-times "ldrh" 1 } } */
+/* { dg-final { scan-assembler-times "ldmia" 1 { target { ! { arm_prefer_ldrd_strd } } } } } */
+/* { dg-final { scan-assembler-times "ldrd" 1 { target { arm_prefer_ldrd_strd } } } } */
+/* { dg-final { scan-assembler-times "strd" 0 } } */
+/* { dg-final { scan-assembler-times "stm" 0 } } */
+/* { dg-final { scan-assembler-times "ldrh" 1 { target { ! { arm_prefer_ldrd_strd } } } } } */
 /* { dg-final { scan-assembler-times "strh" 1 } } */
-/* { dg-final { scan-assembler-times "ldrb" 1 } } */
+/* { dg-final { scan-assembler-times "ldrb" 1 { target { ! { arm_prefer_ldrd_strd } } } } } */
 /* { dg-final { scan-assembler-times "strb" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/pr46975-2.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr46975-2.c
@@ -0,0 +1,10 @@
+/* { dg-options "-mthumb -O2" } */
+/* { dg-require-effective-target arm_thumb2_ok } */
+/* { dg-final { scan-assembler "sub" } } */
+/* { dg-final { scan-assembler "clz" } } */
+/* { dg-final { scan-assembler "lsr.*#5" } } */
+
+int foo (int s)
+{
+      return s == 1;
+}
Index: b/src/gcc/testsuite/gcc.target/arm/neon-vceq_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/neon-vceq_p64.c
@@ -0,0 +1,38 @@
+/* { dg-do run } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-require-effective-target arm_neon_hw } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+#include <stdio.h>
+
+extern void abort (void);
+
+int
+main (void)
+{
+  uint64_t args[] = { 0x0, 0xdeadbeef, ~0xdeadbeef, 0xffff,
+                      ~0xffff, 0xffffffff, ~0xffffffff, ~0x0 };
+  int i, j;
+
+  for (i = 0; i < sizeof (args) / sizeof (args[0]); ++i)
+    {
+       for (j = 0; j < sizeof (args) / sizeof (args[0]); ++j)
+         {
+           uint64_t a1 = args[i];
+           uint64_t a2 = args[j];
+           uint64_t res = vceq_p64 (vreinterpret_p64_u64 (a1),
+                                    vreinterpret_p64_u64 (a2));
+           uint64_t exp = (a1 == a2) ? ~0x0 : 0x0;
+
+           if (res != exp)
+             {
+               fprintf (stderr, "vceq_p64 (a1= %lx, a2= %lx)"
+                                " returned %lx, expected %lx\n",
+                                 a1, a2, res, exp);
+               abort ();
+             }
+         }
+    }
+  return 0;
+}
Index: b/src/gcc/testsuite/gcc.target/arm/anddi3-opt2.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/anddi3-opt2.c
@@ -0,0 +1,9 @@
+/* { dg-do compile } */
+/* { dg-options "-O1" } */
+
+long long muld(long long X, long long Y)
+{
+  return X & ~1;
+}
+
+/* { dg-final { scan-assembler-not "and\[\\t \]+.+,\[\\t \]*.+,\[\\t \]*.+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon-vcond-ltgt.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/arm/neon-vcond-ltgt.c
+++ b/src/gcc/testsuite/gcc.target/arm/neon-vcond-ltgt.c
@@ -15,4 +15,4 @@ void foo (int ilast,float* w, float* w2)
 
 /* { dg-final { scan-assembler-times "vcgt\\.f32\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" 2 } } */
 /* { dg-final { scan-assembler "vorr\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vbsl\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
+/* { dg-final { scan-assembler "vbsl|vbit|vbif\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha256h2q_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha256h2q_u32.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32x4_t a = {0xd, 0xe, 0xa, 0xd};
+  uint32x4_t b = {0, 1, 2, 3};
+  uint32x4_t c = {3, 2, 1, 0};
+
+  uint32x4_t res = vsha256h2q_u32 (a, b, c);
+  return res[0];
+}
+
+/* { dg-final { scan-assembler "sha256h2.32\tq\[0-9\]+, q\[0-9\]+, q\[0-9\]" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselltdf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselltdf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+double
+foo (double x, double y)
+{
+  volatile int i = 0;
+  return i < 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselge.f64\td\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/unaligned-memcpy-4.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/arm/unaligned-memcpy-4.c
+++ b/src/gcc/testsuite/gcc.target/arm/unaligned-memcpy-4.c
@@ -4,8 +4,8 @@
 
 #include <string.h>
 
-char src[16];
-char dest[16];
+char src[16] = { 0 };
+char dest[16] = { 0 };
 
 void aligned_both (void)
 {
@@ -14,5 +14,9 @@ void aligned_both (void)
 
 /* We know both src and dest to be aligned: expect multiword loads/stores.  */
 
-/* { dg-final { scan-assembler-times "ldmia" 1 } } */
-/* { dg-final { scan-assembler-times "stmia" 1 } } */
+/* { dg-final { scan-assembler-times "ldmia" 1 { target { ! { arm_prefer_ldrd_strd } } } } } */
+/* { dg-final { scan-assembler-times "stmia" 1 { target { ! { arm_prefer_ldrd_strd } } } } } */
+/* { dg-final { scan-assembler "ldrd" { target { arm_prefer_ldrd_strd } } } } */
+/* { dg-final { scan-assembler-times "ldm" 0 { target { arm_prefer_ldrd_strd } } } } */
+/* { dg-final { scan-assembler "strd" { target { arm_prefer_ldrd_strd } } } } */
+/* { dg-final { scan-assembler-times "stm" 0 { target { arm_prefer_ldrd_strd } } } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vseleqdf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vseleqdf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+double
+foo (double x, double y)
+{
+  volatile int i = 0;
+  return i == 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vseleq.f64\td\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-op-acquire.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-op-acquire.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-op-acquire.x"
+
+/* { dg-final { scan-assembler-times "ldaex\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-times "strex\t...?, r\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vsellesf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vsellesf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+float
+foo (float x, float y)
+{
+  volatile int i = 0;
+  return i <= 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselgt.f32\ts\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/neon-vcond-unordered.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/arm/neon-vcond-unordered.c
+++ b/src/gcc/testsuite/gcc.target/arm/neon-vcond-unordered.c
@@ -16,4 +16,4 @@ void foo (int ilast,float* w, float* w2)
 /* { dg-final { scan-assembler "vcgt\\.f32\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
 /* { dg-final { scan-assembler "vcge\\.f32\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
 /* { dg-final { scan-assembler "vorr\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vbsl\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
+/* { dg-final { scan-assembler "vbsl|vbit|vbif\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1su0q_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1su0q_u32.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32x4_t a = {0xd, 0xe, 0xa, 0xd};
+  uint32x4_t b = {0, 1, 2, 3};
+  uint32x4_t c = {3, 2, 1, 0};
+
+  uint32x4_t res = vsha1su0q_u32 (a, b, c);
+  return res[0];
+}
+
+/* { dg-final { scan-assembler "sha1su0.32\tq\[0-9\]+, q\[0-9\]+, q\[0-9\]" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vmull_high_p64.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vmull_high_p64.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+poly128_t
+foo (void)
+{
+  poly64x2_t a = { 0xdeadbeef, 0xadabcaca };
+  poly64x2_t b = { 0xdcdcdcdc, 0xbdbdbdbd };
+  return vmull_high_p64 (a, b);
+}
+
+/* { dg-final { scan-assembler "vmull.p64.*" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-op-int.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-op-int.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-op-int.x"
+
+/* { dg-final { scan-assembler-times "ldrex\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-times "strex\t...?, r\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1mq_u32.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vsha1mq_u32.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint32_t hash = 0xdeadbeef;
+  uint32x4_t a = {0, 1, 2, 3};
+  uint32x4_t b = {3, 2, 1, 0};
+
+  uint32x4_t res = vsha1mq_u32 (a, hash, b);
+  return res[0];
+}
+
+/* { dg-final { scan-assembler "sha1m.32\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vldrq_p128.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vldrq_p128.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+poly128_t
+foo (poly128_t* ptr)
+{
+  return vldrq_p128 (ptr);
+}
+
+/* { dg-final { scan-assembler "vld1.64\t{d\[0-9\]+-d\[0-9\]+}.*" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/ldrd-strd-offset.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/ldrd-strd-offset.c
@@ -0,0 +1,17 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+typedef struct
+{
+  int x;
+  int i, j;
+} off_struct;
+
+int foo (char *str, int *a, int b, int c)
+{
+  off_struct *p = (off_struct *)(str + 3);
+  b = p->i;
+  c = p->j;
+  *a = b + c;
+  return 0;
+}
Index: b/src/gcc/testsuite/gcc.target/arm/atomic-op-short.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/atomic-op-short.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v8a_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_arch_v8a } */
+
+#include "../aarch64/atomic-op-short.x"
+
+/* { dg-final { scan-assembler-times "ldrexh\tr\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-times "strexh\t...?, r\[0-9\]+, \\\[r\[0-9\]+\\\]" 6 } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/pr40887.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/arm/pr40887.c
+++ b/src/gcc/testsuite/gcc.target/arm/pr40887.c
@@ -2,9 +2,9 @@
 /* { dg-options "-O2 -march=armv5te" }  */
 /* { dg-final { scan-assembler "blx" } } */
 
-int (*indirect_func)();
+int (*indirect_func)(int x);
 
 int indirect_call()
 {
-    return indirect_func();
+  return indirect_func(20) + indirect_func (40);
 }
Index: b/src/gcc/testsuite/gcc.target/arm/crypto-vaesimcq_u8.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/crypto-vaesimcq_u8.c
@@ -0,0 +1,20 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_crypto_ok } */
+/* { dg-add-options arm_crypto } */
+
+#include "arm_neon.h"
+
+int
+foo (void)
+{
+  uint8x16_t a, b;
+  int i = 0;
+
+  for (i = 0; i < 16; ++i)
+    a[i] = i;
+
+  b = vaesimcq_u8 (a);
+  return b[0];
+}
+
+/* { dg-final { scan-assembler "aesimc.8\tq\[0-9\]+, q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vect-rounding-ceilf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vect-rounding-ceilf.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_neon_ok } */
+/* { dg-options "-O2 -ffast-math -ftree-vectorize" } */
+/* { dg-add-options arm_v8_neon } */
+
+#define N 32
+
+void
+foo (float *output, float *input)
+{
+  int i = 0;
+  /* Vectorizable.  */
+  for (i = 0; i < N; i++)
+    output[i] = __builtin_ceilf (input[i]);
+}
+
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 1 "vect" { target vect_call_ceilf } } } */
+/* { dg-final { cleanup-tree-dump "vect" } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselledf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselledf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+double
+foo (double x, double y)
+{
+  volatile int i = 0;
+  return i <= 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselgt.f64\td\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/vselgtsf.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/vselgtsf.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_v8_vfp_ok } */
+/* { dg-options "-O2" } */
+/* { dg-add-options arm_v8_vfp } */
+
+float
+foo (float x, float y)
+{
+  volatile int i = 0;
+  return i > 0 ? x : y;
+}
+
+/* { dg-final { scan-assembler-times "vselgt.f32\ts\[0-9\]+" 1 } } */
Index: b/src/gcc/testsuite/gcc.target/arm/pr58578.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr58578.c
@@ -0,0 +1,54 @@
+
+/* PR target/58578 */
+/* { dg-do run } */
+/* { dg-options "-O1" } */
+
+#include <stdlib.h>
+
+typedef struct {
+    long _prec;
+    int _flag;
+    long _exp;
+} __my_st_t;
+
+typedef __my_st_t *__my_st_ptr;
+
+int
+_test_fn (__my_st_ptr y, const __my_st_ptr xt)
+{
+  int inexact;
+  if (xt->_exp != -2147483647L)
+    {
+      (y->_flag = xt->_flag);
+    }
+
+  do {
+      __my_st_ptr _y = y;
+      long _err1 = -2 * xt->_exp;
+      long _err2 = 2;
+      if (0 < _err1)
+	{
+	  unsigned long _err = (unsigned long) _err1 + _err2;
+	  if (__builtin_expect(!!(_err > _y->_prec + 1), 0))
+	    return 2;
+	  return 3;
+	}
+  } while (0);
+
+  return 0;
+}
+
+int main ()
+{
+  __my_st_t x, y;
+  long pz;
+  int inex;
+
+  x._prec = 914;
+  y._exp = 18;
+  if (_test_fn (&x, &y))
+    {
+      abort();
+    }
+  return 0;
+}
Index: b/src/gcc/testsuite/gcc.target/arm/neon-vcond-gt.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/arm/neon-vcond-gt.c
+++ b/src/gcc/testsuite/gcc.target/arm/neon-vcond-gt.c
@@ -14,4 +14,4 @@ void foo (int ilast,float* w, float* w2)
 }
 
 /* { dg-final { scan-assembler "vcgt\\.f32\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vbit\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
+/* { dg-final { scan-assembler "vbsl|vbit|vbif\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+,\[\\t \]*q\[0-9\]+" } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/insv_2.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/insv_2.c
@@ -0,0 +1,85 @@
+/* { dg-do run { target aarch64*-*-* } } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target aarch64_big_endian } */
+
+extern void abort (void);
+
+typedef struct bitfield
+{
+  unsigned short eight: 8;
+  unsigned short four: 4;
+  unsigned short five: 5;
+  unsigned short seven: 7;
+  unsigned int sixteen: 16;
+} bitfield;
+
+bitfield
+bfi1 (bitfield a)
+{
+  /* { dg-final { scan-assembler "bfi\tx\[0-9\]+, x\[0-9\]+, 56, 8" } } */
+  a.eight = 3;
+  return a;
+}
+
+bitfield
+bfi2 (bitfield a)
+{
+  /* { dg-final { scan-assembler "bfi\tx\[0-9\]+, x\[0-9\]+, 43, 5" } } */
+  a.five = 7;
+  return a;
+}
+
+bitfield
+movk (bitfield a)
+{
+  /* { dg-final { scan-assembler "movk\tx\[0-9\]+, 0x1d6b, lsl 16" } } */
+  a.sixteen = 7531;
+  return a;
+}
+
+bitfield
+set1 (bitfield a)
+{
+  /* { dg-final { scan-assembler "orr\tx\[0-9\]+, x\[0-9\]+, 272678883688448" } } */
+  a.five = 0x1f;
+  return a;
+}
+
+bitfield
+set0 (bitfield a)
+{
+  /* { dg-final { scan-assembler "and\tx\[0-9\]+, x\[0-9\]+, -272678883688449" } } */
+  a.five = 0;
+  return a;
+}
+
+
+int
+main (int argc, char** argv)
+{
+  static bitfield a;
+  bitfield b = bfi1 (a);
+  bitfield c = bfi2 (b);
+  bitfield d = movk (c);
+
+  if (d.eight != 3)
+    abort ();
+
+  if (d.five != 7)
+    abort ();
+
+  if (d.sixteen != 7531)
+    abort ();
+
+  d = set1 (d);
+  if (d.five != 0x1f)
+    abort ();
+
+  d = set0 (d);
+  if (d.five != 0)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vrecps.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vrecps.c
@@ -0,0 +1,144 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps" } */
+
+#include <arm_neon.h>
+#include <math.h>
+#include <stdlib.h>
+
+int
+test_frecps_float32_t (void)
+{
+  int i;
+  float32_t value = 0.2;
+  float32_t reciprocal = 5.0;
+  float32_t step = vrecpes_f32 (value);
+  /* 3 steps should give us within ~0.001 accuracy.  */
+  for (i = 0; i < 3; i++)
+    step = step * vrecpss_f32 (step, value);
+
+  return fabs (step - reciprocal) < 0.001;
+}
+
+/* { dg-final { scan-assembler "frecpe\\ts\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "frecps\\ts\[0-9\]+, s\[0-9\]+, s\[0-9\]+" } } */
+
+int
+test_frecps_float32x2_t (void)
+{
+  int i;
+  int ret = 1;
+
+  const float32_t value_pool[] = {0.2, 0.4};
+  const float32_t reciprocal_pool[] = {5.0, 2.5};
+  float32x2_t value = vld1_f32 (value_pool);
+  float32x2_t reciprocal = vld1_f32 (reciprocal_pool);
+
+  float32x2_t step = vrecpe_f32 (value);
+  /* 3 steps should give us within ~0.001 accuracy.  */
+  for (i = 0; i < 3; i++)
+    step = step * vrecps_f32 (step, value);
+
+  ret &= fabs (vget_lane_f32 (step, 0)
+	       - vget_lane_f32 (reciprocal, 0)) < 0.001;
+  ret &= fabs (vget_lane_f32 (step, 1)
+	       - vget_lane_f32 (reciprocal, 1)) < 0.001;
+
+  return ret;
+}
+
+/* { dg-final { scan-assembler "frecpe\\tv\[0-9\]+.2s, v\[0-9\]+.2s" } } */
+/* { dg-final { scan-assembler "frecps\\tv\[0-9\]+.2s, v\[0-9\]+.2s, v\[0-9\]+.2s" } } */
+
+int
+test_frecps_float32x4_t (void)
+{
+  int i;
+  int ret = 1;
+
+  const float32_t value_pool[] = {0.2, 0.4, 0.5, 0.8};
+  const float32_t reciprocal_pool[] = {5.0, 2.5, 2.0, 1.25};
+  float32x4_t value = vld1q_f32 (value_pool);
+  float32x4_t reciprocal = vld1q_f32 (reciprocal_pool);
+
+  float32x4_t step = vrecpeq_f32 (value);
+  /* 3 steps should give us within ~0.001 accuracy.  */
+  for (i = 0; i < 3; i++)
+    step = step * vrecpsq_f32 (step, value);
+
+  ret &= fabs (vgetq_lane_f32 (step, 0)
+	       - vgetq_lane_f32 (reciprocal, 0)) < 0.001;
+  ret &= fabs (vgetq_lane_f32 (step, 1)
+	       - vgetq_lane_f32 (reciprocal, 1)) < 0.001;
+  ret &= fabs (vgetq_lane_f32 (step, 2)
+	       - vgetq_lane_f32 (reciprocal, 2)) < 0.001;
+  ret &= fabs (vgetq_lane_f32 (step, 3)
+	       - vgetq_lane_f32 (reciprocal, 3)) < 0.001;
+
+  return ret;
+}
+
+/* { dg-final { scan-assembler "frecpe\\tv\[0-9\]+.4s, v\[0-9\]+.4s" } } */
+/* { dg-final { scan-assembler "frecps\\tv\[0-9\]+.4s, v\[0-9\]+.4s, v\[0-9\]+.4s" } } */
+
+int
+test_frecps_float64_t (void)
+{
+  int i;
+  float64_t value = 0.2;
+  float64_t reciprocal = 5.0;
+  float64_t step = vrecped_f64 (value);
+  /* 3 steps should give us within ~0.001 accuracy.  */
+  for (i = 0; i < 3; i++)
+    step = step * vrecpsd_f64 (step, value);
+
+  return fabs (step - reciprocal) < 0.001;
+}
+
+/* { dg-final { scan-assembler "frecpe\\td\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "frecps\\td\[0-9\]+, d\[0-9\]+, d\[0-9\]+" } } */
+
+int
+test_frecps_float64x2_t (void)
+{
+  int i;
+  int ret = 1;
+
+  const float64_t value_pool[] = {0.2, 0.4};
+  const float64_t reciprocal_pool[] = {5.0, 2.5};
+  float64x2_t value = vld1q_f64 (value_pool);
+  float64x2_t reciprocal = vld1q_f64 (reciprocal_pool);
+
+  float64x2_t step = vrecpeq_f64 (value);
+  /* 3 steps should give us within ~0.001 accuracy.  */
+  for (i = 0; i < 3; i++)
+    step = step * vrecpsq_f64 (step, value);
+
+  ret &= fabs (vgetq_lane_f64 (step, 0)
+	       - vgetq_lane_f64 (reciprocal, 0)) < 0.001;
+  ret &= fabs (vgetq_lane_f64 (step, 1)
+	       - vgetq_lane_f64 (reciprocal, 1)) < 0.001;
+
+  return ret;
+}
+
+/* { dg-final { scan-assembler "frecpe\\tv\[0-9\]+.2d, v\[0-9\]+.2d" } } */
+/* { dg-final { scan-assembler "frecps\\tv\[0-9\]+.2d, v\[0-9\]+.2d, v\[0-9\]+.2d" } } */
+
+int
+main (int argc, char **argv)
+{
+  if (!test_frecps_float32_t ())
+    abort ();
+  if (!test_frecps_float32x2_t ())
+    abort ();
+  if (!test_frecps_float32x4_t ())
+    abort ();
+  if (!test_frecps_float64_t ())
+    abort ();
+  if (!test_frecps_float64x2_t ())
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/ands_2.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/ands_2.c
@@ -0,0 +1,157 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+int
+ands_si_test1 (int a, int b, int c)
+{
+  int d = a & b;
+
+  /* { dg-final { scan-assembler-not "ands\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" } } */
+  /* { dg-final { scan-assembler-times "and\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" 2 } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+ands_si_test2 (int a, int b, int c)
+{
+  int d = a & 0x99999999;
+
+  /* { dg-final { scan-assembler-not "ands\tw\[0-9\]+, w\[0-9\]+, -1717986919" } } */
+  /* { dg-final { scan-assembler "and\tw\[0-9\]+, w\[0-9\]+, -1717986919" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+ands_si_test3 (int a, int b, int c)
+{
+  int d = a & (b << 3);
+
+  /* { dg-final { scan-assembler-not "ands\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  /* { dg-final { scan-assembler "and\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+typedef long long s64;
+
+s64
+ands_di_test1 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & b;
+
+  /* { dg-final { scan-assembler-not "ands\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" } } */
+  /* { dg-final { scan-assembler-times "and\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" 2 } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+ands_di_test2 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & 0xaaaaaaaaaaaaaaaall;
+
+  /* { dg-final { scan-assembler-not "ands\tx\[0-9\]+, x\[0-9\]+, -6148914691236517206" } } */
+  /* { dg-final { scan-assembler "and\tx\[0-9\]+, x\[0-9\]+, -6148914691236517206" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+ands_di_test3 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & (b << 3);
+
+  /* { dg-final { scan-assembler-not "ands\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  /* { dg-final { scan-assembler "and\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+main ()
+{
+  int x;
+  s64 y;
+
+  x = ands_si_test1 (29, 4, 5);
+  if (x != 13)
+    abort ();
+
+  x = ands_si_test1 (5, 2, 20);
+  if (x != 25)
+    abort ();
+
+  x = ands_si_test2 (29, 4, 5);
+  if (x != 34)
+    abort ();
+
+  x = ands_si_test2 (1024, 2, 20);
+  if (x != 1044)
+    abort ();
+
+  x = ands_si_test3 (35, 4, 5);
+  if (x != 41)
+    abort ();
+
+  x = ands_si_test3 (5, 2, 20);
+  if (x != 25)
+    abort ();
+
+  y = ands_di_test1 (0x130000029ll,
+                     0x320000004ll,
+                     0x505050505ll);
+
+  if (y != ((0x130000029ll & 0x320000004ll) + 0x320000004ll + 0x505050505ll))
+    abort ();
+
+  y = ands_di_test1 (0x5000500050005ll,
+                     0x2111211121112ll,
+                     0x0000000002020ll);
+  if (y != 0x5000500052025ll)
+    abort ();
+
+  y = ands_di_test2 (0x130000029ll,
+                     0x320000004ll,
+                     0x505050505ll);
+  if (y != ((0x130000029ll & 0xaaaaaaaaaaaaaaaall) + 0x320000004ll + 0x505050505ll))
+    abort ();
+
+  y = ands_di_test2 (0x540004100ll,
+                     0x320000004ll,
+                     0x805050205ll);
+  if (y != (0x540004100ll + 0x805050205ll))
+    abort ();
+
+  y = ands_di_test3 (0x130000029ll,
+                     0x064000008ll,
+                     0x505050505ll);
+  if (y != ((0x130000029ll & (0x064000008ll << 3))
+	    + 0x064000008ll + 0x505050505ll))
+    abort ();
+
+  y = ands_di_test3 (0x130002900ll,
+                     0x088000008ll,
+                     0x505050505ll);
+  if (y != (0x130002900ll + 0x505050505ll))
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/scalar-vca.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/scalar-vca.c
@@ -0,0 +1,72 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+extern float fabsf (float);
+extern double fabs (double);
+
+#define NUM_TESTS 8
+
+float input_s1[] = {0.1f, -0.1f, 0.4f, 10.3f, 200.0f, -800.0f, -13.0f, -0.5f};
+float input_s2[] = {-0.2f, 0.4f, 0.04f, -100.3f, 2.0f, -80.0f, 13.0f, -0.5f};
+double input_d1[] = {0.1, -0.1, 0.4, 10.3, 200.0, -800.0, -13.0, -0.5};
+double input_d2[] = {-0.2, 0.4, 0.04, -100.3, 2.0, -80.0, 13.0, -0.5};
+
+#define TEST(TEST, CMP, SUFFIX, WIDTH, F)				\
+int									\
+test_fca##TEST##SUFFIX##_float##WIDTH##_t (void)			\
+{									\
+  int ret = 0;								\
+  int i = 0;								\
+  uint##WIDTH##_t output[NUM_TESTS];					\
+									\
+  for (i = 0; i < NUM_TESTS; i++)					\
+    {									\
+      float##WIDTH##_t f1 = fabs##F (input_##SUFFIX##1[i]);		\
+      float##WIDTH##_t f2 = fabs##F (input_##SUFFIX##2[i]);		\
+      /* Inhibit optimization of our linear test loop.  */		\
+      asm volatile ("" : : : "memory");					\
+      output[i] = f1 CMP f2 ? -1 : 0;					\
+    }									\
+									\
+  for (i = 0; i < NUM_TESTS; i++)					\
+    {									\
+      output[i] = vca##TEST##SUFFIX##_f##WIDTH (input_##SUFFIX##1[i],	\
+						input_##SUFFIX##2[i])	\
+						  ^ output[i];		\
+      /* Inhibit autovectorization of our scalar test loop.  */		\
+      asm volatile ("" : : : "memory");					\
+    }									\
+									\
+  for (i = 0; i < NUM_TESTS; i++)					\
+    ret |= output[i];							\
+									\
+  return ret;								\
+}
+
+TEST (ge, >=, s, 32, f)
+/* { dg-final { scan-assembler "facge\\ts\[0-9\]+, s\[0-9\]+, s\[0-9\]+" } } */
+TEST (ge, >=, d, 64, )
+/* { dg-final { scan-assembler "facge\\td\[0-9\]+, d\[0-9\]+, d\[0-9\]+" } } */
+TEST (gt, >, s, 32, f)
+/* { dg-final { scan-assembler "facgt\\ts\[0-9\]+, s\[0-9\]+, s\[0-9\]+" } } */
+TEST (gt, >, d, 64, )
+/* { dg-final { scan-assembler "facgt\\td\[0-9\]+, d\[0-9\]+, d\[0-9\]+" } } */
+
+int
+main (int argc, char **argv)
+{
+  if (test_fcages_float32_t ())
+    abort ();
+  if (test_fcaged_float64_t ())
+    abort ();
+  if (test_fcagts_float32_t ())
+    abort ();
+  if (test_fcagtd_float64_t ())
+    abort ();
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acq_rel.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acq_rel.x
@@ -0,0 +1,37 @@
+int v = 0;
+
+int
+atomic_fetch_add_ACQ_REL (int a)
+{
+  return __atomic_fetch_add (&v, a, __ATOMIC_ACQ_REL);
+}
+
+int
+atomic_fetch_sub_ACQ_REL (int a)
+{
+  return __atomic_fetch_sub (&v, a, __ATOMIC_ACQ_REL);
+}
+
+int
+atomic_fetch_and_ACQ_REL (int a)
+{
+  return __atomic_fetch_and (&v, a, __ATOMIC_ACQ_REL);
+}
+
+int
+atomic_fetch_nand_ACQ_REL (int a)
+{
+  return __atomic_fetch_nand (&v, a, __ATOMIC_ACQ_REL);
+}
+
+int
+atomic_fetch_xor_ACQ_REL (int a)
+{
+  return __atomic_fetch_xor (&v, a, __ATOMIC_ACQ_REL);
+}
+
+int
+atomic_fetch_or_ACQ_REL (int a)
+{
+  return __atomic_fetch_or (&v, a, __ATOMIC_ACQ_REL);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect_smlal_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect_smlal_1.c
@@ -0,0 +1,325 @@
+/* { dg-do run } */
+/* { dg-options "-O3 -fno-inline -save-temps -fno-vect-cost-model" } */
+
+typedef signed char S8_t;
+typedef signed short S16_t;
+typedef signed int S32_t;
+typedef signed long S64_t;
+typedef signed char *__restrict__ pS8_t;
+typedef signed short *__restrict__ pS16_t;
+typedef signed int *__restrict__ pS32_t;
+typedef signed long *__restrict__ pS64_t;
+typedef unsigned char U8_t;
+typedef unsigned short U16_t;
+typedef unsigned int U32_t;
+typedef unsigned long U64_t;
+typedef unsigned char *__restrict__ pU8_t;
+typedef unsigned short *__restrict__ pU16_t;
+typedef unsigned int *__restrict__ pU32_t;
+typedef unsigned long *__restrict__ pU64_t;
+
+extern void abort ();
+
+void
+test_addS64_tS32_t4 (pS64_t a, pS32_t b, pS32_t c)
+{
+  int i;
+  for (i = 0; i < 4; i++)
+    a[i] += (S64_t) b[i] * (S64_t) c[i];
+}
+
+/* { dg-final { scan-assembler "smlal\tv\[0-9\]+\.2d" } } */
+/* { dg-final { scan-assembler "smlal2\tv\[0-9\]+\.2d" } } */
+
+void
+test_addS32_tS16_t8 (pS32_t a, pS16_t b, pS16_t c)
+{
+  int i;
+  for (i = 0; i < 8; i++)
+    a[i] += (S32_t) b[i] * (S32_t) c[i];
+}
+
+/* { dg-final { scan-assembler "smlal\tv\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "smlal2\tv\[0-9\]+\.4s" } } */
+
+void
+test_addS16_tS8_t16 (pS16_t a, pS8_t b, pS8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] += (S16_t) b[i] * (S16_t) c[i];
+}
+
+void
+test_addS16_tS8_t16_neg0 (pS16_t a, pS8_t b, pS8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] += (S16_t) -b[i] * (S16_t) -c[i];
+}
+
+void
+test_addS16_tS8_t16_neg1 (pS16_t a, pS8_t b, pS8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] -= (S16_t) b[i] * (S16_t) -c[i];
+}
+
+void
+test_addS16_tS8_t16_neg2 (pS16_t a, pS8_t b, pS8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] -= (S16_t) -b[i] * (S16_t) c[i];
+}
+
+/* { dg-final { scan-assembler-times "smlal\tv\[0-9\]+\.8h" 4 } } */
+/* { dg-final { scan-assembler-times "smlal2\tv\[0-9\]+\.8h" 4 } } */
+
+void
+test_subS64_tS32_t4 (pS64_t a, pS32_t b, pS32_t c)
+{
+  int i;
+  for (i = 0; i < 4; i++)
+    a[i] -= (S64_t) b[i] * (S64_t) c[i];
+}
+
+/* { dg-final { scan-assembler "smlsl\tv\[0-9\]+\.2d" } } */
+/* { dg-final { scan-assembler "smlsl2\tv\[0-9\]+\.2d" } } */
+
+void
+test_subS32_tS16_t8 (pS32_t a, pS16_t b, pS16_t c)
+{
+  int i;
+  for (i = 0; i < 8; i++)
+    a[i] -= (S32_t) b[i] * (S32_t) c[i];
+}
+
+/* { dg-final { scan-assembler "smlsl\tv\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "smlsl2\tv\[0-9\]+\.4s" } } */
+
+void
+test_subS16_tS8_t16 (pS16_t a, pS8_t b, pS8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] -= (S16_t) b[i] * (S16_t) c[i];
+}
+
+void
+test_subS16_tS8_t16_neg0 (pS16_t a, pS8_t b, pS8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] += (S16_t) -b[i] * (S16_t) c[i];
+}
+
+void
+test_subS16_tS8_t16_neg1 (pS16_t a, pS8_t b, pS8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] += (S16_t) b[i] * (S16_t) -c[i];
+}
+
+void
+test_subS16_tS8_t16_neg2 (pS16_t a, pS8_t b, pS8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] += -((S16_t) b[i] * (S16_t) c[i]);
+}
+
+void
+test_subS16_tS8_t16_neg3 (pS16_t a, pS8_t b, pS8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] -= (S16_t) -b[i] * (S16_t) -c[i];
+}
+
+/* { dg-final { scan-assembler-times "smlsl\tv\[0-9\]+\.8h" 5 } } */
+/* { dg-final { scan-assembler-times "smlsl2\tv\[0-9\]+\.8h" 5 } } */
+
+void
+test_addU64_tU32_t4 (pU64_t a, pU32_t b, pU32_t c)
+{
+  int i;
+  for (i = 0; i < 4; i++)
+    a[i] += (U64_t) b[i] * (U64_t) c[i];
+}
+
+/* { dg-final { scan-assembler "umlal\tv\[0-9\]+\.2d" } } */
+/* { dg-final { scan-assembler "umlal2\tv\[0-9\]+\.2d" } } */
+
+void
+test_addU32_tU16_t8 (pU32_t a, pU16_t b, pU16_t c)
+{
+  int i;
+  for (i = 0; i < 8; i++)
+    a[i] += (U32_t) b[i] * (U32_t) c[i];
+}
+
+/* { dg-final { scan-assembler "umlal\tv\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "umlal2\tv\[0-9\]+\.4s" } } */
+
+void
+test_addU16_tU8_t16 (pU16_t a, pU8_t b, pU8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] += (U16_t) b[i] * (U16_t) c[i];
+}
+
+/* { dg-final { scan-assembler "umlal\tv\[0-9\]+\.8h" } } */
+/* { dg-final { scan-assembler "umlal2\tv\[0-9\]+\.8h" } } */
+
+void
+test_subU64_tU32_t4 (pU64_t a, pU32_t b, pU32_t c)
+{
+  int i;
+  for (i = 0; i < 4; i++)
+    a[i] -= (U64_t) b[i] * (U64_t) c[i];
+}
+
+/* { dg-final { scan-assembler "umlsl\tv\[0-9\]+\.2d" } } */
+/* { dg-final { scan-assembler "umlsl2\tv\[0-9\]+\.2d" } } */
+
+void
+test_subU32_tU16_t8 (pU32_t a, pU16_t b, pU16_t c)
+{
+  int i;
+  for (i = 0; i < 8; i++)
+    a[i] -= (U32_t) b[i] * (U32_t) c[i];
+}
+
+/* { dg-final { scan-assembler "umlsl\tv\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "umlsl2\tv\[0-9\]+\.4s" } } */
+
+void
+test_subU16_tU8_t16 (pU16_t a, pU8_t b, pU8_t c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    a[i] -= (U16_t) b[i] * (U16_t) c[i];
+}
+
+/* { dg-final { scan-assembler "umlsl\tv\[0-9\]+\.8h" } } */
+/* { dg-final { scan-assembler "umlsl2\tv\[0-9\]+\.8h" } } */
+
+
+S64_t add_rS64[4] = { 6, 7, -4, -3 };
+S32_t add_rS32[8] = { 6, 7, -4, -3, 10, 11, 0, 1 };
+S16_t add_rS16[16] =
+ { 6, 7, -4, -3, 10, 11, 0, 1, 14, 15, 4, 5, 18, 19, 8, 9 };
+
+S64_t sub_rS64[4] = { 0, 1, 2, 3 };
+S32_t sub_rS32[8] = { 0, 1, 2, 3, 4, 5, 6, 7 };
+S16_t sub_rS16[16] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };
+
+U64_t add_rU64[4] = { 0x6, 0x7, 0x2fffffffc, 0x2fffffffd };
+
+U32_t add_rU32[8] =
+{
+  0x6, 0x7, 0x2fffc, 0x2fffd,
+  0xa, 0xb, 0x30000, 0x30001
+};
+
+U16_t add_rU16[16] =
+{
+  0x6, 0x7, 0x2fc, 0x2fd, 0xa, 0xb, 0x300, 0x301,
+  0xe, 0xf, 0x304, 0x305, 0x12, 0x13, 0x308, 0x309
+};
+
+U64_t sub_rU64[4] = { 0, 1, 2, 3 };
+U32_t sub_rU32[8] = { 0, 1, 2, 3, 4, 5, 6, 7 };
+U16_t sub_rU16[16] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };
+
+S8_t neg_r[16] = { -6, -5, 8, 9, -2, -1, 12, 13, 2, 3, 16, 17, 6, 7, 20, 21 };
+
+S64_t S64_ta[16] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };
+S32_t S32_tb[16] = { 2, 2, -2, -2, 2, 2, -2, -2, 2, 2, -2, -2, 2, 2, -2, -2 };
+S32_t S32_tc[16] = { 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3 };
+
+S32_t S32_ta[16] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };
+S16_t S16_tb[16] = { 2, 2, -2, -2, 2, 2, -2, -2, 2, 2, -2, -2, 2, 2, -2, -2 };
+S16_t S16_tc[16] = { 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3 };
+
+S16_t S16_ta[16] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };
+S8_t S8_tb[16] = { 2, 2, -2, -2, 2, 2, -2, -2, 2, 2, -2, -2, 2, 2, -2, -2 };
+S8_t S8_tc[16] = { 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3 };
+
+
+#define CHECK(T,N,AS,US)                      \
+do                                            \
+  {                                           \
+    for (i = 0; i < N; i++)                   \
+      if (S##T##_ta[i] != AS##_r##US##T[i])   \
+        abort ();                             \
+  }                                           \
+while (0)
+
+#define SCHECK(T,N,AS) CHECK(T,N,AS,S)
+#define UCHECK(T,N,AS) CHECK(T,N,AS,U)
+
+#define NCHECK(RES)                           \
+do                                            \
+  {                                           \
+    for (i = 0; i < 16; i++)                  \
+      if (S16_ta[i] != RES[i])                \
+        abort ();                             \
+  }                                           \
+while (0)
+
+
+int
+main ()
+{
+  int i;
+
+  test_addS64_tS32_t4 (S64_ta, S32_tb, S32_tc);
+  SCHECK (64, 4, add);
+  test_addS32_tS16_t8 (S32_ta, S16_tb, S16_tc);
+  SCHECK (32, 8, add);
+  test_addS16_tS8_t16 (S16_ta, S8_tb, S8_tc);
+  SCHECK (16, 16, add);
+  test_subS64_tS32_t4 (S64_ta, S32_tb, S32_tc);
+  SCHECK (64, 4, sub);
+  test_subS32_tS16_t8 (S32_ta, S16_tb, S16_tc);
+  SCHECK (32, 8, sub);
+  test_subS16_tS8_t16 (S16_ta, S8_tb, S8_tc);
+  SCHECK (16, 16, sub);
+
+  test_addU64_tU32_t4 (S64_ta, S32_tb, S32_tc);
+  UCHECK (64, 4, add);
+  test_addU32_tU16_t8 (S32_ta, S16_tb, S16_tc);
+  UCHECK (32, 8, add);
+  test_addU16_tU8_t16 (S16_ta, S8_tb, S8_tc);
+  UCHECK (16, 16, add);
+  test_subU64_tU32_t4 (S64_ta, S32_tb, S32_tc);
+  UCHECK (64, 4, sub);
+  test_subU32_tU16_t8 (S32_ta, S16_tb, S16_tc);
+  UCHECK (32, 8, sub);
+  test_subU16_tU8_t16 (S16_ta, S8_tb, S8_tc);
+  UCHECK (16, 16, sub);
+
+  test_addS16_tS8_t16_neg0 (S16_ta, S8_tb, S8_tc);
+  NCHECK (add_rS16);
+  test_subS16_tS8_t16_neg0 (S16_ta, S8_tb, S8_tc);
+  NCHECK (sub_rS16);
+  test_addS16_tS8_t16_neg1 (S16_ta, S8_tb, S8_tc);
+  NCHECK (add_rS16);
+  test_subS16_tS8_t16_neg1 (S16_ta, S8_tb, S8_tc);
+  NCHECK (sub_rS16);
+  test_addS16_tS8_t16_neg2 (S16_ta, S8_tb, S8_tc);
+  NCHECK (add_rS16);
+  test_subS16_tS8_t16_neg2 (S16_ta, S8_tb, S8_tc);
+  NCHECK (sub_rS16);
+  test_subS16_tS8_t16_neg3 (S16_ta, S8_tb, S8_tc);
+  NCHECK (neg_r);
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/extr.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/extr.c
@@ -0,0 +1,34 @@
+/* { dg-options "-O2 --save-temps" } */
+/* { dg-do run } */
+
+extern void abort (void);
+
+int
+test_si (int a, int b)
+{
+  /* { dg-final { scan-assembler "extr\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, 27\n" } } */
+  return (a << 5) | ((unsigned int) b >> 27);
+}
+
+long long
+test_di (long long a, long long b)
+{
+  /* { dg-final { scan-assembler "extr\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, 45\n" } } */
+  return (a << 19) | ((unsigned long long) b >> 45);
+}
+
+int
+main ()
+{
+  int v;
+  long long w;
+  v = test_si (0x00000004, 0x30000000);
+  if (v != 0x00000086)
+    abort();
+  w = test_di (0x0001040040040004ll, 0x0070050066666666ll);
+  if (w != 0x2002002000200380ll)
+    abort();
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-compile.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-compile.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-compile.c
@@ -16,5 +16,7 @@
 /* { dg-final { scan-assembler "uminv" } } */
 /* { dg-final { scan-assembler "smaxv" } } */
 /* { dg-final { scan-assembler "sminv" } } */
+/* { dg-final { scan-assembler "sabd" } } */
+/* { dg-final { scan-assembler "saba" } } */
 /* { dg-final { scan-assembler-times "addv" 2} } */
 /* { dg-final { scan-assembler-times "addp" 2} } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-eq-d.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-eq-d.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-eq-d.c
@@ -2,12 +2,13 @@
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-all -fno-unroll-loops --save-temps -fno-inline" } */
 
 #define FTYPE double
+#define ITYPE long
 #define OP ==
 #define INV_OP !=
 
 #include "vect-fcm.x"
 
-/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 4 "vect" } } */
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 8 "vect" } } */
 /* { dg-final { scan-assembler "fcmeq\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
 /* { dg-final { scan-assembler "fcmeq\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, 0" } } */
 /* { dg-final { cleanup-tree-dump "vect" } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/cpu-diagnostics-3.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/cpu-diagnostics-3.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/cpu-diagnostics-3.c
@@ -1,5 +1,5 @@
 /* { dg-error "unknown" "" {target "aarch64*-*-*" } } */
-/* { dg-options "-O2 -mcpu=example-1+dummy" } */
+/* { dg-options "-O2 -mcpu=cortex-a53+dummy" } */
 
 void f ()
 {
Index: b/src/gcc/testsuite/gcc.target/aarch64/adds3.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/adds3.c
@@ -0,0 +1,61 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+typedef long long s64;
+
+int
+adds_ext (s64 a, int b, int c)
+{
+ s64 d = a + b;
+
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+adds_shift_ext (s64 a, int b, int c)
+{
+ s64 d = (a + ((s64)b << 3));
+
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int main ()
+{
+  int x;
+  s64 y;
+
+  x = adds_ext (0x13000002ll, 41, 15);
+  if (x != 318767203)
+    abort ();
+
+  x = adds_ext (0x50505050ll, 29, 4);
+  if (x != 1347440782)
+    abort ();
+
+  x = adds_ext (0x12121212121ll, 2, 14);
+  if (x != 555819315)
+    abort ();
+
+  x = adds_shift_ext (0x123456789ll, 4, 12);
+  if (x != 591751097)
+    abort ();
+
+  x = adds_shift_ext (0x02020202ll, 9, 8);
+  if (x != 33686107)
+    abort ();
+
+  x = adds_shift_ext (0x987987987987ll, 23, 41);
+  if (x != -2020050305)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { scan-assembler-times "adds\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, sxtw" 2 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/subs2.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/subs2.c
@@ -0,0 +1,155 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+int
+subs_si_test1 (int a, int b, int c)
+{
+  int d = a - b;
+
+  /* { dg-final { scan-assembler-not "subs\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" } } */
+  /* { dg-final { scan-assembler "sub\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+subs_si_test2 (int a, int b, int c)
+{
+  int d = a - 0xfff;
+
+  /* { dg-final { scan-assembler-not "subs\tw\[0-9\]+, w\[0-9\]+, #4095" } } */
+  /* { dg-final { scan-assembler "sub\tw\[0-9\]+, w\[0-9\]+, #4095" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+subs_si_test3 (int a, int b, int c)
+{
+  int d = a - (b << 3);
+
+  /* { dg-final { scan-assembler-not "subs\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  /* { dg-final { scan-assembler "sub\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+typedef long long s64;
+
+s64
+subs_di_test1 (s64 a, s64 b, s64 c)
+{
+  s64 d = a - b;
+
+  /* { dg-final { scan-assembler-not "subs\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" } } */
+  /* { dg-final { scan-assembler "sub\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+subs_di_test2 (s64 a, s64 b, s64 c)
+{
+  s64 d = a - 0x1000ll;
+
+  /* { dg-final { scan-assembler-not "subs\tx\[0-9\]+, x\[0-9\]+, #4096" } } */
+  /* { dg-final { scan-assembler "sub\tx\[0-9\]+, x\[0-9\]+, #4096" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+subs_di_test3 (s64 a, s64 b, s64 c)
+{
+  s64 d = a - (b << 3);
+
+  /* { dg-final { scan-assembler-not "subs\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  /* { dg-final { scan-assembler "sub\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int main ()
+{
+  int x;
+  s64 y;
+
+  x = subs_si_test1 (29, 4, 5);
+  if (x != 34)
+    abort ();
+
+  x = subs_si_test1 (5, 2, 20);
+  if (x != 25)
+    abort ();
+
+  x = subs_si_test2 (29, 4, 5);
+  if (x != 34)
+    abort ();
+
+  x = subs_si_test2 (1024, 2, 20);
+  if (x != 1044)
+    abort ();
+
+  x = subs_si_test3 (35, 4, 5);
+  if (x != 12)
+    abort ();
+
+  x = subs_si_test3 (5, 2, 20);
+  if (x != 25)
+    abort ();
+
+  y = subs_di_test1 (0x130000029ll,
+		     0x320000004ll,
+		     0x505050505ll);
+
+  if (y != 0x63505052e)
+    abort ();
+
+  y = subs_di_test1 (0x5000500050005ll,
+		     0x2111211121112ll,
+		     0x0000000002020ll);
+  if (y != 0x5000500052025)
+    abort ();
+
+  y = subs_di_test2 (0x130000029ll,
+		     0x320000004ll,
+		     0x505050505ll);
+  if (y != 0x95504f532)
+    abort ();
+
+  y = subs_di_test2 (0x540004100ll,
+		     0x320000004ll,
+		     0x805050205ll);
+  if (y != 0x1065053309)
+    abort ();
+
+  y = subs_di_test3 (0x130000029ll,
+		     0x064000008ll,
+		     0x505050505ll);
+  if (y != 0x63505052e)
+    abort ();
+
+  y = subs_di_test3 (0x130002900ll,
+		     0x088000008ll,
+		     0x505050505ll);
+  if (y != 0x635052e05)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/bics_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/bics_1.c
@@ -0,0 +1,107 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+int
+bics_si_test1 (int a, int b, int c)
+{
+  int d = a & ~b;
+
+  /* { dg-final { scan-assembler-times "bics\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" 2 } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+bics_si_test2 (int a, int b, int c)
+{
+  int d = a & ~(b << 3);
+
+  /* { dg-final { scan-assembler "bics\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+typedef long long s64;
+
+s64
+bics_di_test1 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & ~b;
+
+  /* { dg-final { scan-assembler-times "bics\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" 2 } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+bics_di_test2 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & ~(b << 3);
+
+  /* { dg-final { scan-assembler "bics\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+main ()
+{
+  int x;
+  s64 y;
+
+  x = bics_si_test1 (29, ~4, 5);
+  if (x != ((29 & 4) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test1 (5, ~2, 20);
+  if (x != 25)
+    abort ();
+
+  x = bics_si_test2 (35, ~4, 5);
+  if (x != ((35 & ~(~4 << 3)) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test2 (96, ~2, 20);
+  if (x != 116)
+    abort ();
+
+  y = bics_di_test1 (0x130000029ll,
+                     ~0x320000004ll,
+                     0x505050505ll);
+
+  if (y != ((0x130000029ll & 0x320000004ll) + ~0x320000004ll + 0x505050505ll))
+    abort ();
+
+  y = bics_di_test1 (0x5000500050005ll,
+                     ~0x2111211121112ll,
+                     0x0000000002020ll);
+  if (y != 0x5000500052025ll)
+    abort ();
+
+  y = bics_di_test2 (0x130000029ll,
+                     ~0x064000008ll,
+                     0x505050505ll);
+  if (y != ((0x130000029ll & ~(~0x064000008ll << 3))
+	    + ~0x064000008ll + 0x505050505ll))
+    abort ();
+
+  y = bics_di_test2 (0x130002900ll,
+                     ~0x088000008ll,
+                     0x505050505ll);
+  if (y != (0x130002900ll + 0x505050505ll))
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-vmaxv.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-vmaxv.c
@@ -0,0 +1,117 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps -ffast-math" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+
+#define NUM_TESTS 16
+#define DELTA 0.000001
+
+int8_t input_int8[] = {1, 56, 2, -9, -90, 23, 54, 76,
+		       -4, 34, 110, -110, 6, 4, 75, -34};
+int16_t input_int16[] = {1, 56, 2, -9, -90, 23, 54, 76,
+			 -4, 34, 110, -110, 6, 4, 75, -34};
+int32_t input_int32[] = {1, 56, 2, -9, -90, 23, 54, 76,
+			 -4, 34, 110, -110, 6, 4, 75, -34};
+
+uint8_t input_uint8[] = {1, 56, 2, 9, 90, 23, 54, 76,
+			 4, 34, 110, 110, 6, 4, 75, 34};
+uint16_t input_uint16[] = {1, 56, 2, 9, 90, 23, 54, 76,
+			   4, 34, 110, 110, 6, 4, 75, 34};
+uint32_t input_uint32[] = {1, 56, 2, 9, 90, 23, 54, 76,
+			   4, 34, 110, 110, 6, 4, 75, 34};
+
+#define EQUAL(a, b) (a == b)
+
+#define TEST(MAXMIN, CMP_OP, SUFFIX, Q, TYPE, LANES)			\
+int									\
+test_v##MAXMIN##v##SUFFIX##_##TYPE##x##LANES##_t (void)			\
+{									\
+  int i, j;								\
+  int moves = (NUM_TESTS - LANES) + 1;					\
+  TYPE##_t out_l[NUM_TESTS];						\
+  TYPE##_t out_v[NUM_TESTS];						\
+									\
+  /* Calculate linearly.  */						\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      out_l[i] = input_##TYPE[i];					\
+      for (j = 0; j < LANES; j++)					\
+	out_l[i] = input_##TYPE[i + j] CMP_OP out_l[i]  ?		\
+	  input_##TYPE[i + j] : out_l[i];				\
+    }									\
+									\
+  /* Calculate using vector reduction intrinsics.  */			\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      TYPE##x##LANES##_t t1 = vld1##Q##_##SUFFIX (input_##TYPE + i);	\
+      out_v[i] = v##MAXMIN##v##Q##_##SUFFIX (t1);			\
+    }									\
+									\
+  /* Compare.  */							\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      if (!EQUAL (out_v[i], out_l[i]))					\
+	return 0;							\
+    }									\
+  return 1;								\
+}
+
+#define BUILD_VARIANTS(TYPE, STYPE, W32, W64)		\
+TEST (max, >, STYPE,  , TYPE, W32)			\
+TEST (max, >, STYPE, q, TYPE, W64)			\
+TEST (min, <, STYPE,  , TYPE, W32)			\
+TEST (min, <, STYPE, q, TYPE, W64)
+
+BUILD_VARIANTS (int8, s8, 8, 16)
+/* { dg-final { scan-assembler "smaxv\\tb\[0-9\]+, v\[0-9\]+\.8b" } } */
+/* { dg-final { scan-assembler "sminv\\tb\[0-9\]+, v\[0-9\]+\.8b" } } */
+/* { dg-final { scan-assembler "smaxv\\tb\[0-9\]+, v\[0-9\]+\.16b" } } */
+/* { dg-final { scan-assembler "sminv\\tb\[0-9\]+, v\[0-9\]+\.16b" } } */
+BUILD_VARIANTS (uint8, u8, 8, 16)
+/* { dg-final { scan-assembler "umaxv\\tb\[0-9\]+, v\[0-9\]+\.8b" } } */
+/* { dg-final { scan-assembler "uminv\\tb\[0-9\]+, v\[0-9\]+\.8b" } } */
+/* { dg-final { scan-assembler "umaxv\\tb\[0-9\]+, v\[0-9\]+\.16b" } } */
+/* { dg-final { scan-assembler "uminv\\tb\[0-9\]+, v\[0-9\]+\.16b" } } */
+BUILD_VARIANTS (int16, s16, 4, 8)
+/* { dg-final { scan-assembler "smaxv\\th\[0-9\]+, v\[0-9\]+\.4h" } } */
+/* { dg-final { scan-assembler "sminv\\th\[0-9\]+, v\[0-9\]+\.4h" } } */
+/* { dg-final { scan-assembler "smaxv\\th\[0-9\]+, v\[0-9\]+\.8h" } } */
+/* { dg-final { scan-assembler "sminv\\th\[0-9\]+, v\[0-9\]+\.8h" } } */
+BUILD_VARIANTS (uint16, u16, 4, 8)
+/* { dg-final { scan-assembler "umaxv\\th\[0-9\]+, v\[0-9\]+\.4h" } } */
+/* { dg-final { scan-assembler "uminv\\th\[0-9\]+, v\[0-9\]+\.4h" } } */
+/* { dg-final { scan-assembler "umaxv\\th\[0-9\]+, v\[0-9\]+\.8h" } } */
+/* { dg-final { scan-assembler "uminv\\th\[0-9\]+, v\[0-9\]+\.8h" } } */
+BUILD_VARIANTS (int32, s32, 2, 4)
+/* { dg-final { scan-assembler "smaxp\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "sminp\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "smaxv\\ts\[0-9\]+, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "sminv\\ts\[0-9\]+, v\[0-9\]+\.4s" } } */
+BUILD_VARIANTS (uint32, u32, 2, 4)
+/* { dg-final { scan-assembler "umaxp\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "uminp\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "umaxv\\ts\[0-9\]+, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "uminv\\ts\[0-9\]+, v\[0-9\]+\.4s" } } */
+
+#undef TEST
+#define TEST(MAXMIN, CMP_OP, SUFFIX, Q, TYPE, LANES)		\
+{								\
+  if (!test_v##MAXMIN##v##SUFFIX##_##TYPE##x##LANES##_t ())	\
+    abort ();							\
+}
+
+int
+main (int argc, char **argv)
+{
+  BUILD_VARIANTS (int8, s8, 8, 16)
+  BUILD_VARIANTS (uint8, u8, 8, 16)
+  BUILD_VARIANTS (int16, s16, 4, 8)
+  BUILD_VARIANTS (uint16, u16, 4, 8)
+  BUILD_VARIANTS (int32, s32, 2, 4)
+  BUILD_VARIANTS (uint32, u32, 2, 4)
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vrecpx.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vrecpx.c
@@ -0,0 +1,54 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps" } */
+
+#include <arm_neon.h>
+#include <math.h>
+#include <stdlib.h>
+
+float32_t in_f[] =
+{2.0, 4.0, 8.0, 16.0, 1.0, 0.5, 0.25, 0.125};
+float32_t rec_f[] =
+{1.0, 0.5, 0.25, 0.125, 2.0, 4.0, 8.0, 16.0};
+float64_t in_d[] =
+{2.0, 4.0, 8.0, 16.0, 1.0, 0.5, 0.25, 0.125};
+float32_t rec_d[] =
+{1.0, 0.5, 0.25, 0.125, 2.0, 4.0, 8.0, 16.0};
+
+int
+test_frecpx_float32_t (void)
+{
+  int i = 0;
+  int ret = 1;
+  for (i = 0; i < 8; i++)
+    ret &= fabs (vrecpxs_f32 (in_f[i]) - rec_f[i]) < 0.001;
+
+  return ret;
+}
+
+/* { dg-final { scan-assembler "frecpx\\ts\[0-9\]+, s\[0-9\]+" } } */
+
+int
+test_frecpx_float64_t (void)
+{
+  int i = 0;
+  int ret = 1;
+  for (i = 0; i < 8; i++)
+    ret &= fabs (vrecpxd_f64 (in_d[i]) - rec_d[i]) < 0.001;
+
+  return ret;
+}
+
+/* { dg-final { scan-assembler "frecpx\\td\[0-9\]+, d\[0-9\]+" } } */
+
+int
+main (int argc, char **argv)
+{
+  if (!test_frecpx_float32_t ())
+    abort ();
+  if (!test_frecpx_float64_t ())
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-vca.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-vca.c
@@ -0,0 +1,89 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+extern float fabsf (float);
+extern double fabs (double);
+
+#define NUM_TESTS 8
+
+float input_s1[] = {0.1f, -0.1f, 0.4f, 10.3f, 200.0f, -800.0f, -13.0f, -0.5f};
+float input_s2[] = {-0.2f, 0.4f, 0.04f, -100.3f, 2.0f, -80.0f, 13.0f, -0.5f};
+double input_d1[] = {0.1, -0.1, 0.4, 10.3, 200.0, -800.0, -13.0, -0.5};
+double input_d2[] = {-0.2, 0.4, 0.04, -100.3, 2.0, -80.0, 13.0, -0.5};
+
+#define TEST(T, CMP, SUFFIX, WIDTH, LANES, Q, F)			\
+int									\
+test_vca##T##_float##WIDTH##x##LANES##_t (void)				\
+{									\
+  int ret = 0;								\
+  int i = 0;								\
+  uint##WIDTH##_t output[NUM_TESTS];					\
+									\
+  for (i = 0; i < NUM_TESTS; i++)					\
+    {									\
+      float##WIDTH##_t f1 = fabs##F (input_##SUFFIX##1[i]);		\
+      float##WIDTH##_t f2 = fabs##F (input_##SUFFIX##2[i]);		\
+      /* Inhibit optimization of our linear test loop.  */		\
+      asm volatile ("" : : : "memory");					\
+      output[i] = f1 CMP f2 ? -1 : 0;					\
+    }									\
+									\
+  for (i = 0; i < NUM_TESTS; i += LANES)				\
+    {									\
+      float##WIDTH##x##LANES##_t in1 =					\
+	vld1##Q##_f##WIDTH (input_##SUFFIX##1 + i);			\
+      float##WIDTH##x##LANES##_t in2 =					\
+	vld1##Q##_f##WIDTH (input_##SUFFIX##2 + i);			\
+      uint##WIDTH##x##LANES##_t expected_out =				\
+	vld1##Q##_u##WIDTH (output + i);				\
+      uint##WIDTH##x##LANES##_t out =					\
+	veor##Q##_u##WIDTH (vca##T##Q##_f##WIDTH (in1, in2),		\
+			    expected_out);				\
+      vst1##Q##_u##WIDTH (output + i, out);				\
+    }									\
+									\
+  for (i = 0; i < NUM_TESTS; i++)					\
+    ret |= output[i];							\
+									\
+  return ret;								\
+}
+
+#define BUILD_VARIANTS(T, CMP)	\
+TEST (T, CMP, s, 32, 2,  , f)	\
+TEST (T, CMP, s, 32, 4, q, f)	\
+TEST (T, CMP, d, 64, 2, q,  )
+
+BUILD_VARIANTS (ge, >=)
+/* { dg-final { scan-assembler "facge\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "facge\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "facge\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+
+BUILD_VARIANTS (gt, >)
+/* { dg-final { scan-assembler "facgt\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "facgt\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "facgt\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+
+/* No need for another scan-assembler as these tests
+   also generate facge, facgt instructions.  */
+BUILD_VARIANTS (le, <=)
+BUILD_VARIANTS (lt, <)
+
+#undef TEST
+#define TEST(T, CMP, SUFFIX, WIDTH, LANES, Q, F)	\
+if (test_vca##T##_float##WIDTH##x##LANES##_t ())	\
+  abort ();
+
+int
+main (int argc, char **argv)
+{
+BUILD_VARIANTS (ge, >=)
+BUILD_VARIANTS (gt, >)
+BUILD_VARIANTS (le, <=)
+BUILD_VARIANTS (lt, <)
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-vrnd.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-vrnd.c
@@ -0,0 +1,117 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+extern float fabsf (float);
+extern double fabs (double);
+
+extern double trunc (double);
+extern double round (double);
+extern double nearbyint (double);
+extern double floor (double);
+extern double ceil (double);
+extern double rint (double);
+
+extern float truncf (float);
+extern float roundf (float);
+extern float nearbyintf (float);
+extern float floorf (float);
+extern float ceilf (float);
+extern float rintf (float);
+
+#define NUM_TESTS 8
+#define DELTA 0.000001
+
+float input_f32[] = {0.1f, -0.1f, 0.4f, 10.3f,
+		     200.0f, -800.0f, -13.0f, -0.5f};
+double input_f64[] = {0.1, -0.1, 0.4, 10.3,
+		      200.0, -800.0, -13.0, -0.5};
+
+#define TEST(SUFFIX, Q, WIDTH, LANES, C_FN, F)		     		\
+int									\
+test_vrnd##SUFFIX##_float##WIDTH##x##LANES##_t (void)			\
+{									\
+  int ret = 1;								\
+  int i = 0;								\
+  int nlanes = LANES;							\
+  float##WIDTH##_t expected_out[NUM_TESTS];				\
+  float##WIDTH##_t actual_out[NUM_TESTS];				\
+									\
+  for (i = 0; i < NUM_TESTS; i++)					\
+    {									\
+      expected_out[i] = C_FN##F (input_f##WIDTH[i]);			\
+      /* Don't vectorize this.  */					\
+      asm volatile ("" : : : "memory");					\
+    }									\
+									\
+  /* Prevent the compiler from noticing these two loops do the same	\
+     thing and optimizing away the comparison.  */			\
+  asm volatile ("" : : : "memory");					\
+									\
+  for (i = 0; i < NUM_TESTS; i+=nlanes)					\
+    {									\
+      float##WIDTH##x##LANES##_t out =					\
+	vrnd##SUFFIX##Q##_f##WIDTH					\
+		(vld1##Q##_f##WIDTH (input_f##WIDTH + i));		\
+      vst1##Q##_f##WIDTH (actual_out + i, out);				\
+    }									\
+									\
+  for (i = 0; i < NUM_TESTS; i++)					\
+    ret &= fabs##F (expected_out[i] - actual_out[i]) < DELTA;		\
+									\
+  return ret;								\
+}									\
+
+
+#define BUILD_VARIANTS(SUFFIX, C_FN)	\
+TEST (SUFFIX,  , 32, 2, C_FN, f)	\
+TEST (SUFFIX, q, 32, 4, C_FN, f)	\
+TEST (SUFFIX, q, 64, 2, C_FN,  )	\
+
+BUILD_VARIANTS ( , trunc)
+/* { dg-final { scan-assembler "frintz\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "frintz\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "frintz\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+BUILD_VARIANTS (a, round)
+/* { dg-final { scan-assembler "frinta\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "frinta\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "frinta\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+BUILD_VARIANTS (i, nearbyint)
+/* { dg-final { scan-assembler "frinti\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "frinti\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "frinti\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+BUILD_VARIANTS (m, floor)
+/* { dg-final { scan-assembler "frintm\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "frintm\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "frintm\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+BUILD_VARIANTS (p, ceil)
+/* { dg-final { scan-assembler "frintp\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "frintp\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "frintp\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+BUILD_VARIANTS (x, rint)
+/* { dg-final { scan-assembler "frintx\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "frintx\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "frintx\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+
+#undef TEST
+#define TEST(SUFFIX, Q, WIDTH, LANES, C_FN, F)			\
+{								\
+  if (!test_vrnd##SUFFIX##_float##WIDTH##x##LANES##_t ())	\
+    abort ();							\
+}
+
+int
+main (int argc, char **argv)
+{
+  BUILD_VARIANTS ( , trunc)
+  BUILD_VARIANTS (a, round)
+  BUILD_VARIANTS (i, nearbyint)
+  BUILD_VARIANTS (m, floor)
+  BUILD_VARIANTS (p, ceil)
+  BUILD_VARIANTS (x, rint)
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-relaxed.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-op-relaxed.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-relaxed.c
@@ -1,43 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-int v = 0;
-
-int
-atomic_fetch_add_RELAXED (int a)
-{
-  return __atomic_fetch_add (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_sub_RELAXED (int a)
-{
-  return __atomic_fetch_sub (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_and_RELAXED (int a)
-{
-  return __atomic_fetch_and (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_nand_RELAXED (int a)
-{
-  return __atomic_fetch_nand (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_xor_RELAXED (int a)
-{
-  return __atomic_fetch_xor (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_or_RELAXED (int a)
-{
-  return __atomic_fetch_or (&v, a, __ATOMIC_RELAXED);
-}
+#include "atomic-op-relaxed.x"
 
 /* { dg-final { scan-assembler-times "ldxr\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
 /* { dg-final { scan-assembler-times "stxr\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/aes_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/aes_1.c
@@ -0,0 +1,40 @@
+
+/* { dg-do compile } */
+/* { dg-options "-march=armv8-a+crypto" } */
+
+#include "arm_neon.h"
+
+uint8x16_t
+test_vaeseq_u8 (uint8x16_t data, uint8x16_t key)
+{
+  return vaeseq_u8 (data, key);
+}
+
+/* { dg-final { scan-assembler-times "aese\\tv\[0-9\]+\.16b, v\[0-9\]+\.16b" 1 } } */
+
+uint8x16_t
+test_vaesdq_u8 (uint8x16_t data, uint8x16_t key)
+{
+  return vaesdq_u8 (data, key);
+}
+
+/* { dg-final { scan-assembler-times "aesd\\tv\[0-9\]+\.16b, v\[0-9\]+\.16b" 1 } } */
+
+uint8x16_t
+test_vaesmcq_u8 (uint8x16_t data)
+{
+  return vaesmcq_u8 (data);
+}
+
+/* { dg-final { scan-assembler-times "aesmc\\tv\[0-9\]+\.16b, v\[0-9\]+\.16b" 1 } } */
+
+uint8x16_t
+test_vaesimcq_u8 (uint8x16_t data)
+{
+  return vaesimcq_u8 (data);
+}
+
+/* { dg-final { scan-assembler-times "aesimc\\tv\[0-9\]+\.16b, v\[0-9\]+\.16b" 1 } } */
+
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm.x
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fcm.x
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm.x
@@ -13,6 +13,8 @@ FTYPE input2[N] =
  2.0, -4.0, 8.0, -16.0,
  -2.125, 4.25, -8.5, 17.0};
 
+/* Float comparisons, float results.  */
+
 void
 foo (FTYPE *in1, FTYPE *in2, FTYPE *output)
 {
@@ -49,11 +51,52 @@ foobarbar (FTYPE *in1, FTYPE *in2, FTYPE
     output[i] = (in1[i] INV_OP 0.0) ? 4.0 : 2.0;
 }
 
+/* Float comparisons, int results.  */
+
+void
+foo_int (FTYPE *in1, FTYPE *in2, ITYPE *output)
+{
+  int i = 0;
+  /* Vectorizable.  */
+  for (i = 0; i < N; i++)
+    output[i] = (in1[i] OP in2[i]) ? 2 : 4;
+}
+
+void
+bar_int (FTYPE *in1, FTYPE *in2, ITYPE *output)
+{
+  int i = 0;
+  /* Vectorizable.  */
+  for (i = 0; i < N; i++)
+    output[i] = (in1[i] INV_OP in2[i]) ? 4 : 2;
+}
+
+void
+foobar_int (FTYPE *in1, FTYPE *in2, ITYPE *output)
+{
+  int i = 0;
+  /* Vectorizable.  */
+  for (i = 0; i < N; i++)
+    output[i] = (in1[i] OP 0.0) ? 4 : 2;
+}
+
+void
+foobarbar_int (FTYPE *in1, FTYPE *in2, ITYPE *output)
+{
+  int i = 0;
+  /* Vectorizable.  */
+  for (i = 0; i < N; i++)
+    output[i] = (in1[i] INV_OP 0.0) ? 4 : 2;
+}
+
 int
 main (int argc, char **argv)
 {
   FTYPE out1[N];
   FTYPE out2[N];
+  ITYPE outi1[N];
+  ITYPE outi2[N];
+
   int i = 0;
   foo (input1, input2, out1);
   bar (input1, input2, out2);
@@ -65,6 +108,17 @@ main (int argc, char **argv)
   for (i = 0; i < N; i++)
     if (out1[i] == out2[i])
       abort ();
+
+  foo_int (input1, input2, outi1);
+  bar_int (input1, input2, outi2);
+  for (i = 0; i < N; i++)
+    if (outi1[i] != outi2[i])
+      abort ();
+  foobar_int (input1, input2, outi1);
+  foobarbar_int (input1, input2, outi2);
+  for (i = 0; i < N; i++)
+    if (outi1[i] == outi2[i])
+      abort ();
   return 0;
 }
 
Index: b/src/gcc/testsuite/gcc.target/aarch64/movi_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/movi_1.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+void
+dummy (short* b)
+{
+  /* { dg-final { scan-assembler "movi\tv\[0-9\]+\.4h, 0x4, lsl 8" } } */
+  /* { dg-final { scan-assembler-not "movi\tv\[0-9\]+\.4h, 0x400" } } */
+  /* { dg-final { scan-assembler-not "movi\tv\[0-9\]+\.4h, 1024" } } */
+  register short x asm ("h8") = 1024;
+  asm volatile ("" : : "w" (x));
+  *b = x;
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/vaddv-intrinsic-compile.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vaddv-intrinsic-compile.c
@@ -0,0 +1,11 @@
+
+/* { dg-do compile } */
+/* { dg-options "-O3" } */
+
+#include "arm_neon.h"
+
+#include "vaddv-intrinsic.x"
+
+/* { dg-final { scan-assembler "faddp\\ts\[0-9\]+"} } */
+/* { dg-final { scan-assembler-times "faddp\\tv\[0-9\]+\.4s" 2} } */
+/* { dg-final { scan-assembler "faddp\\td\[0-9\]+"} } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vabs_intrinsic_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vabs_intrinsic_1.c
@@ -0,0 +1,101 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+
+#define ETYPE(size) int##size##_t
+#define VTYPE(size, lanes) int##size##x##lanes##_t
+
+#define TEST_VABS(q, size, lanes)				\
+static void							\
+test_vabs##q##_##size (ETYPE (size) * res,			\
+			const ETYPE (size) *in1)		\
+{								\
+  VTYPE (size, lanes) a = vld1##q##_s##size (res);		\
+  VTYPE (size, lanes) b = vld1##q##_s##size (in1);		\
+  a = vabs##q##_s##size (b);					\
+  vst1##q##_s##size (res, a);					\
+}
+
+#define BUILD_VARS(width, n_lanes, n_half_lanes)		\
+TEST_VABS (, width, n_half_lanes)				\
+TEST_VABS (q, width, n_lanes)					\
+
+BUILD_VARS (64, 2, 1)
+BUILD_VARS (32, 4, 2)
+BUILD_VARS (16, 8, 4)
+BUILD_VARS (8, 16, 8)
+
+#define POOL1  {-10}
+#define POOL2  {2, -10}
+#define POOL4  {0, -10, 2, -3}
+#define POOL8  {0, -10, 2, -3, 4, -50, 6, -70}
+#define POOL16 {0, -10, 2, -3, 4, -50, 6, -70,			\
+		-5, 10, -2, 3, -4, 50, -6, 70}
+
+#define EXPECTED1  {10}
+#define EXPECTED2  {2, 10}
+#define EXPECTED4  {0, 10, 2, 3}
+#define EXPECTED8  {0, 10, 2, 3, 4, 50, 6, 70}
+#define EXPECTED16 {0, 10, 2, 3, 4, 50, 6, 70,			\
+		    5, 10, 2, 3, 4, 50, 6, 70}
+
+#define BUILD_TEST(size, lanes_64, lanes_128)			\
+static void							\
+test_##size (void)						\
+{								\
+  int i;							\
+  ETYPE (size) pool1[lanes_64] = POOL##lanes_64;		\
+  ETYPE (size) res1[lanes_64] = {0};				\
+  ETYPE (size) expected1[lanes_64] = EXPECTED##lanes_64;	\
+  ETYPE (size) pool2[lanes_128] = POOL##lanes_128;		\
+  ETYPE (size) res2[lanes_128] = {0};				\
+  ETYPE (size) expected2[lanes_128] = EXPECTED##lanes_128;	\
+								\
+  /* Forcefully avoid optimization.  */				\
+  asm volatile ("" : : : "memory");				\
+  test_vabs_##size (res1, pool1);				\
+  for (i = 0; i < lanes_64; i++)				\
+    if (res1[i] != expected1[i])				\
+      abort ();							\
+								\
+  /* Forcefully avoid optimization.  */				\
+  asm volatile ("" : : : "memory");				\
+  test_vabsq_##size (res2, pool2);				\
+  for (i = 0; i < lanes_128; i++)				\
+    if (res2[i] != expected2[i])				\
+      abort ();							\
+}
+
+/* { dg-final { scan-assembler-times "abs\\tv\[0-9\]+\.8b, v\[0-9\]+\.8b" 1 } } */
+/* { dg-final { scan-assembler-times "abs\\tv\[0-9\]+\.16b, v\[0-9\]+\.16b" 1 } } */
+BUILD_TEST (8 , 8, 16)
+
+/* { dg-final { scan-assembler-times "abs\\tv\[0-9\]+\.4h, v\[0-9\]+\.4h" 1 } } */
+/* { dg-final { scan-assembler-times "abs\\tv\[0-9\]+\.8h, v\[0-9\]+\.8h" 1 } } */
+BUILD_TEST (16, 4, 8)
+
+/* { dg-final { scan-assembler-times "abs\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" 1 } } */
+/* { dg-final { scan-assembler-times "abs\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" 1 } } */
+BUILD_TEST (32, 2, 4)
+
+/* { dg-final { scan-assembler-times "abs\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" 1 } } */
+BUILD_TEST (64, 1, 2)
+
+#undef BUILD_TEST
+
+#define BUILD_TEST(size) test_##size ()
+
+int
+main (int argc, char **argv)
+{
+  BUILD_TEST (8);
+  BUILD_TEST (16);
+  BUILD_TEST (32);
+  BUILD_TEST (64);
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-relaxed.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-relaxed.x
@@ -0,0 +1,37 @@
+int v = 0;
+
+int
+atomic_fetch_add_RELAXED (int a)
+{
+  return __atomic_fetch_add (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_sub_RELAXED (int a)
+{
+  return __atomic_fetch_sub (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_and_RELAXED (int a)
+{
+  return __atomic_fetch_and (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_nand_RELAXED (int a)
+{
+  return __atomic_fetch_nand (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_xor_RELAXED (int a)
+{
+  return __atomic_fetch_xor (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_or_RELAXED (int a)
+{
+  return __atomic_fetch_or (&v, a, __ATOMIC_RELAXED);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect.c
@@ -55,6 +55,8 @@ int main (void)
   int smin_vector[] = {0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15};
   unsigned int umax_vector[] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
   unsigned int umin_vector[] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  int sabd_vector[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+  int saba_vector[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
   int reduce_smax_value = 0;
   int reduce_smin_value = -15;
   unsigned int reduce_umax_value = 15;
@@ -81,6 +83,8 @@ int main (void)
   TEST (smin, s);
   TEST (umax, u);
   TEST (umin, u);
+  TEST (sabd, s);
+  TEST (saba, s);
   TESTV (reduce_smax, s);
   TESTV (reduce_smin, s);
   TESTV (reduce_umax, u);
Index: b/src/gcc/testsuite/gcc.target/aarch64/scalar-mov.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/scalar-mov.c
@@ -0,0 +1,9 @@
+/* { dg-do compile } */
+/* { dg-options "-g -mgeneral-regs-only" } */
+
+void
+foo (const char *c, ...)
+{
+  char buf[256];
+  buf[256 - 1] = '\0';
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-movi.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-movi.c
@@ -0,0 +1,74 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+#define N 16
+
+static void
+movi_msl8 (int *__restrict a)
+{
+  int i;
+
+  /* { dg-final { scan-assembler "movi\\tv\[0-9\]+\.4s, 0xab, msl 8" } } */
+  for (i = 0; i < N; i++)
+    a[i] = 0xabff;
+}
+
+static void
+movi_msl16 (int *__restrict a)
+{
+  int i;
+
+  /* { dg-final { scan-assembler "movi\\tv\[0-9\]+\.4s, 0xab, msl 16" } } */
+  for (i = 0; i < N; i++)
+    a[i] = 0xabffff;
+}
+
+static void
+mvni_msl8 (int *__restrict a)
+{
+  int i;
+
+  /* { dg-final { scan-assembler "mvni\\tv\[0-9\]+\.4s, 0xab, msl 8" } } */
+  for (i = 0; i < N; i++)
+    a[i] = 0xffff5400;
+}
+
+static void
+mvni_msl16 (int *__restrict a)
+{
+  int i;
+
+  /* { dg-final { scan-assembler "mvni\\tv\[0-9\]+\.4s, 0xab, msl 16" } } */
+  for (i = 0; i < N; i++)
+    a[i] = 0xff540000;
+}
+
+int
+main (void)
+{
+  int a[N] = { 0 };
+  int i;
+
+#define CHECK_ARRAY(a, val)	\
+  for (i = 0; i < N; i++)	\
+    if (a[i] != val)		\
+      abort ();
+
+  movi_msl8 (a);
+  CHECK_ARRAY (a, 0xabff);
+
+  movi_msl16 (a);
+  CHECK_ARRAY (a, 0xabffff);
+
+  mvni_msl8 (a);
+  CHECK_ARRAY (a, 0xffff5400);
+
+  mvni_msl16 (a);
+  CHECK_ARRAY (a, 0xff540000);
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-ge-d.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-ge-d.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-ge-d.c
@@ -2,12 +2,13 @@
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-all -fno-unroll-loops --save-temps -fno-inline" } */
 
 #define FTYPE double
+#define ITYPE long
 #define OP >=
 #define INV_OP <
 
 #include "vect-fcm.x"
 
-/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 4 "vect" } } */
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 8 "vect" } } */
 /* { dg-final { scan-assembler "fcmge\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
 /* { dg-final { scan-assembler "fcmge\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, 0" } } */
 /* { dg-final { scan-assembler "fcmlt\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, 0" } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acquire.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acquire.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acquire.c
@@ -1,43 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-int v = 0;
-
-int
-atomic_fetch_add_ACQUIRE (int a)
-{
-  return __atomic_fetch_add (&v, a, __ATOMIC_ACQUIRE);
-}
-
-int
-atomic_fetch_sub_ACQUIRE (int a)
-{
-  return __atomic_fetch_sub (&v, a, __ATOMIC_ACQUIRE);
-}
-
-int
-atomic_fetch_and_ACQUIRE (int a)
-{
-  return __atomic_fetch_and (&v, a, __ATOMIC_ACQUIRE);
-}
-
-int
-atomic_fetch_nand_ACQUIRE (int a)
-{
-  return __atomic_fetch_nand (&v, a, __ATOMIC_ACQUIRE);
-}
-
-int
-atomic_fetch_xor_ACQUIRE (int a)
-{
-  return __atomic_fetch_xor (&v, a, __ATOMIC_ACQUIRE);
-}
-
-int
-atomic_fetch_or_ACQUIRE (int a)
-{
-  return __atomic_fetch_or (&v, a, __ATOMIC_ACQUIRE);
-}
+#include "atomic-op-acquire.x"
 
 /* { dg-final { scan-assembler-times "ldaxr\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
 /* { dg-final { scan-assembler-times "stxr\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/abs_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/abs_1.c
@@ -0,0 +1,53 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -fno-inline --save-temps" } */
+
+extern long long llabs (long long);
+extern void abort (void);
+
+long long
+abs64 (long long a)
+{
+  /* { dg-final { scan-assembler "eor\t" } } */
+  /* { dg-final { scan-assembler "sub\t" } } */
+  return llabs (a);
+}
+
+long long
+abs64_in_dreg (long long a)
+{
+  /* { dg-final { scan-assembler "abs\td\[0-9\]+, d\[0-9\]+" } } */
+  register long long x asm ("d8") = a;
+  register long long y asm ("d9");
+  asm volatile ("" : : "w" (x));
+  y = llabs (x);
+  asm volatile ("" : : "w" (y));
+  return y;
+}
+
+int
+main (void)
+{
+  volatile long long ll0 = 0LL, ll1 = 1LL, llm1 = -1LL;
+
+  if (abs64 (ll0) != 0LL)
+    abort ();
+
+  if (abs64 (ll1) != 1LL)
+    abort ();
+
+  if (abs64 (llm1) != 1LL)
+    abort ();
+
+  if (abs64_in_dreg (ll0) != 0LL)
+    abort ();
+
+  if (abs64_in_dreg (ll1) != 1LL)
+    abort ();
+
+  if (abs64_in_dreg (llm1) != 1LL)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-comp-swap-release-acquire.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-comp-swap-release-acquire.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-comp-swap-release-acquire.c
@@ -1,41 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-#define STRONG 0
-#define WEAK 1
-int v = 0;
-
-int
-atomic_compare_exchange_STRONG_RELEASE_ACQUIRE (int a, int b)
-{
-  return __atomic_compare_exchange (&v, &a, &b,
-				    STRONG, __ATOMIC_RELEASE,
-				    __ATOMIC_ACQUIRE);
-}
-
-int
-atomic_compare_exchange_WEAK_RELEASE_ACQUIRE (int a, int b)
-{
-  return __atomic_compare_exchange (&v, &a, &b,
-				    WEAK, __ATOMIC_RELEASE,
-				    __ATOMIC_ACQUIRE);
-}
-
-int
-atomic_compare_exchange_n_STRONG_RELEASE_ACQUIRE (int a, int b)
-{
-  return __atomic_compare_exchange_n (&v, &a, b,
-				      STRONG, __ATOMIC_RELEASE,
-				      __ATOMIC_ACQUIRE);
-}
-
-int
-atomic_compare_exchange_n_WEAK_RELEASE_ACQUIRE (int a, int b)
-{
-  return __atomic_compare_exchange_n (&v, &a, b,
-				      WEAK, __ATOMIC_RELEASE,
-				      __ATOMIC_ACQUIRE);
-}
+#include "atomic-comp-swap-release-acquire.x"
 
 /* { dg-final { scan-assembler-times "ldaxr\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 4 } } */
 /* { dg-final { scan-assembler-times "stlxr\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 4 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect.x
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect.x
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect.x
@@ -138,3 +138,17 @@ long long reduce_add_s64 (pRINT64 a)
 
   return s;
 }
+
+void sabd (pRINT a, pRINT b, pRINT c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    c[i] = abs (a[i] - b[i]);
+}
+
+void saba (pRINT a, pRINT b, pRINT c)
+{
+  int i;
+  for (i = 0; i < 16; i++)
+    c[i] += abs (a[i] - b[i]);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-clz.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-clz.c
@@ -0,0 +1,35 @@
+/* { dg-do run } */
+/* { dg-options "-O3 -save-temps -fno-inline" } */
+
+extern void abort ();
+
+void
+count_lz_v4si (unsigned *__restrict a, int *__restrict b)
+{
+  int i;
+
+  for (i = 0; i < 4; i++)
+    b[i] = __builtin_clz (a[i]);
+}
+
+/* { dg-final { scan-assembler "clz\tv\[0-9\]+\.4s" } } */
+
+int
+main ()
+{
+  unsigned int x[4] = { 0x0, 0xFFFF, 0x1FFFF, 0xFFFFFFFF };
+  int r[4] = { 32, 16, 15, 0 };
+  int d[4], i;
+
+  count_lz_v4si (x, d);
+
+  for (i = 0; i < 4; i++)
+    {
+      if (d[i] != r[i])
+	abort ();
+    }
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/sha256_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/sha256_1.c
@@ -0,0 +1,40 @@
+
+/* { dg-do compile } */
+/* { dg-options "-march=armv8-a+crypto" } */
+
+#include "arm_neon.h"
+
+uint32x4_t
+test_vsha256hq_u32 (uint32x4_t hash_abcd, uint32x4_t hash_efgh, uint32x4_t wk)
+{
+  return vsha256hq_u32 (hash_abcd, hash_efgh, wk);
+}
+
+/* { dg-final { scan-assembler-times "sha256h\\tq" 1 } } */
+
+uint32x4_t
+test_vsha256h2q_u32 (uint32x4_t hash_efgh, uint32x4_t hash_abcd, uint32x4_t wk)
+{
+  return vsha256h2q_u32 (hash_efgh, hash_abcd, wk);
+}
+
+/* { dg-final { scan-assembler-times "sha256h2\\tq" 1 } } */
+
+uint32x4_t
+test_vsha256su0q_u32 (uint32x4_t w0_3, uint32x4_t w4_7)
+{
+  return vsha256su0q_u32 (w0_3, w4_7);
+}
+
+/* { dg-final { scan-assembler-times "sha256su0\\tv" 1 } } */
+
+uint32x4_t
+test_vsha256su1q_u32 (uint32x4_t tw0_3, uint32x4_t w8_11, uint32x4_t w12_15)
+{
+  return vsha256su1q_u32 (tw0_3, w8_11, w12_15);
+}
+
+/* { dg-final { scan-assembler-times "sha256su1\\tv" 1 } } */
+
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-gt-f.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-gt-f.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-gt-f.c
@@ -2,12 +2,13 @@
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-all -fno-unroll-loops --save-temps -fno-inline" } */
 
 #define FTYPE float
+#define ITYPE int
 #define OP >
 #define INV_OP <=
 
 #include "vect-fcm.x"
 
-/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 4 "vect" } } */
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 8 "vect" } } */
 /* { dg-final { scan-assembler "fcmgt\\tv\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s" } } */
 /* { dg-final { scan-assembler "fcmgt\\tv\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s, 0" } } */
 /* { dg-final { scan-assembler "fcmle\\tv\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s, 0" } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/subs3.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/subs3.c
@@ -0,0 +1,61 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+typedef long long s64;
+
+int
+subs_ext (s64 a, int b, int c)
+{
+ s64 d = a - b;
+
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+subs_shift_ext (s64 a, int b, int c)
+{
+ s64 d = (a - ((s64)b << 3));
+
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int main ()
+{
+  int x;
+  s64 y;
+
+  x = subs_ext (0x13000002ll, 41, 15);
+  if (x != 318767121)
+    abort ();
+
+  x = subs_ext (0x50505050ll, 29, 4);
+  if (x != 1347440724)
+    abort ();
+
+  x = subs_ext (0x12121212121ll, 2, 14);
+  if (x != 555819311)
+    abort ();
+
+  x = subs_shift_ext (0x123456789ll, 4, 12);
+  if (x != 591751033)
+    abort ();
+
+  x = subs_shift_ext (0x02020202ll, 9, 8);
+  if (x != 33685963)
+    abort ();
+
+  x = subs_shift_ext (0x987987987987ll, 23, 41);
+  if (x != -2020050673)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { scan-assembler-times "subs\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, sxtw" 2 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/bics_2.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/bics_2.c
@@ -0,0 +1,111 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+int
+bics_si_test1 (int a, int b, int c)
+{
+  int d = a & ~b;
+
+  /* { dg-final { scan-assembler-not "bics\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" } } */
+  /* { dg-final { scan-assembler-times "bic\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" 2 } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+bics_si_test2 (int a, int b, int c)
+{
+  int d = a & ~(b << 3);
+
+  /* { dg-final { scan-assembler-not "bics\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  /* { dg-final { scan-assembler "bic\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+typedef long long s64;
+
+s64
+bics_di_test1 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & ~b;
+
+  /* { dg-final { scan-assembler-not "bics\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" } } */
+  /* { dg-final { scan-assembler-times "bic\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" 2 } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+bics_di_test2 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & ~(b << 3);
+
+  /* { dg-final { scan-assembler-not "bics\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  /* { dg-final { scan-assembler "bic\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+main ()
+{
+  int x;
+  s64 y;
+
+  x = bics_si_test1 (29, ~4, 5);
+  if (x != ((29 & 4) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test1 (5, ~2, 20);
+  if (x != 25)
+    abort ();
+
+  x = bics_si_test2 (35, ~4, 5);
+  if (x != ((35 & ~(~4 << 3)) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test2 (96, ~2, 20);
+  if (x != 116)
+    abort ();
+
+  y = bics_di_test1 (0x130000029ll,
+                     ~0x320000004ll,
+                     0x505050505ll);
+
+  if (y != ((0x130000029ll & 0x320000004ll) + ~0x320000004ll + 0x505050505ll))
+    abort ();
+
+  y = bics_di_test1 (0x5000500050005ll,
+                     ~0x2111211121112ll,
+                     0x0000000002020ll);
+  if (y != 0x5000500052025ll)
+    abort ();
+
+  y = bics_di_test2 (0x130000029ll,
+                     ~0x064000008ll,
+                     0x505050505ll);
+  if (y != ((0x130000029ll & ~(~0x064000008ll << 3))
+	    + ~0x064000008ll + 0x505050505ll))
+    abort ();
+
+  y = bics_di_test2 (0x130002900ll,
+                     ~0x088000008ll,
+                     0x505050505ll);
+  if (y != (0x130002900ll + 0x505050505ll))
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vaddv-intrinsic.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vaddv-intrinsic.c
@@ -0,0 +1,28 @@
+
+/* { dg-do run } */
+/* { dg-options "-O3" } */
+
+#include "arm_neon.h"
+
+extern void abort (void);
+
+#include "vaddv-intrinsic.x"
+
+int
+main (void)
+{
+  const float32_t pool_v2sf[] = {4.0f, 9.0f};
+  const float32_t pool_v4sf[] = {4.0f, 9.0f, 16.0f, 25.0f};
+  const float64_t pool_v2df[] = {4.0, 9.0};
+
+  if (test_vaddv_v2sf (pool_v2sf) != 13.0f)
+    abort ();
+
+  if (test_vaddv_v4sf (pool_v4sf) != 54.0f)
+    abort ();
+
+  if (test_vaddv_v2df (pool_v2df) != 13.0)
+    abort ();
+
+  return 0;
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acquire.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acquire.x
@@ -0,0 +1,37 @@
+int v = 0;
+
+int
+atomic_fetch_add_ACQUIRE (int a)
+{
+  return __atomic_fetch_add (&v, a, __ATOMIC_ACQUIRE);
+}
+
+int
+atomic_fetch_sub_ACQUIRE (int a)
+{
+  return __atomic_fetch_sub (&v, a, __ATOMIC_ACQUIRE);
+}
+
+int
+atomic_fetch_and_ACQUIRE (int a)
+{
+  return __atomic_fetch_and (&v, a, __ATOMIC_ACQUIRE);
+}
+
+int
+atomic_fetch_nand_ACQUIRE (int a)
+{
+  return __atomic_fetch_nand (&v, a, __ATOMIC_ACQUIRE);
+}
+
+int
+atomic_fetch_xor_ACQUIRE (int a)
+{
+  return __atomic_fetch_xor (&v, a, __ATOMIC_ACQUIRE);
+}
+
+int
+atomic_fetch_or_ACQUIRE (int a)
+{
+  return __atomic_fetch_or (&v, a, __ATOMIC_ACQUIRE);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/sbc.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/sbc.c
@@ -0,0 +1,41 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps" } */
+
+extern void abort (void);
+
+typedef unsigned int u32int;
+typedef unsigned long long u64int;
+
+u32int
+test_si (u32int w1, u32int w2, u32int w3, u32int w4)
+{
+  u32int w0;
+  /* { dg-final { scan-assembler "sbc\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+\n" } } */
+  w0 = w1 - w2 - (w3 < w4);
+  return w0;
+}
+
+u64int
+test_di (u64int x1, u64int x2, u64int x3, u64int x4)
+{
+  u64int x0;
+  /* { dg-final { scan-assembler "sbc\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+\n" } } */
+  x0 = x1 - x2 - (x3 < x4);
+  return x0;
+}
+
+int
+main ()
+{
+  u32int x;
+  u64int y;
+  x = test_si (7, 8, 12, 15);
+  if (x != -2)
+    abort();
+  y = test_di (0x987654321ll, 0x123456789ll, 0x345345345ll, 0x123123123ll);
+  if (y != 0x8641fdb98ll)
+    abort();
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/pmull_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/pmull_1.c
@@ -0,0 +1,23 @@
+
+/* { dg-do compile } */
+/* { dg-options "-march=armv8-a+crypto" } */
+
+#include "arm_neon.h"
+
+poly128_t
+test_vmull_p64 (poly64_t a, poly64_t b)
+{
+  return vmull_p64 (a, b);
+}
+
+/* { dg-final { scan-assembler-times "pmull\\tv" 1 } } */
+
+poly128_t
+test_vmull_high_p64 (poly64x2_t a, poly64x2_t b)
+{
+  return vmull_high_p64 (a, b);
+}
+
+/* { dg-final { scan-assembler-times "pmull2\\tv" 1 } } */
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-comp-swap-release-acquire.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-comp-swap-release-acquire.x
@@ -0,0 +1,36 @@
+
+#define STRONG 0
+#define WEAK 1
+int v = 0;
+
+int
+atomic_compare_exchange_STRONG_RELEASE_ACQUIRE (int a, int b)
+{
+  return __atomic_compare_exchange (&v, &a, &b,
+				    STRONG, __ATOMIC_RELEASE,
+				    __ATOMIC_ACQUIRE);
+}
+
+int
+atomic_compare_exchange_WEAK_RELEASE_ACQUIRE (int a, int b)
+{
+  return __atomic_compare_exchange (&v, &a, &b,
+				    WEAK, __ATOMIC_RELEASE,
+				    __ATOMIC_ACQUIRE);
+}
+
+int
+atomic_compare_exchange_n_STRONG_RELEASE_ACQUIRE (int a, int b)
+{
+  return __atomic_compare_exchange_n (&v, &a, b,
+				      STRONG, __ATOMIC_RELEASE,
+				      __ATOMIC_ACQUIRE);
+}
+
+int
+atomic_compare_exchange_n_WEAK_RELEASE_ACQUIRE (int a, int b)
+{
+  return __atomic_compare_exchange_n (&v, &a, b,
+				      WEAK, __ATOMIC_RELEASE,
+				      __ATOMIC_ACQUIRE);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/scalar_intrinsics.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/scalar_intrinsics.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/scalar_intrinsics.c
@@ -1,5 +1,5 @@
 /* { dg-do compile } */
-/* { dg-options "-O2" } */
+/* { dg-options "-O2 -dp" } */
 
 #include <arm_neon.h>
 
@@ -32,6 +32,18 @@ test_vaddd_s64_2 (int64x1_t a, int64x1_t
 		     vqaddd_s64 (a, d));
 }
 
+/* { dg-final { scan-assembler-times "\\tabs\\td\[0-9\]+, d\[0-9\]+" 1 } } */
+
+int64x1_t
+test_vabs_s64 (int64x1_t a)
+{
+  uint64x1_t res;
+  force_simd (a);
+  res = vabs_s64 (a);
+  force_simd (res);
+  return res;
+}
+
 /* { dg-final { scan-assembler-times "\\tcmeq\\td\[0-9\]+, d\[0-9\]+, d\[0-9\]+" 1 } } */
 
 uint64x1_t
@@ -181,7 +193,7 @@ test_vcltzd_s64 (int64x1_t a)
   return res;
 }
 
-/* { dg-final { scan-assembler-times "\\tdup\\tb\[0-9\]+, v\[0-9\]+\.b" 2 } } */
+/* { dg-final { scan-assembler-times "aarch64_get_lanev16qi" 2 } } */
 
 int8x1_t
 test_vdupb_lane_s8 (int8x16_t a)
@@ -195,7 +207,7 @@ test_vdupb_lane_u8 (uint8x16_t a)
   return vdupb_lane_u8 (a, 2);
 }
 
-/* { dg-final { scan-assembler-times "\\tdup\\th\[0-9\]+, v\[0-9\]+\.h" 2 } } */
+/* { dg-final { scan-assembler-times "aarch64_get_lanev8hi" 2 } } */
 
 int16x1_t
 test_vduph_lane_s16 (int16x8_t a)
@@ -209,7 +221,7 @@ test_vduph_lane_u16 (uint16x8_t a)
   return vduph_lane_u16 (a, 2);
 }
 
-/* { dg-final { scan-assembler-times "\\tdup\\ts\[0-9\]+, v\[0-9\]+\.s" 2 } } */
+/* { dg-final { scan-assembler-times "aarch64_get_lanev4si" 2 } } */
 
 int32x1_t
 test_vdups_lane_s32 (int32x4_t a)
@@ -223,18 +235,18 @@ test_vdups_lane_u32 (uint32x4_t a)
   return vdups_lane_u32 (a, 2);
 }
 
-/* { dg-final { scan-assembler-times "\\tdup\\td\[0-9\]+, v\[0-9\]+\.d" 2 } } */
+/* { dg-final { scan-assembler-times "aarch64_get_lanev2di" 2 } } */
 
 int64x1_t
 test_vdupd_lane_s64 (int64x2_t a)
 {
-  return vdupd_lane_s64 (a, 2);
+  return vdupd_lane_s64 (a, 1);
 }
 
 uint64x1_t
 test_vdupd_lane_u64 (uint64x2_t a)
 {
-  return vdupd_lane_u64 (a, 2);
+  return vdupd_lane_u64 (a, 1);
 }
 
 /* { dg-final { scan-assembler-times "\\tcmtst\\td\[0-9\]+, d\[0-9\]+, d\[0-9\]+" 2 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-int.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-op-int.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-int.c
@@ -1,43 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-int v = 0;
-
-int
-atomic_fetch_add_RELAXED (int a)
-{
-  return __atomic_fetch_add (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_sub_RELAXED (int a)
-{
-  return __atomic_fetch_sub (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_and_RELAXED (int a)
-{
-  return __atomic_fetch_and (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_nand_RELAXED (int a)
-{
-  return __atomic_fetch_nand (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_xor_RELAXED (int a)
-{
-  return __atomic_fetch_xor (&v, a, __ATOMIC_RELAXED);
-}
-
-int
-atomic_fetch_or_RELAXED (int a)
-{
-  return __atomic_fetch_or (&v, a, __ATOMIC_RELAXED);
-}
+#include "atomic-op-int.x"
 
 /* { dg-final { scan-assembler-times "ldxr\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
 /* { dg-final { scan-assembler-times "stxr\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/cmn-neg.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/cmn-neg.c
@@ -0,0 +1,33 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps" } */
+
+extern void abort (void);
+
+void __attribute__ ((noinline))
+foo_s32 (int a, int b)
+{
+  if (a == -b)
+    abort ();
+}
+/* { dg-final { scan-assembler "cmn\tw\[0-9\]" } } */
+
+void __attribute__ ((noinline))
+foo_s64 (long long a, long long b)
+{
+  if (a == -b)
+    abort ();
+}
+/* { dg-final { scan-assembler "cmn\tx\[0-9\]" } } */
+
+
+int
+main (void)
+{
+  int a = 30;
+  int b = 42;
+  foo_s32 (a, b);
+  foo_s64 (a, b);
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-seq_cst.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-op-seq_cst.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-seq_cst.c
@@ -1,43 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-int v = 0;
-
-int
-atomic_fetch_add_SEQ_CST (int a)
-{
-  return __atomic_fetch_add (&v, a, __ATOMIC_SEQ_CST);
-}
-
-int
-atomic_fetch_sub_SEQ_CST (int a)
-{
-  return __atomic_fetch_sub (&v, a, __ATOMIC_SEQ_CST);
-}
-
-int
-atomic_fetch_and_SEQ_CST (int a)
-{
-  return __atomic_fetch_and (&v, a, __ATOMIC_SEQ_CST);
-}
-
-int
-atomic_fetch_nand_SEQ_CST (int a)
-{
-  return __atomic_fetch_nand (&v, a, __ATOMIC_SEQ_CST);
-}
-
-int
-atomic_fetch_xor_SEQ_CST (int a)
-{
-  return __atomic_fetch_xor (&v, a, __ATOMIC_SEQ_CST);
-}
-
-int
-atomic_fetch_or_SEQ_CST (int a)
-{
-  return __atomic_fetch_or (&v, a, __ATOMIC_SEQ_CST);
-}
+#include "atomic-op-seq_cst.x"
 
 /* { dg-final { scan-assembler-times "ldaxr\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
 /* { dg-final { scan-assembler-times "stlxr\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vaddv-intrinsic.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vaddv-intrinsic.x
@@ -0,0 +1,27 @@
+
+float32_t
+test_vaddv_v2sf (const float32_t *pool)
+{
+  float32x2_t val;
+
+  val = vld1_f32 (pool);
+  return vaddv_f32 (val);
+}
+
+float32_t
+test_vaddv_v4sf (const float32_t *pool)
+{
+  float32x4_t val;
+
+  val = vld1q_f32 (pool);
+  return vaddvq_f32 (val);
+}
+
+float64_t
+test_vaddv_v2df (const float64_t *pool)
+{
+  float64x2_t val;
+
+  val = vld1q_f64 (pool);
+  return vaddvq_f64 (val);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/negs.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/negs.c
@@ -0,0 +1,108 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps" } */
+
+extern void abort (void);
+int z;
+
+int
+negs_si_test1 (int a, int b, int c)
+{
+  int d = -b;
+
+  /* { dg-final { scan-assembler "negs\tw\[0-9\]+, w\[0-9\]+" } } */
+  if (d < 0)
+    return a + c;
+
+  z = d;
+    return b + c + d;
+}
+
+int
+negs_si_test3 (int a, int b, int c)
+{
+  int d = -(b) << 3;
+
+  /* { dg-final { scan-assembler "negs\tw\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+
+  z = d;
+    return b + c + d;
+}
+
+typedef long long s64;
+s64 zz;
+
+s64
+negs_di_test1 (s64 a, s64 b, s64 c)
+{
+  s64 d = -b;
+
+  /* { dg-final { scan-assembler "negs\tx\[0-9\]+, x\[0-9\]+" } } */
+  if (d < 0)
+    return a + c;
+
+  zz = d;
+    return b + c + d;
+}
+
+s64
+negs_di_test3 (s64 a, s64 b, s64 c)
+{
+  s64 d = -(b) << 3;
+
+  /* { dg-final { scan-assembler "negs\tx\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+
+  zz = d;
+    return b + c + d;
+}
+
+int main ()
+{
+  int x;
+  s64 y;
+
+  x = negs_si_test1 (2, 12, 5);
+  if (x != 7)
+    abort ();
+
+  x = negs_si_test1 (1, 2, 32);
+  if (x != 33)
+    abort ();
+
+  x = negs_si_test3 (13, 14, 5);
+  if (x != -93)
+    abort ();
+
+  x = negs_si_test3 (15, 21, 2);
+  if (x != -145)
+    abort ();
+
+  y = negs_di_test1 (0x20202020ll,
+		     0x65161611ll,
+		     0x42434243ll);
+  if (y != 0x62636263ll)
+    abort ();
+
+  y = negs_di_test1 (0x1010101010101ll,
+		     0x123456789abcdll,
+		     0x5555555555555ll);
+  if (y != 0x6565656565656ll)
+    abort ();
+
+  y = negs_di_test3 (0x62523781ll,
+		     0x64234978ll,
+		     0x12345123ll);
+  if (y != 0xfffffffd553d4edbll)
+    abort ();
+
+  y = negs_di_test3 (0x763526268ll,
+		     0x101010101ll,
+		     0x222222222ll);
+  if (y != 0xfffffffb1b1b1b1bll)
+    abort ();
+
+  return 0;
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-consume.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-op-consume.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-consume.c
@@ -1,43 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-int v = 0;
-
-int
-atomic_fetch_add_CONSUME (int a)
-{
-  return __atomic_fetch_add (&v, a, __ATOMIC_CONSUME);
-}
-
-int
-atomic_fetch_sub_CONSUME (int a)
-{
-  return __atomic_fetch_sub (&v, a, __ATOMIC_CONSUME);
-}
-
-int
-atomic_fetch_and_CONSUME (int a)
-{
-  return __atomic_fetch_and (&v, a, __ATOMIC_CONSUME);
-}
-
-int
-atomic_fetch_nand_CONSUME (int a)
-{
-  return __atomic_fetch_nand (&v, a, __ATOMIC_CONSUME);
-}
-
-int
-atomic_fetch_xor_CONSUME (int a)
-{
-  return __atomic_fetch_xor (&v, a, __ATOMIC_CONSUME);
-}
-
-int
-atomic_fetch_or_CONSUME (int a)
-{
-  return __atomic_fetch_or (&v, a, __ATOMIC_CONSUME);
-}
+#include "atomic-op-consume.x"
 
 /* { dg-final { scan-assembler-times "ldxr\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
 /* { dg-final { scan-assembler-times "stxr\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-vaddv.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-vaddv.c
@@ -0,0 +1,128 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps -ffast-math" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+extern float fabsf (float);
+extern double fabs (double);
+
+#define NUM_TESTS 16
+#define DELTA 0.000001
+
+int8_t input_int8[] = {1, 56, 2, -9, -90, 23, 54, 76,
+		       -4, 34, 110, -110, 6, 4, 75, -34};
+int16_t input_int16[] = {1, 56, 2, -9, -90, 23, 54, 76,
+			 -4, 34, 110, -110, 6, 4, 75, -34};
+int32_t input_int32[] = {1, 56, 2, -9, -90, 23, 54, 76,
+			 -4, 34, 110, -110, 6, 4, 75, -34};
+int64_t input_int64[] = {1, 56, 2, -9, -90, 23, 54, 76,
+			 -4, 34, 110, -110, 6, 4, 75, -34};
+
+uint8_t input_uint8[] = {1, 56, 2, 9, 90, 23, 54, 76,
+			 4, 34, 110, 110, 6, 4, 75, 34};
+uint16_t input_uint16[] = {1, 56, 2, 9, 90, 23, 54, 76,
+			   4, 34, 110, 110, 6, 4, 75, 34};
+uint32_t input_uint32[] = {1, 56, 2, 9, 90, 23, 54, 76,
+			   4, 34, 110, 110, 6, 4, 75, 34};
+
+uint64_t input_uint64[] = {1, 56, 2, 9, 90, 23, 54, 76,
+			   4, 34, 110, 110, 6, 4, 75, 34};
+
+float input_float32[] = {0.1f, -0.1f, 0.4f, 10.3f,
+			 200.0f, -800.0f, -13.0f, -0.5f,
+			 7.9f, -870.0f, 10.4f, 310.11f,
+			 0.0f, -865.0f, -2213.0f, -1.5f};
+
+double input_float64[] = {0.1, -0.1, 0.4, 10.3,
+			  200.0, -800.0, -13.0, -0.5,
+			  7.9, -870.0, 10.4, 310.11,
+			  0.0, -865.0, -2213.0, -1.5};
+
+#define EQUALF(a, b) (fabsf (a - b) < DELTA)
+#define EQUALD(a, b) (fabs (a - b) < DELTA)
+#define EQUALL(a, b) (a == b)
+
+#define TEST(SUFFIX, Q, TYPE, LANES, FLOAT)				\
+int									\
+test_vaddv##SUFFIX##_##TYPE##x##LANES##_t (void)			\
+{									\
+  int i, j;								\
+  int moves = (NUM_TESTS - LANES) + 1;					\
+  TYPE##_t out_l[NUM_TESTS];						\
+  TYPE##_t out_v[NUM_TESTS];						\
+									\
+  /* Calculate linearly.  */						\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      out_l[i] = input_##TYPE[i];					\
+      for (j = 1; j < LANES; j++)					\
+	out_l[i] += input_##TYPE[i + j];				\
+    }									\
+									\
+  /* Calculate using vector reduction intrinsics.  */			\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      TYPE##x##LANES##_t t1 = vld1##Q##_##SUFFIX (input_##TYPE + i);	\
+      out_v[i] = vaddv##Q##_##SUFFIX (t1);				\
+    }									\
+									\
+  /* Compare.  */							\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      if (!EQUAL##FLOAT (out_v[i], out_l[i]))				\
+	return 0;							\
+    }									\
+  return 1;								\
+}
+
+#define BUILD_VARIANTS(TYPE, STYPE, W32, W64, F)	\
+TEST (STYPE,  , TYPE, W32, F)				\
+TEST (STYPE, q, TYPE, W64, F)				\
+
+BUILD_VARIANTS (int8, s8, 8, 16, L)
+BUILD_VARIANTS (uint8, u8, 8, 16, L)
+/* { dg-final { scan-assembler "addv\\tb\[0-9\]+, v\[0-9\]+\.8b" } } */
+/* { dg-final { scan-assembler "addv\\tb\[0-9\]+, v\[0-9\]+\.16b" } } */
+BUILD_VARIANTS (int16, s16, 4, 8, L)
+BUILD_VARIANTS (uint16, u16, 4, 8, L)
+/* { dg-final { scan-assembler "addv\\th\[0-9\]+, v\[0-9\]+\.4h" } } */
+/* { dg-final { scan-assembler "addv\\th\[0-9\]+, v\[0-9\]+\.8h" } } */
+BUILD_VARIANTS (int32, s32, 2, 4, L)
+BUILD_VARIANTS (uint32, u32, 2, 4, L)
+/* { dg-final { scan-assembler "addp\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "addv\\ts\[0-9\]+, v\[0-9\]+\.4s" } } */
+TEST (s64, q, int64, 2, D)
+TEST (u64, q, uint64, 2, D)
+/* { dg-final { scan-assembler "addp\\td\[0-9\]+\, v\[0-9\]+\.2d" } } */
+
+BUILD_VARIANTS (float32, f32, 2, 4, F)
+/* { dg-final { scan-assembler "faddp\\ts\[0-9\]+, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "faddp\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+TEST (f64, q, float64, 2, D)
+/* { dg-final { scan-assembler "faddp\\td\[0-9\]+\, v\[0-9\]+\.2d" } } */
+
+#undef TEST
+#define TEST(SUFFIX, Q, TYPE, LANES, FLOAT)		\
+{							\
+  if (!test_vaddv##SUFFIX##_##TYPE##x##LANES##_t ())	\
+    abort ();						\
+}
+
+int
+main (int argc, char **argv)
+{
+BUILD_VARIANTS (int8, s8, 8, 16, L)
+BUILD_VARIANTS (uint8, u8, 8, 16, L)
+BUILD_VARIANTS (int16, s16, 4, 8, L)
+BUILD_VARIANTS (uint16, u16, 4, 8, L)
+BUILD_VARIANTS (int32, s32, 2, 4, L)
+BUILD_VARIANTS (uint32, u32, 2, 4, L)
+
+BUILD_VARIANTS (float32, f32, 2, 4, F)
+TEST (f64, q, float64, 2, D)
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-char.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-op-char.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-char.c
@@ -1,43 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-char v = 0;
-
-char
-atomic_fetch_add_RELAXED (char a)
-{
-  return __atomic_fetch_add (&v, a, __ATOMIC_RELAXED);
-}
-
-char
-atomic_fetch_sub_RELAXED (char a)
-{
-  return __atomic_fetch_sub (&v, a, __ATOMIC_RELAXED);
-}
-
-char
-atomic_fetch_and_RELAXED (char a)
-{
-  return __atomic_fetch_and (&v, a, __ATOMIC_RELAXED);
-}
-
-char
-atomic_fetch_nand_RELAXED (char a)
-{
-  return __atomic_fetch_nand (&v, a, __ATOMIC_RELAXED);
-}
-
-char
-atomic_fetch_xor_RELAXED (char a)
-{
-  return __atomic_fetch_xor (&v, a, __ATOMIC_RELAXED);
-}
-
-char
-atomic_fetch_or_RELAXED (char a)
-{
-  return __atomic_fetch_or (&v, a, __ATOMIC_RELAXED);
-}
+#include "atomic-op-char.x"
 
 /* { dg-final { scan-assembler-times "ldxrb\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
 /* { dg-final { scan-assembler-times "stxrb\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-int.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-int.x
@@ -0,0 +1,37 @@
+int v = 0;
+
+int
+atomic_fetch_add_RELAXED (int a)
+{
+  return __atomic_fetch_add (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_sub_RELAXED (int a)
+{
+  return __atomic_fetch_sub (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_and_RELAXED (int a)
+{
+  return __atomic_fetch_and (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_nand_RELAXED (int a)
+{
+  return __atomic_fetch_nand (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_xor_RELAXED (int a)
+{
+  return __atomic_fetch_xor (&v, a, __ATOMIC_RELAXED);
+}
+
+int
+atomic_fetch_or_RELAXED (int a)
+{
+  return __atomic_fetch_or (&v, a, __ATOMIC_RELAXED);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-seq_cst.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-seq_cst.x
@@ -0,0 +1,37 @@
+int v = 0;
+
+int
+atomic_fetch_add_SEQ_CST (int a)
+{
+  return __atomic_fetch_add (&v, a, __ATOMIC_SEQ_CST);
+}
+
+int
+atomic_fetch_sub_SEQ_CST (int a)
+{
+  return __atomic_fetch_sub (&v, a, __ATOMIC_SEQ_CST);
+}
+
+int
+atomic_fetch_and_SEQ_CST (int a)
+{
+  return __atomic_fetch_and (&v, a, __ATOMIC_SEQ_CST);
+}
+
+int
+atomic_fetch_nand_SEQ_CST (int a)
+{
+  return __atomic_fetch_nand (&v, a, __ATOMIC_SEQ_CST);
+}
+
+int
+atomic_fetch_xor_SEQ_CST (int a)
+{
+  return __atomic_fetch_xor (&v, a, __ATOMIC_SEQ_CST);
+}
+
+int
+atomic_fetch_or_SEQ_CST (int a)
+{
+  return __atomic_fetch_or (&v, a, __ATOMIC_SEQ_CST);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/bfxil_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/bfxil_1.c
@@ -0,0 +1,40 @@
+/* { dg-do run { target aarch64*-*-* } } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target aarch64_little_endian } */
+
+extern void abort (void);
+
+typedef struct bitfield
+{
+  unsigned short eight1: 8;
+  unsigned short four: 4;
+  unsigned short eight2: 8;
+  unsigned short seven: 7;
+  unsigned int sixteen: 16;
+} bitfield;
+
+bitfield
+bfxil (bitfield a)
+{
+  /* { dg-final { scan-assembler "bfxil\tx\[0-9\]+, x\[0-9\]+, 16, 8" } } */
+  a.eight1 = a.eight2;
+  return a;
+}
+
+int
+main (void)
+{
+  static bitfield a;
+  bitfield b;
+
+  a.eight1 = 9;
+  a.eight2 = 57;
+  b = bfxil (a);
+
+  if (b.eight1 != a.eight2)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-consume.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-consume.x
@@ -0,0 +1,37 @@
+int v = 0;
+
+int
+atomic_fetch_add_CONSUME (int a)
+{
+  return __atomic_fetch_add (&v, a, __ATOMIC_CONSUME);
+}
+
+int
+atomic_fetch_sub_CONSUME (int a)
+{
+  return __atomic_fetch_sub (&v, a, __ATOMIC_CONSUME);
+}
+
+int
+atomic_fetch_and_CONSUME (int a)
+{
+  return __atomic_fetch_and (&v, a, __ATOMIC_CONSUME);
+}
+
+int
+atomic_fetch_nand_CONSUME (int a)
+{
+  return __atomic_fetch_nand (&v, a, __ATOMIC_CONSUME);
+}
+
+int
+atomic_fetch_xor_CONSUME (int a)
+{
+  return __atomic_fetch_xor (&v, a, __ATOMIC_CONSUME);
+}
+
+int
+atomic_fetch_or_CONSUME (int a)
+{
+  return __atomic_fetch_or (&v, a, __ATOMIC_CONSUME);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-short.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-op-short.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-short.c
@@ -1,43 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-short v = 0;
-
-short
-atomic_fetch_add_RELAXED (short a)
-{
-  return __atomic_fetch_add (&v, a, __ATOMIC_RELAXED);
-}
-
-short
-atomic_fetch_sub_RELAXED (short a)
-{
-  return __atomic_fetch_sub (&v, a, __ATOMIC_RELAXED);
-}
-
-short
-atomic_fetch_and_RELAXED (short a)
-{
-  return __atomic_fetch_and (&v, a, __ATOMIC_RELAXED);
-}
-
-short
-atomic_fetch_nand_RELAXED (short a)
-{
-  return __atomic_fetch_nand (&v, a, __ATOMIC_RELAXED);
-}
-
-short
-atomic_fetch_xor_RELAXED (short a)
-{
-  return __atomic_fetch_xor (&v, a, __ATOMIC_RELAXED);
-}
-
-short
-atomic_fetch_or_RELAXED (short a)
-{
-  return __atomic_fetch_or (&v, a, __ATOMIC_RELAXED);
-}
+#include "atomic-op-short.x"
 
 /* { dg-final { scan-assembler-times "ldxrh\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
 /* { dg-final { scan-assembler-times "stxrh\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-char.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-char.x
@@ -0,0 +1,37 @@
+char v = 0;
+
+char
+atomic_fetch_add_RELAXED (char a)
+{
+  return __atomic_fetch_add (&v, a, __ATOMIC_RELAXED);
+}
+
+char
+atomic_fetch_sub_RELAXED (char a)
+{
+  return __atomic_fetch_sub (&v, a, __ATOMIC_RELAXED);
+}
+
+char
+atomic_fetch_and_RELAXED (char a)
+{
+  return __atomic_fetch_and (&v, a, __ATOMIC_RELAXED);
+}
+
+char
+atomic_fetch_nand_RELAXED (char a)
+{
+  return __atomic_fetch_nand (&v, a, __ATOMIC_RELAXED);
+}
+
+char
+atomic_fetch_xor_RELAXED (char a)
+{
+  return __atomic_fetch_xor (&v, a, __ATOMIC_RELAXED);
+}
+
+char
+atomic_fetch_or_RELAXED (char a)
+{
+  return __atomic_fetch_or (&v, a, __ATOMIC_RELAXED);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-eq-f.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-eq-f.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-eq-f.c
@@ -2,12 +2,13 @@
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-all -fno-unroll-loops --save-temps -fno-inline" } */
 
 #define FTYPE float
+#define ITYPE int
 #define OP ==
 #define INV_OP !=
 
 #include "vect-fcm.x"
 
-/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 4 "vect" } } */
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 8 "vect" } } */
 /* { dg-final { scan-assembler "fcmeq\\tv\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s" } } */
 /* { dg-final { scan-assembler "fcmeq\\tv\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s, 0" } } */
 /* { dg-final { cleanup-tree-dump "vect" } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fp-compile.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fp-compile.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fp-compile.c
@@ -11,3 +11,4 @@
 /* { dg-final { scan-assembler "fdiv\\tv" } } */
 /* { dg-final { scan-assembler "fneg\\tv" } } */
 /* { dg-final { scan-assembler "fabs\\tv" } } */
+/* { dg-final { scan-assembler "fabd\\tv" } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/adds1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/adds1.c
@@ -0,0 +1,149 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+int
+adds_si_test1 (int a, int b, int c)
+{
+  int d = a + b;
+
+  /* { dg-final { scan-assembler "adds\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+adds_si_test2 (int a, int b, int c)
+{
+  int d = a + 0xff;
+
+  /* { dg-final { scan-assembler "adds\tw\[0-9\]+, w\[0-9\]+, 255" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+adds_si_test3 (int a, int b, int c)
+{
+  int d = a + (b << 3);
+
+  /* { dg-final { scan-assembler "adds\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+typedef long long s64;
+
+s64
+adds_di_test1 (s64 a, s64 b, s64 c)
+{
+  s64 d = a + b;
+
+  /* { dg-final { scan-assembler "adds\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+adds_di_test2 (s64 a, s64 b, s64 c)
+{
+  s64 d = a + 0xff;
+
+  /* { dg-final { scan-assembler "adds\tx\[0-9\]+, x\[0-9\]+, 255" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+adds_di_test3 (s64 a, s64 b, s64 c)
+{
+  s64 d = a + (b << 3);
+
+  /* { dg-final { scan-assembler "adds\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int main ()
+{
+  int x;
+  s64 y;
+
+  x = adds_si_test1 (29, 4, 5);
+  if (x != 42)
+    abort ();
+
+  x = adds_si_test1 (5, 2, 20);
+  if (x != 29)
+    abort ();
+
+  x = adds_si_test2 (29, 4, 5);
+  if (x != 293)
+    abort ();
+
+  x = adds_si_test2 (1024, 2, 20);
+  if (x != 1301)
+    abort ();
+
+  x = adds_si_test3 (35, 4, 5);
+  if (x != 76)
+    abort ();
+
+  x = adds_si_test3 (5, 2, 20);
+  if (x != 43)
+    abort ();
+
+  y = adds_di_test1 (0x130000029ll,
+		     0x320000004ll,
+		     0x505050505ll);
+
+  if (y != 0xc75050536)
+    abort ();
+
+  y = adds_di_test1 (0x5000500050005ll,
+		     0x2111211121112ll,
+		     0x0000000002020ll);
+  if (y != 0x9222922294249)
+    abort ();
+
+  y = adds_di_test2 (0x130000029ll,
+		     0x320000004ll,
+		     0x505050505ll);
+  if (y != 0x955050631)
+    abort ();
+
+  y = adds_di_test2 (0x130002900ll,
+		     0x320000004ll,
+		     0x505050505ll);
+  if (y != 0x955052f08)
+    abort ();
+
+  y = adds_di_test3 (0x130000029ll,
+		     0x064000008ll,
+		     0x505050505ll);
+  if (y != 0x9b9050576)
+    abort ();
+
+  y = adds_di_test3 (0x130002900ll,
+		     0x088000008ll,
+		     0x505050505ll);
+  if (y != 0xafd052e4d)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/insv_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/insv_1.c
@@ -0,0 +1,85 @@
+/* { dg-do run { target aarch64*-*-* } } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target aarch64_little_endian } */
+
+extern void abort (void);
+
+typedef struct bitfield
+{
+  unsigned short eight: 8;
+  unsigned short four: 4;
+  unsigned short five: 5;
+  unsigned short seven: 7;
+  unsigned int sixteen: 16;
+} bitfield;
+
+bitfield
+bfi1 (bitfield a)
+{
+  /* { dg-final { scan-assembler "bfi\tx\[0-9\]+, x\[0-9\]+, 0, 8" } } */
+  a.eight = 3;
+  return a;
+}
+
+bitfield
+bfi2 (bitfield a)
+{
+  /* { dg-final { scan-assembler "bfi\tx\[0-9\]+, x\[0-9\]+, 16, 5" } } */
+  a.five = 7;
+  return a;
+}
+
+bitfield
+movk (bitfield a)
+{
+  /* { dg-final { scan-assembler "movk\tx\[0-9\]+, 0x1d6b, lsl 32" } } */
+  a.sixteen = 7531;
+  return a;
+}
+
+bitfield
+set1 (bitfield a)
+{
+  /* { dg-final { scan-assembler "orr\tx\[0-9\]+, x\[0-9\]+, 2031616" } } */
+  a.five = 0x1f;
+  return a;
+}
+
+bitfield
+set0 (bitfield a)
+{
+  /* { dg-final { scan-assembler "and\tx\[0-9\]+, x\[0-9\]+, -2031617" } } */
+  a.five = 0;
+  return a;
+}
+
+
+int
+main (int argc, char** argv)
+{
+  static bitfield a;
+  bitfield b = bfi1 (a);
+  bitfield c = bfi2 (b);
+  bitfield d = movk (c);
+
+  if (d.eight != 3)
+    abort ();
+
+  if (d.five != 7)
+    abort ();
+
+  if (d.sixteen != 7531)
+    abort ();
+
+  d = set1 (d);
+  if (d.five != 0x1f)
+    abort ();
+
+  d = set0 (d);
+  if (d.five != 0)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/ror.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/ror.c
@@ -0,0 +1,34 @@
+/* { dg-options "-O2 --save-temps" } */
+/* { dg-do run } */
+
+extern void abort (void);
+
+int
+test_si (int a)
+{
+  /* { dg-final { scan-assembler "ror\tw\[0-9\]+, w\[0-9\]+, 27\n" } } */
+  return (a << 5) | ((unsigned int) a >> 27);
+}
+
+long long
+test_di (long long a)
+{
+  /* { dg-final { scan-assembler "ror\tx\[0-9\]+, x\[0-9\]+, 45\n" } } */
+  return (a << 19) | ((unsigned long long) a >> 45);
+}
+
+int
+main ()
+{
+  int v;
+  long long w;
+  v = test_si (0x0203050);
+  if (v != 0x4060a00)
+    abort();
+  w = test_di (0x0000020506010304ll);
+  if (w != 0x1028300818200000ll)
+    abort();
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/ands_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/ands_1.c
@@ -0,0 +1,151 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+int
+ands_si_test1 (int a, int b, int c)
+{
+  int d = a & b;
+
+  /* { dg-final { scan-assembler-times "ands\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" 2 } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+ands_si_test2 (int a, int b, int c)
+{
+  int d = a & 0xff;
+
+  /* { dg-final { scan-assembler "ands\tw\[0-9\]+, w\[0-9\]+, 255" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+ands_si_test3 (int a, int b, int c)
+{
+  int d = a & (b << 3);
+
+  /* { dg-final { scan-assembler "ands\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+typedef long long s64;
+
+s64
+ands_di_test1 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & b;
+
+  /* { dg-final { scan-assembler-times "ands\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" 2 } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+ands_di_test2 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & 0xff;
+
+  /* { dg-final { scan-assembler "ands\tx\[0-9\]+, x\[0-9\]+, 255" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+ands_di_test3 (s64 a, s64 b, s64 c)
+{
+  s64 d = a & (b << 3);
+
+  /* { dg-final { scan-assembler "ands\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+main ()
+{
+  int x;
+  s64 y;
+
+  x = ands_si_test1 (29, 4, 5);
+  if (x != 13)
+    abort ();
+
+  x = ands_si_test1 (5, 2, 20);
+  if (x != 25)
+    abort ();
+
+  x = ands_si_test2 (29, 4, 5);
+  if (x != 38)
+    abort ();
+
+  x = ands_si_test2 (1024, 2, 20);
+  if (x != 1044)
+    abort ();
+
+  x = ands_si_test3 (35, 4, 5);
+  if (x != 41)
+    abort ();
+
+  x = ands_si_test3 (5, 2, 20);
+  if (x != 25)
+    abort ();
+
+  y = ands_di_test1 (0x130000029ll,
+                     0x320000004ll,
+                     0x505050505ll);
+
+  if (y != ((0x130000029ll & 0x320000004ll) + 0x320000004ll + 0x505050505ll))
+    abort ();
+
+  y = ands_di_test1 (0x5000500050005ll,
+                     0x2111211121112ll,
+                     0x0000000002020ll);
+  if (y != 0x5000500052025ll)
+    abort ();
+
+  y = ands_di_test2 (0x130000029ll,
+                     0x320000004ll,
+                     0x505050505ll);
+  if (y != ((0x130000029ll & 0xff) + 0x320000004ll + 0x505050505ll))
+    abort ();
+
+  y = ands_di_test2 (0x130002900ll,
+                     0x320000004ll,
+                     0x505050505ll);
+  if (y != (0x130002900ll + 0x505050505ll))
+    abort ();
+
+  y = ands_di_test3 (0x130000029ll,
+                     0x064000008ll,
+                     0x505050505ll);
+  if (y != ((0x130000029ll & (0x064000008ll << 3))
+	    + 0x064000008ll + 0x505050505ll))
+    abort ();
+
+  y = ands_di_test3 (0x130002900ll,
+                     0x088000008ll,
+                     0x505050505ll);
+  if (y != (0x130002900ll + 0x505050505ll))
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-release.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-op-release.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-release.c
@@ -1,43 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-int v = 0;
-
-int
-atomic_fetch_add_RELEASE (int a)
-{
-  return __atomic_fetch_add (&v, a, __ATOMIC_RELEASE);
-}
-
-int
-atomic_fetch_sub_RELEASE (int a)
-{
-  return __atomic_fetch_sub (&v, a, __ATOMIC_RELEASE);
-}
-
-int
-atomic_fetch_and_RELEASE (int a)
-{
-  return __atomic_fetch_and (&v, a, __ATOMIC_RELEASE);
-}
-
-int
-atomic_fetch_nand_RELEASE (int a)
-{
-  return __atomic_fetch_nand (&v, a, __ATOMIC_RELEASE);
-}
-
-int
-atomic_fetch_xor_RELEASE (int a)
-{
-  return __atomic_fetch_xor (&v, a, __ATOMIC_RELEASE);
-}
-
-int
-atomic_fetch_or_RELEASE (int a)
-{
-  return __atomic_fetch_or (&v, a, __ATOMIC_RELEASE);
-}
+#include "atomic-op-release.x"
 
 /* { dg-final { scan-assembler-times "ldxr\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
 /* { dg-final { scan-assembler-times "stlxr\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-vfmaxv.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-vfmaxv.c
@@ -0,0 +1,169 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps -ffast-math" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+
+extern float fabsf (float);
+extern double fabs (double);
+extern int isnan (double);
+extern float fmaxf (float, float);
+extern float fminf (float, float);
+extern double fmax (double, double);
+extern double fmin (double, double);
+
+#define NUM_TESTS 16
+#define DELTA 0.000001
+#define NAN (0.0 / 0.0)
+
+float input_float32[] = {0.1f, -0.1f, 0.4f, 10.3f,
+			 200.0f, -800.0f, -13.0f, -0.5f,
+			 NAN, -870.0f, 10.4f, 310.11f,
+			 0.0f, -865.0f, -2213.0f, -1.5f};
+
+double input_float64[] = {0.1, -0.1, 0.4, 10.3,
+			  200.0, -800.0, -13.0, -0.5,
+			  NAN, -870.0, 10.4, 310.11,
+			  0.0, -865.0, -2213.0, -1.5};
+
+#define EQUALF(a, b) (fabsf (a - b) < DELTA)
+#define EQUALD(a, b) (fabs (a - b) < DELTA)
+
+/* Floating point 'unordered' variants.  */
+
+#undef TEST
+#define TEST(MAXMIN, CMP_OP, SUFFIX, Q, TYPE, LANES, FLOAT)		\
+int									\
+test_v##MAXMIN##v##SUFFIX##_##TYPE##x##LANES##_t (void)			\
+{									\
+  int i, j;								\
+  int moves = (NUM_TESTS - LANES) + 1;					\
+  TYPE##_t out_l[NUM_TESTS];						\
+  TYPE##_t out_v[NUM_TESTS];						\
+									\
+  /* Calculate linearly.  */						\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      out_l[i] = input_##TYPE[i];					\
+      for (j = 0; j < LANES; j++)					\
+	{								\
+	  if (isnan (out_l[i]))						\
+	    continue;							\
+	  if (isnan (input_##TYPE[i + j])				\
+	      || input_##TYPE[i + j] CMP_OP out_l[i])			\
+	    out_l[i] = input_##TYPE[i + j];				\
+	}								\
+    }									\
+									\
+  /* Calculate using vector reduction intrinsics.  */			\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      TYPE##x##LANES##_t t1 = vld1##Q##_##SUFFIX (input_##TYPE + i);	\
+      out_v[i] = v##MAXMIN##v##Q##_##SUFFIX (t1);			\
+    }									\
+									\
+  /* Compare.  */							\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      if (!EQUAL##FLOAT (out_v[i], out_l[i])				\
+			 && !(isnan (out_v[i]) && isnan (out_l[i])))	\
+	return 0;							\
+    }									\
+  return 1;								\
+}
+
+#define BUILD_VARIANTS(TYPE, STYPE, W32, W64, F)	\
+TEST (max, >, STYPE,  , TYPE, W32, F)			\
+TEST (max, >, STYPE, q, TYPE, W64, F)			\
+TEST (min, <, STYPE,  , TYPE, W32, F)			\
+TEST (min, <, STYPE, q, TYPE, W64, F)
+
+BUILD_VARIANTS (float32, f32, 2, 4, F)
+/* { dg-final { scan-assembler "fmaxp\\ts\[0-9\]+, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fminp\\ts\[0-9\]+, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fmaxv\\ts\[0-9\]+, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fminv\\ts\[0-9\]+, v\[0-9\]+\.4s" } } */
+TEST (max, >, f64, q, float64, 2, D)
+/* { dg-final { scan-assembler "fmaxp\\td\[0-9\]+, v\[0-9\]+\.2d" } } */
+TEST (min, <, f64, q, float64, 2, D)
+/* { dg-final { scan-assembler "fminp\\td\[0-9\]+, v\[0-9\]+\.2d" } } */
+
+/* Floating point 'nm' variants.  */
+
+#undef TEST
+#define TEST(MAXMIN, F, SUFFIX, Q, TYPE, LANES, FLOAT)			\
+int									\
+test_v##MAXMIN##nmv##SUFFIX##_##TYPE##x##LANES##_t (void)		\
+{									\
+  int i, j;								\
+  int moves = (NUM_TESTS - LANES) + 1;					\
+  TYPE##_t out_l[NUM_TESTS];						\
+  TYPE##_t out_v[NUM_TESTS];						\
+									\
+  /* Calculate linearly.  */						\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      out_l[i] = input_##TYPE[i];					\
+      for (j = 0; j < LANES; j++)					\
+	out_l[i] = f##MAXMIN##F (input_##TYPE[i + j],  out_l[i]);	\
+    }									\
+									\
+  /* Calculate using vector reduction intrinsics.  */			\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      TYPE##x##LANES##_t t1 = vld1##Q##_##SUFFIX (input_##TYPE + i);	\
+      out_v[i] = v##MAXMIN##nmv##Q##_##SUFFIX (t1);			\
+    }									\
+									\
+  /* Compare.  */							\
+  for (i = 0; i < moves; i++)						\
+    {									\
+      if (!EQUAL##FLOAT (out_v[i], out_l[i]))				\
+	return 0;							\
+    }									\
+  return 1;								\
+}
+
+TEST (max, f, f32, , float32, 2, D)
+/* { dg-final { scan-assembler "fmaxnmp\\ts\[0-9\]+, v\[0-9\]+\.2s" } } */
+TEST (min, f, f32, , float32, 2, D)
+/* { dg-final { scan-assembler "fminnmp\\ts\[0-9\]+, v\[0-9\]+\.2s" } } */
+TEST (max, f, f32, q, float32, 4, D)
+/* { dg-final { scan-assembler "fmaxnmv\\ts\[0-9\]+, v\[0-9\]+\.4s" } } */
+TEST (min, f, f32, q, float32, 4, D)
+/* { dg-final { scan-assembler "fminnmv\\ts\[0-9\]+, v\[0-9\]+\.4s" } } */
+TEST (max, , f64, q, float64, 2, D)
+/* { dg-final { scan-assembler "fmaxnmp\\td\[0-9\]+, v\[0-9\]+\.2d" } } */
+TEST (min, , f64, q, float64, 2, D)
+/* { dg-final { scan-assembler "fminnmp\\td\[0-9\]+, v\[0-9\]+\.2d" } } */
+
+#undef TEST
+#define TEST(MAXMIN, CMP_OP, SUFFIX, Q, TYPE, LANES, FLOAT)		\
+{									\
+  if (!test_v##MAXMIN##v##SUFFIX##_##TYPE##x##LANES##_t ())		\
+    abort ();								\
+}
+
+int
+main (int argc, char **argv)
+{
+  BUILD_VARIANTS (float32, f32, 2, 4, F)
+  TEST (max, >, f64, q, float64, 2, D)
+  TEST (min, <, f64, q, float64, 2, D)
+
+#undef TEST
+#define TEST(MAXMIN, CMP_OP, SUFFIX, Q, TYPE, LANES, FLOAT)		\
+{									\
+  if (!test_v##MAXMIN##nmv##SUFFIX##_##TYPE##x##LANES##_t ())		\
+    abort ();								\
+}
+
+  BUILD_VARIANTS (float32, f32, 2, 4, F)
+  TEST (max, >, f64, q, float64, 2, D)
+  TEST (min, <, f64, q, float64, 2, D)
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-short.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-short.x
@@ -0,0 +1,37 @@
+short v = 0;
+
+short
+atomic_fetch_add_RELAXED (short a)
+{
+  return __atomic_fetch_add (&v, a, __ATOMIC_RELAXED);
+}
+
+short
+atomic_fetch_sub_RELAXED (short a)
+{
+  return __atomic_fetch_sub (&v, a, __ATOMIC_RELAXED);
+}
+
+short
+atomic_fetch_and_RELAXED (short a)
+{
+  return __atomic_fetch_and (&v, a, __ATOMIC_RELAXED);
+}
+
+short
+atomic_fetch_nand_RELAXED (short a)
+{
+  return __atomic_fetch_nand (&v, a, __ATOMIC_RELAXED);
+}
+
+short
+atomic_fetch_xor_RELAXED (short a)
+{
+  return __atomic_fetch_xor (&v, a, __ATOMIC_RELAXED);
+}
+
+short
+atomic_fetch_or_RELAXED (short a)
+{
+  return __atomic_fetch_or (&v, a, __ATOMIC_RELAXED);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-vcvt.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-vcvt.c
@@ -0,0 +1,132 @@
+/* { dg-do run } */
+/* { dg-options "-O3 --save-temps -ffast-math" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+extern double fabs (double);
+
+#define NUM_TESTS 8
+#define DELTA 0.000001
+
+float input_f32[] = {0.1f, -0.1f, 0.4f, 10.3f,
+		     200.0f, -800.0f, -13.0f, -0.5f};
+double input_f64[] = {0.1, -0.1, 0.4, 10.3,
+		      200.0, -800.0, -13.0, -0.5};
+
+#define TEST(SUFFIX, Q, WIDTH, LANES, S, U, D)		     		\
+int									\
+test_vcvt##SUFFIX##_##S##WIDTH##_f##WIDTH##x##LANES##_t (void)		\
+{									\
+  int ret = 1;								\
+  int i = 0;								\
+  int nlanes = LANES;							\
+  U##int##WIDTH##_t expected_out[NUM_TESTS];				\
+  U##int##WIDTH##_t actual_out[NUM_TESTS];				\
+									\
+  for (i = 0; i < NUM_TESTS; i++)					\
+    {									\
+      expected_out[i]							\
+	= vcvt##SUFFIX##D##_##S##WIDTH##_f##WIDTH (input_f##WIDTH[i]);	\
+      /* Don't vectorize this.  */					\
+      asm volatile ("" : : : "memory");					\
+    }									\
+									\
+  for (i = 0; i < NUM_TESTS; i+=nlanes)					\
+    {									\
+      U##int##WIDTH##x##LANES##_t out =					\
+	vcvt##SUFFIX##Q##_##S##WIDTH##_f##WIDTH				\
+		(vld1##Q##_f##WIDTH (input_f##WIDTH + i));		\
+      vst1##Q##_##S##WIDTH (actual_out + i, out);			\
+    }									\
+									\
+  for (i = 0; i < NUM_TESTS; i++)					\
+    ret &= fabs (expected_out[i] - actual_out[i]) < DELTA;		\
+									\
+  return ret;								\
+}									\
+
+
+#define BUILD_VARIANTS(SUFFIX)			\
+TEST (SUFFIX,  , 32, 2, s, ,s)			\
+TEST (SUFFIX, q, 32, 4, s, ,s)			\
+TEST (SUFFIX, q, 64, 2, s, ,d)			\
+TEST (SUFFIX,  , 32, 2, u,u,s)			\
+TEST (SUFFIX, q, 32, 4, u,u,s)			\
+TEST (SUFFIX, q, 64, 2, u,u,d)			\
+
+BUILD_VARIANTS ( )
+/* { dg-final { scan-assembler "fcvtzs\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtzs\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtzs\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtzs\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtzs\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+/* { dg-final { scan-assembler "fcvtzu\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtzu\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtzu\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtzu\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtzu\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+BUILD_VARIANTS (a)
+/* { dg-final { scan-assembler "fcvtas\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtas\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtas\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtas\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtas\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+/* { dg-final { scan-assembler "fcvtau\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtau\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtau\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtau\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtau\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+BUILD_VARIANTS (m)
+/* { dg-final { scan-assembler "fcvtms\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtms\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtms\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtms\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtms\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+/* { dg-final { scan-assembler "fcvtmu\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtmu\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtmu\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtmu\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtmu\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+BUILD_VARIANTS (n)
+/* { dg-final { scan-assembler "fcvtns\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtns\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtns\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtns\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtns\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+/* { dg-final { scan-assembler "fcvtnu\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtnu\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtnu\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtnu\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtnu\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+BUILD_VARIANTS (p)
+/* { dg-final { scan-assembler "fcvtps\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtps\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtps\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtps\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtps\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+/* { dg-final { scan-assembler "fcvtpu\\tw\[0-9\]+, s\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtpu\\tx\[0-9\]+, d\[0-9\]+" } } */
+/* { dg-final { scan-assembler "fcvtpu\\tv\[0-9\]+\.2s, v\[0-9\]+\.2s" } } */
+/* { dg-final { scan-assembler "fcvtpu\\tv\[0-9\]+\.4s, v\[0-9\]+\.4s" } } */
+/* { dg-final { scan-assembler "fcvtpu\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
+
+#undef TEST
+#define TEST(SUFFIX, Q, WIDTH, LANES, S, U, D)		     		\
+{									\
+  if (!test_vcvt##SUFFIX##_##S##WIDTH##_f##WIDTH##x##LANES##_t ())	\
+    abort ();								\
+}
+
+int
+main (int argc, char **argv)
+{
+  BUILD_VARIANTS ( )
+  BUILD_VARIANTS (a)
+  BUILD_VARIANTS (m)
+  BUILD_VARIANTS (n)
+  BUILD_VARIANTS (p)
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-release.x
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-release.x
@@ -0,0 +1,37 @@
+int v = 0;
+
+int
+atomic_fetch_add_RELEASE (int a)
+{
+  return __atomic_fetch_add (&v, a, __ATOMIC_RELEASE);
+}
+
+int
+atomic_fetch_sub_RELEASE (int a)
+{
+  return __atomic_fetch_sub (&v, a, __ATOMIC_RELEASE);
+}
+
+int
+atomic_fetch_and_RELEASE (int a)
+{
+  return __atomic_fetch_and (&v, a, __ATOMIC_RELEASE);
+}
+
+int
+atomic_fetch_nand_RELEASE (int a)
+{
+  return __atomic_fetch_nand (&v, a, __ATOMIC_RELEASE);
+}
+
+int
+atomic_fetch_xor_RELEASE (int a)
+{
+  return __atomic_fetch_xor (&v, a, __ATOMIC_RELEASE);
+}
+
+int
+atomic_fetch_or_RELEASE (int a)
+{
+  return __atomic_fetch_or (&v, a, __ATOMIC_RELEASE);
+}
Index: b/src/gcc/testsuite/gcc.target/aarch64/fabd.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/fabd.c
@@ -0,0 +1,38 @@
+/* { dg-do run } */
+/* { dg-options "-O1 -fno-inline --save-temps" } */
+
+extern double fabs (double);
+extern float fabsf (float);
+extern void abort ();
+extern void exit (int);
+
+void
+fabd_d (double x, double y, double d)
+{
+  if ((fabs (x - y) - d) > 0.00001)
+    abort ();
+}
+
+/* { dg-final { scan-assembler "fabd\td\[0-9\]+" } } */
+
+void
+fabd_f (float x, float y, float d)
+{
+  if ((fabsf (x - y) - d) > 0.00001)
+    abort ();
+}
+
+/* { dg-final { scan-assembler "fabd\ts\[0-9\]+" } } */
+
+int
+main ()
+{
+  fabd_d (10.0, 5.0, 5.0);
+  fabd_d (5.0, 10.0, 5.0);
+  fabd_f (10.0, 5.0, 5.0);
+  fabd_f (5.0, 10.0, 5.0);
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fp.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fp.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fp.c
@@ -117,6 +117,16 @@ int main (void)
 			    9.0, 10.0, 11.0, 12.0,
 			    13.0, 14.0, 15.0, 16.0 };
 
+  F32  fabd_F32_vector[] = { 1.0f, 1.0f, 1.0f, 1.0f,
+			     1.0f, 1.0f, 1.0f, 1.0f,
+			     1.0f, 1.0f, 1.0f, 1.0f,
+			     1.0f, 1.0f, 1.0f, 1.0f };
+
+  F64  fabd_F64_vector[] = { 1.0, 1.0, 1.0, 1.0,
+			     1.0, 1.0, 1.0, 1.0,
+			     1.0, 1.0, 1.0, 1.0,
+			     1.0, 1.0, 1.0, 1.0 };
+
   /* Setup input vectors.  */
   for (i=1; i<=16; i++)
     {
@@ -132,6 +142,7 @@ int main (void)
   TEST (div, 3);
   TEST (neg, 2);
   TEST (abs, 2);
+  TEST (fabd, 3);
 
   return 0;
 }
Index: b/src/gcc/testsuite/gcc.target/aarch64/cmn-neg2.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/cmn-neg2.c
@@ -0,0 +1,34 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps" } */
+
+extern void abort (void);
+
+/* It's unsafe to use CMN in these comparisons.  */
+
+void __attribute__ ((noinline))
+foo_s32 (int a, int b)
+{
+  if (a < -b)
+    abort ();
+}
+
+void __attribute__ ((noinline))
+foo_s64 (unsigned long long a, unsigned long long b)
+{
+  if (a > -b)
+    abort ();
+}
+
+
+int
+main (void)
+{
+  int a = 30;
+  int b = 42;
+  foo_s32 (a, b);
+  foo_s64 (a, b);
+  return 0;
+}
+/* { dg-final { scan-assembler-not "cmn\t" } } */
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/ngc.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/ngc.c
@@ -0,0 +1,66 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+typedef unsigned int u32;
+
+u32
+ngc_si (u32 a, u32 b, u32 c, u32 d)
+{
+  a = -b - (c < d);
+  return a;
+}
+
+typedef unsigned long long u64;
+
+u64
+ngc_si_tst (u64 a, u32 b, u32 c, u32 d)
+{
+  a = -b - (c < d);
+  return a;
+}
+
+u64
+ngc_di (u64 a, u64 b, u64 c, u64 d)
+{
+  a = -b - (c < d);
+  return a;
+}
+
+int
+main ()
+{
+  int x;
+  u64 y;
+
+  x = ngc_si (29, 4, 5, 4);
+  if (x != -4)
+    abort ();
+
+  x = ngc_si (1024, 2, 20, 13);
+  if (x != -2)
+    abort ();
+
+  y = ngc_si_tst (0x130000029ll, 32, 50, 12);
+  if (y != 0xffffffe0)
+    abort ();
+
+  y = ngc_si_tst (0x5000500050005ll, 21, 2, 14);
+  if (y != 0xffffffea)
+    abort ();
+
+  y = ngc_di (0x130000029ll, 0x320000004ll, 0x505050505ll, 0x123123123ll);
+  if (y != 0xfffffffcdffffffc)
+    abort ();
+
+  y = ngc_di (0x5000500050005ll,
+	      0x2111211121112ll, 0x0000000002020ll, 0x1414575046477ll);
+  if (y != 0xfffdeeedeeedeeed)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { scan-assembler-times "ngc\tw\[0-9\]+, w\[0-9\]+" 2 } } */
+/* { dg-final { scan-assembler-times "ngc\tx\[0-9\]+, x\[0-9\]+" 1 } } */
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/sha1_1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/sha1_1.c
@@ -0,0 +1,55 @@
+
+/* { dg-do compile } */
+/* { dg-options "-march=armv8-a+crypto" } */
+
+#include "arm_neon.h"
+
+uint32x4_t
+test_vsha1cq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
+{
+  return vsha1cq_u32 (hash_abcd, hash_e, wk);
+}
+
+/* { dg-final { scan-assembler-times "sha1c\\tq" 1 } } */
+
+uint32x4_t
+test_vsha1mq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
+{
+  return vsha1mq_u32 (hash_abcd, hash_e, wk);
+}
+
+/* { dg-final { scan-assembler-times "sha1m\\tq" 1 } } */
+
+uint32x4_t
+test_vsha1pq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
+{
+  return vsha1pq_u32 (hash_abcd, hash_e, wk);
+}
+
+/* { dg-final { scan-assembler-times "sha1p\\tq" 1 } } */
+
+uint32_t
+test_vsha1h_u32 (uint32_t hash_e)
+{
+  return vsha1h_u32 (hash_e);
+}
+
+/* { dg-final { scan-assembler-times "sha1h\\ts" 1 } } */
+
+uint32x4_t
+test_vsha1su0q_u32 (uint32x4_t w0_3, uint32x4_t w4_7, uint32x4_t w8_11)
+{
+  return vsha1su0q_u32 (w0_3, w4_7, w8_11);
+}
+
+/* { dg-final { scan-assembler-times "sha1su0\\tv" 1 } } */
+
+uint32x4_t
+test_vsha1su1q_u32 (uint32x4_t tw0_3, uint32x4_t w12_15)
+{
+  return vsha1su1q_u32 (tw0_3, w12_15);
+}
+
+/* { dg-final { scan-assembler-times "sha1su1\\tv" 1 } } */
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/cmp.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/cmp.c
@@ -0,0 +1,61 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+int
+cmp_si_test1 (int a, int b, int c)
+{
+  if (a > b)
+    return a + c;
+  else
+    return a + b + c;
+}
+
+int
+cmp_si_test2 (int a, int b, int c)
+{
+  if ((a >> 3) > b)
+    return a + c;
+  else
+    return a + b + c;
+}
+
+typedef long long s64;
+
+s64
+cmp_di_test1 (s64 a, s64 b, s64 c)
+{
+  if (a > b)
+    return a + c;
+  else
+    return a + b + c;
+}
+
+s64
+cmp_di_test2 (s64 a, s64 b, s64 c)
+{
+  if ((a >> 3) > b)
+    return a + c;
+  else
+    return a + b + c;
+}
+
+int
+cmp_di_test3 (int a, s64 b, s64 c)
+{
+  if (a > b)
+    return a + c;
+  else
+    return a + b + c;
+}
+
+int
+cmp_di_test4 (int a, s64 b, s64 c)
+{
+  if (((s64)a << 3) > b)
+    return a + c;
+  else
+    return a + b + c;
+}
+
+/* { dg-final { scan-assembler-times "cmp\tw\[0-9\]+, w\[0-9\]+" 2 } } */
+/* { dg-final { scan-assembler-times "cmp\tx\[0-9\]+, x\[0-9\]+" 4 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-ge-f.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-ge-f.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-ge-f.c
@@ -2,12 +2,13 @@
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-all -fno-unroll-loops --save-temps -fno-inline" } */
 
 #define FTYPE float
+#define ITYPE int
 #define OP >=
 #define INV_OP <
 
 #include "vect-fcm.x"
 
-/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 4 "vect" } } */
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 8 "vect" } } */
 /* { dg-final { scan-assembler "fcmge\\tv\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s" } } */
 /* { dg-final { scan-assembler "fcmge\\tv\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s, 0" } } */
 /* { dg-final { scan-assembler "fcmlt\\tv\[0-9\]+\.\[24\]s, v\[0-9\]+\.\[24\]s, 0" } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/bfxil_2.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/bfxil_2.c
@@ -0,0 +1,42 @@
+/* { dg-do run { target aarch64*-*-* } } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target aarch64_big_endian } */
+
+extern void abort (void);
+
+typedef struct bitfield
+{
+  unsigned short eight1: 8;
+  unsigned short four: 4;
+  unsigned short eight2: 8;
+  unsigned short seven: 7;
+  unsigned int sixteen: 16;
+  unsigned short eight3: 8;
+  unsigned short eight4: 8;
+} bitfield;
+
+bitfield
+bfxil (bitfield a)
+{
+  /* { dg-final { scan-assembler "bfxil\tx\[0-9\]+, x\[0-9\]+, 40, 8" } } */
+  a.eight4 = a.eight2;
+  return a;
+}
+
+int
+main (void)
+{
+  static bitfield a;
+  bitfield b;
+
+  a.eight4 = 9;
+  a.eight2 = 57;
+  b = bfxil (a);
+
+  if (b.eight4 != a.eight2)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fp.x
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fp.x
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fp.x
@@ -7,13 +7,23 @@ typedef double *__restrict__ pRF64;
 extern float fabsf (float);
 extern double fabs (double);
 
+#define DEF3a(fname, type, op) \
+			 void  fname##_##type (pR##type a,   \
+					       pR##type b,   \
+					       pR##type c)   \
+			 {				     \
+			   int i;			     \
+			   for (i = 0; i < 16; i++)	     \
+			     a[i] = op (b[i] - c[i]);	     \
+			 }
+
 #define DEF3(fname, type, op) \
 			void  fname##_##type (pR##type a,   \
 					      pR##type b,   \
 					      pR##type c)   \
 			{				    \
 			  int i; 			    \
-			  for (i=0; i<16; i++)		    \
+			  for (i = 0; i < 16; i++)	    \
 			    a[i] = b[i] op c[i];	    \
 			}
 
@@ -22,11 +32,15 @@ extern double fabs (double);
 					     pR##type b) \
 			{				  \
 			  int i; 			  \
-			  for (i=0; i<16; i++)		  \
+			  for (i = 0; i < 16; i++)	  \
 			    a[i] = op(b[i]);		  \
 			}
 
 
+#define DEFN3a(fname, op) \
+		 DEF3a (fname, F32, op) \
+		 DEF3a (fname, F64, op)
+
 #define DEFN3(fname, op) \
 		DEF3 (fname, F32, op) \
 		DEF3 (fname, F64, op)
@@ -42,3 +56,5 @@ DEFN3 (div, /)
 DEFN2 (neg, -)
 DEF2 (abs, F32, fabsf)
 DEF2 (abs, F64, fabs)
+DEF3a (fabd, F32, fabsf)
+DEF3a (fabd, F64, fabs)
Index: b/src/gcc/testsuite/gcc.target/aarch64/cpu-diagnostics-2.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/cpu-diagnostics-2.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/cpu-diagnostics-2.c
@@ -1,5 +1,5 @@
 /* { dg-error "missing" "" {target "aarch64*-*-*" } } */
-/* { dg-options "-O2 -mcpu=example-1+no" } */
+/* { dg-options "-O2 -mcpu=cortex-a53+no" } */
 
 void f ()
 {
Index: b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acq_rel.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acq_rel.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-op-acq_rel.c
@@ -1,43 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2" } */
 
-int v = 0;
-
-int
-atomic_fetch_add_ACQ_REL (int a)
-{
-  return __atomic_fetch_add (&v, a, __ATOMIC_ACQ_REL);
-}
-
-int
-atomic_fetch_sub_ACQ_REL (int a)
-{
-  return __atomic_fetch_sub (&v, a, __ATOMIC_ACQ_REL);
-}
-
-int
-atomic_fetch_and_ACQ_REL (int a)
-{
-  return __atomic_fetch_and (&v, a, __ATOMIC_ACQ_REL);
-}
-
-int
-atomic_fetch_nand_ACQ_REL (int a)
-{
-  return __atomic_fetch_nand (&v, a, __ATOMIC_ACQ_REL);
-}
-
-int
-atomic_fetch_xor_ACQ_REL (int a)
-{
-  return __atomic_fetch_xor (&v, a, __ATOMIC_ACQ_REL);
-}
-
-int
-atomic_fetch_or_ACQ_REL (int a)
-{
-  return __atomic_fetch_or (&v, a, __ATOMIC_ACQ_REL);
-}
+#include "atomic-op-acq_rel.x"
 
 /* { dg-final { scan-assembler-times "ldaxr\tw\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
 /* { dg-final { scan-assembler-times "stlxr\tw\[0-9\]+, w\[0-9\]+, \\\[x\[0-9\]+\\\]" 6 } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/subs1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/subs1.c
@@ -0,0 +1,149 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+int
+subs_si_test1 (int a, int b, int c)
+{
+  int d = a - c;
+
+  /* { dg-final { scan-assembler "subs\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+subs_si_test2 (int a, int b, int c)
+{
+  int d = a - 0xff;
+
+  /* { dg-final { scan-assembler "subs\tw\[0-9\]+, w\[0-9\]+, #255" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+subs_si_test3 (int a, int b, int c)
+{
+  int d = a - (b << 3);
+
+  /* { dg-final { scan-assembler "subs\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+typedef long long s64;
+
+s64
+subs_di_test1 (s64 a, s64 b, s64 c)
+{
+  s64 d = a - c;
+
+  /* { dg-final { scan-assembler "subs\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+subs_di_test2 (s64 a, s64 b, s64 c)
+{
+  s64 d = a - 0xff;
+
+  /* { dg-final { scan-assembler "subs\tx\[0-9\]+, x\[0-9\]+, #255" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+subs_di_test3 (s64 a, s64 b, s64 c)
+{
+  s64 d = a - (b << 3);
+
+  /* { dg-final { scan-assembler "subs\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int main ()
+{
+  int x;
+  s64 y;
+
+  x = subs_si_test1 (29, 4, 5);
+  if (x != 33)
+    abort ();
+
+  x = subs_si_test1 (5, 2, 20);
+  if (x != 7)
+    abort ();
+
+  x = subs_si_test2 (29, 4, 5);
+  if (x != -217)
+    abort ();
+
+  x = subs_si_test2 (1024, 2, 20);
+  if (x != 791)
+    abort ();
+
+  x = subs_si_test3 (35, 4, 5);
+  if (x != 12)
+    abort ();
+
+  x = subs_si_test3 (5, 2, 20);
+  if (x != 11)
+    abort ();
+
+  y = subs_di_test1 (0x130000029ll,
+		     0x320000004ll,
+		     0x505050505ll);
+
+  if (y != 0x45000002d)
+    abort ();
+
+  y = subs_di_test1 (0x5000500050005ll,
+		     0x2111211121112ll,
+		     0x0000000002020ll);
+  if (y != 0x7111711171117)
+    abort ();
+
+  y = subs_di_test2 (0x130000029ll,
+		     0x320000004ll,
+		     0x505050505ll);
+  if (y != 0x955050433)
+    abort ();
+
+  y = subs_di_test2 (0x130002900ll,
+		     0x320000004ll,
+		     0x505050505ll);
+  if (y != 0x955052d0a)
+    abort ();
+
+  y = subs_di_test3 (0x130000029ll,
+		     0x064000008ll,
+		     0x505050505ll);
+  if (y != 0x3790504f6)
+    abort ();
+
+  y = subs_di_test3 (0x130002900ll,
+		     0x088000008ll,
+		     0x505050505ll);
+  if (y != 0x27d052dcd)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/adds2.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/adds2.c
@@ -0,0 +1,155 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+int
+adds_si_test1 (int a, int b, int c)
+{
+  int d = a + b;
+
+  /* { dg-final { scan-assembler-not "adds\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" } } */
+  /* { dg-final { scan-assembler "add\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+adds_si_test2 (int a, int b, int c)
+{
+  int d = a + 0xfff;
+
+  /* { dg-final { scan-assembler-not "adds\tw\[0-9\]+, w\[0-9\]+, 4095" } } */
+  /* { dg-final { scan-assembler "add\tw\[0-9\]+, w\[0-9\]+, 4095" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+adds_si_test3 (int a, int b, int c)
+{
+  int d = a + (b << 3);
+
+  /* { dg-final { scan-assembler-not "adds\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  /* { dg-final { scan-assembler "add\tw\[0-9\]+, w\[0-9\]+, w\[0-9\]+, lsl 3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+typedef long long s64;
+
+s64
+adds_di_test1 (s64 a, s64 b, s64 c)
+{
+  s64 d = a + b;
+
+  /* { dg-final { scan-assembler-not "adds\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" } } */
+  /* { dg-final { scan-assembler "add\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+adds_di_test2 (s64 a, s64 b, s64 c)
+{
+  s64 d = a + 0x1000ll;
+
+  /* { dg-final { scan-assembler-not "adds\tx\[0-9\]+, x\[0-9\]+, 4096" } } */
+  /* { dg-final { scan-assembler "add\tx\[0-9\]+, x\[0-9\]+, 4096" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+s64
+adds_di_test3 (s64 a, s64 b, s64 c)
+{
+  s64 d = a + (b << 3);
+
+  /* { dg-final { scan-assembler-not "adds\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  /* { dg-final { scan-assembler "add\tx\[0-9\]+, x\[0-9\]+, x\[0-9\]+, lsl 3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int main ()
+{
+  int x;
+  s64 y;
+
+  x = adds_si_test1 (29, 4, 5);
+  if (x != 42)
+    abort ();
+
+  x = adds_si_test1 (5, 2, 20);
+  if (x != 29)
+    abort ();
+
+  x = adds_si_test2 (29, 4, 5);
+  if (x != 4133)
+    abort ();
+
+  x = adds_si_test2 (1024, 2, 20);
+  if (x != 5141)
+    abort ();
+
+  x = adds_si_test3 (35, 4, 5);
+  if (x != 76)
+    abort ();
+
+  x = adds_si_test3 (5, 2, 20);
+  if (x != 43)
+    abort ();
+
+  y = adds_di_test1 (0x130000029ll,
+		     0x320000004ll,
+		     0x505050505ll);
+
+  if (y != 0xc75050536)
+    abort ();
+
+  y = adds_di_test1 (0x5000500050005ll,
+		     0x2111211121112ll,
+		     0x0000000002020ll);
+  if (y != 0x9222922294249)
+    abort ();
+
+  y = adds_di_test2 (0x130000029ll,
+		     0x320000004ll,
+		     0x505050505ll);
+  if (y != 0x955051532)
+    abort ();
+
+  y = adds_di_test2 (0x540004100ll,
+		     0x320000004ll,
+		     0x805050205ll);
+  if (y != 0x1065055309)
+    abort ();
+
+  y = adds_di_test3 (0x130000029ll,
+		     0x064000008ll,
+		     0x505050505ll);
+  if (y != 0x9b9050576)
+    abort ();
+
+  y = adds_di_test3 (0x130002900ll,
+		     0x088000008ll,
+		     0x505050505ll);
+  if (y != 0xafd052e4d)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
Index: b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-gt-d.c
===================================================================
--- a/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-gt-d.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vect-fcm-gt-d.c
@@ -2,12 +2,13 @@
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-all -fno-unroll-loops --save-temps -fno-inline" } */
 
 #define FTYPE double
+#define ITYPE long
 #define OP >
 #define INV_OP <=
 
 #include "vect-fcm.x"
 
-/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 4 "vect" } } */
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 8 "vect" } } */
 /* { dg-final { scan-assembler "fcmgt\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, v\[0-9\]+\.2d" } } */
 /* { dg-final { scan-assembler "fcmgt\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, 0" } } */
 /* { dg-final { scan-assembler "fcmle\\tv\[0-9\]+\.2d, v\[0-9\]+\.2d, 0" } } */
Index: b/src/gcc/testsuite/lib/target-supports.exp
===================================================================
--- a/src/gcc/testsuite/lib/target-supports.exp
+++ b/src/gcc/testsuite/lib/target-supports.exp
@@ -487,13 +487,6 @@ proc check_profiling_available { test_wh
 	return 0
     }
 
-    # We don't yet support profiling for AArch64.
-    if { [istarget aarch64*-*-*]
-	 && ([lindex $test_what 1] == "-p"
-	     || [lindex $test_what 1] == "-pg") } {
-	return 0
-    }
-
     # cygwin does not support -p.
     if { [istarget *-*-cygwin*] && $test_what == "-p" } {
 	return 0
@@ -2073,6 +2066,7 @@ proc check_effective_target_vect_uintflo
 	      || ([istarget powerpc*-*-*]
 		  && ![istarget powerpc-*-linux*paired*])
 	      || [istarget x86_64-*-*] 
+	      || [istarget aarch64*-*-*]
 	      || ([istarget arm*-*-*]
 		  && [check_effective_target_arm_neon_ok])} {
            set et_vect_uintfloat_cvt_saved 1
@@ -2139,6 +2133,15 @@ proc check_effective_target_aarch64_big_
     }]
 }
 
+# Return 1 if this is a AArch64 target supporting little endian
+proc check_effective_target_aarch64_little_endian { } {
+    return [check_no_compiler_messages aarch64_little_endian assembly {
+        #if !defined(__aarch64__) || defined(__AARCH64EB__)
+        #error FOO
+        #endif
+    }]
+}
+
 # Return 1 is this is an arm target using 32-bit instructions
 proc check_effective_target_arm32 { } {
     return [check_no_compiler_messages arm32 assembly {
@@ -2208,22 +2211,6 @@ proc check_effective_target_arm_v8_vfp_o
     }
 }
 
-# Return 1 if this is an ARM target supporting -mfpu=neon-fp-armv8
-# -mfloat-abi=softfp
-proc check_effective_target_arm_v8_neon_ok {} {
-    if { [check_effective_target_arm32] } {
-	return [check_no_compiler_messages arm_v8_neon_ok object {
-	  int foo (void)
-	  {
-	     __asm__ volatile ("vrintn.f32 q0, q0");
-	       return 0;
-	  }
-	} "-mfpu=neon-fp-armv8 -mfloat-abi=softfp"]
-    } else {
-	return 0
-    }
-}
-
 # Return 1 if this is an ARM target supporting -mfpu=vfp
 # -mfloat-abi=hard.  Some multilibs may be incompatible with these
 # options.
@@ -2263,6 +2250,49 @@ proc check_effective_target_arm_unaligne
     }]
 }
 
+# Return 1 if this is an ARM target supporting -mfpu=crypto-neon-fp-armv8
+# -mfloat-abi=softfp or equivalent options.  Some multilibs may be
+# incompatible with these options.  Also set et_arm_crypto_flags to the
+# best options to add.
+
+proc check_effective_target_arm_crypto_ok_nocache { } {
+    global et_arm_crypto_flags
+    set et_arm_crypto_flags ""
+    if { [check_effective_target_arm32] } {
+	foreach flags {"" "-mfloat-abi=softfp" "-mfpu=crypto-neon-fp-armv8" "-mfpu=crypto-neon-fp-armv8 -mfloat-abi=softfp"} {
+	    if { [check_no_compiler_messages_nocache arm_crypto_ok object {
+		#include "arm_neon.h"
+		uint8x16_t
+		foo (uint8x16_t a, uint8x16_t b)
+		{
+	          return vaeseq_u8 (a, b);
+		}
+	    } "$flags"] } {
+		set et_arm_crypto_flags $flags
+		return 1
+	    }
+	}
+    }
+
+    return 0
+}
+
+# Return 1 if this is an ARM target supporting -mfpu=crypto-neon-fp-armv8
+
+proc check_effective_target_arm_crypto_ok { } {
+    return [check_cached_effective_target arm_crypto_ok \
+		check_effective_target_arm_crypto_ok_nocache]
+}
+
+# Add options for crypto extensions.
+proc add_options_for_arm_crypto { flags } {
+    if { ! [check_effective_target_arm_crypto_ok] } {
+        return "$flags"
+    }
+    global et_arm_crypto_flags
+    return "$flags $et_arm_crypto_flags"
+}
+
 # Add the options needed for NEON.  We need either -mfloat-abi=softfp
 # or -mfloat-abi=hard, but if one is already specified by the
 # multilib, use it.  Similarly, if a -mfpu option already enables
@@ -2287,7 +2317,16 @@ proc add_options_for_arm_v8_neon { flags
     if { ! [check_effective_target_arm_v8_neon_ok] } {
         return "$flags"
     }
-    return "$flags -march=armv8-a -mfpu=neon-fp-armv8 -mfloat-abi=softfp"
+    global et_arm_v8_neon_flags
+    return "$flags $et_arm_v8_neon_flags -march=armv8-a"
+}
+
+proc add_options_for_arm_crc { flags } {
+    if { ! [check_effective_target_arm_crc_ok] } {
+        return "$flags"
+    }
+    global et_arm_crc_flags
+    return "$flags $et_arm_crc_flags"
 }
 
 # Add the options needed for NEON.  We need either -mfloat-abi=softfp
@@ -2331,6 +2370,94 @@ proc check_effective_target_arm_neon_ok
 		check_effective_target_arm_neon_ok_nocache]
 }
 
+proc check_effective_target_arm_crc_ok_nocache { } {
+    global et_arm_crc_flags
+    set et_arm_crc_flags "-march=armv8-a+crc"
+    return [check_no_compiler_messages_nocache arm_crc_ok object {
+	#if !defined (__ARM_FEATURE_CRC32)
+	#error FOO
+	#endif
+    } "$et_arm_crc_flags"]
+}
+
+proc check_effective_target_arm_crc_ok { } {
+    return [check_cached_effective_target arm_crc_ok \
+		check_effective_target_arm_crc_ok_nocache]
+}
+
+# Return 1 if this is an ARM target supporting -mfpu=neon-fp16
+# -mfloat-abi=softfp or equivalent options.  Some multilibs may be
+# incompatible with these options.  Also set et_arm_neon_flags to the
+# best options to add.
+
+proc check_effective_target_arm_neon_fp16_ok_nocache { } {
+    global et_arm_neon_fp16_flags
+    set et_arm_neon_fp16_flags ""
+    if { [check_effective_target_arm32] } {
+	foreach flags {"" "-mfloat-abi=softfp" "-mfpu=neon-fp16"
+	               "-mfpu=neon-fp16 -mfloat-abi=softfp"} {
+	    if { [check_no_compiler_messages_nocache arm_neon_fp_16_ok object {
+		#include "arm_neon.h"
+		float16x4_t
+		foo (float32x4_t arg)
+		{
+                  return vcvt_f16_f32 (arg);
+		}
+	    } "$flags"] } {
+		set et_arm_neon_fp16_flags $flags
+		return 1
+	    }
+	}
+    }
+
+    return 0
+}
+
+proc check_effective_target_arm_neon_fp16_ok { } {
+    return [check_cached_effective_target arm_neon_fp16_ok \
+		check_effective_target_arm_neon_fp16_ok_nocache]
+}
+
+proc add_options_for_arm_neon_fp16 { flags } {
+    if { ! [check_effective_target_arm_neon_fp16_ok] } {
+	return "$flags"
+    }
+    global et_arm_neon_fp16_flags
+    return "$flags $et_arm_neon_fp16_flags"
+}
+
+# Return 1 if this is an ARM target supporting -mfpu=neon-fp-armv8
+# -mfloat-abi=softfp or equivalent options.  Some multilibs may be
+# incompatible with these options.  Also set et_arm_v8_neon_flags to the
+# best options to add.
+
+proc check_effective_target_arm_v8_neon_ok_nocache { } {
+    global et_arm_v8_neon_flags
+    set et_arm_v8_neon_flags ""
+    if { [check_effective_target_arm32] } {
+	foreach flags {"" "-mfloat-abi=softfp" "-mfpu=neon-fp-armv8" "-mfpu=neon-fp-armv8 -mfloat-abi=softfp"} {
+	    if { [check_no_compiler_messages_nocache arm_v8_neon_ok object {
+		#include "arm_neon.h"
+		void
+		foo ()
+		{
+	          __asm__ volatile ("vrintn.f32 q0, q0");
+		}
+	    } "$flags"] } {
+		set et_arm_v8_neon_flags $flags
+		return 1
+	    }
+	}
+    }
+
+    return 0
+}
+
+proc check_effective_target_arm_v8_neon_ok { } {
+    return [check_cached_effective_target arm_v8_neon_ok \
+		check_effective_target_arm_v8_neon_ok_nocache]
+}
+
 # Return 1 if this is an ARM target supporting -mfpu=neon-vfpv4
 # -mfloat-abi=softfp or equivalent options.  Some multilibs may be
 # incompatible with these options.  Also set et_arm_neonv2_flags to the
@@ -2393,6 +2520,11 @@ proc check_effective_target_arm_fp16_ok_
 	# Must generate floating-point instructions.
 	return 0
     }
+    if [check_effective_target_arm_hf_eabi] {
+	# Use existing float-abi and force an fpu which supports fp16
+	set et_arm_fp16_flags "-mfpu=vfpv4"
+	return 1;
+    }
     if [check-flags [list "" { *-*-* } { "-mfpu=*" } { "" } ]] {
         # The existing -mfpu value is OK; use it, but add softfp.
 	set et_arm_fp16_flags "-mfloat-abi=softfp"
@@ -2525,6 +2657,17 @@ proc check_effective_target_arm_thumb2 {
     } ""]
 }
 
+# Return 1 if this is an ARM target where conditional execution is available.
+
+proc check_effective_target_arm_cond_exec { } {
+    return [check_no_compiler_messages arm_cond_exec assembly {
+	#if defined(__arm__) && defined(__thumb__) && !defined(__thumb2__)
+	#error FOO
+	#endif
+	int i;
+    } ""]
+}
+
 # Return 1 if this is an ARM cortex-M profile cpu
 
 proc check_effective_target_arm_cortex_m { } {
@@ -2570,6 +2713,24 @@ proc check_effective_target_arm_neonv2_h
     } [add_options_for_arm_neonv2 ""]]
 }
 
+# Return 1 if the target supports executing ARMv8 NEON instructions, 0
+# otherwise.
+
+proc check_effective_target_arm_v8_neon_hw { } {
+    return [check_runtime arm_v8_neon_hw_available {
+        #include "arm_neon.h"
+	int
+	main (void)
+	{
+	  float32x2_t a;
+	  asm ("vrinta.f32 %P0, %P1"
+	       : "=w" (a)
+	       : "0" (a));
+	  return 0;
+	}
+    } [add_options_for_arm_v8_neon ""]]
+}
+
 # Return 1 if this is a ARM target with NEON enabled.
 
 proc check_effective_target_arm_neon { } {
@@ -4720,6 +4881,33 @@ proc check_effective_target_simulator {
     return 0
 }
 
+# Return 1 if programs are intended to be run on hardware rather than
+# on a simulator
+
+proc check_effective_target_hw { } {
+
+    # All "src/sim" simulators set this one.
+    if [board_info target exists is_simulator] {
+	if [board_info target is_simulator] {
+	  return 0
+	} else {
+	  return 1
+	}
+    }
+
+    # The "sid" simulators don't set that one, but at least they set
+    # this one.
+    if [board_info target exists slow_simulator] {
+	if [board_info target slow_simulator] {
+	  return 0
+	} else {
+	  return 1
+	}
+    }
+
+    return 1
+}
+
 # Return 1 if the target is a VxWorks kernel.
 
 proc check_effective_target_vxworks_kernel { } {
Index: b/src/gcc/testsuite/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/ChangeLog.linaro
@@ -0,0 +1,1037 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-26  venkataramanan kumar  <venkataramanan.kumar@linaro.org>
+
+	Backport from trunk r205807.
+	2013-12-09  Richard Earnshaw  <rearnsha@arm.com>
+
+	* gcc.target/arm/ldrd-strd-offset.c: New.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-08-11  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206529
+	2014-01-10  Richard Earnshaw  <rearnsha@arm.com>
+
+	PR target/59744
+	* gcc.target/aarch64/cmn-neg.c: Use equality comparisons.
+	* gcc.target/aarch64/cmn-neg2.c: New test.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r204784
+	2013-11-14  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/cpu-diagnostics-2.c: Change "-mcpu="
+	to "cortex-a53".
+	* gcc.target/aarch64/cpu-diagnostics-3.c: Change "-mcpu="
+	to "cortex-a53".
+
+2014-04-07  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202663
+	2013-09-17  Cong Hou  <congh@google.com>
+
+	* gcc.dg/vect/vect-reduc-dot-s16c.c: Add a test case with dot product
+	on two arrays with short and int types. This should not be recognized
+	as a dot product pattern.
+
+2014-04-02  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	Backport from trunk r208511
+	2014-03-12  Christian Bruel  <christian.bruel@st.com>
+
+	PR target/60264
+	* gcc.target/arm/pr60264.c
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206519
+	2014-01-10  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* lib/target-supports.exp
+	(check_effective_target_arm_crypto_ok_nocache): New.
+	(check_effective_target_arm_crypto_ok): Use above procedure.
+	(add_options_for_arm_crypto): Use et_arm_crypto_flags.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206151
+	2013-12-20  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* gcc.target/arm/neon-vceq_p64.c: New test.
+	* gcc.target/arm/neon-vtst_p64.c: Likewise.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206131
+	2013-12-04  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	 * lib/target-supports.exp (check_effective_target_arm_crypto_ok):
+	 New procedure.
+	 (add_options_for_arm_crypto): Likewise.
+	 * gcc.target/arm/crypto-vaesdq_u8.c: New test.
+	 * gcc.target/arm/crypto-vaeseq_u8.c: Likewise.
+	 * gcc.target/arm/crypto-vaesimcq_u8.c: Likewise.
+	 * gcc.target/arm/crypto-vaesmcq_u8.c: Likewise.
+	 * gcc.target/arm/crypto-vldrq_p128.c: Likewise.
+	 * gcc.target/arm/crypto-vmull_high_p64.c: Likewise.
+	 * gcc.target/arm/crypto-vmullp64.c: Likewise.
+	 * gcc.target/arm/crypto-vsha1cq_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vsha1h_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vsha1mq_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vsha1pq_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vsha1su0q_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vsha1su1q_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vsha256h2q_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vsha256hq_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vsha256su0q_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vsha256su1q_u32.c: Likewise.
+	 * gcc.target/arm/crypto-vstrq_p128.c: Likewise.
+	 * gcc.target/arm/neon/vbslQp64: Generate.
+	 * gcc.target/arm/neon/vbslp64: Likewise.
+	 * gcc.target/arm/neon/vcombinep64: Likewise.
+	 * gcc.target/arm/neon/vcreatep64: Likewise.
+	 * gcc.target/arm/neon/vdupQ_lanep64: Likewise.
+	 * gcc.target/arm/neon/vdupQ_np64: Likewise.
+	 * gcc.target/arm/neon/vdup_lanep64: Likewise.
+	 * gcc.target/arm/neon/vdup_np64: Likewise.
+	 * gcc.target/arm/neon/vextQp64: Likewise.
+	 * gcc.target/arm/neon/vextp64: Likewise.
+	 * gcc.target/arm/neon/vget_highp64: Likewise.
+	 * gcc.target/arm/neon/vget_lowp64: Likewise.
+	 * gcc.target/arm/neon/vld1Q_dupp64: Likewise.
+	 * gcc.target/arm/neon/vld1Q_lanep64: Likewise.
+	 * gcc.target/arm/neon/vld1Qp64: Likewise.
+	 * gcc.target/arm/neon/vld1_dupp64: Likewise.
+	 * gcc.target/arm/neon/vld1_lanep64: Likewise.
+	 * gcc.target/arm/neon/vld1p64: Likewise.
+	 * gcc.target/arm/neon/vld2_dupp64: Likewise.
+	 * gcc.target/arm/neon/vld2p64: Likewise.
+	 * gcc.target/arm/neon/vld3_dupp64: Likewise.
+	 * gcc.target/arm/neon/vld3p64: Likewise.
+	 * gcc.target/arm/neon/vld4_dupp64: Likewise.
+	 * gcc.target/arm/neon/vld4p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQf32_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQf32_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_f32: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_p16: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_p8: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_s16: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_s32: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_s64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_s8: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_u16: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_u32: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_u64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp128_u8: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp16_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp16_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_f32: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_p16: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_p8: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_s16: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_s32: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_s64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_s8: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_u16: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_u32: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_u64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp64_u8: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp8_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQp8_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQs16_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQs16_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQs32_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQs32_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQs64_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQs64_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQs8_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQs8_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQu16_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQu16_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQu32_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQu32_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQu64_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQu64_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQu8_p128: Likewise.
+	 * gcc.target/arm/neon/vreinterpretQu8_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretf32_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp16_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_f32: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_p16: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_p8: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_s16: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_s32: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_s64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_s8: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_u16: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_u32: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_u64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp64_u8: Likewise.
+	 * gcc.target/arm/neon/vreinterpretp8_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterprets16_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterprets32_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterprets64_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterprets8_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretu16_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretu32_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretu64_p64: Likewise.
+	 * gcc.target/arm/neon/vreinterpretu8_p64: Likewise.
+	 * gcc.target/arm/neon/vsliQ_np64: Likewise.
+	 * gcc.target/arm/neon/vsli_np64: Likewise.
+	 * gcc.target/arm/neon/vsriQ_np64: Likewise.
+	 * gcc.target/arm/neon/vsri_np64: Likewise.
+	 * gcc.target/arm/neon/vst1Q_lanep64: Likewise.
+	 * gcc.target/arm/neon/vst1Qp64: Likewise.
+	 * gcc.target/arm/neon/vst1_lanep64: Likewise.
+	 * gcc.target/arm/neon/vst1p64: Likewise.
+	 * gcc.target/arm/neon/vst2p64: Likewise.
+	 * gcc.target/arm/neon/vst3p64: Likewise.
+	 * gcc.target/arm/neon/vst4p64: Likewise.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206128
+	2013-12-19  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* lib/target-supports.exp (add_options_for_arm_crc): New procedure.
+	(check_effective_target_arm_crc_ok_nocache): Likewise.
+	(check_effective_target_arm_crc_ok): Likewise.
+	* gcc.target/arm/acle/: New directory.
+	* gcc.target/arm/acle/acle.exp: New.
+	* gcc.target/arm/acle/crc32b.c: New test.
+	* gcc.target/arm/acle/crc32h.c: Likewise.
+	* gcc.target/arm/acle/crc32w.c: Likewise.
+	* gcc.target/arm/acle/crc32d.c: Likewise.
+	* gcc.target/arm/acle/crc32cb.c: Likewise.
+	* gcc.target/arm/acle/crc32ch.c: Likewise.
+	* gcc.target/arm/acle/crc32cw.c: Likewise.
+	* gcc.target/arm/acle/crc32cd.c: Likewise.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206120
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* gcc.target/aarch64/pmull_1.c: New.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206119
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* gcc.target/aarch64/sha256_1.c: New.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206118
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* gcc.target/aarch64/sha1_1.c: New.
+
+2014-02-10  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r206117
+	2013-12-19  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* gcc.target/aarch64/aes_1.c: New.
+
+2014-02-01  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r203057.
+	2013-10-01  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	PR tree-optimization/58556
+	* gcc.dg/tree-ssa/gen-vect-26.c: Use dynamic vector cost model.
+	* gcc.dg/tree-ssa/gen-vect-28.c: Likewise.
+
+2014-01-21  Zhenqiang Chen <zhenqiang.chen@linaro.org>
+
+	Backport from trunk r205509 and r200103
+	2013-11-29  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* gcc.target/arm/lp1243022.c: Skip target arm-neon.
+
+	Backport mainline r200103
+	2013-06-15  Jeff Law  <law@redhat.com>
+
+	* gcc.dg/tree-ssa/coalesce-1.c: New test.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-12-06  Michael Collison  <michael.collison@linaro.org>
+
+	Backport from trunk r202872.
+	2013-09-24  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* lib/target-supports.exp (check_effective_target_arm_cond_exec):
+	New Procedure
+	* gcc.target/arm/minmax_minus.c: Check for cond_exec target.
+
+2013-12-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r203327.
+	2013-10-09  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	* gcc.dg/tree-ssa/phi-opt-11.c: New test.
+
+2013-12-06  Charles Baylis  <charles.baylis@linaro.org>
+
+	Backport from trunk r203799.
+	2013-10-17  Charles Bayis  <charles.baylis@linaro.org>
+
+	* gcc.dg/builtin-apply2.c: Skip test on arm hardfloat ABI
+	targets.
+	* gcc.dg/tls/pr42894.c: Remove dg-options for arm*-*-* targets.
+	* gcc.target/arm/thumb-ltu.c: Remove dg-skip-if and require
+	effective target arm_thumb1_ok.
+	* lib/target-supports.exp
+	(check_effective_target_arm_fp16_ok_nocache): Don't force
+	-mfloat-abi=soft when building for hardfloat target.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-11-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Revert backport from trunk r197526.
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* gcc.target/arm/negdi-1.c: New test.
+	* gcc.target/arm/negdi-2.c: Likewise.
+	* gcc.target/arm/negdi-3.c: Likewise.
+	* gcc.target/arm/negdi-4.c: Likewise.
+
+2013-11-05  Zhenqiang Chen <zhenqiang.chen@linaro.org>
+
+	Backport from trunk r204247.
+	2013-10-31  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	 * gcc.target/arm/lp1243022.c: New test.
+
+2013-11-04  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	Backport from trunk r204336
+	2013-11-03  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	* gcc.target/arm/neon-vcond-gt.c: Scan for vbsl or vbit or vbif.
+	* gcc.target/arm/neon-vcond-ltgt.c: Scan for vbsl or vbit or vbif.
+	* gcc.target/arm/neon-vcond-unordered.c: Scan for vbsl or vbit or
+	vbif.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-10-09  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198526,200595,200597.
+	2013-05-02  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/bics_1.c: New test.
+	* gcc.target/aarch64/bics_2.c: Likewise.
+
+	2013-07-02  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/bfxil_1.c: New test.
+	* gcc.target/aarch64/bfxil_2.c: Likewise.
+
+	2013-07-02  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/config/aarch64/insv_1.c: Update to show it doesn't work
+	on big endian.
+	* gcc.target/config/aarch64/insv_2.c: New test for big endian.
+	* lib/target-supports.exp: Define aarch64_little_endian.
+
+2013-10-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r202400.
+	2013-09-09  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* gcc.target/aarch64/cmn-neg.c: New test.
+
+2013-10-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r202164.
+	2013-09-02  Bin Cheng  <bin.cheng@arm.com>
+
+	* gcc.target/arm/ivopts-orig_biv-inc.c: New testcase.
+
+2013-10-01  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	Backport from trunk r203059,203116.
+	2013-10-01  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	PR Target/58578
+	* gcc.target/arm/pr58578.c: New test.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-09-06  Venkataramanan Kumar  <venkataramanan.kumar@linaro.org>
+
+	Backport from trunk r201411.
+	2013-08-01  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* gcc.target/arm/pr46972-2.c: New test.
+
+2013-09-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r201267.
+	2013-07-26  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* gcc.target/arm/minmax_minus.c: Scan for absence of mov.
+
+2013-09-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199527,199814,201435.
+	2013-05-31  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	PR target/56315
+	* gcc.target/arm/iordi3-opt.c: New test.
+
+	2013-06-07  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	PR target/56315
+	* gcc.target/arm/xordi3-opt.c: New test.
+
+	2013-08-02  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* gcc.target/arm/neon-for-64bits-2.c: Delete.
+
+2013-09-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201730,201731.
+
+	2013-08-14  Janis Johnson  <janisjo@codesourcery.com>
+
+	* gcc.target/arm/atomic-comp-swap-release-acquire.c: Move dg-do
+	to be the first test directive.
+	* gcc.target/arm/atomic-op-acq_rel.c: Likewise.
+	* gcc.target/arm/atomic-op-acquire.c: Likewise.
+	* gcc.target/arm/atomic-op-char.c: Likewise.
+	* gcc.target/arm/atomic-op-consume.c: Likewise.
+	* gcc.target/arm/atomic-op-int.c: Likewise.
+	* gcc.target/arm/atomic-op-relaxed.c: Likewise.
+	* gcc.target/arm/atomic-op-release.c: Likewise.
+	* gcc.target/arm/atomic-op-seq_cst.c: Likewise.
+	* gcc.target/arm/atomic-op-short.c: Likewise.
+
+	2013-08-14  Janis Johnson  <janisjo@codesourcery.com>
+
+	* gcc.target/arm/pr19599.c: Skip for -mthumb.
+
+2013-09-03  Venkataramanan Kumar  <venkataramanan.kumar@linaro.org>
+
+	Backport from trunk r201624.
+	2013-08-09  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/scalar_intrinsics.c: Update expected
+	output of vdup intrinsics
+
+2013-08-26  Kugan Vivekanandarajah  <kuganv@linaro.org>
+
+	Backport from trunk r201636.
+	2013-08-09  Yufeng Zhang  <yufeng.zhang@arm.com>
+
+	* gcc.dg/lower-subreg-1.c: Skip aarch64*-*-*.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-08-07  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199720
+	2013-06-06  Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* gcc.dg/vect/no-section-anchors-vect-68.c:
+	Add dg-skip-if aarch64_tiny.
+
+2013-08-07  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r201237.
+	2013-07-25  Terry Guo  <terry.guo@arm.com>
+
+	* gcc.target/arm/thumb1-Os-mult.c: New test case.
+
+2013-08-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r200596,201067,201083.
+	2013-07-02  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/abs_1.c: New test.
+
+	2013-07-19  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/scalar_intrinsics.c (test_vabs_s64): Added
+	new testcase.
+
+	2013-07-20  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/vabs_intrinsic_1.c: New file.
+
+2013-08-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198864.
+	2013-05-07  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/ands_1.c: New test.
+	* gcc.target/aarch64/ands_2.c: Likewise
+
+2013-08-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199439,199533,201326.
+
+	2013-05-30  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* gcc.dg/shrink-wrap-alloca.c: New added.
+	* gcc.dg/shrink-wrap-pretend.c: New added.
+	* gcc.dg/shrink-wrap-sibcall.c: New added.
+
+	2013-05-31  Rainer Orth  <ro@CeBiTec.Uni-Bielefeld.DE>
+
+	* gcc.dg/shrink-wrap-alloca.c: Use __builtin_alloca.
+
+	2013-07-30  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* gcc.target/arm/pr57637.c: New testcase.
+
+2013-08-06  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198928,198973,199203,201240,201241.
+	2013-05-15  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	* gcc.target/arm/pr40887.c: Adjust testcase.
+	* gcc.target/arm/pr19599.c: New test.
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200922.
+	2013-07-12  Tejas Belagod  <tejas.belagod@arm.com>
+
+	* gcc.target/aarch64/vect-movi.c: New.
+
+2013-08-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200720.
+	2013-07-05  Marcus Shawcroft  <marcus.shawcroft@arm.com>
+
+	* gcc.dg/pr57518.c: Adjust scan-rtl-dump-not pattern.
+
+2013-07-21  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r200204.
+	2013-06-19  Yufeng Zhang  <yufeng.zhang@arm.com>
+
+	* gcc.dg/torture/stackalign/builtin-apply-2.c: set
+	STACK_ARGUMENTS_SIZE with 0 if __aarch64__ is defined.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-07-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Revert backport from trunk r198928.
+	2013-05-15  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	* gcc.target/arm/pr40887.c: Adjust testcase.
+	* gcc.target/arm/pr19599.c: New test.
+
+2013-07-03  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Revert backport from trunk 199439, 199533
+	2013-05-31  Rainer Orth  <ro@CeBiTec.Uni-Bielefeld.DE>
+
+	* gcc.dg/shrink-wrap-alloca.c: Use __builtin_alloca.
+
+	2013-05-30  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* gcc.dg/shrink-wrap-alloca.c: New added.
+	* gcc.dg/shrink-wrap-pretend.c: New added.
+	* gcc.dg/shrink-wrap-sibcall.c: New added.
+
+2013-07-02  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 200096
+
+	2013-06-14  Vidya Praveen <vidyapraveen@arm.com>
+
+	* gcc.target/aarch64/vect_smlal_1.c: New file.
+
+2013-07-02  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 200019
+	2013-06-12  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	* gcc.target/arm/unaligned-memcpy-4.c (src, dst): Initialize
+	to ensure alignment.
+	* gcc.target/arm/unaligned-memcpy-3.c (src): Likewise.
+
+2013-06-20  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 200152
+	2013-06-17  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* gcc.target/aarch64/scalar_intrinsics.c: Update.
+
+2013-06-20  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 200148
+	2013-06-17  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* gcc.target/arm/unaligned-memcpy-2.c (dest): Initialize to
+	ensure alignment.
+
+2013-06-20  Rob Savoye  <rob.savoye@linaro.org>
+
+	Backport from trunk 199533
+	2013-05-31  Rainer Orth  <ro@CeBiTec.Uni-Bielefeld.DE>
+
+	* gcc.dg/shrink-wrap-alloca.c: Use __builtin_alloca.
+
+2013-06-20  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198683.
+	2013-05-07  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* lib/target-supports.exp (check_effective_target_hw): New
+	function.
+	* c-c++-common/asan/clone-test-1.c: Call
+	check_effective_target_hw.
+	* c-c++-common/asan/rlimit-mmap-test-1.c: Likewise.
+	* c-c++-common/asan/heap-overflow-1.c: Update regexps to accept
+	possible decorations.
+	* c-c++-common/asan/null-deref-1.c: Likewise.
+	* c-c++-common/asan/stack-overflow-1.c: Likewise.
+	* c-c++-common/asan/strncpy-overflow-1.c: Likewise.
+	* c-c++-common/asan/use-after-free-1.c: Likewise.
+	* g++.dg/asan/deep-thread-stack-1.C: Likewise.
+	* g++.dg/asan/large-func-test-1.C: Likewise.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-06-06  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	Backport from mainline r199439.
+	2013-05-30  Zhenqiang Chen  <zhenqiang.chen@linaro.org>
+
+	* gcc.dg/shrink-wrap-alloca.c: New added.
+	* gcc.dg/shrink-wrap-pretend.c: New added.
+	* gcc.dg/shrink-wrap-sibcall.c: New added.
+
+2013-06-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199658.
+	2013-06-04  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/movi_1.c: New test.
+
+2013-06-04  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r199261.
+	2013-05-23  Christian Bruel  <christian.bruel@st.com>
+
+	PR debug/57351
+	* gcc.dg/debug/pr57351.c: New test
+
+2013-06-03  Christophe Lyon  <christophe.lyon@linaro.org>
+	Backport from trunk r198890,199254,199294,199454.
+
+	2013-05-30  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/insv_1.c: New test.
+
+	2013-05-24  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/scalar_intrinsics.c
+	(force_simd): Use a valid instruction.
+	(test_vdupd_lane_s64): Pass a valid lane argument.
+	(test_vdupd_lane_u64): Likewise.
+
+	2013-05-23  Vidya Praveen <vidyapraveen@arm.com>
+
+	* gcc.target/aarch64/vect-clz.c: New file.
+
+	2013-05-14  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/vect-fcm.x: Add cases testing
+	FLOAT cmp FLOAT ? INT : INT.
+	* gcc.target/aarch64/vect-fcm-eq-d.c: Define IMODE.
+	* gcc.target/aarch64/vect-fcm-eq-f.c: Likewise.
+	* gcc.target/aarch64/vect-fcm-ge-d.c: Likewise.
+	* gcc.target/aarch64/vect-fcm-ge-f.c: Likewise.
+	* gcc.target/aarch64/vect-fcm-gt-d.c: Likewise.
+	* gcc.target/aarch64/vect-fcm-gt-f.c: Likewise.
+
+2013-05-29  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198928.
+	2013-05-15  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>
+
+	PR target/19599
+	* gcc.target/arm/pr40887.c: Adjust testcase.
+	* gcc.target/arm/pr19599.c: New test.
+
+2013-05-28  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198680.
+	2013-05-07  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* gcc.target/aarch64/scalar_intrinsics.c: Update.
+
+2013-05-28  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198499-198500.
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+	* gcc.target/aarch64/vect-vaddv.c: New.
+
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/vect-vmaxv.c: New.
+	* gcc.target/aarch64/vect-vfmaxv.c: Likewise.
+
+2013-05-23  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	Backport from trunk r198970.
+	2013-05-16  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* gcc.target/arm/unaligned-memcpy-2.c: Adjust expected output.
+	* gcc.target/arm/unaligned-memcpy-3.c: Likewise.
+	* gcc.target/arm/unaligned-memcpy-4.c: Likewise.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198574-198575.
+	2013-05-03  Vidya Praveen <vidyapraveen@arm.com>
+
+	* gcc.target/aarch64/fabd.c: New file.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198490-198496.
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/scalar-vca.c: New.
+	* gcc.target/aarch64/vect-vca.c: Likewise.
+
+	2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/scalar_intrinsics.c (force_simd): New.
+	(test_vceqd_s64): Force arguments to SIMD registers.
+	(test_vceqzd_s64): Likewise.
+	(test_vcged_s64): Likewise.
+	(test_vcled_s64): Likewise.
+	(test_vcgezd_s64): Likewise.
+	(test_vcged_u64): Likewise.
+	(test_vcgtd_s64): Likewise.
+	(test_vcltd_s64): Likewise.
+	(test_vcgtzd_s64): Likewise.
+	(test_vcgtd_u64): Likewise.
+	(test_vclezd_s64): Likewise.
+	(test_vcltzd_s64): Likewise.
+	(test_vtst_s64): Likewise.
+	(test_vtst_u64): Likewise.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198191.
+	2013-04-23  Sofiane Naci  <sofiane.naci@arm.com>
+
+	* gcc.target/aarch64/scalar-mov.c: New testcase.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197838.
+	2013-04-11   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* gcc.target/aarch64/negs.c: New.
+
+2013-05-02  Matthew Gretton-Dann   <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198019.
+	2013-04-16   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* gcc.target/aarch64/adds1.c: New.
+	* gcc.target/aarch64/adds2.c: New.
+	* gcc.target/aarch64/subs1.c: New.
+	* gcc.target/aarch64/subs2.c: New.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198394,198396-198400,198402-198404,198406.
+	2013-04-29  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* lib/target-supports.exp (vect_uintfloat_cvt): Enable for AArch64.
+
+	2013-04-29  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/vect-vcvt.c: New.
+
+	2013-04-29  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/vect-vrnd.c: New.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198302-198306,198316.
+	2013-04-25  James Greenhalgh  <james.greenhalgh@arm.com>
+	    Tejas Belagod  <tejas.belagod@arm.com>
+
+	* gcc.target/aarch64/vaddv-intrinsic.c: New.
+	* gcc.target/aarch64/vaddv-intrinsic-compile.c: Likewise.
+	* gcc.target/aarch64/vaddv-intrinsic.x: Likewise.
+
+	2013-04-25  Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* gcc.target/aarch64/cmp.c: New.
+
+	2013-04-25  Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* gcc.target/aarch64/ngc.c: New.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198298.
+	2013-04-25  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* lib/target-supports.exp
+	(check_effective_target_arm_neon_fp16_ok_nocache): New procedure.
+	(check_effective_target_arm_neon_fp16_ok): Likewise.
+	(add_options_for_arm_neon_fp16): Likewise.
+	* gcc.target/arm/neon/vcvtf16_f32.c: New test. Generated.
+	* gcc.target/arm/neon/vcvtf32_f16.c: Likewise.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198136-198137,198142,198176
+	2013-04-22  James Greenhalgh  <james.greenhalgh@arm.com>
+
+	* gcc.target/aarch64/vrecps.c: New.
+	* gcc.target/aarch64/vrecpx.c: Likewise.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r198020.
+	2013-04-16   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* gcc.target/aarch64/adds3.c: New.
+	* gcc.target/aarch64/subs3.c: New.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197965.
+	2013-04-15  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* gcc.target/arm/anddi3-opt.c: New test.
+	* gcc.target/arm/anddi3-opt2.c: Likewise.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197642.
+	2013-04-09  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* gcc.target/arm/minmax_minus.c: New test.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197530,197921.
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* gcc.target/arm/peep-ldrd-1.c: New test.
+	* gcc.target/arm/peep-strd-1.c: Likewise.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197523.
+	2013-04-05  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* lib/target-supports.exp (add_options_for_arm_v8_neon):
+	Add -march=armv8-a when we use v8 NEON.
+	(check_effective_target_vect_call_btruncf): Remove arm-*-*-*.
+	(check_effective_target_vect_call_ceilf): Likewise.
+	(check_effective_target_vect_call_floorf): Likewise.
+	(check_effective_target_vect_call_roundf): Likewise.
+	(check_vect_support_and_set_flags): Remove check for arm_v8_neon.
+	* gcc.target/arm/vect-rounding-btruncf.c: New testcase.
+	* gcc.target/arm/vect-rounding-ceilf.c: Likewise.
+	* gcc.target/arm/vect-rounding-floorf.c: Likewise.
+	* gcc.target/arm/vect-rounding-roundf.c: Likewise.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197518-197522,197516-197528.
+	2013-04-05  Greta Yorsh  <Greta.Yorsh@arm.com>
+
+	* gcc.target/arm/negdi-1.c: New test.
+	* gcc.target/arm/negdi-2.c: Likewise.
+	* gcc.target/arm/negdi-3.c: Likewise.
+	* gcc.target/arm/negdi-4.c: Likewise.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197489-197491.
+	2013-04-04  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>
+
+	* lib/target-supports.exp (check_effective_target_arm_v8_neon_hw):
+	New procedure.
+	(check_effective_target_arm_v8_neon_ok_nocache):
+	Likewise.
+	(check_effective_target_arm_v8_neon_ok): Change to use
+	check_effective_target_arm_v8_neon_ok_nocache.
+	(add_options_for_arm_v8_neon): Use et_arm_v8_neon_flags to set ARMv8
+	NEON flags.
+	(check_effective_target_vect_call_btruncf):
+	Enable for arm and ARMv8 NEON.
+	(check_effective_target_vect_call_ceilf): Likewise.
+	(check_effective_target_vect_call_floorf): Likewise.
+	(check_effective_target_vect_call_roundf): Likewise.
+	(check_vect_support_and_set_flags): Handle ARMv8 NEON effective
+	target.
+
+2013-05-02  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r196795-196797,196957.
+	2013-03-19  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/sbc.c: New test.
+
+	2013-03-19  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/ror.c: New test.
+
+	2013-03-19  Ian Bolton  <ian.bolton@arm.com>
+
+	* gcc.target/aarch64/extr.c: New test.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197052.
+	2013-03-25  Kyrylo Tkachov  <kyrylo.tkachov at arm.com>
+
+	* gcc.target/arm/vseleqdf.c: New test.
+	* gcc.target/arm/vseleqsf.c: Likewise.
+	* gcc.target/arm/vselgedf.c: Likewise.
+	* gcc.target/arm/vselgesf.c: Likewise.
+	* gcc.target/arm/vselgtdf.c: Likewise.
+	* gcc.target/arm/vselgtsf.c: Likewise.
+	* gcc.target/arm/vselledf.c: Likewise.
+	* gcc.target/arm/vsellesf.c: Likewise.
+	* gcc.target/arm/vselltdf.c: Likewise.
+	* gcc.target/arm/vselltsf.c: Likewise.
+	* gcc.target/arm/vselnedf.c: Likewise.
+	* gcc.target/arm/vselnesf.c: Likewise.
+	* gcc.target/arm/vselvcdf.c: Likewise.
+	* gcc.target/arm/vselvcsf.c: Likewise.
+	* gcc.target/arm/vselvsdf.c: Likewise.
+	* gcc.target/arm/vselvssf.c: Likewise.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r197051.
+	2013-03-25  Kyrylo Tkachov  <kyrylo.tkachov at arm.com>
+
+	* gcc.target/aarch64/atomic-comp-swap-release-acquire.c: Move test
+	body from here...
+	* gcc.target/aarch64/atomic-comp-swap-release-acquire.x: ... to here.
+	* gcc.target/aarch64/atomic-op-acq_rel.c: Move test body from here...
+	* gcc.target/aarch64/atomic-op-acq_rel.x: ... to here.
+	* gcc.target/aarch64/atomic-op-acquire.c: Move test body from here...
+	* gcc.target/aarch64/atomic-op-acquire.x: ... to here.
+	* gcc.target/aarch64/atomic-op-char.c: Move test body from here...
+	* gcc.target/aarch64/atomic-op-char.x: ... to here.
+	* gcc.target/aarch64/atomic-op-consume.c: Move test body from here...
+	* gcc.target/aarch64/atomic-op-consume.x: ... to here.
+	* gcc.target/aarch64/atomic-op-int.c: Move test body from here...
+	* gcc.target/aarch64/atomic-op-int.x: ... to here.
+	* gcc.target/aarch64/atomic-op-relaxed.c: Move test body from here...
+	* gcc.target/aarch64/atomic-op-relaxed.x: ... to here.
+	* gcc.target/aarch64/atomic-op-release.c: Move test body from here...
+	* gcc.target/aarch64/atomic-op-release.x: ... to here.
+	* gcc.target/aarch64/atomic-op-seq_cst.c: Move test body from here...
+	* gcc.target/aarch64/atomic-op-seq_cst.x: ... to here.
+	* gcc.target/aarch64/atomic-op-short.c: Move test body from here...
+	* gcc.target/aarch64/atomic-op-short.x: ... to here.
+	* gcc.target/arm/atomic-comp-swap-release-acquire.c: New test.
+	* gcc.target/arm/atomic-op-acq_rel.c: Likewise.
+	* gcc.target/arm/atomic-op-acquire.c: Likewise.
+	* gcc.target/arm/atomic-op-char.c: Likewise.
+	* gcc.target/arm/atomic-op-consume.c: Likewise.
+	* gcc.target/arm/atomic-op-int.c: Likewise.
+	* gcc.target/arm/atomic-op-relaxed.c: Likewise.
+	* gcc.target/arm/atomic-op-release.c: Likewise.
+	* gcc.target/arm/atomic-op-seq_cst.c: Likewise.
+	* gcc.target/arm/atomic-op-short.c: Likewise.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r196876.
+	2013-03-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	* gcc.target/arm/neon-for-64bits-1.c: New tests.
+	* gcc.target/arm/neon-for-64bits-2.c: Likewise.
+
+2013-04-08  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	Backport from trunk r196858.
+	2013-03-21   Naveen H.S  <Naveen.Hurugalawadi@caviumnetworks.com>
+
+	* gcc.target/aarch64/vect.c: Test and result vector added
+	for sabd and saba instructions.
+	* gcc.target/aarch64/vect-compile.c: Check for sabd and saba
+	instructions in assembly.
+	* gcc.target/aarch64/vect.x: Add sabd and saba test functions.
+	* gcc.target/aarch64/vect-fp.c: Test and result vector added
+	for fabd instruction.
+	* gcc.target/aarch64/vect-fp-compile.c: Check for fabd
+	instruction in assembly.
+	* gcc.target/aarch64/vect-fp.x: Add fabd test function.
Index: b/src/gcc/testsuite/gcc.dg/shrink-wrap-alloca.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.dg/shrink-wrap-alloca.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -g" } */
+
+int *p;
+
+void
+test (int a)
+{
+  if (a > 0)
+    p = __builtin_alloca (4);
+}
Index: b/src/gcc/testsuite/gcc.dg/shrink-wrap-pretend.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.dg/shrink-wrap-pretend.c
@@ -0,0 +1,36 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -g" } */
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <stdarg.h>
+
+#define DEBUG_BUFFER_SIZE 80
+int unifi_debug = 5;
+
+void
+unifi_trace (void* ospriv, int level, const char *fmt, ...)
+{
+   static char s[DEBUG_BUFFER_SIZE];
+   va_list args;
+   unsigned int len;
+
+   if (!ospriv)
+     return;
+
+   if (unifi_debug >= level)
+     {
+       va_start (args, fmt);
+       len = vsnprintf (&(s)[0], (DEBUG_BUFFER_SIZE), fmt, args);
+       va_end (args);
+
+       if (len >= DEBUG_BUFFER_SIZE)
+	 {
+	   (s)[DEBUG_BUFFER_SIZE - 2] = '\n';
+	   (s)[DEBUG_BUFFER_SIZE - 1] = 0;
+	 }
+
+       printf ("%s", s);
+     }
+}
+
Index: b/src/gcc/testsuite/gcc.dg/debug/pr57351.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.dg/debug/pr57351.c
@@ -0,0 +1,54 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_neon }  */
+/* { dg-options "-std=c99 -Os -g -march=armv7-a" } */
+/* { dg-add-options arm_neon } */
+
+typedef unsigned int size_t;
+typedef int ptrdiff_t;
+typedef signed char int8_t ;
+typedef signed long long int64_t;
+typedef int8_t GFC_INTEGER_1;
+typedef GFC_INTEGER_1 GFC_LOGICAL_1;
+typedef int64_t GFC_INTEGER_8;
+typedef GFC_INTEGER_8 GFC_LOGICAL_8;
+typedef ptrdiff_t index_type;
+typedef struct descriptor_dimension
+{
+  index_type lower_bound;
+  index_type _ubound;
+}
+descriptor_dimension;
+typedef struct { GFC_LOGICAL_1 *base_addr; size_t offset; index_type dtype; descriptor_dimension dim[7];} gfc_array_l1;
+typedef struct { GFC_LOGICAL_8 *base_addr; size_t offset; index_type dtype; descriptor_dimension dim[7];} gfc_array_l8;
+void
+all_l8 (gfc_array_l8 * const restrict retarray,
+ gfc_array_l1 * const restrict array,
+ const index_type * const restrict pdim)
+{
+  GFC_LOGICAL_8 * restrict dest;
+  index_type n;
+  index_type len;
+  index_type delta;
+  index_type dim;
+  dim = (*pdim) - 1;
+  len = ((array)->dim[dim]._ubound + 1 - (array)->dim[dim].lower_bound);
+  for (n = 0; n < dim; n++)
+    {
+      const GFC_LOGICAL_1 * restrict src;
+      GFC_LOGICAL_8 result;
+      {
+  result = 1;
+   {
+     for (n = 0; n < len; n++, src += delta)
+       {
+  if (! *src)
+    {
+      result = 0;
+      break;
+    }
+          }
+     *dest = result;
+   }
+      }
+    }
+}
Index: b/src/gcc/testsuite/gcc.dg/lower-subreg-1.c
===================================================================
--- a/src/gcc/testsuite/gcc.dg/lower-subreg-1.c
+++ b/src/gcc/testsuite/gcc.dg/lower-subreg-1.c
@@ -1,4 +1,4 @@
-/* { dg-do compile { target { ! { mips64 || { arm*-*-* ia64-*-* sparc*-*-* spu-*-* tilegx-*-* } } } } } */
+/* { dg-do compile { target { ! { mips64 || { aarch64*-*-* arm*-*-* ia64-*-* sparc*-*-* spu-*-* tilegx-*-* } } } } } */
 /* { dg-options "-O -fdump-rtl-subreg1" } */
 /* { dg-skip-if "" { { i?86-*-* x86_64-*-* } && x32 } { "*" } { "" } } */
 /* { dg-require-effective-target ilp32 } */
Index: b/src/gcc/testsuite/gcc.dg/shrink-wrap-sibcall.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.dg/shrink-wrap-sibcall.c
@@ -0,0 +1,26 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -g" } */
+
+unsigned char a, b, d, f, g;
+
+int test (void);
+
+int
+baz (int c)
+{
+  if (c == 0) return test ();
+  if (b & 1)
+    {
+      g = 0;
+      int e = (a & 0x0f) - (g & 0x0f);
+
+      if (!a)  b |= 0x80;
+      a = e + test ();
+     f = g/5 + a*3879 + b *2985;
+    }
+   else
+   {
+     f = g + a*39879 + b *25;
+   }
+  return test ();
+}
Index: b/src/gcc/testsuite/gcc.dg/torture/stackalign/builtin-apply-2.c
===================================================================
--- a/src/gcc/testsuite/gcc.dg/torture/stackalign/builtin-apply-2.c
+++ b/src/gcc/testsuite/gcc.dg/torture/stackalign/builtin-apply-2.c
@@ -16,7 +16,7 @@
    E, F and G are passed on stack.  So the size of the stack argument
    data is 20.  */
 #define STACK_ARGUMENTS_SIZE  20
-#elif defined __MMIX__
+#elif defined __aarch64__ || defined __MMIX__
 /* No parameters on stack for bar.  */
 #define STACK_ARGUMENTS_SIZE 0
 #else
Index: b/src/gcc/testsuite/gcc.dg/tree-ssa/coalesce-1.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.dg/tree-ssa/coalesce-1.c
@@ -0,0 +1,195 @@
+/* { dg-do compile } */
+
+/* { dg-options "-O2 -fdump-rtl-expand-details" } */
+
+typedef long unsigned int size_t;
+union tree_node;
+typedef union tree_node *tree;
+union gimple_statement_d;
+typedef union gimple_statement_d *gimple;
+typedef const union tree_node *const_tree;
+typedef const union gimple_statement_d *const_gimple;
+struct gimple_seq_d;
+typedef struct gimple_seq_d *gimple_seq;
+struct edge_def;
+typedef struct edge_def *edge;
+struct basic_block_def;
+typedef struct basic_block_def *basic_block;
+typedef const struct basic_block_def *const_basic_block;
+struct tree_exp
+{
+  tree operands[1];
+};
+typedef struct ssa_use_operand_d
+{
+  tree *use;
+} ssa_use_operand_t;
+struct phi_arg_d
+{
+  struct ssa_use_operand_d imm_use;
+};
+union tree_node
+{
+  struct tree_exp exp;
+};
+struct function
+{
+};
+extern struct function *cfun;
+struct edge_def
+{
+  unsigned int dest_idx;
+};
+static __inline__ void
+VEC_edge_must_be_pointer_type (void)
+{
+  (void) ((edge) 1 == (void *) 1);
+} typedef struct VEC_edge_base
+
+{
+  unsigned num;
+  unsigned alloc;
+  edge vec[1];
+} VEC_edge_base;
+typedef struct VEC_edge_none
+{
+  VEC_edge_base base;
+} VEC_edge_none;
+
+static __inline__ edge
+VEC_edge_base_index (const VEC_edge_base * vec_, unsigned ix_,
+		     const char *file_, unsigned line_, const char *function_)
+{
+  return vec_->vec[ix_];
+}
+
+typedef struct VEC_edge_gc
+{
+  VEC_edge_base base;
+} VEC_edge_gc;
+struct basic_block_def
+{
+  VEC_edge_gc *succs;
+};
+static __inline__ edge
+single_succ_edge (const_basic_block bb)
+{
+  return (VEC_edge_base_index
+	  ((((bb)->succs) ? &((bb)->succs)->base : 0), (0),
+	   "/home/gcc/virgin-gcc/gcc/basic-block.h", 563, __FUNCTION__));
+}
+
+edge find_edge (basic_block, basic_block);
+typedef tree *def_operand_p;
+typedef ssa_use_operand_t *use_operand_p;
+struct gimple_seq_node_d;
+typedef struct gimple_seq_node_d *gimple_seq_node;
+struct gimple_seq_node_d
+{
+  gimple stmt;
+};
+typedef struct
+{
+  gimple_seq_node ptr;
+  gimple_seq seq;
+  basic_block bb;
+} gimple_stmt_iterator;
+struct gimple_statement_phi
+{
+  struct phi_arg_d args[1];
+};
+union gimple_statement_d
+{
+  struct gimple_statement_phi gimple_phi;
+};
+extern size_t const gimple_ops_offset_[];
+static __inline__ tree *
+gimple_ops (gimple gs)
+{
+  size_t off;
+  off = gimple_ops_offset_[gimple_statement_structure (gs)];
+  return (tree *) ((char *) gs + off);
+}
+
+static __inline__ tree
+gimple_op (const_gimple gs, unsigned i)
+{
+  return gimple_ops ((((union
+			{
+			const union gimple_statement_d * _q;
+			union gimple_statement_d * _nq;}) (((gs))))._nq))[i];
+}
+
+static __inline__ struct phi_arg_d *
+gimple_phi_arg (gimple gs, unsigned index)
+{
+  return &(gs->gimple_phi.args[index]);
+}
+
+static __inline__ tree
+gimple_switch_label (const_gimple gs, unsigned index)
+{
+  return gimple_op (gs, index + 1);
+}
+
+gimple_stmt_iterator gsi_start_phis (basic_block);
+extern basic_block label_to_block_fn (struct function *, tree);
+
+static __inline__ tree
+get_use_from_ptr (use_operand_p use)
+{
+  return *(use->use);
+}
+
+static __inline__ use_operand_p
+gimple_phi_arg_imm_use_ptr (gimple gs, int i)
+{
+  return &gimple_phi_arg (gs, i)->imm_use;
+}
+
+struct switch_conv_info
+{
+  basic_block final_bb;
+  basic_block switch_bb;
+  const char *reason;
+  tree *default_values;
+};
+static struct switch_conv_info info;
+
+static void
+gather_default_values (tree default_case)
+{
+  gimple_stmt_iterator gsi;
+  basic_block bb =
+    (label_to_block_fn ((cfun + 0), default_case->exp.operands[2]));
+  edge e;
+  int i = 0;
+  if (bb == info.final_bb)
+    e = find_edge (info.switch_bb, bb);
+  else
+    e = single_succ_edge (bb);
+  for (gsi = gsi_start_phis (info.final_bb);
+       gsi_gsi_start_phis (info.final_bb); gsi_next (&gsi))
+    {
+      gimple phi = gsi.ptr->stmt;
+      tree val = get_use_from_ptr (gimple_phi_arg_imm_use_ptr
+				   ((((phi))), (((e)->dest_idx))));
+      info.default_values[i++] = val;
+    }
+}
+
+unsigned char
+process_switch (gimple swtch)
+{
+  unsigned int i, branch_num = gimple_switch_num_labels (swtch);
+  tree index_type;
+  info.reason = "switch has no labels\n";
+  gather_default_values (gimple_switch_label (swtch, 0));
+}
+
+/* Verify that out-of-ssa coalescing did its job by verifying there are not
+   any partition copies inserted.  */
+
+/* { dg-final { scan-rtl-dump-not "partition copy" "expand"} } */
+/* { dg-final { cleanup-rtl-dump "expand" } } */
+
Index: b/src/gcc/testsuite/gcc.dg/tree-ssa/gen-vect-26.c
===================================================================
--- a/src/gcc/testsuite/gcc.dg/tree-ssa/gen-vect-26.c
+++ b/src/gcc/testsuite/gcc.dg/tree-ssa/gen-vect-26.c
@@ -1,6 +1,6 @@
 /* { dg-do run { target vect_cmdline_needed } } */
-/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details" } */
-/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -mno-sse" { target { i?86-*-* x86_64-*-* } } } */
+/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -fvect-cost-model=dynamic" } */
+/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -fvect-cost-model=dynamic -mno-sse" { target { i?86-*-* x86_64-*-* } } } */
 
 #include <stdlib.h>
 
Index: b/src/gcc/testsuite/gcc.dg/tree-ssa/phi-opt-11.c
===================================================================
--- /dev/null
+++ b/src/gcc/testsuite/gcc.dg/tree-ssa/phi-opt-11.c
@@ -0,0 +1,25 @@
+/* { dg-do compile } */
+/* { dg-options "-O1 -fdump-tree-optimized" } */
+
+int f(int a, int b, int c)
+{
+  if (a == 0 && b > c)
+   return 0;
+ return a;
+}
+
+int g(int a, int b, int c)
+{
+  if (a == 42 && b > c)
+   return 42;
+ return a;
+}
+
+int h(int a, int b, int c, int d)
+{
+  if (a == d && b > c)
+   return d;
+ return a;
+}
+/* { dg-final { scan-tree-dump-times "if" 0 "optimized"} } */
+/* { dg-final { cleanup-tree-dump "optimized" } } */
Index: b/src/gcc/testsuite/gcc.dg/tree-ssa/gen-vect-28.c
===================================================================
--- a/src/gcc/testsuite/gcc.dg/tree-ssa/gen-vect-28.c
+++ b/src/gcc/testsuite/gcc.dg/tree-ssa/gen-vect-28.c
@@ -1,6 +1,6 @@
 /* { dg-do run { target vect_cmdline_needed } } */
-/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details" } */
-/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -mno-sse" { target { i?86-*-* x86_64-*-* } } } */
+/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -fvect-cost-model=dynamic" } */
+/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -fvect-cost-model=dynamic -mno-sse" { target { i?86-*-* x86_64-*-* } } } */
 
 #include <stdlib.h>
 
Index: b/src/gcc/testsuite/gcc.dg/tls/pr42894.c
===================================================================
--- a/src/gcc/testsuite/gcc.dg/tls/pr42894.c
+++ b/src/gcc/testsuite/gcc.dg/tls/pr42894.c
@@ -1,6 +1,5 @@
 /* PR target/42894 */
 /* { dg-do compile } */
-/* { dg-options "-march=armv5te -mthumb" { target arm*-*-* } } */
 /* { dg-require-effective-target tls } */
 
 extern __thread int t;
Index: b/src/gcc/testsuite/gcc.dg/builtin-apply2.c
===================================================================
--- a/src/gcc/testsuite/gcc.dg/builtin-apply2.c
+++ b/src/gcc/testsuite/gcc.dg/builtin-apply2.c
@@ -1,6 +1,6 @@
 /* { dg-do run } */
 /* { dg-skip-if "Variadic funcs have all args on stack. Normal funcs have args in registers." { "aarch64*-*-* avr-*-* " } { "*" } { "" } } */
-/* { dg-skip-if "Variadic funcs use Base AAPCS.  Normal funcs use VFP variant." { "arm*-*-*" } { "-mfloat-abi=hard" } { "" } } */
+/* { dg-skip-if "Variadic funcs use Base AAPCS.  Normal funcs use VFP variant." { arm*-*-* && arm_hf_eabi } { "*" } { "" } } */
 
 /* PR target/12503 */
 /* Origin: <pierre.nguyen-tuong@asim.lip6.fr> */
Index: b/src/gcc/testsuite/gcc.dg/vect/no-section-anchors-vect-68.c
===================================================================
--- a/src/gcc/testsuite/gcc.dg/vect/no-section-anchors-vect-68.c
+++ b/src/gcc/testsuite/gcc.dg/vect/no-section-anchors-vect-68.c
@@ -1,4 +1,6 @@
-/* { dg-require-effective-target vect_int } */
+/* { dg-require-effective-target vect_int }
+   { dg-skip-if "AArch64 tiny code model does not support programs larger than 1MiB" {aarch64_tiny} {"*"} {""} }
+ */
 
 #include <stdarg.h>
 #include "tree-vect.h"
Index: b/src/gcc/testsuite/g++.dg/asan/large-func-test-1.C
===================================================================
--- a/src/gcc/testsuite/g++.dg/asan/large-func-test-1.C
+++ b/src/gcc/testsuite/g++.dg/asan/large-func-test-1.C
@@ -37,9 +37,9 @@ int main() {
 
 // { dg-output "ERROR: AddressSanitizer:? heap-buffer-overflow on address\[^\n\r]*" }
 // { dg-output "0x\[0-9a-f\]+ at pc 0x\[0-9a-f\]+ bp 0x\[0-9a-f\]+ sp 0x\[0-9a-f\]+\[^\n\r]*(\n|\r\n|\r)" }
-// { dg-output "READ of size 4 at 0x\[0-9a-f\]+ thread T0\[^\n\r]*(\n|\r\n|\r)" }
+// { dg-output "\[^\n\r]*READ of size 4 at 0x\[0-9a-f\]+ thread T0\[^\n\r]*(\n|\r\n|\r)" }
 // { dg-output "    #0 0x\[0-9a-f\]+ (in \[^\n\r]*LargeFunction\[^\n\r]*(large-func-test-1.C:18|\[^\n\r]*:0)|\[(\]).*(\n|\r\n|\r)" }
-// { dg-output "0x\[0-9a-f\]+ is located 44 bytes to the right of 400-byte region.*(\n|\r\n|\r)" }
-// { dg-output "allocated by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" }
+// { dg-output "\[^\n\r]*0x\[0-9a-f\]+ is located 44 bytes to the right of 400-byte region.*(\n|\r\n|\r)" }
+// { dg-output "\[^\n\r]*allocated by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" }
 // { dg-output "    #0( 0x\[0-9a-f\]+ (in _*(interceptor_|)malloc|\[(\])\[^\n\r]*(\n|\r\n|\r)" }
 // { dg-output "    #1|) 0x\[0-9a-f\]+ (in (operator new|_*_Zn\[aw\]\[mj\])|\[(\])\[^\n\r]*(\n|\r\n|\r)" }
Index: b/src/gcc/testsuite/g++.dg/asan/deep-thread-stack-1.C
===================================================================
--- a/src/gcc/testsuite/g++.dg/asan/deep-thread-stack-1.C
+++ b/src/gcc/testsuite/g++.dg/asan/deep-thread-stack-1.C
@@ -45,9 +45,9 @@ int main(int argc, char *argv[]) {
 }
 
 // { dg-output "ERROR: AddressSanitizer: heap-use-after-free.*(\n|\r\n|\r)" }
-// { dg-output "WRITE of size 4 at 0x\[0-9a-f\]+ thread T(\[0-9\]+).*(\n|\r\n|\r)" }
-// { dg-output "freed by thread T(\[0-9\]+) here:.*(\n|\r\n|\r)" }
-// { dg-output "previously allocated by thread T(\[0-9\]+) here:.*(\n|\r\n|\r)" }
+// { dg-output "\[^\n\r]*WRITE of size 4 at 0x\[0-9a-f\]+ thread T(\[0-9\]+).*(\n|\r\n|\r)" }
+// { dg-output "\[^\n\r]*freed by thread T(\[0-9\]+) here:.*(\n|\r\n|\r)" }
+// { dg-output "\[^\n\r]*previously allocated by thread T(\[0-9\]+) here:.*(\n|\r\n|\r)" }
 // { dg-output "Thread T\\2 created by T(\[0-9\]+) here:.*(\n|\r\n|\r)" }
 // { dg-output "Thread T\\8 created by T0 here:.*(\n|\r\n|\r)" }
 // { dg-output "Thread T\\4 created by T(\[0-9\]+) here:.*(\n|\r\n|\r)" }
Index: b/src/gcc/testsuite/c-c++-common/asan/strncpy-overflow-1.c
===================================================================
--- a/src/gcc/testsuite/c-c++-common/asan/strncpy-overflow-1.c
+++ b/src/gcc/testsuite/c-c++-common/asan/strncpy-overflow-1.c
@@ -15,7 +15,7 @@ int main(int argc, char **argv) {
 /* { dg-output "WRITE of size \[0-9\]* at 0x\[0-9a-f\]+ thread T0\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #0 0x\[0-9a-f\]+ (in _*(interceptor_|)strncpy|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #1 0x\[0-9a-f\]+ (in _*main (\[^\n\r]*strncpy-overflow-1.c:11|\[^\n\r]*:0)|\[(\]).*(\n|\r\n|\r)" } */
-/* { dg-output "0x\[0-9a-f\]+ is located 0 bytes to the right of 9-byte region\[^\n\r]*(\n|\r\n|\r)" } */
-/* { dg-output "allocated by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" } */
+/* { dg-output "\[^\n\r]*0x\[0-9a-f\]+ is located 0 bytes to the right of 9-byte region\[^\n\r]*(\n|\r\n|\r)" } */
+/* { dg-output "\[^\n\r]*allocated by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #0 0x\[0-9a-f\]+ (in _*(interceptor_|)malloc|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #1 0x\[0-9a-f\]+ (in _*main (\[^\n\r]*strncpy-overflow-1.c:10|\[^\n\r]*:0)|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
Index: b/src/gcc/testsuite/c-c++-common/asan/rlimit-mmap-test-1.c
===================================================================
--- a/src/gcc/testsuite/c-c++-common/asan/rlimit-mmap-test-1.c
+++ b/src/gcc/testsuite/c-c++-common/asan/rlimit-mmap-test-1.c
@@ -2,6 +2,7 @@
 
 /* { dg-do run { target setrlimit } } */
 /* { dg-skip-if "" { *-*-* } { "*" } { "-O0" } } */
+/* { dg-require-effective-target hw } */
 /* { dg-shouldfail "asan" } */
 
 #include <stdlib.h>
Index: b/src/gcc/testsuite/c-c++-common/asan/stack-overflow-1.c
===================================================================
--- a/src/gcc/testsuite/c-c++-common/asan/stack-overflow-1.c
+++ b/src/gcc/testsuite/c-c++-common/asan/stack-overflow-1.c
@@ -19,4 +19,4 @@ int main() {
 
 /* { dg-output "READ of size 1 at 0x\[0-9a-f\]+ thread T0\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #0 0x\[0-9a-f\]+ (in _*main (\[^\n\r]*stack-overflow-1.c:16|\[^\n\r]*:0)|\[(\]).*(\n|\r\n|\r)" } */
-/* { dg-output "Address 0x\[0-9a-f\]+ is\[^\n\r]*frame <main>" } */
+/* { dg-output "\[^\n\r]*Address 0x\[0-9a-f\]+ is\[^\n\r]*frame <main>" } */
Index: b/src/gcc/testsuite/c-c++-common/asan/use-after-free-1.c
===================================================================
--- a/src/gcc/testsuite/c-c++-common/asan/use-after-free-1.c
+++ b/src/gcc/testsuite/c-c++-common/asan/use-after-free-1.c
@@ -11,12 +11,12 @@ int main() {
 
 /* { dg-output "ERROR: AddressSanitizer:? heap-use-after-free on address\[^\n\r]*" } */
 /* { dg-output "0x\[0-9a-f\]+ at pc 0x\[0-9a-f\]+ bp 0x\[0-9a-f\]+ sp 0x\[0-9a-f\]+\[^\n\r]*(\n|\r\n|\r)" } */
-/* { dg-output "READ of size 1 at 0x\[0-9a-f\]+ thread T0\[^\n\r]*(\n|\r\n|\r)" } */
+/* { dg-output "\[^\n\r]*READ of size 1 at 0x\[0-9a-f\]+ thread T0\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #0 0x\[0-9a-f\]+ (in _*main (\[^\n\r]*use-after-free-1.c:9|\[^\n\r]*:0)|\[(\]).*(\n|\r\n|\r)" } */
-/* { dg-output "0x\[0-9a-f\]+ is located 5 bytes inside of 10-byte region .0x\[0-9a-f\]+,0x\[0-9a-f\]+\[^\n\r]*(\n|\r\n|\r)" } */
-/* { dg-output "freed by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" } */
+/* { dg-output "\[^\n\r]*0x\[0-9a-f\]+ is located 5 bytes inside of 10-byte region .0x\[0-9a-f\]+,0x\[0-9a-f\]+\[^\n\r]*(\n|\r\n|\r)" } */
+/* { dg-output "\[^\n\r]*freed by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #0 0x\[0-9a-f\]+ (in _*(interceptor_|)free|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #1 0x\[0-9a-f\]+ (in _*main (\[^\n\r]*use-after-free-1.c:8|\[^\n\r]*:0)|\[(\]).*(\n|\r\n|\r)" } */
-/* { dg-output "previously allocated by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" } */
+/* { dg-output "\[^\n\r]*previously allocated by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #0 0x\[0-9a-f\]+ (in _*(interceptor_|)malloc|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #1 0x\[0-9a-f\]+ (in _*main (\[^\n\r]*use-after-free-1.c:7|\[^\n\r]*:0)|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
Index: b/src/gcc/testsuite/c-c++-common/asan/clone-test-1.c
===================================================================
--- a/src/gcc/testsuite/c-c++-common/asan/clone-test-1.c
+++ b/src/gcc/testsuite/c-c++-common/asan/clone-test-1.c
@@ -3,6 +3,7 @@
 
 /* { dg-do run { target { *-*-linux* } } } */
 /* { dg-require-effective-target clone } */
+/* { dg-require-effective-target hw } */
 /* { dg-options "-D_GNU_SOURCE" } */
 
 #include <stdio.h>
Index: b/src/gcc/testsuite/c-c++-common/asan/heap-overflow-1.c
===================================================================
--- a/src/gcc/testsuite/c-c++-common/asan/heap-overflow-1.c
+++ b/src/gcc/testsuite/c-c++-common/asan/heap-overflow-1.c
@@ -25,7 +25,7 @@ int main(int argc, char **argv) {
 
 /* { dg-output "READ of size 1 at 0x\[0-9a-f\]+ thread T0.*(\n|\r\n|\r)" } */
 /* { dg-output "    #0 0x\[0-9a-f\]+ (in _*main (\[^\n\r]*heap-overflow-1.c:21|\[^\n\r]*:0)|\[(\]).*(\n|\r\n|\r)" } */
-/* { dg-output "0x\[0-9a-f\]+ is located 0 bytes to the right of 10-byte region\[^\n\r]*(\n|\r\n|\r)" } */
-/* { dg-output "allocated by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" } */
+/* { dg-output "\[^\n\r]*0x\[0-9a-f\]+ is located 0 bytes to the right of 10-byte region\[^\n\r]*(\n|\r\n|\r)" } */
+/* { dg-output "\[^\n\r]*allocated by thread T0 here:\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #0 0x\[0-9a-f\]+ (in _*(interceptor_|)malloc|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #1 0x\[0-9a-f\]+ (in _*main (\[^\n\r]*heap-overflow-1.c:19|\[^\n\r]*:0)|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
Index: b/src/gcc/testsuite/c-c++-common/asan/null-deref-1.c
===================================================================
--- a/src/gcc/testsuite/c-c++-common/asan/null-deref-1.c
+++ b/src/gcc/testsuite/c-c++-common/asan/null-deref-1.c
@@ -18,6 +18,6 @@ int main()
 
 /* { dg-output "ERROR: AddressSanitizer:? SEGV on unknown address\[^\n\r]*" } */
 /* { dg-output "0x\[0-9a-f\]+ \[^\n\r]*pc 0x\[0-9a-f\]+\[^\n\r]*(\n|\r\n|\r)" } */
-/* { dg-output "AddressSanitizer can not provide additional info.*(\n|\r\n|\r)" } */
+/* { dg-output "\[^\n\r]*AddressSanitizer can not provide additional info.*(\n|\r\n|\r)" } */
 /* { dg-output "    #0 0x\[0-9a-f\]+ (in \[^\n\r]*NullDeref\[^\n\r]* (\[^\n\r]*null-deref-1.c:10|\[^\n\r]*:0)|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
 /* { dg-output "    #1 0x\[0-9a-f\]+ (in _*main (\[^\n\r]*null-deref-1.c:15|\[^\n\r]*:0)|\[(\])\[^\n\r]*(\n|\r\n|\r)" } */
Index: b/src/gcc/objcp/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/objcp/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/cp/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/cp/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/tree-ssa-loop-ivopts.c
===================================================================
--- a/src/gcc/tree-ssa-loop-ivopts.c
+++ b/src/gcc/tree-ssa-loop-ivopts.c
@@ -4832,22 +4832,36 @@ set_autoinc_for_original_candidates (str
   for (i = 0; i < n_iv_cands (data); i++)
     {
       struct iv_cand *cand = iv_cand (data, i);
-      struct iv_use *closest = NULL;
+      struct iv_use *closest_before = NULL;
+      struct iv_use *closest_after = NULL;
       if (cand->pos != IP_ORIGINAL)
 	continue;
+
       for (j = 0; j < n_iv_uses (data); j++)
 	{
 	  struct iv_use *use = iv_use (data, j);
 	  unsigned uid = gimple_uid (use->stmt);
-	  if (gimple_bb (use->stmt) != gimple_bb (cand->incremented_at)
-	      || uid > gimple_uid (cand->incremented_at))
+
+	  if (gimple_bb (use->stmt) != gimple_bb (cand->incremented_at))
 	    continue;
-	  if (closest == NULL || uid > gimple_uid (closest->stmt))
-	    closest = use;
+
+	  if (uid < gimple_uid (cand->incremented_at)
+	      && (closest_before == NULL
+		  || uid > gimple_uid (closest_before->stmt)))
+	    closest_before = use;
+
+	  if (uid > gimple_uid (cand->incremented_at)
+	      && (closest_after == NULL
+		  || uid < gimple_uid (closest_after->stmt)))
+	    closest_after = use;
 	}
-      if (closest == NULL || !autoinc_possible_for_pair (data, closest, cand))
-	continue;
-      cand->ainc_use = closest;
+
+      if (closest_before != NULL
+	  && autoinc_possible_for_pair (data, closest_before, cand))
+	cand->ainc_use = closest_before;
+      else if (closest_after != NULL
+	       && autoinc_possible_for_pair (data, closest_after, cand))
+	cand->ainc_use = closest_after;
     }
 }
 
Index: b/src/gcc/rtl.def
===================================================================
--- a/src/gcc/rtl.def
+++ b/src/gcc/rtl.def
@@ -937,8 +937,9 @@ DEF_RTL_EXPR(DEFINE_ASM_ATTRIBUTES, "def
       relational operator.  Operands should have only one alternative.
    1: A C expression giving an additional condition for recognizing
       the generated pattern.
-   2: A template or C code to produce assembler output.  */
-DEF_RTL_EXPR(DEFINE_COND_EXEC, "define_cond_exec", "Ess", RTX_EXTRA)
+   2: A template or C code to produce assembler output.
+   3: A vector of attributes to append to the resulting cond_exec insn.  */
+DEF_RTL_EXPR(DEFINE_COND_EXEC, "define_cond_exec", "EssV", RTX_EXTRA)
 
 /* Definition of an operand predicate.  The difference between
    DEFINE_PREDICATE and DEFINE_SPECIAL_PREDICATE is that genrecog will
Index: b/src/gcc/go/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/go/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/opts.c
===================================================================
--- a/src/gcc/opts.c
+++ b/src/gcc/opts.c
@@ -484,6 +484,7 @@ static const struct default_options defa
     { OPT_LEVELS_2_PLUS, OPT_falign_labels, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_falign_functions, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_ftree_tail_merge, NULL, 1 },
+    { OPT_LEVELS_2_PLUS, OPT_fvect_cost_model_, NULL, VECT_COST_MODEL_CHEAP },
     { OPT_LEVELS_2_PLUS_SPEED_ONLY, OPT_foptimize_strlen, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_fhoist_adjacent_loads, NULL, 1 },
 
@@ -497,7 +498,7 @@ static const struct default_options defa
     { OPT_LEVELS_3_PLUS, OPT_funswitch_loops, NULL, 1 },
     { OPT_LEVELS_3_PLUS, OPT_fgcse_after_reload, NULL, 1 },
     { OPT_LEVELS_3_PLUS, OPT_ftree_vectorize, NULL, 1 },
-    { OPT_LEVELS_3_PLUS, OPT_fvect_cost_model, NULL, 1 },
+    { OPT_LEVELS_3_PLUS, OPT_fvect_cost_model_, NULL, VECT_COST_MODEL_DYNAMIC },
     { OPT_LEVELS_3_PLUS, OPT_fipa_cp_clone, NULL, 1 },
     { OPT_LEVELS_3_PLUS, OPT_ftree_partial_pre, NULL, 1 },
 
@@ -822,6 +823,17 @@ finish_options (struct gcc_options *opts
 	}
     }
 
+  /* Tune vectorization related parametees according to cost model.  */
+  if (opts->x_flag_vect_cost_model == VECT_COST_MODEL_CHEAP)
+    {
+      maybe_set_param_value (PARAM_VECT_MAX_VERSION_FOR_ALIAS_CHECKS,
+            6, opts->x_param_values, opts_set->x_param_values);
+      maybe_set_param_value (PARAM_VECT_MAX_VERSION_FOR_ALIGNMENT_CHECKS,
+            0, opts->x_param_values, opts_set->x_param_values);
+      maybe_set_param_value (PARAM_VECT_MAX_PEELING_FOR_ALIGNMENT,
+            0, opts->x_param_values, opts_set->x_param_values);
+    }
+
   /* Set PARAM_MAX_STORES_TO_SINK to 0 if either vectorization or if-conversion
      is disabled.  */
   if (!opts->x_flag_tree_vectorize || !opts->x_flag_tree_loop_if_convert)
@@ -1592,7 +1604,7 @@ common_handle_option (struct gcc_options
       if (!opts_set->x_flag_tree_vectorize)
 	opts->x_flag_tree_vectorize = value;
       if (!opts_set->x_flag_vect_cost_model)
-	opts->x_flag_vect_cost_model = value;
+	opts->x_flag_vect_cost_model = VECT_COST_MODEL_DYNAMIC;
       if (!opts_set->x_flag_tree_loop_distribute_patterns)
 	opts->x_flag_tree_loop_distribute_patterns = value;
       break;
Index: b/src/gcc/ada/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/ada/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/common/config/aarch64/aarch64-common.c
===================================================================
--- a/src/gcc/common/config/aarch64/aarch64-common.c
+++ b/src/gcc/common/config/aarch64/aarch64-common.c
@@ -44,6 +44,8 @@ static const struct default_options aarc
   {
     /* Enable section anchors by default at -O1 or higher.  */
     { OPT_LEVELS_1_PLUS, OPT_fsection_anchors, NULL, 1 },
+    /* Enable redundant extension instructions removal at -O2 and higher.  */
+    { OPT_LEVELS_2_PLUS, OPT_free, NULL, 1 },
     { OPT_LEVELS_NONE, 0, NULL, 0 }
   };
 
Index: b/src/gcc/common/config/i386/i386-common.c
===================================================================
--- a/src/gcc/common/config/i386/i386-common.c
+++ b/src/gcc/common/config/i386/i386-common.c
@@ -729,7 +729,6 @@ ix86_option_init_struct (struct gcc_opti
 
   opts->x_flag_pcc_struct_return = 2;
   opts->x_flag_asynchronous_unwind_tables = 2;
-  opts->x_flag_vect_cost_model = 1;
 }
 
 /* On the x86 -fsplit-stack and -fstack-protector both use the same
Index: b/src/gcc/asan.c
===================================================================
--- a/src/gcc/asan.c
+++ b/src/gcc/asan.c
@@ -906,7 +906,7 @@ asan_clear_shadow (rtx shadow_mem, HOST_
 
   gcc_assert ((len & 3) == 0);
   top_label = gen_label_rtx ();
-  addr = force_reg (Pmode, XEXP (shadow_mem, 0));
+  addr = copy_to_mode_reg (Pmode, XEXP (shadow_mem, 0));
   shadow_mem = adjust_automodify_address (shadow_mem, SImode, addr, 0);
   end = force_reg (Pmode, plus_constant (Pmode, addr, len));
   emit_label (top_label);
Index: b/src/gcc/fortran/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/fortran/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/configure.ac
===================================================================
--- a/src/gcc/configure.ac
+++ b/src/gcc/configure.ac
@@ -813,7 +813,7 @@ AC_ARG_WITH(specs,
 )
 AC_SUBST(CONFIGURE_SPECS)
 
-ACX_PKGVERSION([GCC])
+ACX_PKGVERSION([Linaro GCC `cat $srcdir/LINARO-VERSION`])
 ACX_BUGURL([http://gcc.gnu.org/bugs.html])
 
 # Sanity check enable_languages in case someone does not run the toplevel
@@ -4233,8 +4233,9 @@ esac
 # ??? Once 2.11 is released, probably need to add first known working
 # version to the per-target configury.
 case "$cpu_type" in
-  alpha | arm | avr | bfin | cris | i386 | m32c | m68k | microblaze | mips \
-  | pa | rs6000 | score | sparc | spu | tilegx | tilepro | xstormy16 | xtensa)
+  aarch64 | alpha | arm | avr | bfin | cris | i386 | m32c | m68k | microblaze \
+  | mips | pa | rs6000 | score | sparc | spu | tilegx | tilepro | xstormy16 \
+  | xtensa)
     insn="nop"
     ;;
   ia64 | s390)
Index: b/src/gcc/tree-vectorizer.h
===================================================================
--- a/src/gcc/tree-vectorizer.h
+++ b/src/gcc/tree-vectorizer.h
@@ -838,6 +838,14 @@ known_alignment_for_access_p (struct dat
   return (DR_MISALIGNMENT (data_ref_info) != -1);
 }
 
+
+/* Return true if the vect cost model is unlimited.  */
+static inline bool
+unlimited_cost_model ()
+{
+  return flag_vect_cost_model == VECT_COST_MODEL_UNLIMITED;
+}
+
 /* Source location */
 extern LOC vect_location;
 
Index: b/src/gcc/tree-vect-loop.c
===================================================================
--- a/src/gcc/tree-vect-loop.c
+++ b/src/gcc/tree-vect-loop.c
@@ -2636,7 +2636,7 @@ vect_estimate_min_profitable_iters (loop
   void *target_cost_data = LOOP_VINFO_TARGET_COST_DATA (loop_vinfo);
 
   /* Cost model disabled.  */
-  if (!flag_vect_cost_model)
+  if (unlimited_cost_model ())
     {
       dump_printf_loc (MSG_NOTE, vect_location, "cost model disabled.");
       *ret_min_profitable_niters = 0;
Index: b/src/gcc/flag-types.h
===================================================================
--- a/src/gcc/flag-types.h
+++ b/src/gcc/flag-types.h
@@ -191,4 +191,13 @@ enum fp_contract_mode {
   FP_CONTRACT_FAST = 2
 };
 
+/* Vectorizer cost-model.  */
+enum vect_cost_model {
+  VECT_COST_MODEL_UNLIMITED = 0,
+  VECT_COST_MODEL_CHEAP = 1,
+  VECT_COST_MODEL_DYNAMIC = 2,
+  VECT_COST_MODEL_DEFAULT = 3
+};
+
+
 #endif /* ! GCC_FLAG_TYPES_H */
Index: b/src/gcc/tree-vect-data-refs.c
===================================================================
--- a/src/gcc/tree-vect-data-refs.c
+++ b/src/gcc/tree-vect-data-refs.c
@@ -1328,7 +1328,7 @@ vect_peeling_hash_insert (loop_vec_info
       *new_slot = slot;
     }
 
-  if (!supportable_dr_alignment && !flag_vect_cost_model)
+  if (!supportable_dr_alignment && unlimited_cost_model ())
     slot->count += VECT_MAX_COST;
 }
 
@@ -1438,7 +1438,7 @@ vect_peeling_hash_choose_best_peeling (l
    res.peel_info.dr = NULL;
    res.body_cost_vec = stmt_vector_for_cost();
 
-   if (flag_vect_cost_model)
+   if (!unlimited_cost_model ())
      {
        res.inside_cost = INT_MAX;
        res.outside_cost = INT_MAX;
@@ -1668,7 +1668,7 @@ vect_enhance_data_refs_alignment (loop_v
                  vectorization factor.
                  We do this automtically for cost model, since we calculate cost
                  for every peeling option.  */
-              if (!flag_vect_cost_model)
+              if (unlimited_cost_model ())
                 possible_npeel_number = vf /nelements;
 
               /* Handle the aligned case. We may decide to align some other
@@ -1676,7 +1676,7 @@ vect_enhance_data_refs_alignment (loop_v
               if (DR_MISALIGNMENT (dr) == 0)
                 {
                   npeel_tmp = 0;
-                  if (!flag_vect_cost_model)
+                  if (unlimited_cost_model ())
                     possible_npeel_number++;
                 }
 
@@ -1926,6 +1926,30 @@ vect_enhance_data_refs_alignment (loop_v
 
       if (do_peeling)
         {
+          unsigned max_allowed_peel
+            = PARAM_VALUE (PARAM_VECT_MAX_PEELING_FOR_ALIGNMENT);
+          if (max_allowed_peel != (unsigned)-1)
+            {
+              unsigned max_peel = npeel;
+              if (max_peel == 0)
+                {
+                  gimple dr_stmt = DR_STMT (dr0);
+                  stmt_vec_info vinfo = vinfo_for_stmt (dr_stmt);
+                  tree vtype = STMT_VINFO_VECTYPE (vinfo);
+                  max_peel = TYPE_VECTOR_SUBPARTS (vtype) - 1;
+                }
+              if (max_peel > max_allowed_peel)
+                {
+                  do_peeling = false;
+                  if (dump_enabled_p ())
+                    dump_printf_loc (MSG_NOTE, vect_location,
+                        "Disable peeling, max peels reached: %d\n", max_peel);
+                }
+            }
+        }
+
+      if (do_peeling)
+        {
 	  stmt_info_for_cost *si;
 	  void *data = LOOP_VINFO_TARGET_COST_DATA (loop_vinfo);
 
@@ -1979,16 +2003,14 @@ vect_enhance_data_refs_alignment (loop_v
   /* (2) Versioning to force alignment.  */
 
   /* Try versioning if:
-     1) flag_tree_vect_loop_version is TRUE
-     2) optimize loop for speed
-     3) there is at least one unsupported misaligned data ref with an unknown
+     1) optimize loop for speed
+     2) there is at least one unsupported misaligned data ref with an unknown
         misalignment, and
-     4) all misaligned data refs with a known misalignment are supported, and
-     5) the number of runtime alignment checks is within reason.  */
+     3) all misaligned data refs with a known misalignment are supported, and
+     4) the number of runtime alignment checks is within reason.  */
 
   do_versioning =
-	flag_tree_vect_loop_version
-	&& optimize_loop_nest_for_speed_p (loop)
+	optimize_loop_nest_for_speed_p (loop)
 	&& (!loop->inner); /* FORNOW */
 
   if (do_versioning)
Index: b/src/gcc/coretypes.h
===================================================================
--- a/src/gcc/coretypes.h
+++ b/src/gcc/coretypes.h
@@ -62,6 +62,8 @@ union gimple_statement_d;
 typedef union gimple_statement_d *gimple;
 typedef const union gimple_statement_d *const_gimple;
 typedef gimple gimple_seq;
+struct gimple_stmt_iterator_d;
+typedef struct gimple_stmt_iterator_d gimple_stmt_iterator;
 union section;
 typedef union section section;
 struct gcc_options;
Index: b/src/gcc/tree-ssa-phiopt.c
===================================================================
--- a/src/gcc/tree-ssa-phiopt.c
+++ b/src/gcc/tree-ssa-phiopt.c
@@ -108,6 +108,26 @@ static bool gate_hoist_loads (void);
    This opportunity can sometimes occur as a result of other
    optimizations.
 
+
+   Another case caught by value replacement looks like this:
+
+     bb0:
+       t1 = a == CONST;
+       t2 = b > c;
+       t3 = t1 & t2;
+       if (t3 != 0) goto bb1; else goto bb2;
+     bb1:
+     bb2:
+       x = PHI (CONST, a)
+
+   Gets replaced with:
+     bb0:
+     bb2:
+       t1 = a == CONST;
+       t2 = b > c;
+       t3 = t1 & t2;
+       x = a;
+
    ABS Replacement
    ---------------
 
@@ -153,7 +173,7 @@ static bool gate_hoist_loads (void);
 
    Adjacent Load Hoisting
    ----------------------
-   
+
    This transformation replaces
 
      bb0:
@@ -275,7 +295,7 @@ single_non_singleton_phi_for_edges (gimp
    phi optimizations.  Both share much of the infrastructure in how
    to match applicable basic block patterns.  DO_STORE_ELIM is true
    when we want to do conditional store replacement, false otherwise.
-   DO_HOIST_LOADS is true when we want to hoist adjacent loads out 
+   DO_HOIST_LOADS is true when we want to hoist adjacent loads out
    of diamond control flow patterns, false otherwise.  */
 static unsigned int
 tree_ssa_phiopt_worker (bool do_store_elim, bool do_hoist_loads)
@@ -378,7 +398,7 @@ tree_ssa_phiopt_worker (bool do_store_el
 	  continue;
 	}
       else
-	continue;      
+	continue;
 
       e1 = EDGE_SUCC (bb1, 0);
 
@@ -426,7 +446,7 @@ tree_ssa_phiopt_worker (bool do_store_el
 
 	  if (!candorest)
 	    continue;
-	  
+
 	  phi = single_non_singleton_phi_for_edges (phis, e1, e2);
 	  if (!phi)
 	    continue;
@@ -714,6 +734,93 @@ jump_function_from_stmt (tree *arg, gimp
   return false;
 }
 
+/* RHS is a source argument in a BIT_AND_EXPR which feeds a conditional
+   of the form SSA_NAME NE 0.
+
+   If RHS is fed by a simple EQ_EXPR comparison of two values, see if
+   the two input values of the EQ_EXPR match arg0 and arg1.
+
+   If so update *code and return TRUE.  Otherwise return FALSE.  */
+
+static bool
+rhs_is_fed_for_value_replacement (const_tree arg0, const_tree arg1,
+				  enum tree_code *code, const_tree rhs)
+{
+  /* Obviously if RHS is not an SSA_NAME, we can't look at the defining
+     statement.  */
+  if (TREE_CODE (rhs) == SSA_NAME)
+    {
+      gimple def1 = SSA_NAME_DEF_STMT (rhs);
+
+      /* Verify the defining statement has an EQ_EXPR on the RHS.  */
+      if (is_gimple_assign (def1) && gimple_assign_rhs_code (def1) == EQ_EXPR)
+	{
+	  /* Finally verify the source operands of the EQ_EXPR are equal
+	     to arg0 and arg1.  */
+	  tree op0 = gimple_assign_rhs1 (def1);
+	  tree op1 = gimple_assign_rhs2 (def1);
+	  if ((operand_equal_for_phi_arg_p (arg0, op0)
+	       && operand_equal_for_phi_arg_p (arg1, op1))
+	      || (operand_equal_for_phi_arg_p (arg0, op1)
+               && operand_equal_for_phi_arg_p (arg1, op0)))
+	    {
+	      /* We will perform the optimization.  */
+	      *code = gimple_assign_rhs_code (def1);
+	      return true;
+	    }
+	}
+    }
+  return false;
+}
+
+/* Return TRUE if arg0/arg1 are equal to the rhs/lhs or lhs/rhs of COND. 
+
+   Also return TRUE if arg0/arg1 are equal to the source arguments of a
+   an EQ comparison feeding a BIT_AND_EXPR which feeds COND. 
+
+   Return FALSE otherwise.  */
+
+static bool
+operand_equal_for_value_replacement (const_tree arg0, const_tree arg1,
+				     enum tree_code *code, gimple cond)
+{
+  gimple def;
+  tree lhs = gimple_cond_lhs (cond);
+  tree rhs = gimple_cond_rhs (cond);
+
+  if ((operand_equal_for_phi_arg_p (arg0, lhs)
+       && operand_equal_for_phi_arg_p (arg1, rhs))
+      || (operand_equal_for_phi_arg_p (arg1, lhs)
+	  && operand_equal_for_phi_arg_p (arg0, rhs)))
+    return true;
+
+  /* Now handle more complex case where we have an EQ comparison
+     which feeds a BIT_AND_EXPR which feeds COND.
+
+     First verify that COND is of the form SSA_NAME NE 0.  */
+  if (*code != NE_EXPR || !integer_zerop (rhs)
+      || TREE_CODE (lhs) != SSA_NAME)
+    return false;
+
+  /* Now ensure that SSA_NAME is set by a BIT_AND_EXPR.  */
+  def = SSA_NAME_DEF_STMT (lhs);
+  if (!is_gimple_assign (def) || gimple_assign_rhs_code (def) != BIT_AND_EXPR)
+    return false;
+
+  /* Now verify arg0/arg1 correspond to the source arguments of an 
+     EQ comparison feeding the BIT_AND_EXPR.  */
+     
+  tree tmp = gimple_assign_rhs1 (def);
+  if (rhs_is_fed_for_value_replacement (arg0, arg1, code, tmp))
+    return true;
+
+  tmp = gimple_assign_rhs2 (def);
+  if (rhs_is_fed_for_value_replacement (arg0, arg1, code, tmp))
+    return true;
+
+  return false;
+}
+
 /*  The function value_replacement does the main work of doing the value
     replacement.  Return non-zero if the replacement is done.  Otherwise return
     0.  If we remove the middle basic block, return 2.
@@ -783,10 +890,7 @@ value_replacement (basic_block cond_bb,
      We now need to verify that the two arguments in the PHI node match
      the two arguments to the equality comparison.  */
 
-  if ((operand_equal_for_phi_arg_p (arg0, gimple_cond_lhs (cond))
-       && operand_equal_for_phi_arg_p (arg1, gimple_cond_rhs (cond)))
-      || (operand_equal_for_phi_arg_p (arg1, gimple_cond_lhs (cond))
-	  && operand_equal_for_phi_arg_p (arg0, gimple_cond_rhs (cond))))
+  if (operand_equal_for_value_replacement (arg0, arg1, &code, cond))
     {
       edge e;
       tree arg;
@@ -1807,7 +1911,7 @@ local_mem_dependence (gimple stmt, basic
 
 /* Given a "diamond" control-flow pattern where BB0 tests a condition,
    BB1 and BB2 are "then" and "else" blocks dependent on this test,
-   and BB3 rejoins control flow following BB1 and BB2, look for 
+   and BB3 rejoins control flow following BB1 and BB2, look for
    opportunities to hoist loads as follows.  If BB3 contains a PHI of
    two loads, one each occurring in BB1 and BB2, and the loads are
    provably of adjacent fields in the same structure, then move both
@@ -1857,7 +1961,7 @@ hoist_adjacent_loads (basic_block bb0, b
 
       arg1 = gimple_phi_arg_def (phi_stmt, 0);
       arg2 = gimple_phi_arg_def (phi_stmt, 1);
-      
+
       if (TREE_CODE (arg1) != SSA_NAME
 	  || TREE_CODE (arg2) != SSA_NAME
 	  || SSA_NAME_IS_DEFAULT_DEF (arg1)
Index: b/src/gcc/tree-ssa-coalesce.c
===================================================================
--- a/src/gcc/tree-ssa-coalesce.c
+++ b/src/gcc/tree-ssa-coalesce.c
@@ -979,8 +979,7 @@ create_outofssa_var_map (coalesce_list_p
 		continue;
 
 	      register_ssa_partition (map, arg);
-	      if ((SSA_NAME_VAR (arg) == SSA_NAME_VAR (res)
-		   && TREE_TYPE (arg) == TREE_TYPE (res))
+	      if (gimple_can_coalesce_p (arg, res)
 		  || (e->flags & EDGE_ABNORMAL))
 		{
 		  saw_copy = true;
@@ -1021,8 +1020,7 @@ create_outofssa_var_map (coalesce_list_p
 		if (gimple_assign_copy_p (stmt)
                     && TREE_CODE (lhs) == SSA_NAME
 		    && TREE_CODE (rhs1) == SSA_NAME
-		    && SSA_NAME_VAR (lhs) == SSA_NAME_VAR (rhs1)
-		    && TREE_TYPE (lhs) == TREE_TYPE (rhs1))
+		    && gimple_can_coalesce_p (lhs, rhs1))
 		  {
 		    v1 = SSA_NAME_VERSION (lhs);
 		    v2 = SSA_NAME_VERSION (rhs1);
@@ -1073,8 +1071,7 @@ create_outofssa_var_map (coalesce_list_p
 		    v1 = SSA_NAME_VERSION (outputs[match]);
 		    v2 = SSA_NAME_VERSION (input);
 
-		    if (SSA_NAME_VAR (outputs[match]) == SSA_NAME_VAR (input)
-			&& TREE_TYPE (outputs[match]) == TREE_TYPE (input))
+		    if (gimple_can_coalesce_p (outputs[match], input))
 		      {
 			cost = coalesce_cost (REG_BR_PROB_BASE,
 					      optimize_bb_for_size_p (bb));
@@ -1108,8 +1105,7 @@ create_outofssa_var_map (coalesce_list_p
 		first = var;
 	      else
 		{
-		  gcc_assert (SSA_NAME_VAR (var) == SSA_NAME_VAR (first)
-			      && TREE_TYPE (var) == TREE_TYPE (first));
+		  gcc_assert (gimple_can_coalesce_p (var, first));
 		  v1 = SSA_NAME_VERSION (first);
 		  v2 = SSA_NAME_VERSION (var);
 		  bitmap_set_bit (used_in_copy, v1);
@@ -1246,8 +1242,7 @@ coalesce_partitions (var_map map, ssa_co
       var2 = ssa_name (y);
 
       /* Assert the coalesces have the same base variable.  */
-      gcc_assert (SSA_NAME_VAR (var1) == SSA_NAME_VAR (var2)
-		  && TREE_TYPE (var1) == TREE_TYPE (var2));
+      gcc_assert (gimple_can_coalesce_p (var1, var2));
 
       if (debug)
 	fprintf (debug, "Coalesce list: ");
@@ -1377,3 +1372,38 @@ coalesce_ssa_name (void)
 
   return map;
 }
+
+/* Given SSA_NAMEs NAME1 and NAME2, return true if they are candidates for
+   coalescing together, false otherwise.
+
+   This must stay consistent with var_map_base_init in tree-ssa-live.c.  */
+
+bool
+gimple_can_coalesce_p (tree name1, tree name2)
+{
+  /* First check the SSA_NAME's associated DECL.  We only want to
+     coalesce if they have the same DECL or both have no associated DECL.  */
+  if (SSA_NAME_VAR (name1) != SSA_NAME_VAR (name2))
+    return false;
+
+  /* Now check the types.  If the types are the same, then we should
+     try to coalesce V1 and V2.  */
+  tree t1 = TREE_TYPE (name1);
+  tree t2 = TREE_TYPE (name2);
+  if (t1 == t2)
+    return true;
+
+  /* If the types are not the same, check for a canonical type match.  This
+     (for example) allows coalescing when the types are fundamentally the
+     same, but just have different names. 
+
+     Note pointer types with different address spaces may have the same
+     canonical type.  Those are rejected for coalescing by the
+     types_compatible_p check.  */
+  if (TYPE_CANONICAL (t1)
+      && TYPE_CANONICAL (t1) == TYPE_CANONICAL (t2)
+      && types_compatible_p (t1, t2))
+    return true;
+
+  return false;
+}
Index: b/src/gcc/lower-subreg.c
===================================================================
--- a/src/gcc/lower-subreg.c
+++ b/src/gcc/lower-subreg.c
@@ -966,7 +966,20 @@ resolve_simple_move (rtx set, rtx insn)
       rtx reg;
 
       reg = gen_reg_rtx (orig_mode);
+
+#ifdef AUTO_INC_DEC
+      {
+	rtx move = emit_move_insn (reg, src);
+	if (MEM_P (src))
+	  {
+	    rtx note = find_reg_note (insn, REG_INC, NULL_RTX);
+	    if (note)
+	      add_reg_note (move, REG_INC, XEXP (note, 0));
+	  }
+      }
+#else
       emit_move_insn (reg, src);
+#endif
       src = reg;
     }
 
@@ -1056,6 +1069,16 @@ resolve_simple_move (rtx set, rtx insn)
 	mdest = simplify_gen_subreg (orig_mode, dest, GET_MODE (dest), 0);
       minsn = emit_move_insn (real_dest, mdest);
 
+#ifdef AUTO_INC_DEC
+  if (MEM_P (real_dest)
+      && !(resolve_reg_p (real_dest) || resolve_subreg_p (real_dest)))
+    {
+      rtx note = find_reg_note (insn, REG_INC, NULL_RTX);
+      if (note)
+	add_reg_note (minsn, REG_INC, XEXP (note, 0));
+    }
+#endif
+
       smove = single_set (minsn);
       gcc_assert (smove != NULL_RTX);
 
Index: b/src/gcc/gimple-fold.c
===================================================================
--- a/src/gcc/gimple-fold.c
+++ b/src/gcc/gimple-fold.c
@@ -1151,6 +1151,8 @@ gimple_fold_call (gimple_stmt_iterator *
 	    gimplify_and_update_call_from_tree (gsi, result);
 	  changed = true;
 	}
+      else if (DECL_BUILT_IN_CLASS (callee) == BUILT_IN_MD)
+	changed |= targetm.gimple_fold_builtin (gsi);
     }
 
   return changed;
Index: b/src/gcc/tree-ssa-live.c
===================================================================
--- a/src/gcc/tree-ssa-live.c
+++ b/src/gcc/tree-ssa-live.c
@@ -88,8 +88,12 @@ var_map_base_init (var_map map)
 	   as it restricts the sets we compute conflicts for.
 	   Using TREE_TYPE to generate sets is the easies as
 	   type equivalency also holds for SSA names with the same
-	   underlying decl.  */
-	m->base.from = TREE_TYPE (var);
+	   underlying decl. 
+
+	   Check gimple_can_coalesce_p when changing this code.  */
+	m->base.from = (TYPE_CANONICAL (TREE_TYPE (var))
+			? TYPE_CANONICAL (TREE_TYPE (var))
+			: TREE_TYPE (var));
       /* If base variable hasn't been seen, set it up.  */
       slot = (struct tree_int_map **) htab_find_slot (tree_to_index,
 						      m, INSERT);
Index: b/src/gcc/lto/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/lto/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/po/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/gcc/po/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/gcc/common.opt
===================================================================
--- a/src/gcc/common.opt
+++ b/src/gcc/common.opt
@@ -2237,13 +2237,33 @@ ftree-slp-vectorize
 Common Report Var(flag_tree_slp_vectorize) Init(2) Optimization
 Enable basic block vectorization (SLP) on trees
 
+fvect-cost-model=
+Common Joined RejectNegative Enum(vect_cost_model) Var(flag_vect_cost_model) Init(VECT_COST_MODEL_DEFAULT)
+Specifies the cost model for vectorization
+ 
+Enum
+Name(vect_cost_model) Type(enum vect_cost_model) UnknownError(unknown vectorizer cost model %qs)
+
+EnumValue
+Enum(vect_cost_model) String(unlimited) Value(VECT_COST_MODEL_UNLIMITED)
+
+EnumValue
+Enum(vect_cost_model) String(dynamic) Value(VECT_COST_MODEL_DYNAMIC)
+
+EnumValue
+Enum(vect_cost_model) String(cheap) Value(VECT_COST_MODEL_CHEAP)
+
 fvect-cost-model
-Common Report Var(flag_vect_cost_model) Optimization
-Enable use of cost model in vectorization
+Common RejectNegative Alias(fvect-cost-model=,dynamic)
+Enables the dynamic vectorizer cost model.  Preserved for backward compatibility.
+
+fno-vect-cost-model
+Common RejectNegative Alias(fvect-cost-model=,unlimited)
+Enables the unlimited vectorizer cost model.  Preserved for backward compatibility.
 
 ftree-vect-loop-version
-Common Report Var(flag_tree_vect_loop_version) Init(1) Optimization
-Enable loop versioning when doing loop vectorization on trees
+Common Ignore
+Does nothing. Preserved for backward compatibility.
 
 ftree-scev-cprop
 Common Report Var(flag_tree_scev_cprop) Init(1) Optimization
Index: b/src/gcc/combine.c
===================================================================
--- a/src/gcc/combine.c
+++ b/src/gcc/combine.c
@@ -11996,6 +11996,13 @@ simplify_comparison (enum rtx_code code,
 	    }
 	}
 
+  /* We may have changed the comparison operands.  Re-canonicalize.  */
+  if (swap_commutative_operands_p (op0, op1))
+    {
+      tem = op0, op0 = op1, op1 = tem;
+      code = swap_condition (code);
+    }
+
   /* If this machine only supports a subset of valid comparisons, see if we
      can convert an unsupported one into a supported one.  */
   target_canonicalize_comparison (&code, &op0, &op1, 0);
Index: b/src/gcc/var-tracking.c
===================================================================
--- a/src/gcc/var-tracking.c
+++ b/src/gcc/var-tracking.c
@@ -5915,7 +5915,8 @@ add_stores (rtx loc, const_rtx expr, voi
     {
       cselib_val *oval = cselib_lookup (oloc, GET_MODE (oloc), 0, VOIDmode);
 
-      gcc_assert (oval != v);
+      if (oval == v)
+	return;
       gcc_assert (REG_P (oloc) || MEM_P (oloc));
 
       if (oval && !cselib_preserved_value_p (oval))
Index: b/src/gcc/config.gcc
===================================================================
--- a/src/gcc/config.gcc
+++ b/src/gcc/config.gcc
@@ -312,7 +312,7 @@ aarch64*-*-*)
 	cpu_type=aarch64
 	need_64bit_hwint=yes
 	extra_headers="arm_neon.h"
-	extra_objs="aarch64-builtins.o"
+	extra_objs="aarch64-builtins.o aarch-common.o"
 	target_has_targetm_common=yes
 	;;
 alpha*-*-*)
@@ -325,10 +325,12 @@ am33_2.0-*-linux*)
 	;;
 arm*-*-*)
 	cpu_type=arm
-	extra_headers="mmintrin.h arm_neon.h"
+	extra_headers="mmintrin.h arm_neon.h arm_acle.h"
+	extra_objs="aarch-common.o"
 	target_type_format_char='%'
 	c_target_objs="arm-c.o"
 	cxx_target_objs="arm-c.o"
+	need_64bit_hwint=yes
 	extra_options="${extra_options} arm/arm-tables.opt"
 	;;
 avr-*-*)
@@ -497,6 +499,9 @@ then
 fi
 
 case ${target} in
+aarch64*-*-*)
+	tm_p_file="${tm_p_file} arm/aarch-common-protos.h"
+	;;
 i[34567]86-*-*)
 	if test "x$with_abi" != x; then
 		echo "This target does not support --with-abi."
@@ -537,7 +542,11 @@ x86_64-*-*)
 	fi
 	tm_file="vxworks-dummy.h ${tm_file}"
 	;;
-arm*-*-* | mips*-*-* | sh*-*-* | sparc*-*-*)
+arm*-*-*)
+	tm_p_file="${tm_p_file} arm/aarch-common-protos.h"
+	tm_file="vxworks-dummy.h ${tm_file}"
+	;;
+mips*-*-* | sh*-*-* | sparc*-*-*)
 	tm_file="vxworks-dummy.h ${tm_file}"
 	;;
 esac
@@ -877,7 +886,7 @@ arm*-*-linux-*)			# ARM GNU/Linux with E
 	    tm_defines="${tm_defines} TARGET_BIG_ENDIAN_DEFAULT=1"
 	    ;;
 	esac
-	tmake_file="${tmake_file} arm/t-arm arm/t-arm-elf arm/t-bpabi arm/t-linux-eabi"
+	tmake_file="${tmake_file} arm/t-arm arm/t-arm-elf arm/t-bpabi arm/t-linux-eabi arm/t-mlibs"
 	tm_file="$tm_file arm/bpabi.h arm/linux-eabi.h arm/aout.h vxworks-dummy.h arm/arm.h"
 	# Define multilib configuration for arm-linux-androideabi.
 	case ${target} in
@@ -885,10 +894,6 @@ arm*-*-linux-*)			# ARM GNU/Linux with E
 	    tmake_file="$tmake_file arm/t-linux-androideabi"
 	    ;;
 	esac
-  	# The BPABI long long divmod functions return a 128-bit value in
-	# registers r0-r3.  Correctly modeling that requires the use of
-	# TImode.
-	need_64bit_hwint=yes
 	# The EABI requires the use of __cxa_atexit.
 	default_use_cxa_atexit=yes
 	with_tls=${with_tls:-gnu}
@@ -897,10 +902,6 @@ arm*-*-uclinux*eabi*)		# ARM ucLinux
 	tm_file="dbxelf.h elfos.h arm/unknown-elf.h arm/elf.h arm/linux-gas.h arm/uclinux-elf.h glibc-stdint.h"
 	tmake_file="arm/t-arm arm/t-arm-elf arm/t-bpabi"
 	tm_file="$tm_file arm/bpabi.h arm/uclinux-eabi.h arm/aout.h vxworks-dummy.h arm/arm.h"
-	# The BPABI long long divmod functions return a 128-bit value in
-	# registers r0-r3.  Correctly modeling that requires the use of
-	# TImode.
-	need_64bit_hwint=yes
 	# The EABI requires the use of __cxa_atexit.
 	default_use_cxa_atexit=yes
 	;;
@@ -909,10 +910,6 @@ arm*-*-eabi* | arm*-*-symbianelf* | arm*
 	arm*eb-*-eabi*)
 	  tm_defines="${tm_defines} TARGET_BIG_ENDIAN_DEFAULT=1"
 	esac
-	# The BPABI long long divmod functions return a 128-bit value in
-	# registers r0-r3.  Correctly modeling that requires the use of
-	# TImode.
-	need_64bit_hwint=yes
 	default_use_cxa_atexit=yes
 	tm_file="dbxelf.h elfos.h arm/unknown-elf.h arm/elf.h arm/bpabi.h"
 	tmake_file="arm/t-arm arm/t-arm-elf"
@@ -3340,6 +3337,43 @@ case "${target}" in
 		if test "x$with_arch" != x && test "x$with_cpu" != x; then
 			echo "Warning: --with-arch overrides --with-cpu=$with_cpu" 1>&2
 		fi
+
+		# Add extra multilibs
+		if test "x$with_multilib_list" != x; then
+			arm_multilibs=`echo $with_multilib_list | sed -e 's/,/ /g'`
+			for arm_multilib in ${arm_multilibs}; do
+				case ${arm_multilib} in
+				aprofile)
+				# Note that arm/t-aprofile is a
+				# stand-alone make file fragment to be
+				# used only with itself.  We do not
+				# specifically use the
+				# TM_MULTILIB_OPTION framework because
+				# this shorthand is more
+				# pragmatic. Additionally it is only
+				# designed to work without any
+				# with-cpu, with-arch with-mode
+				# with-fpu or with-float options.
+					if test "x$with_arch" != x \
+					    || test "x$with_cpu" != x \
+					    || test "x$with_float" != x \
+					    || test "x$with_fpu" != x \
+					    || test "x$with_mode" != x ; then
+					    echo "Error: You cannot use any of --with-arch/cpu/fpu/float/mode with --with-multilib-list=aprofile" 1>&2
+					    exit 1
+					fi
+					tmake_file="${tmake_file} arm/t-aprofile"
+					break
+					;;
+				default)
+					;;
+				*)
+					echo "Error: --with-multilib-list=${with_multilib_list} not supported." 1>&2
+					exit 1
+					;;
+				esac
+			done
+		fi
 		;;
 
 	fr*-*-*linux*)
Index: b/src/gcc/Makefile.in
===================================================================
--- a/src/gcc/Makefile.in
+++ b/src/gcc/Makefile.in
@@ -800,10 +800,12 @@ BASEVER     := $(srcdir)/BASE-VER  # 4.x
 DEVPHASE    := $(srcdir)/DEV-PHASE # experimental, prerelease, ""
 DATESTAMP   := $(srcdir)/DATESTAMP # YYYYMMDD or empty
 REVISION    := $(srcdir)/REVISION  # [BRANCH revision XXXXXX]
+LINAROVER   := $(srcdir)/LINARO-VERSION # M.x-YYYY.MM[-S][~dev]
 
 BASEVER_c   := $(shell cat $(BASEVER))
 DEVPHASE_c  := $(shell cat $(DEVPHASE))
 DATESTAMP_c := $(shell cat $(DATESTAMP))
+LINAROVER_c := $(shell cat $(LINAROVER))
 
 ifeq (,$(wildcard $(REVISION)))
 REVISION_c  :=
@@ -824,6 +826,7 @@ DEVPHASE_s  := "\"$(if $(DEVPHASE_c), ($
 DATESTAMP_s := "\"$(if $(DEVPHASE_c), $(DATESTAMP_c))\""
 PKGVERSION_s:= "\"@PKGVERSION@\""
 BUGURL_s    := "\"@REPORT_BUGS_TO@\""
+LINAROVER_s := "\"$(LINAROVER_c)\""
 
 PKGVERSION  := @PKGVERSION@
 BUGURL_TEXI := @REPORT_BUGS_TEXI@
@@ -4017,10 +4020,9 @@ PREPROCESSOR_DEFINES = \
   -DSTANDARD_EXEC_PREFIX=\"$(libdir)/gcc/\" \
   @TARGET_SYSTEM_ROOT_DEFINE@
 
-CFLAGS-cppbuiltin.o += $(PREPROCESSOR_DEFINES) -DBASEVER=$(BASEVER_s)
-cppbuiltin.o: cppbuiltin.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
-	$(TARGET_H) $(TARGET_DEF) $(TREE_H) $(CPP_ID_DATA_H) \
-	cppbuiltin.h version.h Makefile
+CFLAGS-cppbuiltin.o += $(PREPROCESSOR_DEFINES) -DBASEVER=$(BASEVER_s) \
+	-DLINAROVER=$(LINAROVER_s)
+cppbuiltin.o: $(BASEVER) $(LINAROVER)
 
 CFLAGS-cppdefault.o += $(PREPROCESSOR_DEFINES)
 cppdefault.o: cppdefault.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
@@ -4283,7 +4285,8 @@ TEXI_GCC_FILES = gcc.texi gcc-common.tex
 	 gcov.texi trouble.texi bugreport.texi service.texi		\
 	 contribute.texi compat.texi funding.texi gnu.texi gpl_v3.texi	\
 	 fdl.texi contrib.texi cppenv.texi cppopts.texi avr-mmcu.texi	\
-	 implement-c.texi implement-cxx.texi arm-neon-intrinsics.texi
+	 implement-c.texi implement-cxx.texi arm-neon-intrinsics.texi	\
+	 arm-acle-intrinsics.texi
 
 # we explicitly use $(srcdir)/doc/tm.texi here to avoid confusion with
 # the generated tm.texi; the latter might have a more recent timestamp,
Index: b/src/gcc/gimple.h
===================================================================
--- a/src/gcc/gimple.h
+++ b/src/gcc/gimple.h
@@ -130,7 +130,7 @@ enum plf_mask {
 
 /* Iterator object for GIMPLE statement sequences.  */
 
-typedef struct
+struct gimple_stmt_iterator_d
 {
   /* Sequence node holding the current statement.  */
   gimple_seq_node ptr;
@@ -141,8 +141,7 @@ typedef struct
      block/sequence is removed.  */
   gimple_seq *seq;
   basic_block bb;
-} gimple_stmt_iterator;
-
+};
 
 /* Data structure definitions for GIMPLE tuples.  NOTE: word markers
    are for 64 bit hosts.  */
@@ -1036,6 +1035,9 @@ extern tree tree_ssa_strip_useless_type_
 extern bool useless_type_conversion_p (tree, tree);
 extern bool types_compatible_p (tree, tree);
 
+/* In tree-ssa-coalesce.c */
+extern bool gimple_can_coalesce_p (tree, tree);
+
 /* Return the first node in GIMPLE sequence S.  */
 
 static inline gimple_seq_node
Index: b/src/gcc/config/i386/linux-common.h
===================================================================
--- a/src/gcc/config/i386/linux-common.h
+++ b/src/gcc/config/i386/linux-common.h
@@ -40,7 +40,7 @@ along with GCC; see the file COPYING3.
 #undef  LIB_SPEC
 #define LIB_SPEC \
   LINUX_OR_ANDROID_LD (GNU_USER_TARGET_LIB_SPEC, \
-		       GNU_USER_TARGET_LIB_SPEC " " ANDROID_LIB_SPEC)
+		    GNU_USER_TARGET_NO_PTHREADS_LIB_SPEC " " ANDROID_LIB_SPEC)
 
 #undef  STARTFILE_SPEC
 #define STARTFILE_SPEC \
Index: b/src/gcc/config/i386/i386.c
===================================================================
--- a/src/gcc/config/i386/i386.c
+++ b/src/gcc/config/i386/i386.c
@@ -42294,20 +42294,17 @@ ix86_add_stmt_cost (void *data, int coun
   unsigned *cost = (unsigned *) data;
   unsigned retval = 0;
 
-  if (flag_vect_cost_model)
-    {
-      tree vectype = stmt_info ? stmt_vectype (stmt_info) : NULL_TREE;
-      int stmt_cost = ix86_builtin_vectorization_cost (kind, vectype, misalign);
+  tree vectype = stmt_info ? stmt_vectype (stmt_info) : NULL_TREE;
+  int stmt_cost = ix86_builtin_vectorization_cost (kind, vectype, misalign);
 
-      /* Statements in an inner loop relative to the loop being
-	 vectorized are weighted more heavily.  The value here is
-	 arbitrary and could potentially be improved with analysis.  */
-      if (where == vect_body && stmt_info && stmt_in_inner_loop_p (stmt_info))
-	count *= 50;  /* FIXME.  */
+  /* Statements in an inner loop relative to the loop being
+     vectorized are weighted more heavily.  The value here is
+      arbitrary and could potentially be improved with analysis.  */
+  if (where == vect_body && stmt_info && stmt_in_inner_loop_p (stmt_info))
+    count *= 50;  /* FIXME.  */
 
-      retval = (unsigned) (count * stmt_cost);
-      cost[where] += retval;
-    }
+  retval = (unsigned) (count * stmt_cost);
+  cost[where] += retval;
 
   return retval;
 }
Index: b/src/gcc/config/gnu-user.h
===================================================================
--- a/src/gcc/config/gnu-user.h
+++ b/src/gcc/config/gnu-user.h
@@ -73,10 +73,14 @@ see the files COPYING3 and COPYING.RUNTI
 #undef CPLUSPLUS_CPP_SPEC
 #define CPLUSPLUS_CPP_SPEC "-D_GNU_SOURCE %(cpp)"
 
-#define GNU_USER_TARGET_LIB_SPEC \
-  "%{pthread:-lpthread} \
-   %{shared:-lc} \
+#define GNU_USER_TARGET_NO_PTHREADS_LIB_SPEC \
+  "%{shared:-lc} \
    %{!shared:%{mieee-fp:-lieee} %{profile:-lc_p}%{!profile:-lc}}"
+
+#define GNU_USER_TARGET_LIB_SPEC \
+  "%{pthread:-lpthread} " \
+  GNU_USER_TARGET_NO_PTHREADS_LIB_SPEC
+
 #undef  LIB_SPEC
 #define LIB_SPEC GNU_USER_TARGET_LIB_SPEC
 
Index: b/src/gcc/config/aarch64/large.md
===================================================================
--- a/src/gcc/config/aarch64/large.md
+++ b/src/gcc/config/aarch64/large.md
@@ -1,312 +0,0 @@
-;; Copyright (C) 2012-2013 Free Software Foundation, Inc.
-;;
-;; Contributed by ARM Ltd.
-;;
-;; This file is part of GCC.
-;;
-;; GCC is free software; you can redistribute it and/or modify it
-;; under the terms of the GNU General Public License as published by
-;; the Free Software Foundation; either version 3, or (at your option)
-;; any later version.
-;;
-;; GCC is distributed in the hope that it will be useful, but
-;; WITHOUT ANY WARRANTY; without even the implied warranty of
-;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;; General Public License for more details.
-;;
-;; You should have received a copy of the GNU General Public License
-;; along with GCC; see the file COPYING3.  If not see
-;; <http://www.gnu.org/licenses/>.
-
-;; In the absence of any ARMv8-A implementations, two examples derived
-;; from ARM's most recent ARMv7-A cores (Cortex-A7 and Cortex-A15) are
-;; included by way of example.  This is a temporary measure.
-
-;; Example pipeline description for an example 'large' core
-;; implementing AArch64
-
-;;-------------------------------------------------------
-;; General Description
-;;-------------------------------------------------------
-
-(define_automaton "large_cpu")
-
-;; The core is modelled as a triple issue pipeline that has
-;; the following dispatch units.
-;; 1. Two pipelines for simple integer operations: int1, int2
-;; 2. Two pipelines for SIMD and FP data-processing operations: fpsimd1, fpsimd2
-;; 3. One pipeline for branch operations: br
-;; 4. One pipeline for integer multiply and divide operations: multdiv
-;; 5. Two pipelines for load and store operations: ls1, ls2
-;;
-;; We can issue into three pipelines per-cycle.
-;;
-;; We assume that where we have unit pairs xxx1 is always filled before xxx2.
-
-;;-------------------------------------------------------
-;; CPU Units and Reservations
-;;-------------------------------------------------------
-
-;; The three issue units
-(define_cpu_unit "large_cpu_unit_i1, large_cpu_unit_i2, large_cpu_unit_i3" "large_cpu")
-
-(define_reservation "large_cpu_resv_i1"
-		    "(large_cpu_unit_i1 | large_cpu_unit_i2 | large_cpu_unit_i3)")
-
-(define_reservation "large_cpu_resv_i2"
-		    "((large_cpu_unit_i1 + large_cpu_unit_i2) | (large_cpu_unit_i2 + large_cpu_unit_i3))")
-
-(define_reservation "large_cpu_resv_i3"
-		    "(large_cpu_unit_i1 + large_cpu_unit_i2 + large_cpu_unit_i3)")
-
-(final_presence_set "large_cpu_unit_i2" "large_cpu_unit_i1")
-(final_presence_set "large_cpu_unit_i3" "large_cpu_unit_i2")
-
-;; The main dispatch units
-(define_cpu_unit "large_cpu_unit_int1, large_cpu_unit_int2" "large_cpu")
-(define_cpu_unit "large_cpu_unit_fpsimd1, large_cpu_unit_fpsimd2" "large_cpu")
-(define_cpu_unit "large_cpu_unit_ls1, large_cpu_unit_ls2" "large_cpu")
-(define_cpu_unit "large_cpu_unit_br" "large_cpu")
-(define_cpu_unit "large_cpu_unit_multdiv" "large_cpu")
-
-(define_reservation "large_cpu_resv_ls" "(large_cpu_unit_ls1 | large_cpu_unit_ls2)")
-
-;; The extended load-store pipeline
-(define_cpu_unit "large_cpu_unit_load, large_cpu_unit_store" "large_cpu")
-
-;; The extended ALU pipeline
-(define_cpu_unit "large_cpu_unit_int1_alu, large_cpu_unit_int2_alu" "large_cpu")
-(define_cpu_unit "large_cpu_unit_int1_shf, large_cpu_unit_int2_shf" "large_cpu")
-(define_cpu_unit "large_cpu_unit_int1_sat, large_cpu_unit_int2_sat" "large_cpu")
-
-
-;;-------------------------------------------------------
-;; Simple ALU Instructions
-;;-------------------------------------------------------
-
-;; Simple ALU operations without shift
-(define_insn_reservation "large_cpu_alu" 2
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "adc,alu,alu_ext"))
-  "large_cpu_resv_i1, \
-   (large_cpu_unit_int1, large_cpu_unit_int1_alu) |\
-     (large_cpu_unit_int2, large_cpu_unit_int2_alu)")
-
-(define_insn_reservation "large_cpu_logic" 2
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "logic,logic_imm"))
-  "large_cpu_resv_i1, \
-   (large_cpu_unit_int1, large_cpu_unit_int1_alu) |\
-     (large_cpu_unit_int2, large_cpu_unit_int2_alu)")
-
-(define_insn_reservation "large_cpu_shift" 2
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "shift,shift_imm"))
-  "large_cpu_resv_i1, \
-   (large_cpu_unit_int1, large_cpu_unit_int1_shf) |\
-     (large_cpu_unit_int2, large_cpu_unit_int2_shf)")
-
-;; Simple ALU operations with immediate shift
-(define_insn_reservation "large_cpu_alu_shift" 3
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "alu_shift"))
-  "large_cpu_resv_i1, \
-   (large_cpu_unit_int1,
-     large_cpu_unit_int1 + large_cpu_unit_int1_shf, large_cpu_unit_int1_alu) | \
-   (large_cpu_unit_int2,
-     large_cpu_unit_int2 + large_cpu_unit_int2_shf, large_cpu_unit_int2_alu)")
-
-(define_insn_reservation "large_cpu_logic_shift" 3
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "logic_shift"))
-  "large_cpu_resv_i1, \
-   (large_cpu_unit_int1, large_cpu_unit_int1_alu) |\
-     (large_cpu_unit_int2, large_cpu_unit_int2_alu)")
-
-
-;;-------------------------------------------------------
-;; Multiplication/Division
-;;-------------------------------------------------------
-
-;; Simple multiplication
-(define_insn_reservation "large_cpu_mult_single" 3
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "mult,madd") (eq_attr "mode" "SI")))
-  "large_cpu_resv_i1, large_cpu_unit_multdiv")
-
-(define_insn_reservation "large_cpu_mult_double" 4
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "mult,madd") (eq_attr "mode" "DI")))
-  "large_cpu_resv_i1, large_cpu_unit_multdiv")
-
-;; 64-bit multiplication
-(define_insn_reservation "large_cpu_mull" 4
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "mull,mulh,maddl"))
-  "large_cpu_resv_i1, large_cpu_unit_multdiv * 2")
-
-;; Division
-(define_insn_reservation "large_cpu_udiv_single" 9
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "udiv") (eq_attr "mode" "SI")))
-  "large_cpu_resv_i1, large_cpu_unit_multdiv")
-
-(define_insn_reservation "large_cpu_udiv_double" 18
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "udiv") (eq_attr "mode" "DI")))
-  "large_cpu_resv_i1, large_cpu_unit_multdiv")
-
-(define_insn_reservation "large_cpu_sdiv_single" 10
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "sdiv") (eq_attr "mode" "SI")))
-  "large_cpu_resv_i1, large_cpu_unit_multdiv")
-
-(define_insn_reservation "large_cpu_sdiv_double" 20
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "sdiv") (eq_attr "mode" "DI")))
-  "large_cpu_resv_i1, large_cpu_unit_multdiv")
-
-
-;;-------------------------------------------------------
-;; Branches
-;;-------------------------------------------------------
-
-;; Branches take one issue slot.
-;; No latency as there is no result
-(define_insn_reservation "large_cpu_branch" 0
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "branch"))
-  "large_cpu_resv_i1, large_cpu_unit_br")
-
-
-;; Calls take up all issue slots, and form a block in the
-;; pipeline.  The result however is available the next cycle.
-;; Addition of new units requires this to be updated.
-(define_insn_reservation "large_cpu_call" 1
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "call"))
-  "large_cpu_resv_i3 | large_cpu_resv_i2, \
-   large_cpu_unit_int1 + large_cpu_unit_int2 + large_cpu_unit_br + \
-     large_cpu_unit_multdiv + large_cpu_unit_fpsimd1 + large_cpu_unit_fpsimd2 + \
-     large_cpu_unit_ls1 + large_cpu_unit_ls2,\
-   large_cpu_unit_int1_alu + large_cpu_unit_int1_shf + large_cpu_unit_int1_sat + \
-     large_cpu_unit_int2_alu + large_cpu_unit_int2_shf + \
-     large_cpu_unit_int2_sat + large_cpu_unit_load + large_cpu_unit_store")
-
-
-;;-------------------------------------------------------
-;; Load/Store Instructions
-;;-------------------------------------------------------
-
-;; Loads of up to two words.
-(define_insn_reservation "large_cpu_load1" 4
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "load_acq,load1,load2"))
-  "large_cpu_resv_i1, large_cpu_resv_ls, large_cpu_unit_load, nothing")
-
-;; Stores of up to two words.
-(define_insn_reservation "large_cpu_store1" 0
-  (and (eq_attr "tune" "large") (eq_attr "v8type" "store_rel,store1,store2"))
-  "large_cpu_resv_i1, large_cpu_resv_ls, large_cpu_unit_store")
-
-
-;;-------------------------------------------------------
-;; Floating-point arithmetic.
-;;-------------------------------------------------------
-
-(define_insn_reservation "large_cpu_fpalu" 4
-  (and (eq_attr "tune" "large")
-       (eq_attr "v8type" "ffarith,fadd,fccmp,fcvt,fcmp"))
-  "large_cpu_resv_i1 + large_cpu_unit_fpsimd1")
-
-(define_insn_reservation "large_cpu_fconst" 3
-  (and (eq_attr "tune" "large")
-       (eq_attr "v8type" "fconst"))
-  "large_cpu_resv_i1 + large_cpu_unit_fpsimd1")
-
-(define_insn_reservation "large_cpu_fpmuls" 4
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "fmul,fmadd") (eq_attr "mode" "SF")))
-  "large_cpu_resv_i1 + large_cpu_unit_fpsimd1")
-
-(define_insn_reservation "large_cpu_fpmuld" 7
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "fmul,fmadd") (eq_attr "mode" "DF")))
-  "large_cpu_resv_i1 + large_cpu_unit_fpsimd1, large_cpu_unit_fpsimd1 * 2,\
-   large_cpu_resv_i1 + large_cpu_unit_fpsimd1")
-
-
-;;-------------------------------------------------------
-;; Floating-point Division
-;;-------------------------------------------------------
-
-;; Single-precision divide takes 14 cycles to complete, and this
-;; includes the time taken for the special instruction used to collect the
-;; result to travel down the multiply pipeline.
-
-(define_insn_reservation "large_cpu_fdivs" 14
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "fdiv,fsqrt") (eq_attr "mode" "SF")))
-  "large_cpu_resv_i1, large_cpu_unit_fpsimd1 * 13")
-
-(define_insn_reservation "large_cpu_fdivd" 29
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "fdiv,fsqrt") (eq_attr "mode" "DF")))
-  "large_cpu_resv_i1, large_cpu_unit_fpsimd1 * 28")
-
-
-
-;;-------------------------------------------------------
-;; Floating-point Transfers
-;;-------------------------------------------------------
-
-(define_insn_reservation "large_cpu_i2f" 4
-  (and (eq_attr "tune" "large")
-       (eq_attr "v8type" "fmovi2f"))
-  "large_cpu_resv_i1")
-
-(define_insn_reservation "large_cpu_f2i" 2
-  (and (eq_attr "tune" "large")
-       (eq_attr "v8type" "fmovf2i"))
-  "large_cpu_resv_i1")
-
-
-;;-------------------------------------------------------
-;; Floating-point Load/Store
-;;-------------------------------------------------------
-
-(define_insn_reservation "large_cpu_floads" 4
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "fpsimd_load,fpsimd_load2") (eq_attr "mode" "SF")))
-  "large_cpu_resv_i1")
-
-(define_insn_reservation "large_cpu_floadd" 5
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "fpsimd_load,fpsimd_load2") (eq_attr "mode" "DF")))
-  "large_cpu_resv_i1 + large_cpu_unit_br, large_cpu_resv_i1")
-
-(define_insn_reservation "large_cpu_fstores" 0
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "fpsimd_store,fpsimd_store2") (eq_attr "mode" "SF")))
-  "large_cpu_resv_i1")
-
-(define_insn_reservation "large_cpu_fstored" 0
-  (and (eq_attr "tune" "large")
-       (and (eq_attr "v8type" "fpsimd_store,fpsimd_store2") (eq_attr "mode" "DF")))
-  "large_cpu_resv_i1 + large_cpu_unit_br, large_cpu_resv_i1")
-
-
-;;-------------------------------------------------------
-;; Bypasses
-;;-------------------------------------------------------
-
-(define_bypass 1 "large_cpu_alu, large_cpu_logic, large_cpu_shift"
-  "large_cpu_alu, large_cpu_alu_shift, large_cpu_logic, large_cpu_logic_shift, large_cpu_shift")
-
-(define_bypass 2 "large_cpu_alu_shift, large_cpu_logic_shift"
-  "large_cpu_alu, large_cpu_alu_shift, large_cpu_logic, large_cpu_logic_shift, large_cpu_shift")
-
-(define_bypass 1 "large_cpu_alu, large_cpu_logic, large_cpu_shift" "large_cpu_load1")
-
-(define_bypass 2 "large_cpu_alu_shift, large_cpu_logic_shift" "large_cpu_load1")
-
-(define_bypass 2 "large_cpu_floads"
-                 "large_cpu_fpalu, large_cpu_fpmuld,\
-		  large_cpu_fdivs, large_cpu_fdivd,\
-		  large_cpu_f2i")
-
-(define_bypass 3 "large_cpu_floadd"
-                 "large_cpu_fpalu, large_cpu_fpmuld,\
-		  large_cpu_fdivs, large_cpu_fdivd,\
-		  large_cpu_f2i")
Index: b/src/gcc/config/aarch64/aarch64-generic.md
===================================================================
--- a/src/gcc/config/aarch64/aarch64-generic.md
+++ b/src/gcc/config/aarch64/aarch64-generic.md
@@ -1,38 +0,0 @@
-;; Machine description for AArch64 architecture.
-;; Copyright (C) 2009-2013 Free Software Foundation, Inc.
-;; Contributed by ARM Ltd.
-;;
-;; This file is part of GCC.
-;;
-;; GCC is free software; you can redistribute it and/or modify it
-;; under the terms of the GNU General Public License as published by
-;; the Free Software Foundation; either version 3, or (at your option)
-;; any later version.
-;;
-;; GCC is distributed in the hope that it will be useful, but
-;; WITHOUT ANY WARRANTY; without even the implied warranty of
-;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;; General Public License for more details.
-;;
-;; You should have received a copy of the GNU General Public License
-;; along with GCC; see the file COPYING3.  If not see
-;; <http://www.gnu.org/licenses/>.
-
-;; Generic scheduler
-
-(define_automaton "aarch64")
-
-(define_cpu_unit "core" "aarch64")
-
-(define_attr "is_load" "yes,no"
-  (if_then_else (eq_attr "v8type" "fpsimd_load,fpsimd_load2,load1,load2")
-	(const_string "yes")
-	(const_string "no")))
-
-(define_insn_reservation "load" 2
-  (eq_attr "is_load" "yes")
-  "core")
-
-(define_insn_reservation "nonload" 1
-  (eq_attr "is_load" "no")
-  "core")
Index: b/src/gcc/config/aarch64/small.md
===================================================================
--- a/src/gcc/config/aarch64/small.md
+++ b/src/gcc/config/aarch64/small.md
@@ -1,287 +0,0 @@
-;; Copyright (C) 2012-2013 Free Software Foundation, Inc.
-;;
-;; Contributed by ARM Ltd.
-;;
-;; This file is part of GCC.
-;;
-;; GCC is free software; you can redistribute it and/or modify it
-;; under the terms of the GNU General Public License as published by
-;; the Free Software Foundation; either version 3, or (at your option)
-;; any later version.
-;;
-;; GCC is distributed in the hope that it will be useful, but
-;; WITHOUT ANY WARRANTY; without even the implied warranty of
-;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-;; General Public License for more details.
-;;
-;; You should have received a copy of the GNU General Public License
-;; along with GCC; see the file COPYING3.  If not see
-;; <http://www.gnu.org/licenses/>.
-
-;; In the absence of any ARMv8-A implementations, two examples derived
-;; from ARM's most recent ARMv7-A cores (Cortex-A7 and Cortex-A15) are
-;; included by way of example.  This is a temporary measure.
-
-;; Example pipeline description for an example 'small' core
-;; implementing AArch64
-
-;;-------------------------------------------------------
-;; General Description
-;;-------------------------------------------------------
-
-(define_automaton "small_cpu")
-
-;; The core is modelled as a single issue pipeline with the following
-;; dispatch units.
-;; 1. One pipeline for simple intructions.
-;; 2. One pipeline for branch intructions.
-;;
-;; There are five pipeline stages.
-;; The decode/issue stages operate the same for all instructions.
-;; Instructions always advance one stage per cycle in order.
-;; Only branch instructions may dual-issue with other instructions, except
-;; when those instructions take multiple cycles to issue.
-
-
-;;-------------------------------------------------------
-;; CPU Units and Reservations
-;;-------------------------------------------------------
-
-(define_cpu_unit "small_cpu_unit_i" "small_cpu")
-(define_cpu_unit "small_cpu_unit_br" "small_cpu")
-
-;; Pseudo-unit for blocking the multiply pipeline when a double-precision
-;; multiply is in progress.
-(define_cpu_unit "small_cpu_unit_fpmul_pipe" "small_cpu")
-
-;; The floating-point add pipeline, used to model the usage
-;; of the add pipeline by fp alu instructions.
-(define_cpu_unit "small_cpu_unit_fpadd_pipe" "small_cpu")
-
-;; Floating-point division pipeline (long latency, out-of-order completion).
-(define_cpu_unit "small_cpu_unit_fpdiv" "small_cpu")
-
-
-;;-------------------------------------------------------
-;; Simple ALU Instructions
-;;-------------------------------------------------------
-
-;; Simple ALU operations without shift
-(define_insn_reservation "small_cpu_alu" 2
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "adc,alu,alu_ext"))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_logic" 2
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "logic,logic_imm"))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_shift" 2
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "shift,shift_imm"))
-  "small_cpu_unit_i")
-
-;; Simple ALU operations with immediate shift
-(define_insn_reservation "small_cpu_alu_shift" 2
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "alu_shift"))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_logic_shift" 2
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "logic_shift"))
-  "small_cpu_unit_i")
-
-
-;;-------------------------------------------------------
-;; Multiplication/Division
-;;-------------------------------------------------------
-
-;; Simple multiplication
-(define_insn_reservation "small_cpu_mult_single" 2
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "mult,madd") (eq_attr "mode" "SI")))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_mult_double" 3
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "mult,madd") (eq_attr "mode" "DI")))
-  "small_cpu_unit_i")
-
-;; 64-bit multiplication
-(define_insn_reservation "small_cpu_mull" 3
-  (and (eq_attr "tune" "small") (eq_attr "v8type" "mull,mulh,maddl"))
-  "small_cpu_unit_i * 2")
-
-;; Division
-(define_insn_reservation "small_cpu_udiv_single" 5
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "udiv") (eq_attr "mode" "SI")))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_udiv_double" 10
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "udiv") (eq_attr "mode" "DI")))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_sdiv_single" 6
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "sdiv") (eq_attr "mode" "SI")))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_sdiv_double" 12
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "sdiv") (eq_attr "mode" "DI")))
-  "small_cpu_unit_i")
-
-
-;;-------------------------------------------------------
-;; Load/Store Instructions
-;;-------------------------------------------------------
-
-(define_insn_reservation "small_cpu_load1" 2
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "load_acq,load1"))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_store1" 0
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "store_rel,store1"))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_load2" 3
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "load2"))
-  "small_cpu_unit_i + small_cpu_unit_br, small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_store2" 0
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "store2"))
-  "small_cpu_unit_i + small_cpu_unit_br, small_cpu_unit_i")
-
-
-;;-------------------------------------------------------
-;; Branches
-;;-------------------------------------------------------
-
-;; Direct branches are the only instructions that can dual-issue.
-;; The latency here represents when the branch actually takes place.
-
-(define_insn_reservation "small_cpu_unit_br" 3
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "branch,call"))
-  "small_cpu_unit_br")
-
-
-;;-------------------------------------------------------
-;; Floating-point arithmetic.
-;;-------------------------------------------------------
-
-(define_insn_reservation "small_cpu_fpalu" 4
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "ffarith,fadd,fccmp,fcvt,fcmp"))
-  "small_cpu_unit_i + small_cpu_unit_fpadd_pipe")
-
-(define_insn_reservation "small_cpu_fconst" 3
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "fconst"))
-  "small_cpu_unit_i + small_cpu_unit_fpadd_pipe")
-
-(define_insn_reservation "small_cpu_fpmuls" 4
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "fmul") (eq_attr "mode" "SF")))
-  "small_cpu_unit_i + small_cpu_unit_fpmul_pipe")
-
-(define_insn_reservation "small_cpu_fpmuld" 7
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "fmul") (eq_attr "mode" "DF")))
-  "small_cpu_unit_i + small_cpu_unit_fpmul_pipe, small_cpu_unit_fpmul_pipe * 2,\
-   small_cpu_unit_i + small_cpu_unit_fpmul_pipe")
-
-
-;;-------------------------------------------------------
-;; Floating-point Division
-;;-------------------------------------------------------
-
-;; Single-precision divide takes 14 cycles to complete, and this
-;; includes the time taken for the special instruction used to collect the
-;; result to travel down the multiply pipeline.
-
-(define_insn_reservation "small_cpu_fdivs" 14
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "fdiv,fsqrt") (eq_attr "mode" "SF")))
-  "small_cpu_unit_i, small_cpu_unit_fpdiv * 13")
-
-(define_insn_reservation "small_cpu_fdivd" 29
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "fdiv,fsqrt") (eq_attr "mode" "DF")))
-  "small_cpu_unit_i, small_cpu_unit_fpdiv * 28")
-
-
-;;-------------------------------------------------------
-;; Floating-point Transfers
-;;-------------------------------------------------------
-
-(define_insn_reservation "small_cpu_i2f" 4
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "fmovi2f"))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_f2i" 2
-  (and (eq_attr "tune" "small")
-       (eq_attr "v8type" "fmovf2i"))
-  "small_cpu_unit_i")
-
-
-;;-------------------------------------------------------
-;; Floating-point Load/Store
-;;-------------------------------------------------------
-
-(define_insn_reservation "small_cpu_floads" 4
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "fpsimd_load") (eq_attr "mode" "SF")))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_floadd" 5
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "fpsimd_load") (eq_attr "mode" "DF")))
-  "small_cpu_unit_i + small_cpu_unit_br, small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_fstores" 0
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "fpsimd_store") (eq_attr "mode" "SF")))
-  "small_cpu_unit_i")
-
-(define_insn_reservation "small_cpu_fstored" 0
-  (and (eq_attr "tune" "small")
-       (and (eq_attr "v8type" "fpsimd_store") (eq_attr "mode" "DF")))
-  "small_cpu_unit_i + small_cpu_unit_br, small_cpu_unit_i")
-
-
-;;-------------------------------------------------------
-;; Bypasses
-;;-------------------------------------------------------
-
-;; Forwarding path for unshifted operands.
-
-(define_bypass 1 "small_cpu_alu, small_cpu_alu_shift" 
-  "small_cpu_alu, small_cpu_alu_shift, small_cpu_logic, small_cpu_logic_shift, small_cpu_shift")
-
-(define_bypass 1 "small_cpu_logic, small_cpu_logic_shift" 
-  "small_cpu_alu, small_cpu_alu_shift, small_cpu_logic, small_cpu_logic_shift, small_cpu_shift")
-
-(define_bypass 1 "small_cpu_shift" 
-  "small_cpu_alu, small_cpu_alu_shift, small_cpu_logic, small_cpu_logic_shift, small_cpu_shift")
-
-;; Load-to-use for floating-point values has a penalty of one cycle.
-
-(define_bypass 2 "small_cpu_floads"
-                 "small_cpu_fpalu, small_cpu_fpmuld,\
-		  small_cpu_fdivs, small_cpu_fdivd,\
-		  small_cpu_f2i")
-
-(define_bypass 3 "small_cpu_floadd"
-                 "small_cpu_fpalu, small_cpu_fpmuld,\
-		  small_cpu_fdivs, small_cpu_fdivd,\
-		  small_cpu_f2i")
Index: b/src/gcc/config/aarch64/aarch64-simd.md
===================================================================
--- a/src/gcc/config/aarch64/aarch64-simd.md
+++ b/src/gcc/config/aarch64/aarch64-simd.md
@@ -18,298 +18,6 @@
 ;; along with GCC; see the file COPYING3.  If not see
 ;; <http://www.gnu.org/licenses/>.
 
-
-; Main data types used by the insntructions
-
-(define_attr "simd_mode" "unknown,none,V8QI,V16QI,V4HI,V8HI,V2SI,V4SI,V2DI,V2SF,V4SF,V2DF,OI,CI,XI,DI,DF,SI,SF,HI,QI"
-  (const_string "unknown"))
-
-
-; Classification of AdvSIMD instructions for scheduling purposes.
-; Do not set this attribute and the "v8type" attribute together in
-; any instruction pattern.
-
-; simd_abd              integer absolute difference and accumulate.
-; simd_abdl             integer absolute difference and accumulate (long).
-; simd_adal             integer add and accumulate (long).
-; simd_add              integer addition/subtraction.
-; simd_addl             integer addition/subtraction (long).
-; simd_addlv            across lanes integer sum (long).
-; simd_addn             integer addition/subtraction (narrow).
-; simd_addn2            integer addition/subtraction (narrow, high).
-; simd_addv             across lanes integer sum.
-; simd_cls              count leading sign/zero bits.
-; simd_cmp              compare / create mask.
-; simd_cnt              population count.
-; simd_dup              duplicate element.
-; simd_dupgp            duplicate general purpose register.
-; simd_ext              bitwise extract from pair.
-; simd_fadd             floating point add/sub.
-; simd_fcmp             floating point compare.
-; simd_fcvti            floating point convert to integer.
-; simd_fcvtl            floating-point convert upsize.
-; simd_fcvtn            floating-point convert downsize (narrow).
-; simd_fcvtn2           floating-point convert downsize (narrow, high).
-; simd_fdiv             floating point division.
-; simd_fminmax          floating point min/max.
-; simd_fminmaxv         across lanes floating point min/max.
-; simd_fmla             floating point multiply-add.
-; simd_fmla_elt         floating point multiply-add (by element).
-; simd_fmul             floating point multiply.
-; simd_fmul_elt         floating point multiply (by element).
-; simd_fnegabs          floating point neg/abs.
-; simd_frcpe            floating point reciprocal estimate.
-; simd_frcps            floating point reciprocal step.
-; simd_frecx            floating point reciprocal exponent.
-; simd_frint            floating point round to integer.
-; simd_fsqrt            floating point square root.
-; simd_icvtf            integer convert to floating point.
-; simd_ins              insert element.
-; simd_insgp            insert general purpose register.
-; simd_load1            load multiple structures to one register (LD1).
-; simd_load1r           load single structure to all lanes of one register (LD1R).
-; simd_load1s           load single structure to one lane of one register (LD1 [index]).
-; simd_load2            load multiple structures to two registers (LD1, LD2).
-; simd_load2r           load single structure to all lanes of two registers (LD1R, LD2R).
-; simd_load2s           load single structure to one lane of two registers (LD2 [index]).
-; simd_load3            load multiple structures to three registers (LD1, LD3).
-; simd_load3r           load single structure to all lanes of three registers (LD3R).
-; simd_load3s           load single structure to one lane of three registers (LD3 [index]).
-; simd_load4            load multiple structures to four registers (LD1, LD2, LD4).
-; simd_load4r           load single structure to all lanes of four registers (LD4R).
-; simd_load4s           load single structure to one lane of four registers (LD4 [index]).
-; simd_logic            logical operation.
-; simd_logic_imm        logcial operation (immediate).
-; simd_minmax           integer min/max.
-; simd_minmaxv          across lanes integer min/max,
-; simd_mla              integer multiply-accumulate.
-; simd_mla_elt          integer multiply-accumulate (by element).
-; simd_mlal             integer multiply-accumulate (long).
-; simd_mlal_elt         integer multiply-accumulate (by element, long).
-; simd_move             move register.
-; simd_move_imm         move immediate.
-; simd_movgp            move element to general purpose register.
-; simd_mul              integer multiply.
-; simd_mul_elt          integer multiply (by element).
-; simd_mull             integer multiply (long).
-; simd_mull_elt         integer multiply (by element, long).
-; simd_negabs           integer negate/absolute.
-; simd_rbit             bitwise reverse.
-; simd_rcpe             integer reciprocal estimate.
-; simd_rcps             integer reciprocal square root.
-; simd_rev              element reverse.
-; simd_sat_add          integer saturating addition/subtraction.
-; simd_sat_mlal         integer saturating multiply-accumulate (long).
-; simd_sat_mlal_elt     integer saturating multiply-accumulate (by element, long).
-; simd_sat_mul          integer saturating multiply.
-; simd_sat_mul_elt      integer saturating multiply (by element).
-; simd_sat_mull         integer saturating multiply (long).
-; simd_sat_mull_elt     integer saturating multiply (by element, long).
-; simd_sat_negabs       integer saturating negate/absolute.
-; simd_sat_shift        integer saturating shift.
-; simd_sat_shift_imm    integer saturating shift (immediate).
-; simd_sat_shiftn_imm   integer saturating shift (narrow, immediate).
-; simd_sat_shiftn2_imm  integer saturating shift (narrow, high, immediate).
-; simd_shift            shift register/vector.
-; simd_shift_acc        shift accumulate.
-; simd_shift_imm        shift immediate.
-; simd_shift_imm_acc    shift immediate and accumualte.
-; simd_shiftl           shift register/vector (long).
-; simd_shiftl_imm       shift register/vector (long, immediate).
-; simd_shiftn_imm       shift register/vector (narrow, immediate).
-; simd_shiftn2_imm      shift register/vector (narrow, high, immediate).
-; simd_store1           store multiple structures from one register (ST1).
-; simd_store1s          store single structure from one lane of one register (ST1 [index]).
-; simd_store2           store multiple structures from two registers (ST1, ST2).
-; simd_store2s          store single structure from one lane of two registers (ST2 [index]).
-; simd_store3           store multiple structures from three registers (ST1, ST3).
-; simd_store3s          store single structure from one lane of three register (ST3 [index]).
-; simd_store4           store multiple structures from four registers (ST1, ST2, ST4).
-; simd_store4s          store single structure from one lane for four registers (ST4 [index]).
-; simd_tbl              table lookup.
-; simd_trn              transpose.
-; simd_uzp              unzip.
-; simd_zip              zip.
-
-(define_attr "simd_type"
-   "simd_abd,\
-   simd_abdl,\
-   simd_adal,\
-   simd_add,\
-   simd_addl,\
-   simd_addlv,\
-   simd_addn,\
-   simd_addn2,\
-   simd_addv,\
-   simd_cls,\
-   simd_cmp,\
-   simd_cnt,\
-   simd_dup,\
-   simd_dupgp,\
-   simd_ext,\
-   simd_fadd,\
-   simd_fcmp,\
-   simd_fcvti,\
-   simd_fcvtl,\
-   simd_fcvtn,\
-   simd_fcvtn2,\
-   simd_fdiv,\
-   simd_fminmax,\
-   simd_fminmaxv,\
-   simd_fmla,\
-   simd_fmla_elt,\
-   simd_fmul,\
-   simd_fmul_elt,\
-   simd_fnegabs,\
-   simd_frcpe,\
-   simd_frcps,\
-   simd_frecx,\
-   simd_frint,\
-   simd_fsqrt,\
-   simd_icvtf,\
-   simd_ins,\
-   simd_insgp,\
-   simd_load1,\
-   simd_load1r,\
-   simd_load1s,\
-   simd_load2,\
-   simd_load2r,\
-   simd_load2s,\
-   simd_load3,\
-   simd_load3r,\
-   simd_load3s,\
-   simd_load4,\
-   simd_load4r,\
-   simd_load4s,\
-   simd_logic,\
-   simd_logic_imm,\
-   simd_minmax,\
-   simd_minmaxv,\
-   simd_mla,\
-   simd_mla_elt,\
-   simd_mlal,\
-   simd_mlal_elt,\
-   simd_movgp,\
-   simd_move,\
-   simd_move_imm,\
-   simd_mul,\
-   simd_mul_elt,\
-   simd_mull,\
-   simd_mull_elt,\
-   simd_negabs,\
-   simd_rbit,\
-   simd_rcpe,\
-   simd_rcps,\
-   simd_rev,\
-   simd_sat_add,\
-   simd_sat_mlal,\
-   simd_sat_mlal_elt,\
-   simd_sat_mul,\
-   simd_sat_mul_elt,\
-   simd_sat_mull,\
-   simd_sat_mull_elt,\
-   simd_sat_negabs,\
-   simd_sat_shift,\
-   simd_sat_shift_imm,\
-   simd_sat_shiftn_imm,\
-   simd_sat_shiftn2_imm,\
-   simd_shift,\
-   simd_shift_acc,\
-   simd_shift_imm,\
-   simd_shift_imm_acc,\
-   simd_shiftl,\
-   simd_shiftl_imm,\
-   simd_shiftn_imm,\
-   simd_shiftn2_imm,\
-   simd_store1,\
-   simd_store1s,\
-   simd_store2,\
-   simd_store2s,\
-   simd_store3,\
-   simd_store3s,\
-   simd_store4,\
-   simd_store4s,\
-   simd_tbl,\
-   simd_trn,\
-   simd_uzp,\
-   simd_zip,\
-   none"
-  (const_string "none"))
-
-
-; The "neon_type" attribute is used by the AArch32 backend.  Below is a mapping
-; from "simd_type" to "neon_type".
-
-(define_attr "neon_type"
-   "neon_int_1,neon_int_2,neon_int_3,neon_int_4,neon_int_5,neon_vqneg_vqabs,
-   neon_vmov,neon_vaba,neon_vsma,neon_vaba_qqq,
-   neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,neon_mul_qqq_8_16_32_ddd_32,
-   neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar,
-   neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,neon_mla_qqq_8_16,
-   neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long,
-   neon_mla_qqq_32_qqd_32_scalar,neon_mul_ddd_16_scalar_32_16_long_scalar,
-   neon_mul_qqd_32_scalar,neon_mla_ddd_16_scalar_qdd_32_16_long_scalar,
-   neon_shift_1,neon_shift_2,neon_shift_3,neon_vshl_ddd,
-   neon_vqshl_vrshl_vqrshl_qqq,neon_vsra_vrsra,neon_fp_vadd_ddd_vabs_dd,
-   neon_fp_vadd_qqq_vabs_qq,neon_fp_vsum,neon_fp_vmul_ddd,neon_fp_vmul_qqd,
-   neon_fp_vmla_ddd,neon_fp_vmla_qqq,neon_fp_vmla_ddd_scalar,
-   neon_fp_vmla_qqq_scalar,neon_fp_vrecps_vrsqrts_ddd,
-   neon_fp_vrecps_vrsqrts_qqq,neon_bp_simple,neon_bp_2cycle,neon_bp_3cycle,
-   neon_ldr,neon_str,neon_vld1_1_2_regs,neon_vld1_3_4_regs,
-   neon_vld2_2_regs_vld1_vld2_all_lanes,neon_vld2_4_regs,neon_vld3_vld4,
-   neon_vst1_1_2_regs_vst2_2_regs,neon_vst1_3_4_regs,
-   neon_vst2_4_regs_vst3_vst4,neon_vst3_vst4,neon_vld1_vld2_lane,
-   neon_vld3_vld4_lane,neon_vst1_vst2_lane,neon_vst3_vst4_lane,
-   neon_vld3_vld4_all_lanes,neon_mcr,neon_mcr_2_mcrr,neon_mrc,neon_mrrc,
-   neon_ldm_2,neon_stm_2,none,unknown"
-  (cond [
-	  (eq_attr "simd_type" "simd_dup") (const_string "neon_bp_simple")
-	  (eq_attr "simd_type" "simd_movgp") (const_string "neon_bp_simple")
-	  (eq_attr "simd_type" "simd_add,simd_logic,simd_logic_imm") (const_string "neon_int_1")
-	  (eq_attr "simd_type" "simd_negabs,simd_addlv") (const_string "neon_int_3")
-	  (eq_attr "simd_type" "simd_addn,simd_addn2,simd_addl,simd_sat_add,simd_sat_negabs") (const_string "neon_int_4")
-	  (eq_attr "simd_type" "simd_move") (const_string "neon_vmov")
-	  (eq_attr "simd_type" "simd_ins") (const_string "neon_mcr")
-	  (and (eq_attr "simd_type" "simd_mul,simd_sat_mul") (eq_attr "simd_mode" "V8QI,V4HI")) (const_string "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long")
-	  (and (eq_attr "simd_type" "simd_mul,simd_sat_mul") (eq_attr "simd_mode" "V2SI,V8QI,V16QI,V2SI")) (const_string "neon_mul_qqq_8_16_32_ddd_32")
-	  (and (eq_attr "simd_type" "simd_mull,simd_sat_mull") (eq_attr "simd_mode" "V8QI,V16QI,V4HI,V8HI")) (const_string "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long")
-	  (and (eq_attr "simd_type" "simd_mull,simd_sat_mull") (eq_attr "simd_mode" "V2SI,V4SI,V2DI")) (const_string "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")
-	  (and (eq_attr "simd_type" "simd_mla,simd_sat_mlal") (eq_attr "simd_mode" "V8QI,V4HI")) (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-	  (and (eq_attr "simd_type" "simd_mla,simd_sat_mlal") (eq_attr "simd_mode" "V2SI")) (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")
-	  (and (eq_attr "simd_type" "simd_mla,simd_sat_mlal") (eq_attr "simd_mode" "V16QI,V8HI")) (const_string "neon_mla_qqq_8_16")
-	  (and (eq_attr "simd_type" "simd_mla,simd_sat_mlal") (eq_attr "simd_mode" "V4SI")) (const_string "neon_mla_qqq_32_qqd_32_scalar")
-	  (and (eq_attr "simd_type" "simd_mlal") (eq_attr "simd_mode" "V8QI,V16QI,V4HI,V8HI")) (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-	  (and (eq_attr "simd_type" "simd_mlal") (eq_attr "simd_mode" "V2SI,V4SI,V2DI")) (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")
-	  (and (eq_attr "simd_type" "simd_fmla") (eq_attr "simd_mode" "V2SF")) (const_string "neon_fp_vmla_ddd")
-	  (and (eq_attr "simd_type" "simd_fmla") (eq_attr "simd_mode" "V4SF,V2DF")) (const_string "neon_fp_vmla_qqq")
-	  (and (eq_attr "simd_type" "simd_fmla_elt") (eq_attr "simd_mode" "V2SF")) (const_string "neon_fp_vmla_ddd_scalar")
-	  (and (eq_attr "simd_type" "simd_fmla_elt") (eq_attr "simd_mode" "V4SF,V2DF")) (const_string "neon_fp_vmla_qqq_scalar")
-	  (and (eq_attr "simd_type" "simd_fmul,simd_fmul_elt,simd_fdiv,simd_fsqrt") (eq_attr "simd_mode" "V2SF")) (const_string "neon_fp_vmul_ddd")
-	  (and (eq_attr "simd_type" "simd_fmul,simd_fmul_elt,simd_fdiv,simd_fsqrt") (eq_attr "simd_mode" "V4SF,V2DF")) (const_string "neon_fp_vmul_qqd")
-	  (and (eq_attr "simd_type" "simd_fadd") (eq_attr "simd_mode" "V2SF")) (const_string "neon_fp_vadd_ddd_vabs_dd")
-	  (and (eq_attr "simd_type" "simd_fadd") (eq_attr "simd_mode" "V4SF,V2DF")) (const_string "neon_fp_vadd_qqq_vabs_qq")
-	  (and (eq_attr "simd_type" "simd_fnegabs,simd_fminmax,simd_fminmaxv") (eq_attr "simd_mode" "V2SF")) (const_string "neon_fp_vadd_ddd_vabs_dd")
-	  (and (eq_attr "simd_type" "simd_fnegabs,simd_fminmax,simd_fminmaxv") (eq_attr "simd_mode" "V4SF,V2DF")) (const_string "neon_fp_vadd_qqq_vabs_qq")
-	  (and (eq_attr "simd_type" "simd_shift,simd_shift_acc") (eq_attr "simd_mode" "V8QI,V4HI,V2SI")) (const_string "neon_vshl_ddd")
-	  (and (eq_attr "simd_type" "simd_shift,simd_shift_acc") (eq_attr "simd_mode" "V16QI,V8HI,V4SI,V2DI")) (const_string "neon_shift_3")
-	  (eq_attr "simd_type" "simd_minmax,simd_minmaxv") (const_string "neon_int_5")
-	  (eq_attr "simd_type" "simd_shiftn_imm,simd_shiftn2_imm,simd_shiftl_imm,") (const_string "neon_shift_1")
-	  (eq_attr "simd_type" "simd_load1,simd_load2") (const_string "neon_vld1_1_2_regs")
-	  (eq_attr "simd_type" "simd_load3,simd_load3") (const_string "neon_vld1_3_4_regs")
-	  (eq_attr "simd_type" "simd_load1r,simd_load2r,simd_load3r,simd_load4r") (const_string "neon_vld2_2_regs_vld1_vld2_all_lanes")
-	  (eq_attr "simd_type" "simd_load1s,simd_load2s") (const_string "neon_vld1_vld2_lane")
-	  (eq_attr "simd_type" "simd_load3s,simd_load4s") (const_string "neon_vld3_vld4_lane")
-	  (eq_attr "simd_type" "simd_store1,simd_store2") (const_string "neon_vst1_1_2_regs_vst2_2_regs")
-	  (eq_attr "simd_type" "simd_store3,simd_store4") (const_string "neon_vst1_3_4_regs")
-	  (eq_attr "simd_type" "simd_store1s,simd_store2s") (const_string "neon_vst1_vst2_lane")
-	  (eq_attr "simd_type" "simd_store3s,simd_store4s") (const_string "neon_vst3_vst4_lane")
-	  (and (eq_attr "simd_type" "simd_frcpe,simd_frcps") (eq_attr "simd_mode" "V2SF")) (const_string "neon_fp_vrecps_vrsqrts_ddd")
-	  (and (eq_attr "simd_type" "simd_frcpe,simd_frcps") (eq_attr "simd_mode" "V4SF,V2DF")) (const_string "neon_fp_vrecps_vrsqrts_qqq")
-	  (eq_attr "simd_type" "none") (const_string "none")
-  ]
-  (const_string "unknown")))
-
-
 (define_expand "mov<mode>"
   [(set (match_operand:VALL 0 "aarch64_simd_nonimmediate_operand" "")
 	(match_operand:VALL 1 "aarch64_simd_general_operand" ""))]
@@ -338,8 +46,7 @@
         (vec_duplicate:VDQ (match_operand:<VEL> 1 "register_operand" "r")))]
   "TARGET_SIMD"
   "dup\\t%0.<Vtype>, %<vw>1"
-  [(set_attr "simd_type" "simd_dupgp")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_from_gp<q>")]
 )
 
 (define_insn "aarch64_dup_lane<mode>"
@@ -351,20 +58,7 @@
           )))]
   "TARGET_SIMD"
   "dup\\t%<v>0<Vmtype>, %1.<Vetype>[%2]"
-  [(set_attr "simd_type" "simd_dup")
-   (set_attr "simd_mode" "<MODE>")]
-)
-
-(define_insn "aarch64_dup_lane<mode>"
-  [(set (match_operand:SDQ_I 0 "register_operand" "=w")
-	(vec_select:<VEL>
-	  (match_operand:<VCON> 1 "register_operand" "w")
-	  (parallel [(match_operand:SI 2 "immediate_operand" "i")])
-        ))]
-  "TARGET_SIMD"
-  "dup\\t%<v>0<Vmtype>, %1.<Vetype>[%2]"
-  [(set_attr "simd_type" "simd_dup")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_dup<q>")]
 )
 
 (define_insn "aarch64_simd_dup<mode>"
@@ -372,8 +66,7 @@
         (vec_duplicate:VDQF (match_operand:<VEL> 1 "register_operand" "w")))]
   "TARGET_SIMD"
   "dup\\t%0.<Vtype>, %1.<Vetype>[0]"
-  [(set_attr "simd_type" "simd_dup")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_dup<q>")]
 )
 
 (define_insn "*aarch64_simd_mov<mode>"
@@ -394,13 +87,14 @@
      case 4: return "ins\t%0.d[0], %1";
      case 5: return "mov\t%0, %1";
      case 6:
-	return aarch64_output_simd_mov_immediate (&operands[1],
+	return aarch64_output_simd_mov_immediate (operands[1],
 						  <MODE>mode, 64);
      default: gcc_unreachable ();
      }
 }
-  [(set_attr "simd_type" "simd_load1,simd_store1,simd_move,simd_movgp,simd_insgp,simd_move,simd_move_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_load1_1reg<q>, neon_store1_1reg<q>,\
+                     neon_logic<q>, neon_to_gp<q>, neon_from_gp<q>,\
+                     mov_reg, neon_move<q>")]
 )
 
 (define_insn "*aarch64_simd_mov<mode>"
@@ -414,20 +108,25 @@
 {
   switch (which_alternative)
     {
-    case 0: return "ld1\t{%0.<Vtype>}, %1";
-    case 1: return "st1\t{%1.<Vtype>}, %0";
-    case 2: return "orr\t%0.<Vbtype>, %1.<Vbtype>, %1.<Vbtype>";
-    case 3: return "umov\t%0, %1.d[0]\;umov\t%H0, %1.d[1]";
-    case 4: return "ins\t%0.d[0], %1\;ins\t%0.d[1], %H1";
-    case 5: return "#";
+    case 0:
+	return "ld1\t{%0.<Vtype>}, %1";
+    case 1:
+	return "st1\t{%1.<Vtype>}, %0";
+    case 2:
+	return "orr\t%0.<Vbtype>, %1.<Vbtype>, %1.<Vbtype>";
+    case 3:
+    case 4:
+    case 5:
+	return "#";
     case 6:
-	return aarch64_output_simd_mov_immediate (&operands[1],
-						  <MODE>mode, 128);
-    default: gcc_unreachable ();
+	return aarch64_output_simd_mov_immediate (operands[1], <MODE>mode, 128);
+    default:
+	gcc_unreachable ();
     }
 }
-  [(set_attr "simd_type" "simd_load1,simd_store1,simd_move,simd_movgp,simd_insgp,simd_move,simd_move_imm")
-   (set_attr "simd_mode" "<MODE>")
+  [(set_attr "type" "neon_load1_1reg<q>, neon_store1_1reg<q>,\
+                     neon_logic<q>, multiple, multiple, multiple,\
+                     neon_move<q>")
    (set_attr "length" "4,4,4,8,8,8,4")]
 )
 
@@ -452,14 +151,82 @@
   aarch64_simd_disambiguate_copy (operands, dest, src, 2);
 })
 
+(define_split
+  [(set (match_operand:VQ 0 "register_operand" "")
+        (match_operand:VQ 1 "register_operand" ""))]
+  "TARGET_SIMD && reload_completed
+   && ((FP_REGNUM_P (REGNO (operands[0])) && GP_REGNUM_P (REGNO (operands[1])))
+       || (GP_REGNUM_P (REGNO (operands[0])) && FP_REGNUM_P (REGNO (operands[1]))))"
+  [(const_int 0)]
+{
+  aarch64_split_simd_move (operands[0], operands[1]);
+  DONE;
+})
+
+(define_expand "aarch64_split_simd_mov<mode>"
+  [(set (match_operand:VQ 0)
+        (match_operand:VQ 1))]
+  "TARGET_SIMD"
+  {
+    rtx dst = operands[0];
+    rtx src = operands[1];
+
+    if (GP_REGNUM_P (REGNO (src)))
+      {
+        rtx src_low_part = gen_lowpart (<VHALF>mode, src);
+        rtx src_high_part = gen_highpart (<VHALF>mode, src);
+
+        emit_insn
+          (gen_move_lo_quad_<mode> (dst, src_low_part));
+        emit_insn
+          (gen_move_hi_quad_<mode> (dst, src_high_part));
+      }
+
+    else
+      {
+        rtx dst_low_part = gen_lowpart (<VHALF>mode, dst);
+        rtx dst_high_part = gen_highpart (<VHALF>mode, dst);
+        rtx lo = aarch64_simd_vect_par_cnst_half (<MODE>mode, false);
+        rtx hi = aarch64_simd_vect_par_cnst_half (<MODE>mode, true);
+
+        emit_insn
+          (gen_aarch64_simd_mov_from_<mode>low (dst_low_part, src, lo));
+        emit_insn
+          (gen_aarch64_simd_mov_from_<mode>high (dst_high_part, src, hi));
+      }
+    DONE;
+  }
+)
+
+(define_insn "aarch64_simd_mov_from_<mode>low"
+  [(set (match_operand:<VHALF> 0 "register_operand" "=r")
+        (vec_select:<VHALF>
+          (match_operand:VQ 1 "register_operand" "w")
+          (match_operand:VQ 2 "vect_par_cnst_lo_half" "")))]
+  "TARGET_SIMD && reload_completed"
+  "umov\t%0, %1.d[0]"
+  [(set_attr "type" "neon_to_gp<q>")
+   (set_attr "length" "4")
+  ])
+
+(define_insn "aarch64_simd_mov_from_<mode>high"
+  [(set (match_operand:<VHALF> 0 "register_operand" "=r")
+        (vec_select:<VHALF>
+          (match_operand:VQ 1 "register_operand" "w")
+          (match_operand:VQ 2 "vect_par_cnst_hi_half" "")))]
+  "TARGET_SIMD && reload_completed"
+  "umov\t%0, %1.d[1]"
+  [(set_attr "type" "neon_to_gp<q>")
+   (set_attr "length" "4")
+  ])
+
 (define_insn "orn<mode>3"
  [(set (match_operand:VDQ 0 "register_operand" "=w")
        (ior:VDQ (not:VDQ (match_operand:VDQ 1 "register_operand" "w"))
 		(match_operand:VDQ 2 "register_operand" "w")))]
  "TARGET_SIMD"
  "orn\t%0.<Vbtype>, %2.<Vbtype>, %1.<Vbtype>"
-  [(set_attr "simd_type" "simd_logic")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 (define_insn "bic<mode>3"
@@ -468,8 +235,7 @@
 		(match_operand:VDQ 2 "register_operand" "w")))]
  "TARGET_SIMD"
  "bic\t%0.<Vbtype>, %2.<Vbtype>, %1.<Vbtype>"
-  [(set_attr "simd_type" "simd_logic")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 (define_insn "add<mode>3"
@@ -478,8 +244,7 @@
 		  (match_operand:VDQ 2 "register_operand" "w")))]
   "TARGET_SIMD"
   "add\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_add")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_add<q>")]
 )
 
 (define_insn "sub<mode>3"
@@ -488,8 +253,7 @@
 		   (match_operand:VDQ 2 "register_operand" "w")))]
   "TARGET_SIMD"
   "sub\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_add")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sub<q>")]
 )
 
 (define_insn "mul<mode>3"
@@ -498,17 +262,15 @@
 		   (match_operand:VDQM 2 "register_operand" "w")))]
   "TARGET_SIMD"
   "mul\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_mul_<Vetype><q>")]
 )
 
 (define_insn "neg<mode>2"
-  [(set (match_operand:VDQM 0 "register_operand" "=w")
-        (neg:VDQM (match_operand:VDQM 1 "register_operand" "w")))]
+  [(set (match_operand:VDQ 0 "register_operand" "=w")
+	(neg:VDQ (match_operand:VDQ 1 "register_operand" "w")))]
   "TARGET_SIMD"
   "neg\t%0.<Vtype>, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_negabs")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_neg<q>")]
 )
 
 (define_insn "abs<mode>2"
@@ -516,8 +278,48 @@
         (abs:VDQ (match_operand:VDQ 1 "register_operand" "w")))]
   "TARGET_SIMD"
   "abs\t%0.<Vtype>, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_negabs")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_abs<q>")]
+)
+
+(define_insn "abd<mode>_3"
+  [(set (match_operand:VDQ_BHSI 0 "register_operand" "=w")
+	(abs:VDQ_BHSI (minus:VDQ_BHSI
+		       (match_operand:VDQ_BHSI 1 "register_operand" "w")
+		       (match_operand:VDQ_BHSI 2 "register_operand" "w"))))]
+  "TARGET_SIMD"
+  "sabd\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
+  [(set_attr "type" "neon_abd<q>")]
+)
+
+(define_insn "aba<mode>_3"
+  [(set (match_operand:VDQ_BHSI 0 "register_operand" "=w")
+	(plus:VDQ_BHSI (abs:VDQ_BHSI (minus:VDQ_BHSI
+			 (match_operand:VDQ_BHSI 1 "register_operand" "w")
+			 (match_operand:VDQ_BHSI 2 "register_operand" "w")))
+		       (match_operand:VDQ_BHSI 3 "register_operand" "0")))]
+  "TARGET_SIMD"
+  "saba\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
+  [(set_attr "type" "neon_arith_acc<q>")]
+)
+
+(define_insn "fabd<mode>_3"
+  [(set (match_operand:VDQF 0 "register_operand" "=w")
+	(abs:VDQF (minus:VDQF
+		   (match_operand:VDQF 1 "register_operand" "w")
+		   (match_operand:VDQF 2 "register_operand" "w"))))]
+  "TARGET_SIMD"
+  "fabd\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
+  [(set_attr "type" "neon_fp_abd_<Vetype><q>")]
+)
+
+(define_insn "*fabd_scalar<mode>3"
+  [(set (match_operand:GPF 0 "register_operand" "=w")
+        (abs:GPF (minus:GPF
+                 (match_operand:GPF 1 "register_operand" "w")
+                 (match_operand:GPF 2 "register_operand" "w"))))]
+  "TARGET_SIMD"
+  "fabd\t%<s>0, %<s>1, %<s>2"
+  [(set_attr "type" "neon_fp_abd_<Vetype><q>")]
 )
 
 (define_insn "and<mode>3"
@@ -526,8 +328,7 @@
 		 (match_operand:VDQ 2 "register_operand" "w")))]
   "TARGET_SIMD"
   "and\t%0.<Vbtype>, %1.<Vbtype>, %2.<Vbtype>"
-  [(set_attr "simd_type" "simd_logic")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 (define_insn "ior<mode>3"
@@ -536,8 +337,7 @@
 		 (match_operand:VDQ 2 "register_operand" "w")))]
   "TARGET_SIMD"
   "orr\t%0.<Vbtype>, %1.<Vbtype>, %2.<Vbtype>"
-  [(set_attr "simd_type" "simd_logic")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 (define_insn "xor<mode>3"
@@ -546,8 +346,7 @@
 		 (match_operand:VDQ 2 "register_operand" "w")))]
   "TARGET_SIMD"
   "eor\t%0.<Vbtype>, %1.<Vbtype>, %2.<Vbtype>"
-  [(set_attr "simd_type" "simd_logic")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 (define_insn "one_cmpl<mode>2"
@@ -555,8 +354,7 @@
         (not:VDQ (match_operand:VDQ 1 "register_operand" "w")))]
   "TARGET_SIMD"
   "not\t%0.<Vbtype>, %1.<Vbtype>"
-  [(set_attr "simd_type" "simd_logic")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 (define_insn "aarch64_simd_vec_set<mode>"
@@ -568,8 +366,7 @@
 	    (match_operand:SI 2 "immediate_operand" "i")))]
   "TARGET_SIMD"
   "ins\t%0.<Vetype>[%p2], %w1";
-  [(set_attr "simd_type" "simd_insgp")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_from_gp<q>")]
 )
 
 (define_insn "aarch64_simd_lshr<mode>"
@@ -578,8 +375,7 @@
 		     (match_operand:VDQ  2 "aarch64_simd_rshift_imm" "Dr")))]
  "TARGET_SIMD"
  "ushr\t%0.<Vtype>, %1.<Vtype>, %2"
-  [(set_attr "simd_type" "simd_shift_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_imm<q>")]
 )
 
 (define_insn "aarch64_simd_ashr<mode>"
@@ -588,8 +384,7 @@
 		     (match_operand:VDQ  2 "aarch64_simd_rshift_imm" "Dr")))]
  "TARGET_SIMD"
  "sshr\t%0.<Vtype>, %1.<Vtype>, %2"
-  [(set_attr "simd_type" "simd_shift_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_imm<q>")]
 )
 
 (define_insn "aarch64_simd_imm_shl<mode>"
@@ -598,8 +393,7 @@
 		   (match_operand:VDQ  2 "aarch64_simd_lshift_imm" "Dl")))]
  "TARGET_SIMD"
   "shl\t%0.<Vtype>, %1.<Vtype>, %2"
-  [(set_attr "simd_type" "simd_shift_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_imm<q>")]
 )
 
 (define_insn "aarch64_simd_reg_sshl<mode>"
@@ -608,8 +402,7 @@
 		   (match_operand:VDQ 2 "register_operand" "w")))]
  "TARGET_SIMD"
  "sshl\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_shift")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_reg<q>")]
 )
 
 (define_insn "aarch64_simd_reg_shl<mode>_unsigned"
@@ -619,8 +412,7 @@
 		   UNSPEC_ASHIFT_UNSIGNED))]
  "TARGET_SIMD"
  "ushl\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_shift")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_reg<q>")]
 )
 
 (define_insn "aarch64_simd_reg_shl<mode>_signed"
@@ -630,8 +422,7 @@
 		   UNSPEC_ASHIFT_SIGNED))]
  "TARGET_SIMD"
  "sshl\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_shift")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_reg<q>")]
 )
 
 (define_expand "ashl<mode>3"
@@ -837,8 +628,7 @@
 	    (match_operand:SI 2 "immediate_operand" "i")))]
   "TARGET_SIMD"
   "ins\t%0.d[%p2], %1";
-  [(set_attr "simd_type" "simd_insgp")
-   (set_attr "simd_mode" "V2DI")]
+  [(set_attr "type" "neon_from_gp")]
 )
 
 (define_expand "vec_setv2di"
@@ -863,8 +653,7 @@
 	    (match_operand:SI 2 "immediate_operand" "i")))]
   "TARGET_SIMD"
   "ins\t%0.<Vetype>[%p2], %1.<Vetype>[0]";
-  [(set_attr "simd_type" "simd_ins")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_ins<q>")]
 )
 
 (define_expand "vec_set<mode>"
@@ -888,8 +677,7 @@
 		  (match_operand:VQ_S 1 "register_operand" "0")))]
  "TARGET_SIMD"
  "mla\t%0.<Vtype>, %2.<Vtype>, %3.<Vtype>"
-  [(set_attr "simd_type" "simd_mla")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_mla_<Vetype><q>")]
 )
 
 (define_insn "aarch64_mls<mode>"
@@ -899,47 +687,52 @@
 			      (match_operand:VQ_S 3 "register_operand" "w"))))]
  "TARGET_SIMD"
  "mls\t%0.<Vtype>, %2.<Vtype>, %3.<Vtype>"
-  [(set_attr "simd_type" "simd_mla")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_mla_<Vetype><q>")]
 )
 
 ;; Max/Min operations.
-(define_insn "<maxmin><mode>3"
+(define_insn "<su><maxmin><mode>3"
  [(set (match_operand:VQ_S 0 "register_operand" "=w")
        (MAXMIN:VQ_S (match_operand:VQ_S 1 "register_operand" "w")
 		    (match_operand:VQ_S 2 "register_operand" "w")))]
  "TARGET_SIMD"
- "<maxmin>\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_minmax")
-   (set_attr "simd_mode" "<MODE>")]
+ "<su><maxmin>\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
+  [(set_attr "type" "neon_minmax<q>")]
 )
 
 ;; Move into low-half clearing high half to 0.
 
 (define_insn "move_lo_quad_<mode>"
-  [(set (match_operand:VQ 0 "register_operand" "=w")
+  [(set (match_operand:VQ 0 "register_operand" "=w,w,w")
         (vec_concat:VQ
-	  (match_operand:<VHALF> 1 "register_operand" "w")
+	  (match_operand:<VHALF> 1 "register_operand" "w,r,r")
 	  (vec_duplicate:<VHALF> (const_int 0))))]
   "TARGET_SIMD"
-  "mov\\t%d0, %d1";
-  [(set_attr "simd_type" "simd_dup")
-   (set_attr "simd_mode" "<MODE>")]
+  "@
+   dup\\t%d0, %1.d[0]
+   fmov\\t%d0, %1
+   dup\\t%d0, %1"
+  [(set_attr "type" "neon_dup<q>,fmov,neon_dup<q>")
+   (set_attr "simd" "yes,*,yes")
+   (set_attr "fp" "*,yes,*")
+   (set_attr "length" "4")]
 )
 
 ;; Move into high-half.
 
 (define_insn "aarch64_simd_move_hi_quad_<mode>"
-  [(set (match_operand:VQ 0 "register_operand" "+w")
+  [(set (match_operand:VQ 0 "register_operand" "+w,w")
         (vec_concat:VQ
           (vec_select:<VHALF>
                 (match_dup 0)
                 (match_operand:VQ 2 "vect_par_cnst_lo_half" ""))
-	  (match_operand:<VHALF> 1 "register_operand" "w")))]
+	  (match_operand:<VHALF> 1 "register_operand" "w,r")))]
   "TARGET_SIMD"
-  "ins\\t%0.d[1], %1.d[0]";
-  [(set_attr "simd_type" "simd_ins")
-   (set_attr "simd_mode" "<MODE>")]
+  "@
+   ins\\t%0.d[1], %1.d[0]
+   ins\\t%0.d[1], %1"
+  [(set_attr "type" "neon_ins")
+   (set_attr "length" "4")]
 )
 
 (define_expand "move_hi_quad_<mode>"
@@ -961,8 +754,7 @@
        (truncate:<VNARROWQ> (match_operand:VQN 1 "register_operand" "w")))]
  "TARGET_SIMD"
  "xtn\\t%0.<Vntype>, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_shiftn_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_imm_narrow_q")]
 )
 
 (define_expand "vec_pack_trunc_<mode>"
@@ -988,8 +780,7 @@
 	 (truncate:<VNARROWQ> (match_operand:VQN 2 "register_operand" "w"))))]
  "TARGET_SIMD"
  "xtn\\t%0.<Vntype>, %1.<Vtype>\;xtn2\\t%0.<V2ntype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_shiftn2_imm")
-   (set_attr "simd_mode" "<MODE>")
+  [(set_attr "type" "multiple")
    (set_attr "length" "8")]
 )
 
@@ -1003,8 +794,7 @@
 			    )))]
   "TARGET_SIMD"
   "<su>shll %0.<Vwtype>, %1.<Vhalftype>, 0"
-  [(set_attr "simd_type" "simd_shiftl_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_imm_long")]
 )
 
 (define_insn "aarch64_simd_vec_unpack<su>_hi_<mode>"
@@ -1015,8 +805,7 @@
 			    )))]
   "TARGET_SIMD"
   "<su>shll2 %0.<Vwtype>, %1.<Vtype>, 0"
-  [(set_attr "simd_type" "simd_shiftl_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_imm_long")]
 )
 
 (define_expand "vec_unpack<su>_hi_<mode>"
@@ -1045,6 +834,98 @@
 
 ;; Widening arithmetic.
 
+(define_insn "*aarch64_<su>mlal_lo<mode>"
+  [(set (match_operand:<VWIDE> 0 "register_operand" "=w")
+        (plus:<VWIDE>
+          (mult:<VWIDE>
+              (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>
+                 (match_operand:VQW 2 "register_operand" "w")
+                 (match_operand:VQW 3 "vect_par_cnst_lo_half" "")))
+              (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>
+                 (match_operand:VQW 4 "register_operand" "w")
+                 (match_dup 3))))
+          (match_operand:<VWIDE> 1 "register_operand" "0")))]
+  "TARGET_SIMD"
+  "<su>mlal\t%0.<Vwtype>, %2.<Vhalftype>, %4.<Vhalftype>"
+  [(set_attr "type" "neon_mla_<Vetype>_long")]
+)
+
+(define_insn "*aarch64_<su>mlal_hi<mode>"
+  [(set (match_operand:<VWIDE> 0 "register_operand" "=w")
+        (plus:<VWIDE>
+          (mult:<VWIDE>
+              (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>
+                 (match_operand:VQW 2 "register_operand" "w")
+                 (match_operand:VQW 3 "vect_par_cnst_hi_half" "")))
+              (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>
+                 (match_operand:VQW 4 "register_operand" "w")
+                 (match_dup 3))))
+          (match_operand:<VWIDE> 1 "register_operand" "0")))]
+  "TARGET_SIMD"
+  "<su>mlal2\t%0.<Vwtype>, %2.<Vtype>, %4.<Vtype>"
+  [(set_attr "type" "neon_mla_<Vetype>_long")]
+)
+
+(define_insn "*aarch64_<su>mlsl_lo<mode>"
+  [(set (match_operand:<VWIDE> 0 "register_operand" "=w")
+        (minus:<VWIDE>
+          (match_operand:<VWIDE> 1 "register_operand" "0")
+          (mult:<VWIDE>
+              (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>
+                 (match_operand:VQW 2 "register_operand" "w")
+                 (match_operand:VQW 3 "vect_par_cnst_lo_half" "")))
+              (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>
+                 (match_operand:VQW 4 "register_operand" "w")
+                 (match_dup 3))))))]
+  "TARGET_SIMD"
+  "<su>mlsl\t%0.<Vwtype>, %2.<Vhalftype>, %4.<Vhalftype>"
+  [(set_attr "type" "neon_mla_<Vetype>_long")]
+)
+
+(define_insn "*aarch64_<su>mlsl_hi<mode>"
+  [(set (match_operand:<VWIDE> 0 "register_operand" "=w")
+        (minus:<VWIDE>
+          (match_operand:<VWIDE> 1 "register_operand" "0")
+          (mult:<VWIDE>
+              (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>
+                 (match_operand:VQW 2 "register_operand" "w")
+                 (match_operand:VQW 3 "vect_par_cnst_hi_half" "")))
+              (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>
+                 (match_operand:VQW 4 "register_operand" "w")
+                 (match_dup 3))))))]
+  "TARGET_SIMD"
+  "<su>mlsl2\t%0.<Vwtype>, %2.<Vtype>, %4.<Vtype>"
+  [(set_attr "type" "neon_mla_<Vetype>_long")]
+)
+
+(define_insn "*aarch64_<su>mlal<mode>"
+  [(set (match_operand:<VWIDE> 0 "register_operand" "=w")
+        (plus:<VWIDE>
+          (mult:<VWIDE>
+            (ANY_EXTEND:<VWIDE>
+              (match_operand:VDW 1 "register_operand" "w"))
+            (ANY_EXTEND:<VWIDE>
+              (match_operand:VDW 2 "register_operand" "w")))
+          (match_operand:<VWIDE> 3 "register_operand" "0")))]
+  "TARGET_SIMD"
+  "<su>mlal\t%0.<Vwtype>, %1.<Vtype>, %2.<Vtype>"
+  [(set_attr "type" "neon_mla_<Vetype>_long")]
+)
+
+(define_insn "*aarch64_<su>mlsl<mode>"
+  [(set (match_operand:<VWIDE> 0 "register_operand" "=w")
+        (minus:<VWIDE>
+          (match_operand:<VWIDE> 1 "register_operand" "0")
+          (mult:<VWIDE>
+            (ANY_EXTEND:<VWIDE>
+              (match_operand:VDW 2 "register_operand" "w"))
+            (ANY_EXTEND:<VWIDE>
+              (match_operand:VDW 3 "register_operand" "w")))))]
+  "TARGET_SIMD"
+  "<su>mlsl\t%0.<Vwtype>, %2.<Vtype>, %3.<Vtype>"
+  [(set_attr "type" "neon_mla_<Vetype>_long")]
+)
+
 (define_insn "aarch64_simd_vec_<su>mult_lo_<mode>"
  [(set (match_operand:<VWIDE> 0 "register_operand" "=w")
        (mult:<VWIDE> (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>
@@ -1055,8 +936,7 @@
                            (match_dup 3)))))]
   "TARGET_SIMD"
   "<su>mull\\t%0.<Vwtype>, %1.<Vhalftype>, %2.<Vhalftype>"
-  [(set_attr "simd_type" "simd_mull")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_mul_<Vetype>_long")]
 )
 
 (define_expand "vec_widen_<su>mult_lo_<mode>"
@@ -1083,8 +963,7 @@
 			    (match_dup 3)))))]
   "TARGET_SIMD"
   "<su>mull2\\t%0.<Vwtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_mull")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_mul_<Vetype>_long")]
 )
 
 (define_expand "vec_widen_<su>mult_hi_<mode>"
@@ -1133,8 +1012,7 @@
 		  (match_operand:VDQF 2 "register_operand" "w")))]
  "TARGET_SIMD"
  "fadd\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_fadd")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_addsub_<Vetype><q>")]
 )
 
 (define_insn "sub<mode>3"
@@ -1143,8 +1021,7 @@
 		   (match_operand:VDQF 2 "register_operand" "w")))]
  "TARGET_SIMD"
  "fsub\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_fadd")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_addsub_<Vetype><q>")]
 )
 
 (define_insn "mul<mode>3"
@@ -1153,8 +1030,7 @@
 		  (match_operand:VDQF 2 "register_operand" "w")))]
  "TARGET_SIMD"
  "fmul\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_fmul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_mul_<Vetype><q>")]
 )
 
 (define_insn "div<mode>3"
@@ -1163,8 +1039,7 @@
 		 (match_operand:VDQF 2 "register_operand" "w")))]
  "TARGET_SIMD"
  "fdiv\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_fdiv")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_div_<Vetype><q>")]
 )
 
 (define_insn "neg<mode>2"
@@ -1172,8 +1047,7 @@
        (neg:VDQF (match_operand:VDQF 1 "register_operand" "w")))]
  "TARGET_SIMD"
  "fneg\\t%0.<Vtype>, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_fnegabs")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_neg_<Vetype><q>")]
 )
 
 (define_insn "abs<mode>2"
@@ -1181,8 +1055,7 @@
        (abs:VDQF (match_operand:VDQF 1 "register_operand" "w")))]
  "TARGET_SIMD"
  "fabs\\t%0.<Vtype>, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_fnegabs")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_abs_<Vetype><q>")]
 )
 
 (define_insn "fma<mode>4"
@@ -1192,50 +1065,161 @@
                 (match_operand:VDQF 3 "register_operand" "0")))]
   "TARGET_SIMD"
  "fmla\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_fmla")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_mla_<Vetype><q>")]
 )
 
-(define_insn "aarch64_frint<frint_suffix><mode>"
+;; Vector versions of the floating-point frint patterns.
+;; Expands to btrunc, ceil, floor, nearbyint, rint, round.
+(define_insn "<frint_pattern><mode>2"
   [(set (match_operand:VDQF 0 "register_operand" "=w")
 	(unspec:VDQF [(match_operand:VDQF 1 "register_operand" "w")]
 		      FRINT))]
   "TARGET_SIMD"
   "frint<frint_suffix>\\t%0.<Vtype>, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_frint")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_round_<Vetype><q>")]
 )
 
-;; Vector versions of the floating-point frint patterns.
-;; Expands to btrunc, ceil, floor, nearbyint, rint, round.
-(define_expand "<frint_pattern><mode>2"
-  [(set (match_operand:VDQF 0 "register_operand")
-	(unspec:VDQF [(match_operand:VDQF 1 "register_operand")]
-		      FRINT))]
-  "TARGET_SIMD"
-  {})
-
-(define_insn "aarch64_fcvt<frint_suffix><su><mode>"
+;; Vector versions of the fcvt standard patterns.
+;; Expands to lbtrunc, lround, lceil, lfloor
+(define_insn "l<fcvt_pattern><su_optab><VDQF:mode><fcvt_target>2"
   [(set (match_operand:<FCVT_TARGET> 0 "register_operand" "=w")
 	(FIXUORS:<FCVT_TARGET> (unspec:<FCVT_TARGET>
 			       [(match_operand:VDQF 1 "register_operand" "w")]
 			       FCVT)))]
   "TARGET_SIMD"
   "fcvt<frint_suffix><su>\\t%0.<Vtype>, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_fcvti")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_to_int_<Vetype><q>")]
 )
 
-;; Vector versions of the fcvt standard patterns.
-;; Expands to lbtrunc, lround, lceil, lfloor
-(define_expand "l<fcvt_pattern><su_optab><fcvt_target><VDQF:mode>2"
+(define_expand "<optab><VDQF:mode><fcvt_target>2"
   [(set (match_operand:<FCVT_TARGET> 0 "register_operand")
 	(FIXUORS:<FCVT_TARGET> (unspec:<FCVT_TARGET>
 			       [(match_operand:VDQF 1 "register_operand")]
-			       FCVT)))]
+			       UNSPEC_FRINTZ)))]
+  "TARGET_SIMD"
+  {})
+
+(define_expand "<fix_trunc_optab><VDQF:mode><fcvt_target>2"
+  [(set (match_operand:<FCVT_TARGET> 0 "register_operand")
+	(FIXUORS:<FCVT_TARGET> (unspec:<FCVT_TARGET>
+			       [(match_operand:VDQF 1 "register_operand")]
+			       UNSPEC_FRINTZ)))]
   "TARGET_SIMD"
   {})
 
+(define_expand "ftrunc<VDQF:mode>2"
+  [(set (match_operand:VDQF 0 "register_operand")
+	(unspec:VDQF [(match_operand:VDQF 1 "register_operand")]
+		      UNSPEC_FRINTZ))]
+  "TARGET_SIMD"
+  {})
+
+(define_insn "<optab><fcvt_target><VDQF:mode>2"
+  [(set (match_operand:VDQF 0 "register_operand" "=w")
+	(FLOATUORS:VDQF
+	  (match_operand:<FCVT_TARGET> 1 "register_operand" "w")))]
+  "TARGET_SIMD"
+  "<su_optab>cvtf\\t%0.<Vtype>, %1.<Vtype>"
+  [(set_attr "type" "neon_int_to_fp_<Vetype><q>")]
+)
+
+;; Conversions between vectors of floats and doubles.
+;; Contains a mix of patterns to match standard pattern names
+;; and those for intrinsics.
+
+;; Float widening operations.
+
+(define_insn "vec_unpacks_lo_v4sf"
+  [(set (match_operand:V2DF 0 "register_operand" "=w")
+	(float_extend:V2DF
+	  (vec_select:V2SF
+	    (match_operand:V4SF 1 "register_operand" "w")
+	    (parallel [(const_int 0) (const_int 1)])
+	  )))]
+  "TARGET_SIMD"
+  "fcvtl\\t%0.2d, %1.2s"
+  [(set_attr "type" "neon_fp_cvt_widen_s")]
+)
+
+(define_insn "aarch64_float_extend_lo_v2df"
+  [(set (match_operand:V2DF 0 "register_operand" "=w")
+	(float_extend:V2DF
+	  (match_operand:V2SF 1 "register_operand" "w")))]
+  "TARGET_SIMD"
+  "fcvtl\\t%0.2d, %1.2s"
+  [(set_attr "type" "neon_fp_cvt_widen_s")]
+)
+
+(define_insn "vec_unpacks_hi_v4sf"
+  [(set (match_operand:V2DF 0 "register_operand" "=w")
+	(float_extend:V2DF
+	  (vec_select:V2SF
+	    (match_operand:V4SF 1 "register_operand" "w")
+	    (parallel [(const_int 2) (const_int 3)])
+	  )))]
+  "TARGET_SIMD"
+  "fcvtl2\\t%0.2d, %1.4s"
+  [(set_attr "type" "neon_fp_cvt_widen_s")]
+)
+
+;; Float narrowing operations.
+
+(define_insn "aarch64_float_truncate_lo_v2sf"
+  [(set (match_operand:V2SF 0 "register_operand" "=w")
+      (float_truncate:V2SF
+	(match_operand:V2DF 1 "register_operand" "w")))]
+  "TARGET_SIMD"
+  "fcvtn\\t%0.2s, %1.2d"
+  [(set_attr "type" "neon_fp_cvt_narrow_d_q")]
+)
+
+(define_insn "aarch64_float_truncate_hi_v4sf"
+  [(set (match_operand:V4SF 0 "register_operand" "=w")
+    (vec_concat:V4SF
+      (match_operand:V2SF 1 "register_operand" "0")
+      (float_truncate:V2SF
+	(match_operand:V2DF 2 "register_operand" "w"))))]
+  "TARGET_SIMD"
+  "fcvtn2\\t%0.4s, %2.2d"
+  [(set_attr "type" "neon_fp_cvt_narrow_d_q")]
+)
+
+(define_expand "vec_pack_trunc_v2df"
+  [(set (match_operand:V4SF 0 "register_operand")
+      (vec_concat:V4SF
+	(float_truncate:V2SF
+	    (match_operand:V2DF 1 "register_operand"))
+	(float_truncate:V2SF
+	    (match_operand:V2DF 2 "register_operand"))
+	  ))]
+  "TARGET_SIMD"
+  {
+    rtx tmp = gen_reg_rtx (V2SFmode);
+    emit_insn (gen_aarch64_float_truncate_lo_v2sf (tmp, operands[1]));
+    emit_insn (gen_aarch64_float_truncate_hi_v4sf (operands[0],
+						   tmp, operands[2]));
+    DONE;
+  }
+)
+
+(define_expand "vec_pack_trunc_df"
+  [(set (match_operand:V2SF 0 "register_operand")
+      (vec_concat:V2SF
+	(float_truncate:SF
+	    (match_operand:DF 1 "register_operand"))
+	(float_truncate:SF
+	    (match_operand:DF 2 "register_operand"))
+	  ))]
+  "TARGET_SIMD"
+  {
+    rtx tmp = gen_reg_rtx (V2SFmode);
+    emit_insn (gen_move_lo_quad_v2df (tmp, operands[1]));
+    emit_insn (gen_move_hi_quad_v2df (tmp, operands[2]));
+    emit_insn (gen_aarch64_float_truncate_lo_v2sf (operands[0], tmp));
+    DONE;
+  }
+)
+
 (define_insn "aarch64_vmls<mode>"
   [(set (match_operand:VDQF 0 "register_operand" "=w")
        (minus:VDQF (match_operand:VDQF 1 "register_operand" "0")
@@ -1243,8 +1227,7 @@
 			      (match_operand:VDQF 3 "register_operand" "w"))))]
   "TARGET_SIMD"
  "fmls\\t%0.<Vtype>, %2.<Vtype>, %3.<Vtype>"
-  [(set_attr "simd_type" "simd_fmla")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_mla_<Vetype>_scalar<q>")]
 )
 
 ;; FP Max/Min
@@ -1261,253 +1244,201 @@
 ;; only introduces MIN_EXPR/MAX_EXPR in fast math mode or when not honouring
 ;; NaNs.
 
-(define_insn "smax<mode>3"
+(define_insn "<su><maxmin><mode>3"
   [(set (match_operand:VDQF 0 "register_operand" "=w")
-        (smax:VDQF (match_operand:VDQF 1 "register_operand" "w")
+        (FMAXMIN:VDQF (match_operand:VDQF 1 "register_operand" "w")
 		   (match_operand:VDQF 2 "register_operand" "w")))]
   "TARGET_SIMD"
-  "fmaxnm\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_fminmax")
-   (set_attr "simd_mode" "<MODE>")]
+  "f<maxmin>nm\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
+  [(set_attr "type" "neon_fp_minmax_<Vetype><q>")]
 )
 
-(define_insn "smin<mode>3"
+(define_insn "<maxmin_uns><mode>3"
   [(set (match_operand:VDQF 0 "register_operand" "=w")
-        (smin:VDQF (match_operand:VDQF 1 "register_operand" "w")
-		   (match_operand:VDQF 2 "register_operand" "w")))]
+       (unspec:VDQF [(match_operand:VDQF 1 "register_operand" "w")
+		     (match_operand:VDQF 2 "register_operand" "w")]
+		    FMAXMIN_UNS))]
   "TARGET_SIMD"
-  "fminnm\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_fminmax")
-   (set_attr "simd_mode" "<MODE>")]
+  "<maxmin_uns_op>\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
+  [(set_attr "type" "neon_fp_minmax_<Vetype><q>")]
 )
 
-;; FP 'across lanes' max and min ops.
+;; 'across lanes' add.
 
-(define_insn "reduc_s<fmaxminv>_v4sf"
- [(set (match_operand:V4SF 0 "register_operand" "=w")
-       (unspec:V4SF [(match_operand:V4SF 1 "register_operand" "w")]
-		    FMAXMINV))]
+(define_insn "reduc_<sur>plus_<mode>"
+ [(set (match_operand:VDQV 0 "register_operand" "=w")
+       (unspec:VDQV [(match_operand:VDQV 1 "register_operand" "w")]
+		    SUADDV))]
  "TARGET_SIMD"
- "f<fmaxminv>nmv\\t%s0, %1.4s";
-  [(set_attr "simd_type" "simd_fminmaxv")
-   (set_attr "simd_mode" "V4SF")]
+ "addv\\t%<Vetype>0, %1.<Vtype>"
+  [(set_attr "type" "neon_reduc_add<q>")]
 )
 
-(define_insn "reduc_s<fmaxminv>_<mode>"
- [(set (match_operand:V2F 0 "register_operand" "=w")
-       (unspec:V2F [(match_operand:V2F 1 "register_operand" "w")]
-		    FMAXMINV))]
+(define_insn "reduc_<sur>plus_v2di"
+ [(set (match_operand:V2DI 0 "register_operand" "=w")
+       (unspec:V2DI [(match_operand:V2DI 1 "register_operand" "w")]
+		    SUADDV))]
  "TARGET_SIMD"
- "f<fmaxminv>nmp\\t%0.<Vtype>, %1.<Vtype>, %1.<Vtype>";
-  [(set_attr "simd_type" "simd_fminmax")
-   (set_attr "simd_mode" "<MODE>")]
+ "addp\\t%d0, %1.2d"
+  [(set_attr "type" "neon_reduc_add_q")]
 )
 
-;; FP 'across lanes' add.
-
-(define_insn "aarch64_addvv4sf"
- [(set (match_operand:V4SF 0 "register_operand" "=w")
-       (unspec:V4SF [(match_operand:V4SF 1 "register_operand" "w")]
-		    UNSPEC_FADDV))]
+(define_insn "reduc_<sur>plus_v2si"
+ [(set (match_operand:V2SI 0 "register_operand" "=w")
+       (unspec:V2SI [(match_operand:V2SI 1 "register_operand" "w")]
+		    SUADDV))]
  "TARGET_SIMD"
- "faddp\\t%0.4s, %1.4s, %1.4s"
-  [(set_attr "simd_type" "simd_fadd")
-   (set_attr "simd_mode" "V4SF")]
+ "addp\\t%0.2s, %1.2s, %1.2s"
+  [(set_attr "type" "neon_reduc_add")]
 )
 
-(define_expand "reduc_uplus_v4sf"
- [(set (match_operand:V4SF 0 "register_operand" "=w")
-       (match_operand:V4SF 1 "register_operand" "w"))]
- "TARGET_SIMD"
-{
-  rtx tmp = gen_reg_rtx (V4SFmode);
-  emit_insn (gen_aarch64_addvv4sf (tmp, operands[1]));
-  emit_insn (gen_aarch64_addvv4sf (operands[0], tmp));
-  DONE;
-})
-
-(define_expand "reduc_splus_v4sf"
- [(set (match_operand:V4SF 0 "register_operand" "=w")
-       (match_operand:V4SF 1 "register_operand" "w"))]
- "TARGET_SIMD"
-{
-  rtx tmp = gen_reg_rtx (V4SFmode);
-  emit_insn (gen_aarch64_addvv4sf (tmp, operands[1]));
-  emit_insn (gen_aarch64_addvv4sf (operands[0], tmp));
-  DONE;
-})
-
-(define_insn "aarch64_addv<mode>"
+(define_insn "reduc_<sur>plus_<mode>"
  [(set (match_operand:V2F 0 "register_operand" "=w")
        (unspec:V2F [(match_operand:V2F 1 "register_operand" "w")]
-		    UNSPEC_FADDV))]
+		    SUADDV))]
  "TARGET_SIMD"
  "faddp\\t%<Vetype>0, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_fadd")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_reduc_add_<Vetype><q>")]
 )
 
-(define_expand "reduc_uplus_<mode>"
- [(set (match_operand:V2F 0 "register_operand" "=w")
-       (unspec:V2F [(match_operand:V2F 1 "register_operand" "w")]
+(define_insn "aarch64_addpv4sf"
+ [(set (match_operand:V4SF 0 "register_operand" "=w")
+       (unspec:V4SF [(match_operand:V4SF 1 "register_operand" "w")]
 		    UNSPEC_FADDV))]
  "TARGET_SIMD"
- ""
+ "faddp\\t%0.4s, %1.4s, %1.4s"
+  [(set_attr "type" "neon_fp_reduc_add_s_q")]
 )
 
-(define_expand "reduc_splus_<mode>"
- [(set (match_operand:V2F 0 "register_operand" "=w")
-       (unspec:V2F [(match_operand:V2F 1 "register_operand" "w")]
-		    UNSPEC_FADDV))]
+(define_expand "reduc_<sur>plus_v4sf"
+ [(set (match_operand:V4SF 0 "register_operand")
+       (unspec:V4SF [(match_operand:V4SF 1 "register_operand")]
+		    SUADDV))]
  "TARGET_SIMD"
- ""
-)
-
-;; Reduction across lanes.
+{
+  rtx tmp = gen_reg_rtx (V4SFmode);
+  emit_insn (gen_aarch64_addpv4sf (tmp, operands[1]));
+  emit_insn (gen_aarch64_addpv4sf (operands[0], tmp));
+  DONE;
+})
 
-(define_insn "aarch64_addv<mode>"
- [(set (match_operand:VDQV 0 "register_operand" "=w")
-       (unspec:VDQV [(match_operand:VDQV 1 "register_operand" "w")]
-		    UNSPEC_ADDV))]
+(define_insn "clz<mode>2"
+ [(set (match_operand:VDQ_BHSI 0 "register_operand" "=w")
+       (clz:VDQ_BHSI (match_operand:VDQ_BHSI 1 "register_operand" "w")))]
  "TARGET_SIMD"
- "addv\\t%<Vetype>0, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_addv")
-   (set_attr "simd_mode" "<MODE>")]
+ "clz\\t%0.<Vtype>, %1.<Vtype>"
+  [(set_attr "type" "neon_cls<q>")]
 )
 
-(define_expand "reduc_splus_<mode>"
- [(set (match_operand:VDQV 0 "register_operand" "=w")
-       (unspec:VDQV [(match_operand:VDQV 1 "register_operand" "w")]
-		    UNSPEC_ADDV))]
- "TARGET_SIMD"
- ""
-)
+;; 'across lanes' max and min ops.
 
-(define_expand "reduc_uplus_<mode>"
+(define_insn "reduc_<maxmin_uns>_<mode>"
  [(set (match_operand:VDQV 0 "register_operand" "=w")
        (unspec:VDQV [(match_operand:VDQV 1 "register_operand" "w")]
-		    UNSPEC_ADDV))]
- "TARGET_SIMD"
- ""
-)
-
-(define_insn "aarch64_addvv2di"
- [(set (match_operand:V2DI 0 "register_operand" "=w")
-       (unspec:V2DI [(match_operand:V2DI 1 "register_operand" "w")]
-		    UNSPEC_ADDV))]
- "TARGET_SIMD"
- "addp\\t%d0, %1.2d"
-  [(set_attr "simd_type" "simd_add")
-   (set_attr "simd_mode" "V2DI")]
-)
-
-(define_expand "reduc_uplus_v2di"
- [(set (match_operand:V2DI 0 "register_operand" "=w")
-       (unspec:V2DI [(match_operand:V2DI 1 "register_operand" "w")]
-		    UNSPEC_ADDV))]
+		    MAXMINV))]
  "TARGET_SIMD"
- ""
+ "<maxmin_uns_op>v\\t%<Vetype>0, %1.<Vtype>"
+  [(set_attr "type" "neon_reduc_minmax<q>")]
 )
 
-(define_expand "reduc_splus_v2di"
+(define_insn "reduc_<maxmin_uns>_v2di"
  [(set (match_operand:V2DI 0 "register_operand" "=w")
        (unspec:V2DI [(match_operand:V2DI 1 "register_operand" "w")]
-		    UNSPEC_ADDV))]
- "TARGET_SIMD"
- ""
-)
-
-(define_insn "aarch64_addvv2si"
- [(set (match_operand:V2SI 0 "register_operand" "=w")
-       (unspec:V2SI [(match_operand:V2SI 1 "register_operand" "w")]
-		    UNSPEC_ADDV))]
- "TARGET_SIMD"
- "addp\\t%0.2s, %1.2s, %1.2s"
-  [(set_attr "simd_type" "simd_add")
-   (set_attr "simd_mode" "V2SI")]
-)
-
-(define_expand "reduc_uplus_v2si"
- [(set (match_operand:V2SI 0 "register_operand" "=w")
-       (unspec:V2SI [(match_operand:V2SI 1 "register_operand" "w")]
-		    UNSPEC_ADDV))]
+		    MAXMINV))]
  "TARGET_SIMD"
- ""
+ "<maxmin_uns_op>p\\t%d0, %1.2d"
+  [(set_attr "type" "neon_reduc_minmax_q")]
 )
 
-(define_expand "reduc_splus_v2si"
+(define_insn "reduc_<maxmin_uns>_v2si"
  [(set (match_operand:V2SI 0 "register_operand" "=w")
        (unspec:V2SI [(match_operand:V2SI 1 "register_operand" "w")]
-		    UNSPEC_ADDV))]
+		    MAXMINV))]
  "TARGET_SIMD"
- ""
+ "<maxmin_uns_op>p\\t%0.2s, %1.2s, %1.2s"
+  [(set_attr "type" "neon_reduc_minmax")]
 )
 
-(define_insn "reduc_<maxminv>_<mode>"
- [(set (match_operand:VDQV 0 "register_operand" "=w")
-       (unspec:VDQV [(match_operand:VDQV 1 "register_operand" "w")]
-		    MAXMINV))]
+(define_insn "reduc_<maxmin_uns>_<mode>"
+ [(set (match_operand:V2F 0 "register_operand" "=w")
+       (unspec:V2F [(match_operand:V2F 1 "register_operand" "w")]
+		    FMAXMINV))]
  "TARGET_SIMD"
- "<maxminv>v\\t%<Vetype>0, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_minmaxv")
-   (set_attr "simd_mode" "<MODE>")]
+ "<maxmin_uns_op>p\\t%<Vetype>0, %1.<Vtype>"
+  [(set_attr "type" "neon_fp_reduc_minmax_<Vetype><q>")]
 )
 
-(define_insn "reduc_<maxminv>_v2si"
- [(set (match_operand:V2SI 0 "register_operand" "=w")
-       (unspec:V2SI [(match_operand:V2SI 1 "register_operand" "w")]
-		    MAXMINV))]
+(define_insn "reduc_<maxmin_uns>_v4sf"
+ [(set (match_operand:V4SF 0 "register_operand" "=w")
+       (unspec:V4SF [(match_operand:V4SF 1 "register_operand" "w")]
+		    FMAXMINV))]
  "TARGET_SIMD"
- "<maxminv>p\\t%0.2s, %1.2s, %1.2s"
-  [(set_attr "simd_type" "simd_minmax")
-   (set_attr "simd_mode" "V2SI")]
+ "<maxmin_uns_op>v\\t%s0, %1.4s"
+  [(set_attr "type" "neon_fp_reduc_minmax_s_q")]
 )
 
-;; vbsl_* intrinsics may compile to any of bsl/bif/bit depending on register
-;; allocation.  For an intrinsic of form:
-;;   vD = bsl_* (vS, vN, vM)
+;; aarch64_simd_bsl may compile to any of bsl/bif/bit depending on register
+;; allocation.
+;; Operand 1 is the mask, operands 2 and 3 are the bitfields from which
+;; to select.
+;;
+;; Thus our BSL is of the form:
+;;   op0 = bsl (mask, op2, op3)
 ;; We can use any of:
-;;   bsl vS, vN, vM  (if D = S)
-;;   bit vD, vN, vS  (if D = M, so 1-bits in vS choose bits from vN, else vM)
-;;   bif vD, vM, vS  (if D = N, so 0-bits in vS choose bits from vM, else vN)
+;;
+;;   if (op0 = mask)
+;;     bsl mask, op1, op2
+;;   if (op0 = op1) (so 1-bits in mask choose bits from op2, else op0)
+;;     bit op0, op2, mask
+;;   if (op0 = op2) (so 0-bits in mask choose bits from op1, else op0)
+;;     bif op0, op1, mask
 
 (define_insn "aarch64_simd_bsl<mode>_internal"
   [(set (match_operand:VALL 0 "register_operand"		"=w,w,w")
-	(unspec:VALL
-	 [(match_operand:<V_cmp_result> 1 "register_operand"	" 0,w,w")
-	  (match_operand:VALL 2 "register_operand"		" w,w,0")
-	  (match_operand:VALL 3 "register_operand"		" w,0,w")]
-	 UNSPEC_BSL))]
+	(ior:VALL
+	   (and:VALL
+	     (match_operand:<V_cmp_result> 1 "register_operand"	" 0,w,w")
+	     (match_operand:VALL 2 "register_operand"		" w,w,0"))
+	   (and:VALL
+	     (not:<V_cmp_result>
+		(match_dup:<V_cmp_result> 1))
+	     (match_operand:VALL 3 "register_operand"		" w,0,w"))
+	))]
   "TARGET_SIMD"
   "@
   bsl\\t%0.<Vbtype>, %2.<Vbtype>, %3.<Vbtype>
   bit\\t%0.<Vbtype>, %2.<Vbtype>, %1.<Vbtype>
   bif\\t%0.<Vbtype>, %3.<Vbtype>, %1.<Vbtype>"
+  [(set_attr "type" "neon_bsl<q>")]
 )
 
 (define_expand "aarch64_simd_bsl<mode>"
-  [(set (match_operand:VALL 0 "register_operand")
-	(unspec:VALL [(match_operand:<V_cmp_result> 1 "register_operand")
-		      (match_operand:VALL 2 "register_operand")
-		      (match_operand:VALL 3 "register_operand")]
-		     UNSPEC_BSL))]
-  "TARGET_SIMD"
+  [(match_operand:VALL 0 "register_operand")
+   (match_operand:<V_cmp_result> 1 "register_operand")
+   (match_operand:VALL 2 "register_operand")
+   (match_operand:VALL 3 "register_operand")]
+ "TARGET_SIMD"
 {
   /* We can't alias operands together if they have different modes.  */
   operands[1] = gen_lowpart (<V_cmp_result>mode, operands[1]);
+  emit_insn (gen_aarch64_simd_bsl<mode>_internal (operands[0], operands[1],
+						  operands[2], operands[3]));
+  DONE;
 })
 
-(define_expand "aarch64_vcond_internal<mode>"
+(define_expand "aarch64_vcond_internal<mode><mode>"
   [(set (match_operand:VDQ 0 "register_operand")
 	(if_then_else:VDQ
 	  (match_operator 3 "comparison_operator"
 	    [(match_operand:VDQ 4 "register_operand")
 	     (match_operand:VDQ 5 "nonmemory_operand")])
-	  (match_operand:VDQ 1 "register_operand")
-	  (match_operand:VDQ 2 "register_operand")))]
+	  (match_operand:VDQ 1 "nonmemory_operand")
+	  (match_operand:VDQ 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
   int inverse = 0, has_zero_imm_form = 0;
+  rtx op1 = operands[1];
+  rtx op2 = operands[2];
   rtx mask = gen_reg_rtx (<MODE>mode);
 
   switch (GET_CODE (operands[3]))
@@ -1566,30 +1497,47 @@
     }
 
   if (inverse)
-    emit_insn (gen_aarch64_simd_bsl<mode> (operands[0], mask, operands[2],
-				    operands[1]));
-  else
-    emit_insn (gen_aarch64_simd_bsl<mode> (operands[0], mask, operands[1],
-				    operands[2]));
+    {
+      op1 = operands[2];
+      op2 = operands[1];
+    }
+
+    /* If we have (a = (b CMP c) ? -1 : 0);
+       Then we can simply move the generated mask.  */
+
+    if (op1 == CONSTM1_RTX (<V_cmp_result>mode)
+	&& op2 == CONST0_RTX (<V_cmp_result>mode))
+      emit_move_insn (operands[0], mask);
+    else
+      {
+	if (!REG_P (op1))
+	  op1 = force_reg (<MODE>mode, op1);
+	if (!REG_P (op2))
+	  op2 = force_reg (<MODE>mode, op2);
+	emit_insn (gen_aarch64_simd_bsl<mode> (operands[0], mask,
+					       op1, op2));
+      }
 
   DONE;
 })
 
-(define_expand "aarch64_vcond_internal<mode>"
-  [(set (match_operand:VDQF 0 "register_operand")
+(define_expand "aarch64_vcond_internal<VDQF_COND:mode><VDQF:mode>"
+  [(set (match_operand:VDQF_COND 0 "register_operand")
 	(if_then_else:VDQF
 	  (match_operator 3 "comparison_operator"
 	    [(match_operand:VDQF 4 "register_operand")
 	     (match_operand:VDQF 5 "nonmemory_operand")])
-	  (match_operand:VDQF 1 "register_operand")
-	  (match_operand:VDQF 2 "register_operand")))]
+	  (match_operand:VDQF_COND 1 "nonmemory_operand")
+	  (match_operand:VDQF_COND 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
   int inverse = 0;
   int use_zero_form = 0;
   int swap_bsl_operands = 0;
-  rtx mask = gen_reg_rtx (<V_cmp_result>mode);
-  rtx tmp = gen_reg_rtx (<V_cmp_result>mode);
+  rtx op1 = operands[1];
+  rtx op2 = operands[2];
+  rtx mask = gen_reg_rtx (<VDQF_COND:V_cmp_result>mode);
+  rtx tmp = gen_reg_rtx (<VDQF_COND:V_cmp_result>mode);
 
   rtx (*base_comparison) (rtx, rtx, rtx);
   rtx (*complimentary_comparison) (rtx, rtx, rtx);
@@ -1609,7 +1557,7 @@
       /* Fall through.  */
     default:
       if (!REG_P (operands[5]))
-	operands[5] = force_reg (<MODE>mode, operands[5]);
+	operands[5] = force_reg (<VDQF:MODE>mode, operands[5]);
     }
 
   switch (GET_CODE (operands[3]))
@@ -1622,8 +1570,8 @@
     case UNGE:
     case ORDERED:
     case UNORDERED:
-      base_comparison = gen_aarch64_cmge<mode>;
-      complimentary_comparison = gen_aarch64_cmgt<mode>;
+      base_comparison = gen_aarch64_cmge<VDQF:mode>;
+      complimentary_comparison = gen_aarch64_cmgt<VDQF:mode>;
       break;
     case LE:
     case UNLE:
@@ -1631,14 +1579,14 @@
       /* Fall through.  */
     case GT:
     case UNGT:
-      base_comparison = gen_aarch64_cmgt<mode>;
-      complimentary_comparison = gen_aarch64_cmge<mode>;
+      base_comparison = gen_aarch64_cmgt<VDQF:mode>;
+      complimentary_comparison = gen_aarch64_cmge<VDQF:mode>;
       break;
     case EQ:
     case NE:
     case UNEQ:
-      base_comparison = gen_aarch64_cmeq<mode>;
-      complimentary_comparison = gen_aarch64_cmeq<mode>;
+      base_comparison = gen_aarch64_cmeq<VDQF:mode>;
+      complimentary_comparison = gen_aarch64_cmeq<VDQF:mode>;
       break;
     default:
       gcc_unreachable ();
@@ -1666,10 +1614,10 @@
 	  switch (GET_CODE (operands[3]))
 	    {
 	    case LT:
-	      base_comparison = gen_aarch64_cmlt<mode>;
+	      base_comparison = gen_aarch64_cmlt<VDQF:mode>;
 	      break;
 	    case LE:
-	      base_comparison = gen_aarch64_cmle<mode>;
+	      base_comparison = gen_aarch64_cmle<VDQF:mode>;
 	      break;
 	    default:
 	      /* Do nothing, other zero form cases already have the correct
@@ -1712,9 +1660,9 @@
 	 true iff !(a != b && a ORDERED b), swapping the operands to BSL
 	 will then give us (a == b ||  a UNORDERED b) as intended.  */
 
-      emit_insn (gen_aarch64_cmgt<mode> (mask, operands[4], operands[5]));
-      emit_insn (gen_aarch64_cmgt<mode> (tmp, operands[5], operands[4]));
-      emit_insn (gen_ior<v_cmp_result>3 (mask, mask, tmp));
+      emit_insn (gen_aarch64_cmgt<VDQF:mode> (mask, operands[4], operands[5]));
+      emit_insn (gen_aarch64_cmgt<VDQF:mode> (tmp, operands[5], operands[4]));
+      emit_insn (gen_ior<VDQF_COND:v_cmp_result>3 (mask, mask, tmp));
       swap_bsl_operands = 1;
       break;
     case UNORDERED:
@@ -1723,20 +1671,36 @@
      swap_bsl_operands = 1;
      /* Fall through.  */
     case ORDERED:
-      emit_insn (gen_aarch64_cmgt<mode> (tmp, operands[4], operands[5]));
-      emit_insn (gen_aarch64_cmge<mode> (mask, operands[5], operands[4]));
-      emit_insn (gen_ior<v_cmp_result>3 (mask, mask, tmp));
+      emit_insn (gen_aarch64_cmgt<VDQF:mode> (tmp, operands[4], operands[5]));
+      emit_insn (gen_aarch64_cmge<VDQF:mode> (mask, operands[5], operands[4]));
+      emit_insn (gen_ior<VDQF_COND:v_cmp_result>3 (mask, mask, tmp));
       break;
     default:
       gcc_unreachable ();
     }
 
   if (swap_bsl_operands)
-    emit_insn (gen_aarch64_simd_bsl<mode> (operands[0], mask, operands[2],
-				    operands[1]));
-  else
-    emit_insn (gen_aarch64_simd_bsl<mode> (operands[0], mask, operands[1],
-				    operands[2]));
+    {
+      op1 = operands[2];
+      op2 = operands[1];
+    }
+
+    /* If we have (a = (b CMP c) ? -1 : 0);
+       Then we can simply move the generated mask.  */
+
+    if (op1 == CONSTM1_RTX (<VDQF_COND:V_cmp_result>mode)
+	&& op2 == CONST0_RTX (<VDQF_COND:V_cmp_result>mode))
+      emit_move_insn (operands[0], mask);
+    else
+      {
+	if (!REG_P (op1))
+	  op1 = force_reg (<VDQF_COND:MODE>mode, op1);
+	if (!REG_P (op2))
+	  op2 = force_reg (<VDQF_COND:MODE>mode, op2);
+	emit_insn (gen_aarch64_simd_bsl<VDQF_COND:mode> (operands[0], mask,
+					       op1, op2));
+      }
+
   DONE;
 })
 
@@ -1746,16 +1710,32 @@
 	  (match_operator 3 "comparison_operator"
 	    [(match_operand:VALL 4 "register_operand")
 	     (match_operand:VALL 5 "nonmemory_operand")])
-	  (match_operand:VALL 1 "register_operand")
-	  (match_operand:VALL 2 "register_operand")))]
+	  (match_operand:VALL 1 "nonmemory_operand")
+	  (match_operand:VALL 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
-  emit_insn (gen_aarch64_vcond_internal<mode> (operands[0], operands[1],
+  emit_insn (gen_aarch64_vcond_internal<mode><mode> (operands[0], operands[1],
 					       operands[2], operands[3],
 					       operands[4], operands[5]));
   DONE;
 })
 
+(define_expand "vcond<v_cmp_result><mode>"
+  [(set (match_operand:<V_cmp_result> 0 "register_operand")
+	(if_then_else:<V_cmp_result>
+	  (match_operator 3 "comparison_operator"
+	    [(match_operand:VDQF 4 "register_operand")
+	     (match_operand:VDQF 5 "nonmemory_operand")])
+	  (match_operand:<V_cmp_result> 1 "nonmemory_operand")
+	  (match_operand:<V_cmp_result> 2 "nonmemory_operand")))]
+  "TARGET_SIMD"
+{
+  emit_insn (gen_aarch64_vcond_internal<v_cmp_result><mode> (
+						operands[0], operands[1],
+						operands[2], operands[3],
+						operands[4], operands[5]));
+  DONE;
+})
 
 (define_expand "vcondu<mode><mode>"
   [(set (match_operand:VDQ 0 "register_operand")
@@ -1763,11 +1743,11 @@
 	  (match_operator 3 "comparison_operator"
 	    [(match_operand:VDQ 4 "register_operand")
 	     (match_operand:VDQ 5 "nonmemory_operand")])
-	  (match_operand:VDQ 1 "register_operand")
-	  (match_operand:VDQ 2 "register_operand")))]
+	  (match_operand:VDQ 1 "nonmemory_operand")
+	  (match_operand:VDQ 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
-  emit_insn (gen_aarch64_vcond_internal<mode> (operands[0], operands[1],
+  emit_insn (gen_aarch64_vcond_internal<mode><mode> (operands[0], operands[1],
 					       operands[2], operands[3],
 					       operands[4], operands[5]));
   DONE;
@@ -1785,45 +1765,47 @@
   DONE;
 })
 
-(define_insn "aarch64_get_lane_signed<mode>"
-  [(set (match_operand:<VEL> 0 "register_operand" "=r")
-	(sign_extend:<VEL>
+;; Lane extraction with sign extension to general purpose register.
+(define_insn "*aarch64_get_lane_extend<GPI:mode><VDQQH:mode>"
+  [(set (match_operand:GPI 0 "register_operand" "=r")
+	(sign_extend:GPI
 	  (vec_select:<VEL>
-	    (match_operand:VQ_S 1 "register_operand" "w")
+	    (match_operand:VDQQH 1 "register_operand" "w")
 	    (parallel [(match_operand:SI 2 "immediate_operand" "i")]))))]
   "TARGET_SIMD"
-  "smov\\t%0, %1.<Vetype>[%2]"
-  [(set_attr "simd_type" "simd_movgp")
-   (set_attr "simd_mode" "<MODE>")]
+  "smov\\t%<GPI:w>0, %1.<VDQQH:Vetype>[%2]"
+  [(set_attr "type" "neon_to_gp<q>")]
 )
 
-(define_insn "aarch64_get_lane_unsigned<mode>"
-  [(set (match_operand:<VEL> 0 "register_operand" "=r")
-	(zero_extend:<VEL>
+(define_insn "*aarch64_get_lane_zero_extendsi<mode>"
+  [(set (match_operand:SI 0 "register_operand" "=r")
+	(zero_extend:SI
 	  (vec_select:<VEL>
-	    (match_operand:VDQ 1 "register_operand" "w")
+	    (match_operand:VDQQH 1 "register_operand" "w")
 	    (parallel [(match_operand:SI 2 "immediate_operand" "i")]))))]
   "TARGET_SIMD"
-  "umov\\t%<vw>0, %1.<Vetype>[%2]"
-  [(set_attr "simd_type" "simd_movgp")
-   (set_attr "simd_mode" "<MODE>")]
+  "umov\\t%w0, %1.<Vetype>[%2]"
+  [(set_attr "type" "neon_to_gp<q>")]
 )
 
+;; Lane extraction of a value, neither sign nor zero extension
+;; is guaranteed so upper bits should be considered undefined.
 (define_insn "aarch64_get_lane<mode>"
-  [(set (match_operand:<VEL> 0 "register_operand" "=w")
+  [(set (match_operand:<VEL> 0 "register_operand" "=r, w")
 	(vec_select:<VEL>
-	    (match_operand:VDQF 1 "register_operand" "w")
-	    (parallel [(match_operand:SI 2 "immediate_operand" "i")])))]
+	  (match_operand:VALL 1 "register_operand" "w, w")
+	  (parallel [(match_operand:SI 2 "immediate_operand" "i, i")])))]
   "TARGET_SIMD"
-  "mov\\t%0.<Vetype>[0], %1.<Vetype>[%2]"
-  [(set_attr "simd_type" "simd_ins")
-   (set_attr "simd_mode" "<MODE>")]
+  "@
+   umov\\t%<vwcore>0, %1.<Vetype>[%2]
+   dup\\t%<Vetype>0, %1.<Vetype>[%2]"
+  [(set_attr "type" "neon_to_gp<q>, neon_dup<q>")]
 )
 
 (define_expand "aarch64_get_lanedi"
-  [(match_operand:DI 0 "register_operand" "=r")
-   (match_operand:DI 1 "register_operand" "w")
-   (match_operand:SI 2 "immediate_operand" "i")]
+  [(match_operand:DI 0 "register_operand")
+   (match_operand:DI 1 "register_operand")
+   (match_operand:SI 2 "immediate_operand")]
   "TARGET_SIMD"
 {
   aarch64_simd_lane_bounds (operands[2], 0, 1);
@@ -1940,18 +1922,35 @@
 	   (match_operand:VDIC 2 "aarch64_simd_imm_zero" "Dz")))]
   "TARGET_SIMD"
   "mov\\t%0.8b, %1.8b"
-  [(set_attr "simd_type" "simd_move")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_move<q>")]
 )
 
-(define_insn "aarch64_combine<mode>"
+(define_insn_and_split "aarch64_combine<mode>"
   [(set (match_operand:<VDBL> 0 "register_operand" "=&w")
         (vec_concat:<VDBL> (match_operand:VDC 1 "register_operand" "w")
 			   (match_operand:VDC 2 "register_operand" "w")))]
   "TARGET_SIMD"
-  "mov\\t%0.d[0], %1.d[0]\;ins\\t%0.d[1], %2.d[0]"
-  [(set_attr "simd_type" "simd_ins")
-   (set_attr "simd_mode" "<MODE>")]
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+{
+  aarch64_split_simd_combine (operands[0], operands[1], operands[2]);
+  DONE;
+}
+[(set_attr "type" "multiple")]
+)
+
+(define_expand "aarch64_simd_combine<mode>"
+  [(set (match_operand:<VDBL> 0 "register_operand" "=&w")
+        (vec_concat:<VDBL> (match_operand:VDC 1 "register_operand" "w")
+  (match_operand:VDC 2 "register_operand" "w")))]
+  "TARGET_SIMD"
+  {
+    emit_insn (gen_move_lo_quad_<Vdbl> (operands[0], operands[1]));
+    emit_insn (gen_move_hi_quad_<Vdbl> (operands[0], operands[2]));
+    DONE;
+  }
+[(set_attr "type" "multiple")]
 )
 
 ;; <su><addsub>l<q>.
@@ -1966,8 +1965,7 @@
 			   (match_dup 3)))))]
   "TARGET_SIMD"
   "<ANY_EXTEND:su><ADDSUB:optab>l2 %0.<Vwtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_addl")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_<ADDSUB:optab>_long")]
 )
 
 (define_expand "aarch64_saddl2<mode>"
@@ -2026,8 +2024,7 @@
 			   (match_operand:VDW 2 "register_operand" "w"))))]
   "TARGET_SIMD"
   "<ANY_EXTEND:su><ADDSUB:optab>l %0.<Vwtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_addl")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_<ADDSUB:optab>_long")]
 )
 
 ;; <su><addsub>w<q>.
@@ -2039,8 +2036,7 @@
 			  (match_operand:VDW 2 "register_operand" "w"))))]
   "TARGET_SIMD"
   "<ANY_EXTEND:su><ADDSUB:optab>w\\t%0.<Vwtype>, %1.<Vwtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_addl")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_<ADDSUB:optab>_widen")]
 )
 
 (define_insn "aarch64_<ANY_EXTEND:su><ADDSUB:optab>w2<mode>_internal"
@@ -2052,8 +2048,7 @@
 			   (match_operand:VQW 3 "vect_par_cnst_hi_half" "")))))]
   "TARGET_SIMD"
   "<ANY_EXTEND:su><ADDSUB:optab>w2\\t%0.<Vwtype>, %1.<Vwtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_addl")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_<ADDSUB:optab>_widen")]
 )
 
 (define_expand "aarch64_saddw2<mode>"
@@ -2114,8 +2109,7 @@
 		     HADDSUB))]
   "TARGET_SIMD"
   "<sur>h<addsub>\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_add")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_<addsub>_halve<q>")]
 )
 
 ;; <r><addsub>hn<q>.
@@ -2127,8 +2121,7 @@
                            ADDSUBHN))]
   "TARGET_SIMD"
   "<sur><addsub>hn\\t%0.<Vntype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_addn")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_<addsub>_halve_narrow_q")]
 )
 
 (define_insn "aarch64_<sur><addsub>hn2<mode>"
@@ -2139,8 +2132,7 @@
                             ADDSUBHN2))]
   "TARGET_SIMD"
   "<sur><addsub>hn2\\t%0.<V2ntype>, %2.<Vtype>, %3.<Vtype>"
-  [(set_attr "simd_type" "simd_addn2")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_<addsub>_halve_narrow_q")]
 )
 
 ;; pmul.
@@ -2152,8 +2144,7 @@
 		   UNSPEC_PMUL))]
  "TARGET_SIMD"
  "pmul\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_mul_<Vetype><q>")]
 )
 
 ;; <su>q<addsub>
@@ -2164,8 +2155,7 @@
 			  (match_operand:VSDQ_I 2 "register_operand" "w")))]
   "TARGET_SIMD"
   "<su_optab><optab>\\t%<v>0<Vmtype>, %<v>1<Vmtype>, %<v>2<Vmtype>"
-  [(set_attr "simd_type" "simd_add")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_<optab><q>")]
 )
 
 ;; suqadd and usqadd
@@ -2177,8 +2167,7 @@
 		       USSUQADD))]
   "TARGET_SIMD"
   "<sur>qadd\\t%<v>0<Vmtype>, %<v>2<Vmtype>"
-  [(set_attr "simd_type" "simd_sat_add")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_qadd<q>")]
 )
 
 ;; sqmovun
@@ -2189,8 +2178,7 @@
                             UNSPEC_SQXTUN))]
    "TARGET_SIMD"
    "sqxtun\\t%<vn2>0<Vmntype>, %<v>1<Vmtype>"
-   [(set_attr "simd_type" "simd_sat_shiftn_imm")
-    (set_attr "simd_mode" "<MODE>")]
+   [(set_attr "type" "neon_sat_shift_imm_narrow_q")]
  )
 
 ;; sqmovn and uqmovn
@@ -2201,8 +2189,7 @@
                             SUQMOVN))]
   "TARGET_SIMD"
   "<sur>qxtn\\t%<vn2>0<Vmntype>, %<v>1<Vmtype>"
-   [(set_attr "simd_type" "simd_sat_shiftn_imm")
-    (set_attr "simd_mode" "<MODE>")]
+   [(set_attr "type" "neon_sat_shift_imm_narrow_q")]
  )
 
 ;; <su>q<absneg>
@@ -2213,8 +2200,7 @@
 	  (match_operand:VSDQ_I_BHSI 1 "register_operand" "w")))]
   "TARGET_SIMD"
   "s<optab>\\t%<v>0<Vmtype>, %<v>1<Vmtype>"
-  [(set_attr "simd_type" "simd_sat_negabs")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_<optab><q>")]
 )
 
 ;; sq<r>dmulh.
@@ -2227,8 +2213,7 @@
 	 VQDMULH))]
   "TARGET_SIMD"
   "sq<r>dmulh\\t%<v>0<Vmtype>, %<v>1<Vmtype>, %<v>2<Vmtype>"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype><q>")]
 )
 
 ;; sq<r>dmulh_lane
@@ -2245,8 +2230,7 @@
   "*
    aarch64_simd_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<VCOND>mode));
    return \"sq<r>dmulh\\t%0.<Vtype>, %1.<Vtype>, %2.<Vetype>[%3]\";"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_scalar<q>")]
 )
 
 (define_insn "aarch64_sq<r>dmulh_laneq<mode>"
@@ -2261,8 +2245,7 @@
   "*
    aarch64_simd_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<VCONQ>mode));
    return \"sq<r>dmulh\\t%0.<Vtype>, %1.<Vtype>, %2.<Vetype>[%3]\";"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_scalar<q>")]
 )
 
 (define_insn "aarch64_sq<r>dmulh_lane<mode>"
@@ -2277,8 +2260,7 @@
   "*
    aarch64_simd_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<VCONQ>mode));
    return \"sq<r>dmulh\\t%<v>0, %<v>1, %2.<v>[%3]\";"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_scalar<q>")]
 )
 
 ;; vqdml[sa]l
@@ -2296,8 +2278,7 @@
 	      (const_int 1))))]
   "TARGET_SIMD"
   "sqdml<SBINQOPS:as>l\\t%<vw2>0<Vmwtype>, %<v>2<Vmtype>, %<v>3<Vmtype>"
-  [(set_attr "simd_type" "simd_sat_mlal")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mla_<Vetype>_long")]
 )
 
 ;; vqdml[sa]l_lane
@@ -2319,8 +2300,7 @@
 	    (const_int 1))))]
   "TARGET_SIMD"
   "sqdml<SBINQOPS:as>l\\t%<vw2>0<Vmwtype>, %<v>2<Vmtype>, %3.<Vetype>[%4]"
-  [(set_attr "simd_type" "simd_sat_mlal")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mla_<Vetype>_scalar_long")]
 )
 
 (define_insn "aarch64_sqdml<SBINQOPS:as>l_lane<mode>_internal"
@@ -2339,8 +2319,7 @@
 	    (const_int 1))))]
   "TARGET_SIMD"
   "sqdml<SBINQOPS:as>l\\t%<vw2>0<Vmwtype>, %<v>2<Vmtype>, %3.<Vetype>[%4]"
-  [(set_attr "simd_type" "simd_sat_mlal")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mla_<Vetype>_scalar_long")]
 )
 
 (define_expand "aarch64_sqdmlal_lane<mode>"
@@ -2419,8 +2398,7 @@
 	      (const_int 1))))]
   "TARGET_SIMD"
   "sqdml<SBINQOPS:as>l\\t%<vw2>0<Vmwtype>, %<v>2<Vmtype>, %3.<Vetype>[0]"
-  [(set_attr "simd_type" "simd_sat_mlal")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mla_<Vetype>_scalar_long")]
 )
 
 ;; sqdml[as]l2
@@ -2442,8 +2420,7 @@
              (const_int 1))))]
   "TARGET_SIMD"
   "sqdml<SBINQOPS:as>l2\\t%<vw2>0<Vmwtype>, %<v>2<Vmtype>, %<v>3<Vmtype>"
-  [(set_attr "simd_type" "simd_sat_mlal")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mla_<Vetype>_scalar_long")]
 )
 
 (define_expand "aarch64_sqdmlal2<mode>"
@@ -2493,8 +2470,7 @@
 	      (const_int 1))))]
   "TARGET_SIMD"
   "sqdml<SBINQOPS:as>l2\\t%<vw2>0<Vmwtype>, %<v>2<Vmtype>, %3.<Vetype>[%4]"
-  [(set_attr "simd_type" "simd_sat_mlal")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mla_<Vetype>_scalar_long")]
 )
 
 (define_expand "aarch64_sqdmlal2_lane<mode>"
@@ -2577,8 +2553,7 @@
 	    (const_int 1))))]
   "TARGET_SIMD"
   "sqdml<SBINQOPS:as>l2\\t%<vw2>0<Vmwtype>, %<v>2<Vmtype>, %3.<Vetype>[0]"
-  [(set_attr "simd_type" "simd_sat_mlal")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mla_<Vetype>_scalar_long")]
 )
 
 (define_expand "aarch64_sqdmlal2_n<mode>"
@@ -2622,8 +2597,7 @@
 	     (const_int 1)))]
   "TARGET_SIMD"
   "sqdmull\\t%<vw2>0<Vmwtype>, %<v>1<Vmtype>, %<v>2<Vmtype>"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_long")]
 )
 
 ;; vqdmull_lane
@@ -2643,8 +2617,7 @@
 	     (const_int 1)))]
   "TARGET_SIMD"
   "sqdmull\\t%<vw2>0<Vmwtype>, %<v>1<Vmtype>, %2.<Vetype>[%3]"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_scalar_long")]
 )
 
 (define_insn "aarch64_sqdmull_lane<mode>_internal"
@@ -2661,8 +2634,7 @@
 	     (const_int 1)))]
   "TARGET_SIMD"
   "sqdmull\\t%<vw2>0<Vmwtype>, %<v>1<Vmtype>, %2.<Vetype>[%3]"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_scalar_long")]
 )
 
 (define_expand "aarch64_sqdmull_lane<mode>"
@@ -2706,8 +2678,7 @@
 	     (const_int 1)))]
   "TARGET_SIMD"
   "sqdmull\\t%<vw2>0<Vmwtype>, %<v>1<Vmtype>, %2.<Vetype>[0]"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_scalar_long")]
 )
 
 ;; vqdmull2
@@ -2730,8 +2701,7 @@
 	     (const_int 1)))]
   "TARGET_SIMD"
   "sqdmull2\\t%<vw2>0<Vmwtype>, %<v>1<Vmtype>, %<v>2<Vmtype>"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_scalar_long")]
 )
 
 (define_expand "aarch64_sqdmull2<mode>"
@@ -2765,8 +2735,7 @@
 	     (const_int 1)))]
   "TARGET_SIMD"
   "sqdmull2\\t%<vw2>0<Vmwtype>, %<v>1<Vmtype>, %2.<Vetype>[%3]"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_scalar_long")]
 )
 
 (define_expand "aarch64_sqdmull2_lane<mode>"
@@ -2816,8 +2785,7 @@
 	     (const_int 1)))]
   "TARGET_SIMD"
   "sqdmull2\\t%<vw2>0<Vmwtype>, %<v>1<Vmtype>, %2.<Vetype>[0]"
-  [(set_attr "simd_type" "simd_sat_mul")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_mul_<Vetype>_scalar_long")]
 )
 
 (define_expand "aarch64_sqdmull2_n<mode>"
@@ -2842,8 +2810,7 @@
          VSHL))]
   "TARGET_SIMD"
   "<sur>shl\\t%<v>0<Vmtype>, %<v>1<Vmtype>, %<v>2<Vmtype>";
-  [(set_attr "simd_type" "simd_shift")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_reg<q>")]
 )
 
 
@@ -2857,32 +2824,9 @@
          VQSHL))]
   "TARGET_SIMD"
   "<sur>q<r>shl\\t%<v>0<Vmtype>, %<v>1<Vmtype>, %<v>2<Vmtype>";
-  [(set_attr "simd_type" "simd_sat_shift")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_shift_reg<q>")]
 )
 
-;; vshl_n
-
-(define_expand "aarch64_sshl_n<mode>"
-  [(match_operand:VSDQ_I_DI 0 "register_operand" "=w")
-   (match_operand:VSDQ_I_DI 1 "register_operand" "w")
-   (match_operand:SI 2 "immediate_operand" "i")]
-  "TARGET_SIMD"
-{
-  emit_insn (gen_ashl<mode>3 (operands[0], operands[1], operands[2]));
-  DONE;
-})
-
-(define_expand "aarch64_ushl_n<mode>"
-  [(match_operand:VSDQ_I_DI 0 "register_operand" "=w")
-   (match_operand:VSDQ_I_DI 1 "register_operand" "w")
-   (match_operand:SI 2 "immediate_operand" "i")]
-  "TARGET_SIMD"
-{
-  emit_insn (gen_ashl<mode>3 (operands[0], operands[1], operands[2]));
-  DONE;
-})
-
 ;; vshll_n
 
 (define_insn "aarch64_<sur>shll_n<mode>"
@@ -2901,8 +2845,7 @@
   else {
     return \"<sur>shll\\t%0.<Vwtype>, %1.<Vtype>, %2\";
   }"
-  [(set_attr "simd_type" "simd_shift_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_imm_long")]
 )
 
 ;; vshll_high_n
@@ -2923,32 +2866,9 @@
   else {
     return \"<sur>shll2\\t%0.<Vwtype>, %1.<Vtype>, %2\";
   }"
-  [(set_attr "simd_type" "simd_shift_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_imm_long")]
 )
 
-;; vshr_n
-
-(define_expand "aarch64_sshr_n<mode>"
-  [(match_operand:VSDQ_I_DI 0 "register_operand" "=w")
-   (match_operand:VSDQ_I_DI 1 "register_operand" "w")
-   (match_operand:SI 2 "immediate_operand" "i")]
-  "TARGET_SIMD"
-{
-  emit_insn (gen_ashr<mode>3 (operands[0], operands[1], operands[2]));
-  DONE;
-})
-
-(define_expand "aarch64_ushr_n<mode>"
-  [(match_operand:VSDQ_I_DI 0 "register_operand" "=w")
-   (match_operand:VSDQ_I_DI 1 "register_operand" "w")
-   (match_operand:SI 2 "immediate_operand" "i")]
-  "TARGET_SIMD"
-{
-  emit_insn (gen_lshr<mode>3 (operands[0], operands[1], operands[2]));
-  DONE;
-})
-
 ;; vrshr_n
 
 (define_insn "aarch64_<sur>shr_n<mode>"
@@ -2961,8 +2881,7 @@
   int bit_width = GET_MODE_UNIT_SIZE (<MODE>mode) * BITS_PER_UNIT;
   aarch64_simd_const_bounds (operands[2], 1, bit_width + 1);
   return \"<sur>shr\\t%<v>0<Vmtype>, %<v>1<Vmtype>, %2\";"
-  [(set_attr "simd_type" "simd_shift_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_shift_imm<q>")]
 )
 
 ;; v(r)sra_n
@@ -2978,8 +2897,7 @@
   int bit_width = GET_MODE_UNIT_SIZE (<MODE>mode) * BITS_PER_UNIT;
   aarch64_simd_const_bounds (operands[3], 1, bit_width + 1);
   return \"<sur>sra\\t%<v>0<Vmtype>, %<v>2<Vmtype>, %3\";"
-  [(set_attr "simd_type" "simd_shift_imm_acc")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_acc<q>")]
 )
 
 ;; vs<lr>i_n
@@ -2996,8 +2914,7 @@
   aarch64_simd_const_bounds (operands[3], 1 - <VSLRI:offsetlr>,
                              bit_width - <VSLRI:offsetlr> + 1);
   return \"s<lr>i\\t%<v>0<Vmtype>, %<v>2<Vmtype>, %3\";"
-  [(set_attr "simd_type" "simd_shift_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_shift_imm<q>")]
 )
 
 ;; vqshl(u)
@@ -3012,8 +2929,7 @@
   int bit_width = GET_MODE_UNIT_SIZE (<MODE>mode) * BITS_PER_UNIT;
   aarch64_simd_const_bounds (operands[2], 0, bit_width);
   return \"<sur>qshl<u>\\t%<v>0<Vmtype>, %<v>1<Vmtype>, %2\";"
-  [(set_attr "simd_type" "simd_sat_shift_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_shift_imm<q>")]
 )
 
 
@@ -3029,8 +2945,7 @@
   int bit_width = GET_MODE_UNIT_SIZE (<MODE>mode) * BITS_PER_UNIT;
   aarch64_simd_const_bounds (operands[2], 1, bit_width + 1);
   return \"<sur>q<r>shr<u>n\\t%<vn2>0<Vmntype>, %<v>1<Vmtype>, %2\";"
-  [(set_attr "simd_type" "simd_sat_shiftn_imm")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_sat_shift_imm_narrow_q")]
 )
 
 
@@ -3049,8 +2964,7 @@
   "@
   cm<n_optab>\t%<v>0<Vmtype>, %<v><cmp_1><Vmtype>, %<v><cmp_2><Vmtype>
   cm<optab>\t%<v>0<Vmtype>, %<v>1<Vmtype>, #0"
-  [(set_attr "simd_type" "simd_cmp")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_compare<q>, neon_compare_zero<q>")]
 )
 
 (define_insn_and_split "aarch64_cm<optab>di"
@@ -3059,7 +2973,8 @@
 	  (COMPARISONS:DI
 	    (match_operand:DI 1 "register_operand" "w,w,r")
 	    (match_operand:DI 2 "aarch64_simd_reg_or_zero" "w,ZDz,r")
-	  )))]
+	  )))
+     (clobber (reg:CC CC_REGNUM))]
   "TARGET_SIMD"
   "@
   cm<n_optab>\t%d0, %d<cmp_1>, %d<cmp_2>
@@ -3070,15 +2985,7 @@
       happening in the 'w' constraint cases.  */
    && GP_REGNUM_P (REGNO (operands[0]))
    && GP_REGNUM_P (REGNO (operands[1]))"
-  [(set (reg:CC CC_REGNUM)
-    (compare:CC
-      (match_dup 1)
-      (match_dup 2)))
-  (set (match_dup 0)
-    (neg:DI
-      (COMPARISONS:DI
-	(match_operand 3 "cc_register" "")
-	(const_int 0))))]
+  [(const_int 0)]
   {
     enum machine_mode mode = SELECT_CC_MODE (<CMP>, operands[1], operands[2]);
     rtx cc_reg = aarch64_gen_compare_reg (<CMP>, operands[1], operands[2]);
@@ -3086,8 +2993,7 @@
     emit_insn (gen_cstoredi_neg (operands[0], comparison, cc_reg));
     DONE;
   }
-  [(set_attr "simd_type" "simd_cmp")
-   (set_attr "simd_mode" "DI")]
+  [(set_attr "type" "neon_compare, neon_compare_zero, multiple")]
 )
 
 ;; cm(hs|hi)
@@ -3101,8 +3007,7 @@
 	  )))]
   "TARGET_SIMD"
   "cm<n_optab>\t%<v>0<Vmtype>, %<v><cmp_1><Vmtype>, %<v><cmp_2><Vmtype>"
-  [(set_attr "simd_type" "simd_cmp")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_compare<q>")]
 )
 
 (define_insn_and_split "aarch64_cm<optab>di"
@@ -3111,7 +3016,8 @@
 	  (UCOMPARISONS:DI
 	    (match_operand:DI 1 "register_operand" "w,r")
 	    (match_operand:DI 2 "aarch64_simd_reg_or_zero" "w,r")
-	  )))]
+	  )))
+    (clobber (reg:CC CC_REGNUM))]
   "TARGET_SIMD"
   "@
   cm<n_optab>\t%d0, %d<cmp_1>, %d<cmp_2>
@@ -3121,24 +3027,15 @@
       happening in the 'w' constraint cases.  */
    && GP_REGNUM_P (REGNO (operands[0]))
    && GP_REGNUM_P (REGNO (operands[1]))"
-  [(set (reg:CC CC_REGNUM)
-    (compare:CC
-      (match_dup 1)
-      (match_dup 2)))
-  (set (match_dup 0)
-    (neg:DI
-      (UCOMPARISONS:DI
-	(match_operand 3 "cc_register" "")
-	(const_int 0))))]
+  [(const_int 0)]
   {
-    enum machine_mode mode = SELECT_CC_MODE (<CMP>, operands[1], operands[2]);
+    enum machine_mode mode = CCmode;
     rtx cc_reg = aarch64_gen_compare_reg (<CMP>, operands[1], operands[2]);
     rtx comparison = gen_rtx_<CMP> (mode, operands[1], operands[2]);
     emit_insn (gen_cstoredi_neg (operands[0], comparison, cc_reg));
     DONE;
   }
-  [(set_attr "simd_type" "simd_cmp")
-   (set_attr "simd_mode" "DI")]
+  [(set_attr "type" "neon_compare, neon_compare_zero")]
 )
 
 ;; cmtst
@@ -3153,8 +3050,7 @@
 	    (vec_duplicate:<V_cmp_result> (const_int 0)))))]
   "TARGET_SIMD"
   "cmtst\t%<v>0<Vmtype>, %<v>1<Vmtype>, %<v>2<Vmtype>"
-  [(set_attr "simd_type" "simd_cmp")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_tst<q>")]
 )
 
 (define_insn_and_split "aarch64_cmtstdi"
@@ -3164,7 +3060,8 @@
 	    (and:DI
 	      (match_operand:DI 1 "register_operand" "w,r")
 	      (match_operand:DI 2 "register_operand" "w,r"))
-	    (const_int 0))))]
+	    (const_int 0))))
+    (clobber (reg:CC CC_REGNUM))]
   "TARGET_SIMD"
   "@
   cmtst\t%d0, %d1, %d2
@@ -3174,16 +3071,7 @@
       happening in the 'w' constraint cases.  */
    && GP_REGNUM_P (REGNO (operands[0]))
    && GP_REGNUM_P (REGNO (operands[1]))"
-   [(set (reg:CC_NZ CC_REGNUM)
-	(compare:CC_NZ
-	 (and:DI (match_dup 1)
-		  (match_dup 2))
-	 (const_int 0)))
-  (set (match_dup 0)
-    (neg:DI
-      (ne:DI
-	(match_operand 3 "cc_register" "")
-	(const_int 0))))]
+  [(const_int 0)]
   {
     rtx and_tree = gen_rtx_AND (DImode, operands[1], operands[2]);
     enum machine_mode mode = SELECT_CC_MODE (NE, and_tree, const0_rtx);
@@ -3192,8 +3080,7 @@
     emit_insn (gen_cstoredi_neg (operands[0], comparison, cc_reg));
     DONE;
   }
-  [(set_attr "simd_type" "simd_cmp")
-   (set_attr "simd_mode" "DI")]
+  [(set_attr "type" "neon_tst")]
 )
 
 ;; fcm(eq|ge|gt|le|lt)
@@ -3209,8 +3096,23 @@
   "@
   fcm<n_optab>\t%<v>0<Vmtype>, %<v><cmp_1><Vmtype>, %<v><cmp_2><Vmtype>
   fcm<optab>\t%<v>0<Vmtype>, %<v>1<Vmtype>, 0"
-  [(set_attr "simd_type" "simd_fcmp")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_compare_<Vetype><q>")]
+)
+
+;; fac(ge|gt)
+;; Note we can also handle what would be fac(le|lt) by
+;; generating fac(ge|gt).
+
+(define_insn "*aarch64_fac<optab><mode>"
+  [(set (match_operand:<V_cmp_result> 0 "register_operand" "=w")
+	(neg:<V_cmp_result>
+	  (FAC_COMPARISONS:<V_cmp_result>
+	    (abs:VALLF (match_operand:VALLF 1 "register_operand" "w"))
+	    (abs:VALLF (match_operand:VALLF 2 "register_operand" "w"))
+  )))]
+  "TARGET_SIMD"
+  "fac<n_optab>\t%<v>0<Vmtype>, %<v><cmp_1><Vmtype>, %<v><cmp_2><Vmtype>"
+  [(set_attr "type" "neon_fp_compare_<Vetype><q>")]
 )
 
 ;; addp
@@ -3223,8 +3125,7 @@
           UNSPEC_ADDP))]
   "TARGET_SIMD"
   "addp\t%<v>0<Vmtype>, %<v>1<Vmtype>, %<v>2<Vmtype>"
-  [(set_attr "simd_type" "simd_add")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_reduc_add<q>")]
 )
 
 (define_insn "aarch64_addpdi"
@@ -3234,32 +3135,7 @@
           UNSPEC_ADDP))]
   "TARGET_SIMD"
   "addp\t%d0, %1.2d"
-  [(set_attr "simd_type" "simd_add")
-   (set_attr "simd_mode" "DI")]
-)
-
-;; v(max|min)
-
-(define_expand "aarch64_<maxmin><mode>"
- [(set (match_operand:VDQ_BHSI 0 "register_operand" "=w")
-       (MAXMIN:VDQ_BHSI (match_operand:VDQ_BHSI 1 "register_operand" "w")
-			(match_operand:VDQ_BHSI 2 "register_operand" "w")))]
- "TARGET_SIMD"
-{
-  emit_insn (gen_<maxmin><mode>3 (operands[0], operands[1], operands[2]));
-  DONE;
-})
-
-
-(define_insn "aarch64_<fmaxmin><mode>"
-  [(set (match_operand:VDQF 0 "register_operand" "=w")
-        (unspec:VDQF [(match_operand:VDQF 1 "register_operand" "w")
-		      (match_operand:VDQF 2 "register_operand" "w")]
-		      FMAXMIN))]
-  "TARGET_SIMD"
-  "<fmaxmin>\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_fminmax")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_reduc_add")]
 )
 
 ;; sqrt
@@ -3269,20 +3145,9 @@
         (sqrt:VDQF (match_operand:VDQF 1 "register_operand" "w")))]
   "TARGET_SIMD"
   "fsqrt\\t%0.<Vtype>, %1.<Vtype>"
-  [(set_attr "simd_type" "simd_fsqrt")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_fp_sqrt_<Vetype><q>")]
 )
 
-(define_expand "aarch64_sqrt<mode>"
-  [(match_operand:VDQF 0 "register_operand" "=w")
-   (match_operand:VDQF 1 "register_operand" "w")]
-  "TARGET_SIMD"
-{
-  emit_insn (gen_sqrt<mode>2 (operands[0], operands[1]));
-  DONE;
-})
-
-
 ;; Patterns for vector struct loads and stores.
 
 (define_insn "vec_load_lanesoi<mode>"
@@ -3292,8 +3157,8 @@
 		   UNSPEC_LD2))]
   "TARGET_SIMD"
   "ld2\\t{%S0.<Vtype> - %T0.<Vtype>}, %1"
-  [(set_attr "simd_type" "simd_load2")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load2_2reg<q>")]
+)
 
 (define_insn "vec_store_lanesoi<mode>"
   [(set (match_operand:OI 0 "aarch64_simd_struct_operand" "=Utv")
@@ -3302,8 +3167,8 @@
                    UNSPEC_ST2))]
   "TARGET_SIMD"
   "st2\\t{%S1.<Vtype> - %T1.<Vtype>}, %0"
-  [(set_attr "simd_type" "simd_store2")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_store2_2reg<q>")]
+)
 
 (define_insn "vec_load_lanesci<mode>"
   [(set (match_operand:CI 0 "register_operand" "=w")
@@ -3312,8 +3177,8 @@
 		   UNSPEC_LD3))]
   "TARGET_SIMD"
   "ld3\\t{%S0.<Vtype> - %U0.<Vtype>}, %1"
-  [(set_attr "simd_type" "simd_load3")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load3_3reg<q>")]
+)
 
 (define_insn "vec_store_lanesci<mode>"
   [(set (match_operand:CI 0 "aarch64_simd_struct_operand" "=Utv")
@@ -3322,8 +3187,8 @@
                    UNSPEC_ST3))]
   "TARGET_SIMD"
   "st3\\t{%S1.<Vtype> - %U1.<Vtype>}, %0"
-  [(set_attr "simd_type" "simd_store3")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_store3_3reg<q>")]
+)
 
 (define_insn "vec_load_lanesxi<mode>"
   [(set (match_operand:XI 0 "register_operand" "=w")
@@ -3332,8 +3197,8 @@
 		   UNSPEC_LD4))]
   "TARGET_SIMD"
   "ld4\\t{%S0.<Vtype> - %V0.<Vtype>}, %1"
-  [(set_attr "simd_type" "simd_load4")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load4_4reg<q>")]
+)
 
 (define_insn "vec_store_lanesxi<mode>"
   [(set (match_operand:XI 0 "aarch64_simd_struct_operand" "=Utv")
@@ -3342,8 +3207,8 @@
                    UNSPEC_ST4))]
   "TARGET_SIMD"
   "st4\\t{%S1.<Vtype> - %V1.<Vtype>}, %0"
-  [(set_attr "simd_type" "simd_store4")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_store4_4reg<q>")]
+)
 
 ;; Reload patterns for AdvSIMD register list operands.
 
@@ -3375,9 +3240,10 @@
     default: gcc_unreachable ();
     }
 }
-  [(set_attr "simd_type" "simd_move,simd_store<nregs>,simd_load<nregs>")
-   (set (attr "length") (symbol_ref "aarch64_simd_attr_length_move (insn)"))
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_move,neon_store<nregs>_<nregs>reg_q,\
+                     neon_load<nregs>_<nregs>reg_q")
+   (set (attr "length") (symbol_ref "aarch64_simd_attr_length_move (insn)"))]
+)
 
 (define_split
   [(set (match_operand:OI 0 "register_operand" "")
@@ -3459,8 +3325,8 @@
 	     (vec_duplicate:VD (const_int 0)))) 0))]
   "TARGET_SIMD"
   "ld2\\t{%S0.<Vtype> - %T0.<Vtype>}, %1"
-  [(set_attr "simd_type" "simd_load2")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load2_2reg<q>")]
+)
 
 (define_insn "aarch64_ld2<mode>_dreg"
   [(set (match_operand:OI 0 "register_operand" "=w")
@@ -3476,8 +3342,8 @@
 	     (const_int 0))) 0))]
   "TARGET_SIMD"
   "ld1\\t{%S0.1d - %T0.1d}, %1"
-  [(set_attr "simd_type" "simd_load2")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load1_2reg<q>")]
+)
 
 (define_insn "aarch64_ld3<mode>_dreg"
   [(set (match_operand:CI 0 "register_operand" "=w")
@@ -3498,8 +3364,8 @@
 	     (vec_duplicate:VD (const_int 0)))) 0))]
   "TARGET_SIMD"
   "ld3\\t{%S0.<Vtype> - %U0.<Vtype>}, %1"
-  [(set_attr "simd_type" "simd_load3")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load3_3reg<q>")]
+)
 
 (define_insn "aarch64_ld3<mode>_dreg"
   [(set (match_operand:CI 0 "register_operand" "=w")
@@ -3520,8 +3386,8 @@
 	     (const_int 0))) 0))]
   "TARGET_SIMD"
   "ld1\\t{%S0.1d - %U0.1d}, %1"
-  [(set_attr "simd_type" "simd_load3")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load1_3reg<q>")]
+)
 
 (define_insn "aarch64_ld4<mode>_dreg"
   [(set (match_operand:XI 0 "register_operand" "=w")
@@ -3547,8 +3413,8 @@
 	       (vec_duplicate:VD (const_int 0))))) 0))]
   "TARGET_SIMD"
   "ld4\\t{%S0.<Vtype> - %V0.<Vtype>}, %1"
-  [(set_attr "simd_type" "simd_load4")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load4_4reg<q>")]
+)
 
 (define_insn "aarch64_ld4<mode>_dreg"
   [(set (match_operand:XI 0 "register_operand" "=w")
@@ -3574,8 +3440,8 @@
 	       (const_int 0)))) 0))]
   "TARGET_SIMD"
   "ld1\\t{%S0.1d - %V0.1d}, %1"
-  [(set_attr "simd_type" "simd_load4")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load1_4reg<q>")]
+)
 
 (define_expand "aarch64_ld<VSTRUCT:nregs><VDC:mode>"
  [(match_operand:VSTRUCT 0 "register_operand" "=w")
@@ -3689,8 +3555,7 @@
 		   UNSPEC_TBL))]
   "TARGET_SIMD"
   "tbl\\t%0.<Vtype>, {%1.16b}, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_tbl")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_tbl1<q>")]
 )
 
 ;; Two source registers.
@@ -3702,8 +3567,7 @@
 		      UNSPEC_TBL))]
   "TARGET_SIMD"
   "tbl\\t%0.16b, {%S1.16b - %T1.16b}, %2.16b"
-  [(set_attr "simd_type" "simd_tbl")
-   (set_attr "simd_mode" "V16QI")]
+  [(set_attr "type" "neon_tbl2_q")]
 )
 
 (define_insn_and_split "aarch64_combinev16qi"
@@ -3718,7 +3582,9 @@
 {
   aarch64_split_combinev16qi (operands);
   DONE;
-})
+}
+[(set_attr "type" "multiple")]
+)
 
 (define_insn "aarch64_<PERMUTE:perm_insn><PERMUTE:perm_hilo><mode>"
   [(set (match_operand:VALL 0 "register_operand" "=w")
@@ -3727,8 +3593,7 @@
 		       PERMUTE))]
   "TARGET_SIMD"
   "<PERMUTE:perm_insn><PERMUTE:perm_hilo>\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
-  [(set_attr "simd_type" "simd_<PERMUTE:perm_insn>")
-   (set_attr "simd_mode" "<MODE>")]
+  [(set_attr "type" "neon_permute<q>")]
 )
 
 (define_insn "aarch64_st2<mode>_dreg"
@@ -3738,8 +3603,8 @@
                    UNSPEC_ST2))]
   "TARGET_SIMD"
   "st2\\t{%S1.<Vtype> - %T1.<Vtype>}, %0"
-  [(set_attr "simd_type" "simd_store2")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_store2_2reg")]
+)
 
 (define_insn "aarch64_st2<mode>_dreg"
   [(set (match_operand:TI 0 "aarch64_simd_struct_operand" "=Utv")
@@ -3748,8 +3613,8 @@
                    UNSPEC_ST2))]
   "TARGET_SIMD"
   "st1\\t{%S1.1d - %T1.1d}, %0"
-  [(set_attr "simd_type" "simd_store2")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_store1_2reg")]
+)
 
 (define_insn "aarch64_st3<mode>_dreg"
   [(set (match_operand:EI 0 "aarch64_simd_struct_operand" "=Utv")
@@ -3758,8 +3623,8 @@
                    UNSPEC_ST3))]
   "TARGET_SIMD"
   "st3\\t{%S1.<Vtype> - %U1.<Vtype>}, %0"
-  [(set_attr "simd_type" "simd_store3")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_store3_3reg")]
+)
 
 (define_insn "aarch64_st3<mode>_dreg"
   [(set (match_operand:EI 0 "aarch64_simd_struct_operand" "=Utv")
@@ -3768,8 +3633,8 @@
                    UNSPEC_ST3))]
   "TARGET_SIMD"
   "st1\\t{%S1.1d - %U1.1d}, %0"
-  [(set_attr "simd_type" "simd_store3")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_store1_3reg")]
+)
 
 (define_insn "aarch64_st4<mode>_dreg"
   [(set (match_operand:OI 0 "aarch64_simd_struct_operand" "=Utv")
@@ -3778,8 +3643,8 @@
                    UNSPEC_ST4))]
   "TARGET_SIMD"
   "st4\\t{%S1.<Vtype> - %V1.<Vtype>}, %0"
-  [(set_attr "simd_type" "simd_store4")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_store4_4reg")]
+)
 
 (define_insn "aarch64_st4<mode>_dreg"
   [(set (match_operand:OI 0 "aarch64_simd_struct_operand" "=Utv")
@@ -3788,8 +3653,8 @@
                    UNSPEC_ST4))]
   "TARGET_SIMD"
   "st1\\t{%S1.1d - %V1.1d}, %0"
-  [(set_attr "simd_type" "simd_store4")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_store1_4reg")]
+)
 
 (define_expand "aarch64_st<VSTRUCT:nregs><VDC:mode>"
  [(match_operand:DI 0 "register_operand" "r")
@@ -3867,5 +3732,154 @@
 	  (match_operand:<VEL> 1 "aarch64_simd_struct_operand" "Utv")))]
   "TARGET_SIMD"
   "ld1r\\t{%0.<Vtype>}, %1"
-  [(set_attr "simd_type" "simd_load1r")
-   (set_attr "simd_mode" "<MODE>")])
+  [(set_attr "type" "neon_load1_all_lanes")]
+)
+
+(define_insn "aarch64_frecpe<mode>"
+  [(set (match_operand:VDQF 0 "register_operand" "=w")
+	(unspec:VDQF [(match_operand:VDQF 1 "register_operand" "w")]
+		    UNSPEC_FRECPE))]
+  "TARGET_SIMD"
+  "frecpe\\t%0.<Vtype>, %1.<Vtype>"
+  [(set_attr "type" "neon_fp_recpe_<Vetype><q>")]
+)
+
+(define_insn "aarch64_frecp<FRECP:frecp_suffix><mode>"
+  [(set (match_operand:GPF 0 "register_operand" "=w")
+	(unspec:GPF [(match_operand:GPF 1 "register_operand" "w")]
+		    FRECP))]
+  "TARGET_SIMD"
+  "frecp<FRECP:frecp_suffix>\\t%<s>0, %<s>1"
+  [(set_attr "type" "neon_fp_recp<FRECP:frecp_suffix>_<GPF:Vetype><GPF:q>")]
+)
+
+(define_insn "aarch64_frecps<mode>"
+  [(set (match_operand:VALLF 0 "register_operand" "=w")
+	(unspec:VALLF [(match_operand:VALLF 1 "register_operand" "w")
+		     (match_operand:VALLF 2 "register_operand" "w")]
+		    UNSPEC_FRECPS))]
+  "TARGET_SIMD"
+  "frecps\\t%<v>0<Vmtype>, %<v>1<Vmtype>, %<v>2<Vmtype>"
+  [(set_attr "type" "neon_fp_recps_<Vetype><q>")]
+)
+
+;; aes
+
+(define_insn "aarch64_crypto_aes<aes_op>v16qi"
+  [(set (match_operand:V16QI 0 "register_operand" "=w")
+        (unspec:V16QI [(match_operand:V16QI 1 "register_operand" "0")
+		       (match_operand:V16QI 2 "register_operand" "w")]
+         CRYPTO_AES))]
+  "TARGET_SIMD && TARGET_CRYPTO"
+  "aes<aes_op>\\t%0.16b, %2.16b"
+  [(set_attr "type" "crypto_aes")]
+)
+
+(define_insn "aarch64_crypto_aes<aesmc_op>v16qi"
+  [(set (match_operand:V16QI 0 "register_operand" "=w")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand" "w")]
+	 CRYPTO_AESMC))]
+  "TARGET_SIMD && TARGET_CRYPTO"
+  "aes<aesmc_op>\\t%0.16b, %1.16b"
+  [(set_attr "type" "crypto_aes")]
+)
+
+;; sha1
+
+(define_insn "aarch64_crypto_sha1hsi"
+  [(set (match_operand:SI 0 "register_operand" "=w")
+        (unspec:SI [(match_operand:SI 1
+                       "register_operand" "w")]
+         UNSPEC_SHA1H))]
+  "TARGET_SIMD && TARGET_CRYPTO"
+  "sha1h\\t%s0, %s1"
+   [(set_attr "type" "crypto_sha1_fast")]
+)
+
+(define_insn "aarch64_crypto_sha1su1v4si"
+  [(set (match_operand:V4SI 0 "register_operand" "=w")
+        (unspec:V4SI [(match_operand:V4SI 1 "register_operand" "0")
+                      (match_operand:V4SI 2 "register_operand" "w")]
+         UNSPEC_SHA1SU1))]
+  "TARGET_SIMD && TARGET_CRYPTO"
+  "sha1su1\\t%0.4s, %2.4s"
+  [(set_attr "type" "crypto_sha1_fast")]
+)
+
+(define_insn "aarch64_crypto_sha1<sha1_op>v4si"
+  [(set (match_operand:V4SI 0 "register_operand" "=w")
+        (unspec:V4SI [(match_operand:V4SI 1 "register_operand" "0")
+                      (match_operand:SI 2 "register_operand" "w")
+                      (match_operand:V4SI 3 "register_operand" "w")]
+         CRYPTO_SHA1))]
+  "TARGET_SIMD && TARGET_CRYPTO"
+  "sha1<sha1_op>\\t%q0, %s2, %3.4s"
+  [(set_attr "type" "crypto_sha1_slow")]
+)
+
+(define_insn "aarch64_crypto_sha1su0v4si"
+  [(set (match_operand:V4SI 0 "register_operand" "=w")
+        (unspec:V4SI [(match_operand:V4SI 1 "register_operand" "0")
+                      (match_operand:V4SI 2 "register_operand" "w")
+                      (match_operand:V4SI 3 "register_operand" "w")]
+         UNSPEC_SHA1SU0))]
+  "TARGET_SIMD && TARGET_CRYPTO"
+  "sha1su0\\t%0.4s, %2.4s, %3.4s"
+  [(set_attr "type" "crypto_sha1_xor")]
+)
+
+;; sha256
+
+(define_insn "aarch64_crypto_sha256h<sha256_op>v4si"
+  [(set (match_operand:V4SI 0 "register_operand" "=w")
+        (unspec:V4SI [(match_operand:V4SI 1 "register_operand" "0")
+                      (match_operand:V4SI 2 "register_operand" "w")
+                      (match_operand:V4SI 3 "register_operand" "w")]
+         CRYPTO_SHA256))]
+  "TARGET_SIMD && TARGET_CRYPTO"
+  "sha256h<sha256_op>\\t%q0, %q2, %3.4s"
+  [(set_attr "type" "crypto_sha256_slow")]
+)
+
+(define_insn "aarch64_crypto_sha256su0v4si"
+  [(set (match_operand:V4SI 0 "register_operand" "=w")
+        (unspec:V4SI [(match_operand:V4SI 1 "register_operand" "0")
+                      (match_operand:V4SI 2 "register_operand" "w")]
+         UNSPEC_SHA256SU0))]
+  "TARGET_SIMD &&TARGET_CRYPTO"
+  "sha256su0\\t%0.4s, %2.4s"
+ [(set_attr "type" "crypto_sha256_fast")]
+)
+
+(define_insn "aarch64_crypto_sha256su1v4si"
+  [(set (match_operand:V4SI 0 "register_operand" "=w")
+        (unspec:V4SI [(match_operand:V4SI 1 "register_operand" "0")
+                      (match_operand:V4SI 2 "register_operand" "w")
+                      (match_operand:V4SI 3 "register_operand" "w")]
+         UNSPEC_SHA256SU1))]
+  "TARGET_SIMD &&TARGET_CRYPTO"
+  "sha256su1\\t%0.4s, %2.4s, %3.4s"
+  [(set_attr "type" "crypto_sha256_slow")]
+)
+
+;; pmull
+
+(define_insn "aarch64_crypto_pmulldi"
+  [(set (match_operand:TI 0 "register_operand" "=w")
+        (unspec:TI  [(match_operand:DI 1 "register_operand" "w")
+		     (match_operand:DI 2 "register_operand" "w")]
+		    UNSPEC_PMULL))]
+ "TARGET_SIMD && TARGET_CRYPTO"
+ "pmull\\t%0.1q, %1.1d, %2.1d"
+  [(set_attr "type" "neon_mul_d_long")]
+)
+
+(define_insn "aarch64_crypto_pmullv2di"
+ [(set (match_operand:TI 0 "register_operand" "=w")
+       (unspec:TI [(match_operand:V2DI 1 "register_operand" "w")
+		   (match_operand:V2DI 2 "register_operand" "w")]
+		  UNSPEC_PMULL2))]
+  "TARGET_SIMD && TARGET_CRYPTO"
+  "pmull2\\t%0.1q, %1.2d, %2.2d"
+  [(set_attr "type" "neon_mul_d_long")]
+)
Index: b/src/gcc/config/aarch64/predicates.md
===================================================================
--- a/src/gcc/config/aarch64/predicates.md
+++ b/src/gcc/config/aarch64/predicates.md
@@ -115,16 +115,11 @@
        (match_test "aarch64_legitimate_address_p (mode, XEXP (op, 0), PARALLEL,
 					       0)")))
 
-(define_predicate "aarch64_const_address"
-  (and (match_code "symbol_ref")
-       (match_test "mode == DImode && CONSTANT_ADDRESS_P (op)")))
-
 (define_predicate "aarch64_valid_symref"
   (match_code "const, symbol_ref, label_ref")
 {
-  enum aarch64_symbol_type symbol_type;
-  return (aarch64_symbolic_constant_p (op, SYMBOL_CONTEXT_ADR, &symbol_type)
-	 && symbol_type != SYMBOL_FORCE_TO_MEM);
+  return (aarch64_classify_symbolic_expression (op, SYMBOL_CONTEXT_ADR)
+	  != SYMBOL_FORCE_TO_MEM);
 })
 
 (define_predicate "aarch64_tls_ie_symref"
@@ -170,15 +165,10 @@
 })
 
 (define_predicate "aarch64_mov_operand"
-  (and (match_code "reg,subreg,mem,const_int,symbol_ref,high")
+  (and (match_code "reg,subreg,mem,const,const_int,symbol_ref,label_ref,high")
        (ior (match_operand 0 "register_operand")
 	    (ior (match_operand 0 "memory_operand")
-		 (ior (match_test "GET_CODE (op) == HIGH
-				   && aarch64_valid_symref (XEXP (op, 0),
-							    GET_MODE (XEXP (op, 0)))")
-		      (ior (match_test "CONST_INT_P (op)
-					&& aarch64_move_imm (INTVAL (op), mode)")
-			   (match_test "aarch64_const_address (op, mode)")))))))
+		 (match_test "aarch64_mov_operand_p (op, SYMBOL_CONTEXT_ADR, mode)")))))
 
 (define_predicate "aarch64_movti_operand"
   (and (match_code "reg,subreg,mem,const_int")
Index: b/src/gcc/config/aarch64/aarch64-elf.h
===================================================================
--- a/src/gcc/config/aarch64/aarch64-elf.h
+++ b/src/gcc/config/aarch64/aarch64-elf.h
@@ -106,7 +106,6 @@
 
 #define ASM_COMMENT_START "//"
 
-#define REGISTER_PREFIX		""
 #define LOCAL_LABEL_PREFIX	"."
 #define USER_LABEL_PREFIX	""
 
Index: b/src/gcc/config/aarch64/arm_neon.h
===================================================================
--- a/src/gcc/config/aarch64/arm_neon.h
+++ b/src/gcc/config/aarch64/arm_neon.h
@@ -29,6 +29,9 @@
 
 #include <stdint.h>
 
+#define __AARCH64_UINT64_C(__C) ((uint64_t) __C)
+#define __AARCH64_INT64_C(__C) ((int64_t) __C)
+
 typedef __builtin_aarch64_simd_qi int8x8_t
   __attribute__ ((__vector_size__ (8)));
 typedef __builtin_aarch64_simd_hi int16x4_t
@@ -72,6 +75,8 @@ typedef __builtin_aarch64_simd_poly8 pol
   __attribute__ ((__vector_size__ (16)));
 typedef __builtin_aarch64_simd_poly16 poly16x8_t
   __attribute__ ((__vector_size__ (16)));
+typedef __builtin_aarch64_simd_poly64 poly64x2_t
+  __attribute__ ((__vector_size__ (16)));
 typedef __builtin_aarch64_simd_uqi uint8x16_t
   __attribute__ ((__vector_size__ (16)));
 typedef __builtin_aarch64_simd_uhi uint16x8_t
@@ -85,6 +90,8 @@ typedef float float32_t;
 typedef double float64_t;
 typedef __builtin_aarch64_simd_poly8 poly8_t;
 typedef __builtin_aarch64_simd_poly16 poly16_t;
+typedef __builtin_aarch64_simd_poly64 poly64_t;
+typedef __builtin_aarch64_simd_poly128 poly128_t;
 
 typedef struct int8x8x2_t
 {
@@ -446,7 +453,66 @@ typedef struct poly16x8x4_t
   poly16x8_t val[4];
 } poly16x8x4_t;
 
+/* vget_lane internal macros.  */
 
+#define __aarch64_vget_lane_any(__size, __cast_ret, __cast_a, __a, __b) \
+  (__cast_ret								\
+     __builtin_aarch64_get_lane##__size (__cast_a __a, __b))
+
+#define __aarch64_vget_lane_f32(__a, __b) \
+  __aarch64_vget_lane_any (v2sf, , , __a, __b)
+#define __aarch64_vget_lane_f64(__a, __b) (__a)
+
+#define __aarch64_vget_lane_p8(__a, __b) \
+  __aarch64_vget_lane_any (v8qi, (poly8_t), (int8x8_t), __a, __b)
+#define __aarch64_vget_lane_p16(__a, __b) \
+  __aarch64_vget_lane_any (v4hi, (poly16_t), (int16x4_t), __a, __b)
+
+#define __aarch64_vget_lane_s8(__a, __b) \
+  __aarch64_vget_lane_any (v8qi, , ,__a, __b)
+#define __aarch64_vget_lane_s16(__a, __b) \
+  __aarch64_vget_lane_any (v4hi, , ,__a, __b)
+#define __aarch64_vget_lane_s32(__a, __b) \
+  __aarch64_vget_lane_any (v2si, , ,__a, __b)
+#define __aarch64_vget_lane_s64(__a, __b) (__a)
+
+#define __aarch64_vget_lane_u8(__a, __b) \
+  __aarch64_vget_lane_any (v8qi, (uint8_t), (int8x8_t), __a, __b)
+#define __aarch64_vget_lane_u16(__a, __b) \
+  __aarch64_vget_lane_any (v4hi, (uint16_t), (int16x4_t), __a, __b)
+#define __aarch64_vget_lane_u32(__a, __b) \
+  __aarch64_vget_lane_any (v2si, (uint32_t), (int32x2_t), __a, __b)
+#define __aarch64_vget_lane_u64(__a, __b) (__a)
+
+#define __aarch64_vgetq_lane_f32(__a, __b) \
+  __aarch64_vget_lane_any (v4sf, , , __a, __b)
+#define __aarch64_vgetq_lane_f64(__a, __b) \
+  __aarch64_vget_lane_any (v2df, , , __a, __b)
+
+#define __aarch64_vgetq_lane_p8(__a, __b) \
+  __aarch64_vget_lane_any (v16qi, (poly8_t), (int8x16_t), __a, __b)
+#define __aarch64_vgetq_lane_p16(__a, __b) \
+  __aarch64_vget_lane_any (v8hi, (poly16_t), (int16x8_t), __a, __b)
+
+#define __aarch64_vgetq_lane_s8(__a, __b) \
+  __aarch64_vget_lane_any (v16qi, , ,__a, __b)
+#define __aarch64_vgetq_lane_s16(__a, __b) \
+  __aarch64_vget_lane_any (v8hi, , ,__a, __b)
+#define __aarch64_vgetq_lane_s32(__a, __b) \
+  __aarch64_vget_lane_any (v4si, , ,__a, __b)
+#define __aarch64_vgetq_lane_s64(__a, __b) \
+  __aarch64_vget_lane_any (v2di, , ,__a, __b)
+
+#define __aarch64_vgetq_lane_u8(__a, __b) \
+  __aarch64_vget_lane_any (v16qi, (uint8_t), (int8x16_t), __a, __b)
+#define __aarch64_vgetq_lane_u16(__a, __b) \
+  __aarch64_vget_lane_any (v8hi, (uint16_t), (int16x8_t), __a, __b)
+#define __aarch64_vgetq_lane_u32(__a, __b) \
+  __aarch64_vget_lane_any (v4si, (uint32_t), (int32x4_t), __a, __b)
+#define __aarch64_vgetq_lane_u64(__a, __b) \
+  __aarch64_vget_lane_any (v2di, (uint64_t), (int64x2_t), __a, __b)
+
+/* vadd  */
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vadd_s8 (int8x8_t __a, int8x8_t __b)
 {
@@ -2307,155 +2373,156 @@ vcreate_p16 (uint64_t __a)
   return (poly16x4_t) __a;
 }
 
+/* vget_lane  */
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vget_lane_f32 (float32x2_t __a, const int __b)
+{
+  return __aarch64_vget_lane_f32 (__a, __b);
+}
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vget_lane_f64 (float64x1_t __a, const int __b)
+{
+  return __aarch64_vget_lane_f64 (__a, __b);
+}
+
+__extension__ static __inline poly8_t __attribute__ ((__always_inline__))
+vget_lane_p8 (poly8x8_t __a, const int __b)
+{
+  return __aarch64_vget_lane_p8 (__a, __b);
+}
+
+__extension__ static __inline poly16_t __attribute__ ((__always_inline__))
+vget_lane_p16 (poly16x4_t __a, const int __b)
+{
+  return __aarch64_vget_lane_p16 (__a, __b);
+}
+
 __extension__ static __inline int8_t __attribute__ ((__always_inline__))
 vget_lane_s8 (int8x8_t __a, const int __b)
 {
-  return (int8_t) __builtin_aarch64_get_lane_signedv8qi (__a, __b);
+  return __aarch64_vget_lane_s8 (__a, __b);
 }
 
 __extension__ static __inline int16_t __attribute__ ((__always_inline__))
 vget_lane_s16 (int16x4_t __a, const int __b)
 {
-  return (int16_t) __builtin_aarch64_get_lane_signedv4hi (__a, __b);
+  return __aarch64_vget_lane_s16 (__a, __b);
 }
 
 __extension__ static __inline int32_t __attribute__ ((__always_inline__))
 vget_lane_s32 (int32x2_t __a, const int __b)
 {
-  return (int32_t) __builtin_aarch64_get_lane_signedv2si (__a, __b);
+  return __aarch64_vget_lane_s32 (__a, __b);
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vget_lane_f32 (float32x2_t __a, const int __b)
+__extension__ static __inline int64_t __attribute__ ((__always_inline__))
+vget_lane_s64 (int64x1_t __a, const int __b)
 {
-  return (float32_t) __builtin_aarch64_get_lanev2sf (__a, __b);
+  return __aarch64_vget_lane_s64 (__a, __b);
 }
 
 __extension__ static __inline uint8_t __attribute__ ((__always_inline__))
 vget_lane_u8 (uint8x8_t __a, const int __b)
 {
-  return (uint8_t) __builtin_aarch64_get_lane_unsignedv8qi ((int8x8_t) __a,
-							    __b);
+  return __aarch64_vget_lane_u8 (__a, __b);
 }
 
 __extension__ static __inline uint16_t __attribute__ ((__always_inline__))
 vget_lane_u16 (uint16x4_t __a, const int __b)
 {
-  return (uint16_t) __builtin_aarch64_get_lane_unsignedv4hi ((int16x4_t) __a,
-							     __b);
+  return __aarch64_vget_lane_u16 (__a, __b);
 }
 
 __extension__ static __inline uint32_t __attribute__ ((__always_inline__))
 vget_lane_u32 (uint32x2_t __a, const int __b)
 {
-  return (uint32_t) __builtin_aarch64_get_lane_unsignedv2si ((int32x2_t) __a,
-							     __b);
+  return __aarch64_vget_lane_u32 (__a, __b);
 }
 
-__extension__ static __inline poly8_t __attribute__ ((__always_inline__))
-vget_lane_p8 (poly8x8_t __a, const int __b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vget_lane_u64 (uint64x1_t __a, const int __b)
 {
-  return (poly8_t) __builtin_aarch64_get_lane_unsignedv8qi ((int8x8_t) __a,
-							    __b);
+  return __aarch64_vget_lane_u64 (__a, __b);
 }
 
-__extension__ static __inline poly16_t __attribute__ ((__always_inline__))
-vget_lane_p16 (poly16x4_t __a, const int __b)
+/* vgetq_lane  */
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vgetq_lane_f32 (float32x4_t __a, const int __b)
 {
-  return (poly16_t) __builtin_aarch64_get_lane_unsignedv4hi ((int16x4_t) __a,
-							     __b);
+  return __aarch64_vgetq_lane_f32 (__a, __b);
 }
 
-__extension__ static __inline int64_t __attribute__ ((__always_inline__))
-vget_lane_s64 (int64x1_t __a, const int __b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vgetq_lane_f64 (float64x2_t __a, const int __b)
 {
-  return (int64_t) __builtin_aarch64_get_lanedi (__a, __b);
+  return __aarch64_vgetq_lane_f64 (__a, __b);
 }
 
-__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
-vget_lane_u64 (uint64x1_t __a, const int __b)
+__extension__ static __inline poly8_t __attribute__ ((__always_inline__))
+vgetq_lane_p8 (poly8x16_t __a, const int __b)
+{
+  return __aarch64_vgetq_lane_p8 (__a, __b);
+}
+
+__extension__ static __inline poly16_t __attribute__ ((__always_inline__))
+vgetq_lane_p16 (poly16x8_t __a, const int __b)
 {
-  return (uint64_t) __builtin_aarch64_get_lanedi ((int64x1_t) __a, __b);
+  return __aarch64_vgetq_lane_p16 (__a, __b);
 }
 
 __extension__ static __inline int8_t __attribute__ ((__always_inline__))
 vgetq_lane_s8 (int8x16_t __a, const int __b)
 {
-  return (int8_t) __builtin_aarch64_get_lane_signedv16qi (__a, __b);
+  return __aarch64_vgetq_lane_s8 (__a, __b);
 }
 
 __extension__ static __inline int16_t __attribute__ ((__always_inline__))
 vgetq_lane_s16 (int16x8_t __a, const int __b)
 {
-  return (int16_t) __builtin_aarch64_get_lane_signedv8hi (__a, __b);
+  return __aarch64_vgetq_lane_s16 (__a, __b);
 }
 
 __extension__ static __inline int32_t __attribute__ ((__always_inline__))
 vgetq_lane_s32 (int32x4_t __a, const int __b)
 {
-  return (int32_t) __builtin_aarch64_get_lane_signedv4si (__a, __b);
-}
-
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vgetq_lane_f32 (float32x4_t __a, const int __b)
-{
-  return (float32_t) __builtin_aarch64_get_lanev4sf (__a, __b);
+  return __aarch64_vgetq_lane_s32 (__a, __b);
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vgetq_lane_f64 (float64x2_t __a, const int __b)
+__extension__ static __inline int64_t __attribute__ ((__always_inline__))
+vgetq_lane_s64 (int64x2_t __a, const int __b)
 {
-  return (float64_t) __builtin_aarch64_get_lanev2df (__a, __b);
+  return __aarch64_vgetq_lane_s64 (__a, __b);
 }
 
 __extension__ static __inline uint8_t __attribute__ ((__always_inline__))
 vgetq_lane_u8 (uint8x16_t __a, const int __b)
 {
-  return (uint8_t) __builtin_aarch64_get_lane_unsignedv16qi ((int8x16_t) __a,
-							     __b);
+  return __aarch64_vgetq_lane_u8 (__a, __b);
 }
 
 __extension__ static __inline uint16_t __attribute__ ((__always_inline__))
 vgetq_lane_u16 (uint16x8_t __a, const int __b)
 {
-  return (uint16_t) __builtin_aarch64_get_lane_unsignedv8hi ((int16x8_t) __a,
-							     __b);
+  return __aarch64_vgetq_lane_u16 (__a, __b);
 }
 
 __extension__ static __inline uint32_t __attribute__ ((__always_inline__))
 vgetq_lane_u32 (uint32x4_t __a, const int __b)
 {
-  return (uint32_t) __builtin_aarch64_get_lane_unsignedv4si ((int32x4_t) __a,
-							     __b);
-}
-
-__extension__ static __inline poly8_t __attribute__ ((__always_inline__))
-vgetq_lane_p8 (poly8x16_t __a, const int __b)
-{
-  return (poly8_t) __builtin_aarch64_get_lane_unsignedv16qi ((int8x16_t) __a,
-							     __b);
-}
-
-__extension__ static __inline poly16_t __attribute__ ((__always_inline__))
-vgetq_lane_p16 (poly16x8_t __a, const int __b)
-{
-  return (poly16_t) __builtin_aarch64_get_lane_unsignedv8hi ((int16x8_t) __a,
-							     __b);
-}
-
-__extension__ static __inline int64_t __attribute__ ((__always_inline__))
-vgetq_lane_s64 (int64x2_t __a, const int __b)
-{
-  return __builtin_aarch64_get_lane_unsignedv2di (__a, __b);
+  return __aarch64_vgetq_lane_u32 (__a, __b);
 }
 
 __extension__ static __inline uint64_t __attribute__ ((__always_inline__))
 vgetq_lane_u64 (uint64x2_t __a, const int __b)
 {
-  return (uint64_t) __builtin_aarch64_get_lane_unsignedv2di ((int64x2_t) __a,
-							     __b);
+  return __aarch64_vgetq_lane_u64 (__a, __b);
 }
 
+/* vreinterpret  */
+
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
 vreinterpret_p8_s8 (int8x8_t __a)
 {
@@ -3805,6 +3872,85 @@ vreinterpretq_u32_p16 (poly16x8_t __a)
   return (uint32x4_t) __builtin_aarch64_reinterpretv4siv8hi ((int16x8_t) __a);
 }
 
+#define __GET_LOW(__TYPE) \
+  uint64x2_t tmp = vreinterpretq_u64_##__TYPE (__a);  \
+  uint64_t lo = vgetq_lane_u64 (tmp, 0);  \
+  return vreinterpret_##__TYPE##_u64 (lo);
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vget_low_f32 (float32x4_t __a)
+{
+  __GET_LOW (f32);
+}
+
+__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))
+vget_low_f64 (float64x2_t __a)
+{
+  return vgetq_lane_f64 (__a, 0);
+}
+
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vget_low_p8 (poly8x16_t __a)
+{
+  __GET_LOW (p8);
+}
+
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vget_low_p16 (poly16x8_t __a)
+{
+  __GET_LOW (p16);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vget_low_s8 (int8x16_t __a)
+{
+  __GET_LOW (s8);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vget_low_s16 (int16x8_t __a)
+{
+  __GET_LOW (s16);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vget_low_s32 (int32x4_t __a)
+{
+  __GET_LOW (s32);
+}
+
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vget_low_s64 (int64x2_t __a)
+{
+  return vgetq_lane_s64 (__a, 0);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vget_low_u8 (uint8x16_t __a)
+{
+  __GET_LOW (u8);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vget_low_u16 (uint16x8_t __a)
+{
+  __GET_LOW (u16);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vget_low_u32 (uint32x4_t __a)
+{
+  __GET_LOW (u32);
+}
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vget_low_u64 (uint64x2_t __a)
+{
+  return vgetq_lane_u64 (__a, 0);
+}
+
+#undef __GET_LOW
+
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vcombine_s8 (int8x8_t __a, int8x8_t __b)
 {
@@ -4468,6790 +4614,6562 @@ vabds_f32 (float32_t a, float32_t b)
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vabs_f32 (float32x2_t a)
+__extension__ static __inline int16_t __attribute__ ((__always_inline__))
+vaddlv_s8 (int8x8_t a)
 {
-  float32x2_t result;
-  __asm__ ("fabs %0.2s,%1.2s"
+  int16_t result;
+  __asm__ ("saddlv %h0,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vabs_s8 (int8x8_t a)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vaddlv_s16 (int16x4_t a)
 {
-  int8x8_t result;
-  __asm__ ("abs %0.8b,%1.8b"
+  int32_t result;
+  __asm__ ("saddlv %s0,%1.4h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vabs_s16 (int16x4_t a)
+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
+vaddlv_u8 (uint8x8_t a)
 {
-  int16x4_t result;
-  __asm__ ("abs %0.4h,%1.4h"
+  uint16_t result;
+  __asm__ ("uaddlv %h0,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vabs_s32 (int32x2_t a)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vaddlv_u16 (uint16x4_t a)
 {
-  int32x2_t result;
-  __asm__ ("abs %0.2s,%1.2s"
+  uint32_t result;
+  __asm__ ("uaddlv %s0,%1.4h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vabsq_f32 (float32x4_t a)
+__extension__ static __inline int16_t __attribute__ ((__always_inline__))
+vaddlvq_s8 (int8x16_t a)
 {
-  float32x4_t result;
-  __asm__ ("fabs %0.4s,%1.4s"
+  int16_t result;
+  __asm__ ("saddlv %h0,%1.16b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vabsq_f64 (float64x2_t a)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vaddlvq_s16 (int16x8_t a)
 {
-  float64x2_t result;
-  __asm__ ("fabs %0.2d,%1.2d"
+  int32_t result;
+  __asm__ ("saddlv %s0,%1.8h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vabsq_s8 (int8x16_t a)
+__extension__ static __inline int64_t __attribute__ ((__always_inline__))
+vaddlvq_s32 (int32x4_t a)
 {
-  int8x16_t result;
-  __asm__ ("abs %0.16b,%1.16b"
+  int64_t result;
+  __asm__ ("saddlv %d0,%1.4s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vabsq_s16 (int16x8_t a)
+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
+vaddlvq_u8 (uint8x16_t a)
 {
-  int16x8_t result;
-  __asm__ ("abs %0.8h,%1.8h"
+  uint16_t result;
+  __asm__ ("uaddlv %h0,%1.16b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vabsq_s32 (int32x4_t a)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vaddlvq_u16 (uint16x8_t a)
 {
-  int32x4_t result;
-  __asm__ ("abs %0.4s,%1.4s"
+  uint32_t result;
+  __asm__ ("uaddlv %s0,%1.8h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vabsq_s64 (int64x2_t a)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vaddlvq_u32 (uint32x4_t a)
 {
-  int64x2_t result;
-  __asm__ ("abs %0.2d,%1.2d"
+  uint64_t result;
+  __asm__ ("uaddlv %d0,%1.4s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vacged_f64 (float64_t a, float64_t b)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vbsl_f32 (uint32x2_t a, float32x2_t b, float32x2_t c)
 {
-  float64_t result;
-  __asm__ ("facge %d0,%d1,%d2"
+  float32x2_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vacges_f32 (float32_t a, float32_t b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vbsl_p8 (uint8x8_t a, poly8x8_t b, poly8x8_t c)
 {
-  float32_t result;
-  __asm__ ("facge %s0,%s1,%s2"
+  poly8x8_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vacgtd_f64 (float64_t a, float64_t b)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vbsl_p16 (uint16x4_t a, poly16x4_t b, poly16x4_t c)
 {
-  float64_t result;
-  __asm__ ("facgt %d0,%d1,%d2"
+  poly16x4_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vacgts_f32 (float32_t a, float32_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vbsl_s8 (uint8x8_t a, int8x8_t b, int8x8_t c)
 {
-  float32_t result;
-  __asm__ ("facgt %s0,%s1,%s2"
+  int8x8_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16_t __attribute__ ((__always_inline__))
-vaddlv_s8 (int8x8_t a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vbsl_s16 (uint16x4_t a, int16x4_t b, int16x4_t c)
 {
-  int16_t result;
-  __asm__ ("saddlv %h0,%1.8b"
+  int16x4_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32_t __attribute__ ((__always_inline__))
-vaddlv_s16 (int16x4_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vbsl_s32 (uint32x2_t a, int32x2_t b, int32x2_t c)
 {
-  int32_t result;
-  __asm__ ("saddlv %s0,%1.4h"
+  int32x2_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
-vaddlv_u8 (uint8x8_t a)
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vbsl_s64 (uint64x1_t a, int64x1_t b, int64x1_t c)
 {
-  uint16_t result;
-  __asm__ ("uaddlv %h0,%1.8b"
+  int64x1_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
-vaddlv_u16 (uint16x4_t a)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vbsl_u8 (uint8x8_t a, uint8x8_t b, uint8x8_t c)
 {
-  uint32_t result;
-  __asm__ ("uaddlv %s0,%1.4h"
+  uint8x8_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16_t __attribute__ ((__always_inline__))
-vaddlvq_s8 (int8x16_t a)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vbsl_u16 (uint16x4_t a, uint16x4_t b, uint16x4_t c)
 {
-  int16_t result;
-  __asm__ ("saddlv %h0,%1.16b"
+  uint16x4_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32_t __attribute__ ((__always_inline__))
-vaddlvq_s16 (int16x8_t a)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vbsl_u32 (uint32x2_t a, uint32x2_t b, uint32x2_t c)
 {
-  int32_t result;
-  __asm__ ("saddlv %s0,%1.8h"
+  uint32x2_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64_t __attribute__ ((__always_inline__))
-vaddlvq_s32 (int32x4_t a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vbsl_u64 (uint64x1_t a, uint64x1_t b, uint64x1_t c)
 {
-  int64_t result;
-  __asm__ ("saddlv %d0,%1.4s"
+  uint64x1_t result;
+  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
-vaddlvq_u8 (uint8x16_t a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vbslq_f32 (uint32x4_t a, float32x4_t b, float32x4_t c)
 {
-  uint16_t result;
-  __asm__ ("uaddlv %h0,%1.16b"
+  float32x4_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
-vaddlvq_u16 (uint16x8_t a)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vbslq_f64 (uint64x2_t a, float64x2_t b, float64x2_t c)
 {
-  uint32_t result;
-  __asm__ ("uaddlv %s0,%1.8h"
+  float64x2_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
-vaddlvq_u32 (uint32x4_t a)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vbslq_p8 (uint8x16_t a, poly8x16_t b, poly8x16_t c)
 {
-  uint64_t result;
-  __asm__ ("uaddlv %d0,%1.4s"
+  poly8x16_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8_t __attribute__ ((__always_inline__))
-vaddv_s8 (int8x8_t a)
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vbslq_p16 (uint16x8_t a, poly16x8_t b, poly16x8_t c)
+{
+  poly16x8_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vbslq_s8 (uint8x16_t a, int8x16_t b, int8x16_t c)
 {
-  int8_t result;
-  __asm__ ("addv %b0,%1.8b"
+  int8x16_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16_t __attribute__ ((__always_inline__))
-vaddv_s16 (int16x4_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vbslq_s16 (uint16x8_t a, int16x8_t b, int16x8_t c)
 {
-  int16_t result;
-  __asm__ ("addv %h0,%1.4h"
+  int16x8_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
-vaddv_u8 (uint8x8_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vbslq_s32 (uint32x4_t a, int32x4_t b, int32x4_t c)
 {
-  uint8_t result;
-  __asm__ ("addv %b0,%1.8b"
+  int32x4_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
-vaddv_u16 (uint16x4_t a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vbslq_s64 (uint64x2_t a, int64x2_t b, int64x2_t c)
 {
-  uint16_t result;
-  __asm__ ("addv %h0,%1.4h"
+  int64x2_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8_t __attribute__ ((__always_inline__))
-vaddvq_s8 (int8x16_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vbslq_u8 (uint8x16_t a, uint8x16_t b, uint8x16_t c)
 {
-  int8_t result;
-  __asm__ ("addv %b0,%1.16b"
+  uint8x16_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16_t __attribute__ ((__always_inline__))
-vaddvq_s16 (int16x8_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vbslq_u16 (uint16x8_t a, uint16x8_t b, uint16x8_t c)
 {
-  int16_t result;
-  __asm__ ("addv %h0,%1.8h"
+  uint16x8_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32_t __attribute__ ((__always_inline__))
-vaddvq_s32 (int32x4_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vbslq_u32 (uint32x4_t a, uint32x4_t b, uint32x4_t c)
 {
-  int32_t result;
-  __asm__ ("addv %s0,%1.4s"
+  uint32x4_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
-vaddvq_u8 (uint8x16_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vbslq_u64 (uint64x2_t a, uint64x2_t b, uint64x2_t c)
+{
+  uint64x2_t result;
+  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vcls_s8 (int8x8_t a)
 {
-  uint8_t result;
-  __asm__ ("addv %b0,%1.16b"
+  int8x8_t result;
+  __asm__ ("cls %0.8b,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
-vaddvq_u16 (uint16x8_t a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vcls_s16 (int16x4_t a)
 {
-  uint16_t result;
-  __asm__ ("addv %h0,%1.8h"
+  int16x4_t result;
+  __asm__ ("cls %0.4h,%1.4h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
-vaddvq_u32 (uint32x4_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vcls_s32 (int32x2_t a)
 {
-  uint32_t result;
-  __asm__ ("addv %s0,%1.4s"
+  int32x2_t result;
+  __asm__ ("cls %0.2s,%1.2s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vbsl_f32 (uint32x2_t a, float32x2_t b, float32x2_t c)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vclsq_s8 (int8x16_t a)
 {
-  float32x2_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  int8x16_t result;
+  __asm__ ("cls %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vbsl_p8 (uint8x8_t a, poly8x8_t b, poly8x8_t c)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vclsq_s16 (int16x8_t a)
 {
-  poly8x8_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  int16x8_t result;
+  __asm__ ("cls %0.8h,%1.8h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vbsl_p16 (uint16x4_t a, poly16x4_t b, poly16x4_t c)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vclsq_s32 (int32x4_t a)
 {
-  poly16x4_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  int32x4_t result;
+  __asm__ ("cls %0.4s,%1.4s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vbsl_s8 (uint8x8_t a, int8x8_t b, int8x8_t c)
+vclz_s8 (int8x8_t a)
 {
   int8x8_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  __asm__ ("clz %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vbsl_s16 (uint16x4_t a, int16x4_t b, int16x4_t c)
+vclz_s16 (int16x4_t a)
 {
   int16x4_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  __asm__ ("clz %0.4h,%1.4h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vbsl_s32 (uint32x2_t a, int32x2_t b, int32x2_t c)
+vclz_s32 (int32x2_t a)
 {
   int32x2_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vbsl_s64 (uint64x1_t a, int64x1_t b, int64x1_t c)
-{
-  int64x1_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  __asm__ ("clz %0.2s,%1.2s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vbsl_u8 (uint8x8_t a, uint8x8_t b, uint8x8_t c)
+vclz_u8 (uint8x8_t a)
 {
   uint8x8_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  __asm__ ("clz %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vbsl_u16 (uint16x4_t a, uint16x4_t b, uint16x4_t c)
+vclz_u16 (uint16x4_t a)
 {
   uint16x4_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  __asm__ ("clz %0.4h,%1.4h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vbsl_u32 (uint32x2_t a, uint32x2_t b, uint32x2_t c)
+vclz_u32 (uint32x2_t a)
 {
   uint32x2_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  __asm__ ("clz %0.2s,%1.2s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vbsl_u64 (uint64x1_t a, uint64x1_t b, uint64x1_t c)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vclzq_s8 (int8x16_t a)
 {
-  uint64x1_t result;
-  __asm__ ("bsl %0.8b, %2.8b, %3.8b"
+  int8x16_t result;
+  __asm__ ("clz %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vbslq_f32 (uint32x4_t a, float32x4_t b, float32x4_t c)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vclzq_s16 (int16x8_t a)
 {
-  float32x4_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
+  int16x8_t result;
+  __asm__ ("clz %0.8h,%1.8h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vbslq_f64 (uint64x2_t a, float64x2_t b, float64x2_t c)
-{
-  float64x2_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vbslq_p8 (uint8x16_t a, poly8x16_t b, poly8x16_t c)
-{
-  poly8x16_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vbslq_p16 (uint16x8_t a, poly16x8_t b, poly16x8_t c)
-{
-  poly16x8_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vbslq_s8 (uint8x16_t a, int8x16_t b, int8x16_t c)
-{
-  int8x16_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vbslq_s16 (uint16x8_t a, int16x8_t b, int16x8_t c)
-{
-  int16x8_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vbslq_s32 (uint32x4_t a, int32x4_t b, int32x4_t c)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vclzq_s32 (int32x4_t a)
 {
   int32x4_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vbslq_s64 (uint64x2_t a, int64x2_t b, int64x2_t c)
-{
-  int64x2_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
+  __asm__ ("clz %0.4s,%1.4s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vbslq_u8 (uint8x16_t a, uint8x16_t b, uint8x16_t c)
+vclzq_u8 (uint8x16_t a)
 {
   uint8x16_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
+  __asm__ ("clz %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vbslq_u16 (uint16x8_t a, uint16x8_t b, uint16x8_t c)
+vclzq_u16 (uint16x8_t a)
 {
   uint16x8_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
+  __asm__ ("clz %0.8h,%1.8h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vbslq_u32 (uint32x4_t a, uint32x4_t b, uint32x4_t c)
+vclzq_u32 (uint32x4_t a)
 {
   uint32x4_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
+  __asm__ ("clz %0.4s,%1.4s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vbslq_u64 (uint64x2_t a, uint64x2_t b, uint64x2_t c)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vcnt_p8 (poly8x8_t a)
 {
-  uint64x2_t result;
-  __asm__ ("bsl %0.16b, %2.16b, %3.16b"
+  poly8x8_t result;
+  __asm__ ("cnt %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcage_f32 (float32x2_t a, float32x2_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vcnt_s8 (int8x8_t a)
 {
-  uint32x2_t result;
-  __asm__ ("facge %0.2s, %1.2s, %2.2s"
+  int8x8_t result;
+  __asm__ ("cnt %0.8b,%1.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcageq_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcnt_u8 (uint8x8_t a)
 {
-  uint32x4_t result;
-  __asm__ ("facge %0.4s, %1.4s, %2.4s"
+  uint8x8_t result;
+  __asm__ ("cnt %0.8b,%1.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcageq_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vcntq_p8 (poly8x16_t a)
 {
-  uint64x2_t result;
-  __asm__ ("facge %0.2d, %1.2d, %2.2d"
+  poly8x16_t result;
+  __asm__ ("cnt %0.16b,%1.16b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcagt_f32 (float32x2_t a, float32x2_t b)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vcntq_s8 (int8x16_t a)
 {
-  uint32x2_t result;
-  __asm__ ("facgt %0.2s, %1.2s, %2.2s"
+  int8x16_t result;
+  __asm__ ("cnt %0.16b,%1.16b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcagtq_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcntq_u8 (uint8x16_t a)
 {
-  uint32x4_t result;
-  __asm__ ("facgt %0.4s, %1.4s, %2.4s"
+  uint8x16_t result;
+  __asm__ ("cnt %0.16b,%1.16b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcagtq_f64 (float64x2_t a, float64x2_t b)
-{
-  uint64x2_t result;
-  __asm__ ("facgt %0.2d, %1.2d, %2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_f32(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t c_ = (c);                                            \
+       float32x4_t a_ = (a);                                            \
+       float32x4_t result;                                              \
+       __asm__ ("ins %0.s[%2], %3.s[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcale_f32 (float32x2_t a, float32x2_t b)
-{
-  uint32x2_t result;
-  __asm__ ("facge %0.2s, %2.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_f64(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x2_t c_ = (c);                                            \
+       float64x2_t a_ = (a);                                            \
+       float64x2_t result;                                              \
+       __asm__ ("ins %0.d[%2], %3.d[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcaleq_f32 (float32x4_t a, float32x4_t b)
-{
-  uint32x4_t result;
-  __asm__ ("facge %0.4s, %2.4s, %1.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_p8(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x16_t c_ = (c);                                             \
+       poly8x16_t a_ = (a);                                             \
+       poly8x16_t result;                                               \
+       __asm__ ("ins %0.b[%2], %3.b[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcaleq_f64 (float64x2_t a, float64x2_t b)
-{
-  uint64x2_t result;
-  __asm__ ("facge %0.2d, %2.2d, %1.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_p16(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x8_t c_ = (c);                                             \
+       poly16x8_t a_ = (a);                                             \
+       poly16x8_t result;                                               \
+       __asm__ ("ins %0.h[%2], %3.h[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcalt_f32 (float32x2_t a, float32x2_t b)
-{
-  uint32x2_t result;
-  __asm__ ("facgt %0.2s, %2.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_s8(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int8x16_t c_ = (c);                                              \
+       int8x16_t a_ = (a);                                              \
+       int8x16_t result;                                                \
+       __asm__ ("ins %0.b[%2], %3.b[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcaltq_f32 (float32x4_t a, float32x4_t b)
-{
-  uint32x4_t result;
-  __asm__ ("facgt %0.4s, %2.4s, %1.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_s16(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t c_ = (c);                                              \
+       int16x8_t a_ = (a);                                              \
+       int16x8_t result;                                                \
+       __asm__ ("ins %0.h[%2], %3.h[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcaltq_f64 (float64x2_t a, float64x2_t b)
-{
-  uint64x2_t result;
-  __asm__ ("facgt %0.2d, %2.2d, %1.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vceq_f32 (float32x2_t a, float32x2_t b)
-{
-  uint32x2_t result;
-  __asm__ ("fcmeq %0.2s, %1.2s, %2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_s32(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t c_ = (c);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("ins %0.s[%2], %3.s[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vceq_f64 (float64x1_t a, float64x1_t b)
-{
-  uint64x1_t result;
-  __asm__ ("fcmeq %d0, %d1, %d2"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_s64(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t c_ = (c);                                              \
+       int64x2_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("ins %0.d[%2], %3.d[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vceqd_f64 (float64_t a, float64_t b)
-{
-  float64_t result;
-  __asm__ ("fcmeq %d0,%d1,%d2"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_u8(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint8x16_t c_ = (c);                                             \
+       uint8x16_t a_ = (a);                                             \
+       uint8x16_t result;                                               \
+       __asm__ ("ins %0.b[%2], %3.b[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vceqq_f32 (float32x4_t a, float32x4_t b)
-{
-  uint32x4_t result;
-  __asm__ ("fcmeq %0.4s, %1.4s, %2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_u16(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t c_ = (c);                                             \
+       uint16x8_t a_ = (a);                                             \
+       uint16x8_t result;                                               \
+       __asm__ ("ins %0.h[%2], %3.h[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vceqq_f64 (float64x2_t a, float64x2_t b)
-{
-  uint64x2_t result;
-  __asm__ ("fcmeq %0.2d, %1.2d, %2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_u32(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t c_ = (c);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("ins %0.s[%2], %3.s[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vceqs_f32 (float32_t a, float32_t b)
-{
-  float32_t result;
-  __asm__ ("fcmeq %s0,%s1,%s2"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcopyq_lane_u64(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t c_ = (c);                                             \
+       uint64x2_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("ins %0.d[%2], %3.d[%4]"                                \
+                : "=w"(result)                                          \
+                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vceqzd_f64 (float64_t a)
-{
-  float64_t result;
-  __asm__ ("fcmeq %d0,%d1,#0"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+/* vcvt_f16_f32 not supported */
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vceqzs_f32 (float32_t a)
-{
-  float32_t result;
-  __asm__ ("fcmeq %s0,%s1,#0"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+/* vcvt_f32_f16 not supported */
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcge_f32 (float32x2_t a, float32x2_t b)
-{
-  uint32x2_t result;
-  __asm__ ("fcmge %0.2s, %1.2s, %2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+/* vcvt_high_f16_f32 not supported */
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcge_f64 (float64x1_t a, float64x1_t b)
-{
-  uint64x1_t result;
-  __asm__ ("fcmge %d0, %d1, %d2"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+/* vcvt_high_f32_f16 not supported */
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcgeq_f32 (float32x4_t a, float32x4_t b)
-{
-  uint32x4_t result;
-  __asm__ ("fcmge %0.4s, %1.4s, %2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+static float32x2_t vdup_n_f32 (float32_t);
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcgeq_f64 (float64x2_t a, float64x2_t b)
-{
-  uint64x2_t result;
-  __asm__ ("fcmge %0.2d, %1.2d, %2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvt_n_f32_s32(a, b)                                            \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x2_t a_ = (a);                                              \
+       float32x2_t result;                                              \
+       __asm__ ("scvtf %0.2s, %1.2s, #%2"                               \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcgt_f32 (float32x2_t a, float32x2_t b)
-{
-  uint32x2_t result;
-  __asm__ ("fcmgt %0.2s, %1.2s, %2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvt_n_f32_u32(a, b)                                            \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x2_t a_ = (a);                                             \
+       float32x2_t result;                                              \
+       __asm__ ("ucvtf %0.2s, %1.2s, #%2"                               \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcgt_f64 (float64x1_t a, float64x1_t b)
-{
-  uint64x1_t result;
-  __asm__ ("fcmgt %d0, %d1, %d2"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvt_n_s32_f32(a, b)                                            \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x2_t a_ = (a);                                            \
+       int32x2_t result;                                                \
+       __asm__ ("fcvtzs %0.2s, %1.2s, #%2"                              \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcgtq_f32 (float32x4_t a, float32x4_t b)
-{
-  uint32x4_t result;
-  __asm__ ("fcmgt %0.4s, %1.4s, %2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvt_n_u32_f32(a, b)                                            \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x2_t a_ = (a);                                            \
+       uint32x2_t result;                                               \
+       __asm__ ("fcvtzu %0.2s, %1.2s, #%2"                              \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcgtq_f64 (float64x2_t a, float64x2_t b)
-{
-  uint64x2_t result;
-  __asm__ ("fcmgt %0.2d, %1.2d, %2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvtd_n_f64_s64(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       int64_t a_ = (a);                                                \
+       int64_t result;                                                  \
+       __asm__ ("scvtf %d0,%d1,%2"                                      \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcle_f32 (float32x2_t a, float32x2_t b)
-{
-  uint32x2_t result;
-  __asm__ ("fcmge %0.2s, %2.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvtd_n_f64_u64(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64_t a_ = (a);                                               \
+       uint64_t result;                                                 \
+       __asm__ ("ucvtf %d0,%d1,%2"                                      \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcle_f64 (float64x1_t a, float64x1_t b)
-{
-  uint64x1_t result;
-  __asm__ ("fcmge %d0, %d2, %d1"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvtd_n_s64_f64(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       float64_t a_ = (a);                                              \
+       float64_t result;                                                \
+       __asm__ ("fcvtzs %d0,%d1,%2"                                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcleq_f32 (float32x4_t a, float32x4_t b)
-{
-  uint32x4_t result;
-  __asm__ ("fcmge %0.4s, %2.4s, %1.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvtd_n_u64_f64(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       float64_t a_ = (a);                                              \
+       float64_t result;                                                \
+       __asm__ ("fcvtzu %d0,%d1,%2"                                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcleq_f64 (float64x2_t a, float64x2_t b)
-{
-  uint64x2_t result;
-  __asm__ ("fcmge %0.2d, %2.2d, %1.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvtq_n_f32_s32(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t a_ = (a);                                              \
+       float32x4_t result;                                              \
+       __asm__ ("scvtf %0.4s, %1.4s, #%2"                               \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vcls_s8 (int8x8_t a)
-{
-  int8x8_t result;
-  __asm__ ("cls %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vcvtq_n_f32_u32(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t a_ = (a);                                             \
+       float32x4_t result;                                              \
+       __asm__ ("ucvtf %0.4s, %1.4s, #%2"                               \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vcls_s16 (int16x4_t a)
+#define vcvtq_n_f64_s64(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t a_ = (a);                                              \
+       float64x2_t result;                                              \
+       __asm__ ("scvtf %0.2d, %1.2d, #%2"                               \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vcvtq_n_f64_u64(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t a_ = (a);                                             \
+       float64x2_t result;                                              \
+       __asm__ ("ucvtf %0.2d, %1.2d, #%2"                               \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vcvtq_n_s32_f32(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t a_ = (a);                                            \
+       int32x4_t result;                                                \
+       __asm__ ("fcvtzs %0.4s, %1.4s, #%2"                              \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vcvtq_n_s64_f64(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x2_t a_ = (a);                                            \
+       int64x2_t result;                                                \
+       __asm__ ("fcvtzs %0.2d, %1.2d, #%2"                              \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vcvtq_n_u32_f32(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t a_ = (a);                                            \
+       uint32x4_t result;                                               \
+       __asm__ ("fcvtzu %0.4s, %1.4s, #%2"                              \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vcvtq_n_u64_f64(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x2_t a_ = (a);                                            \
+       uint64x2_t result;                                               \
+       __asm__ ("fcvtzu %0.2d, %1.2d, #%2"                              \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vcvts_n_f32_s32(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       int32_t a_ = (a);                                                \
+       int32_t result;                                                  \
+       __asm__ ("scvtf %s0,%s1,%2"                                      \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vcvts_n_f32_u32(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32_t a_ = (a);                                               \
+       uint32_t result;                                                 \
+       __asm__ ("ucvtf %s0,%s1,%2"                                      \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vcvts_n_s32_f32(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       float32_t a_ = (a);                                              \
+       float32_t result;                                                \
+       __asm__ ("fcvtzs %s0,%s1,%2"                                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vcvts_n_u32_f32(a, b)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       float32_t a_ = (a);                                              \
+       float32_t result;                                                \
+       __asm__ ("fcvtzu %s0,%s1,%2"                                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vcvtx_f32_f64 (float64x2_t a)
 {
-  int16x4_t result;
-  __asm__ ("cls %0.4h,%1.4h"
+  float32x2_t result;
+  __asm__ ("fcvtxn %0.2s,%1.2d"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vcls_s32 (int32x2_t a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vcvtx_high_f32_f64 (float64x2_t a)
 {
-  int32x2_t result;
-  __asm__ ("cls %0.2s,%1.2s"
+  float32x4_t result;
+  __asm__ ("fcvtxn2 %0.4s,%1.2d"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vclsq_s8 (int8x16_t a)
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vcvtxd_f32_f64 (float64_t a)
 {
-  int8x16_t result;
-  __asm__ ("cls %0.16b,%1.16b"
+  float32_t result;
+  __asm__ ("fcvtxn %s0,%d1"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vclsq_s16 (int16x8_t a)
-{
-  int16x8_t result;
-  __asm__ ("cls %0.8h,%1.8h"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vdup_lane_f32(a, b)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x2_t a_ = (a);                                            \
+       float32x2_t result;                                              \
+       __asm__ ("dup %0.2s,%1.s[%2]"                                    \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vclsq_s32 (int32x4_t a)
-{
-  int32x4_t result;
-  __asm__ ("cls %0.4s,%1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vdup_lane_p8(a, b)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x8_t a_ = (a);                                              \
+       poly8x8_t result;                                                \
+       __asm__ ("dup %0.8b,%1.b[%2]"                                    \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vclt_f32 (float32x2_t a, float32x2_t b)
-{
-  uint32x2_t result;
-  __asm__ ("fcmgt %0.2s, %2.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vdup_lane_p16(a, b)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x4_t a_ = (a);                                             \
+       poly16x4_t result;                                               \
+       __asm__ ("dup %0.4h,%1.h[%2]"                                    \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vclt_f64 (float64x1_t a, float64x1_t b)
+#define vdup_lane_s8(a, b)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       int8x8_t a_ = (a);                                               \
+       int8x8_t result;                                                 \
+       __asm__ ("dup %0.8b,%1.b[%2]"                                    \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vdup_lane_s16(a, b)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x4_t a_ = (a);                                              \
+       int16x4_t result;                                                \
+       __asm__ ("dup %0.4h,%1.h[%2]"                                    \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vdup_lane_s32(a, b)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x2_t a_ = (a);                                              \
+       int32x2_t result;                                                \
+       __asm__ ("dup %0.2s,%1.s[%2]"                                    \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vdup_lane_s64(a, b)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x1_t a_ = (a);                                              \
+       int64x1_t result;                                                \
+       __asm__ ("ins %0.d[0],%1.d[%2]"                                  \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vdup_lane_u8(a, b)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       uint8x8_t a_ = (a);                                              \
+       uint8x8_t result;                                                \
+       __asm__ ("dup %0.8b,%1.b[%2]"                                    \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vdup_lane_u16(a, b)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x4_t a_ = (a);                                             \
+       uint16x4_t result;                                               \
+       __asm__ ("dup %0.4h,%1.h[%2]"                                    \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vdup_lane_u32(a, b)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x2_t a_ = (a);                                             \
+       uint32x2_t result;                                               \
+       __asm__ ("dup %0.2s,%1.s[%2]"                                    \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vdup_lane_u64(a, b)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x1_t a_ = (a);                                             \
+       uint64x1_t result;                                               \
+       __asm__ ("ins %0.d[0],%1.d[%2]"                                  \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vdup_n_f32 (float32_t a)
 {
-  uint64x1_t result;
-  __asm__ ("fcmgt %d0, %d2, %d1"
+  float32x2_t result;
+  __asm__ ("dup %0.2s, %w1"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcltq_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vdup_n_p8 (uint32_t a)
 {
-  uint32x4_t result;
-  __asm__ ("fcmgt %0.4s, %2.4s, %1.4s"
+  poly8x8_t result;
+  __asm__ ("dup %0.8b,%w1"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcltq_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vdup_n_p16 (uint32_t a)
 {
-  uint64x2_t result;
-  __asm__ ("fcmgt %0.2d, %2.2d, %1.2d"
+  poly16x4_t result;
+  __asm__ ("dup %0.4h,%w1"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vclz_s8 (int8x8_t a)
+vdup_n_s8 (int32_t a)
 {
   int8x8_t result;
-  __asm__ ("clz %0.8b,%1.8b"
+  __asm__ ("dup %0.8b,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vclz_s16 (int16x4_t a)
+vdup_n_s16 (int32_t a)
 {
   int16x4_t result;
-  __asm__ ("clz %0.4h,%1.4h"
+  __asm__ ("dup %0.4h,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vclz_s32 (int32x2_t a)
+vdup_n_s32 (int32_t a)
 {
   int32x2_t result;
-  __asm__ ("clz %0.2s,%1.2s"
+  __asm__ ("dup %0.2s,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vdup_n_s64 (int64_t a)
+{
+  int64x1_t result;
+  __asm__ ("ins %0.d[0],%x1"
+           : "=w"(result)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vclz_u8 (uint8x8_t a)
+vdup_n_u8 (uint32_t a)
 {
   uint8x8_t result;
-  __asm__ ("clz %0.8b,%1.8b"
+  __asm__ ("dup %0.8b,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vclz_u16 (uint16x4_t a)
+vdup_n_u16 (uint32_t a)
 {
   uint16x4_t result;
-  __asm__ ("clz %0.4h,%1.4h"
+  __asm__ ("dup %0.4h,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vclz_u32 (uint32x2_t a)
+vdup_n_u32 (uint32_t a)
 {
   uint32x2_t result;
-  __asm__ ("clz %0.2s,%1.2s"
+  __asm__ ("dup %0.2s,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vclzq_s8 (int8x16_t a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vdup_n_u64 (uint64_t a)
 {
-  int8x16_t result;
-  __asm__ ("clz %0.16b,%1.16b"
+  uint64x1_t result;
+  __asm__ ("ins %0.d[0],%x1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vclzq_s16 (int16x8_t a)
-{
-  int16x8_t result;
-  __asm__ ("clz %0.8h,%1.8h"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vdupd_lane_f64(a, b)                                            \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x2_t a_ = (a);                                            \
+       float64_t result;                                                \
+       __asm__ ("dup %d0, %1.d[%2]"                                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vclzq_s32 (int32x4_t a)
-{
-  int32x4_t result;
-  __asm__ ("clz %0.4s,%1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vclzq_u8 (uint8x16_t a)
-{
-  uint8x16_t result;
-  __asm__ ("clz %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vclzq_u16 (uint16x8_t a)
-{
-  uint16x8_t result;
-  __asm__ ("clz %0.8h,%1.8h"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vclzq_u32 (uint32x4_t a)
-{
-  uint32x4_t result;
-  __asm__ ("clz %0.4s,%1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vcnt_p8 (poly8x8_t a)
-{
-  poly8x8_t result;
-  __asm__ ("cnt %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vcnt_s8 (int8x8_t a)
-{
-  int8x8_t result;
-  __asm__ ("cnt %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vcnt_u8 (uint8x8_t a)
-{
-  uint8x8_t result;
-  __asm__ ("cnt %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vcntq_p8 (poly8x16_t a)
-{
-  poly8x16_t result;
-  __asm__ ("cnt %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vcntq_s8 (int8x16_t a)
-{
-  int8x16_t result;
-  __asm__ ("cnt %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vcntq_u8 (uint8x16_t a)
-{
-  uint8x16_t result;
-  __asm__ ("cnt %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-#define vcopyq_lane_f32(a, b, c, d)                                     \
+#define vdupq_lane_f32(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       float32x4_t c_ = (c);                                            \
-       float32x4_t a_ = (a);                                            \
+       float32x2_t a_ = (a);                                            \
        float32x4_t result;                                              \
-       __asm__ ("ins %0.s[%2], %3.s[%4]"                                \
+       __asm__ ("dup %0.4s,%1.s[%2]"                                    \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_f64(a, b, c, d)                                     \
+#define vdupq_lane_f64(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       float64x2_t c_ = (c);                                            \
-       float64x2_t a_ = (a);                                            \
+       float64x1_t a_ = (a);                                            \
        float64x2_t result;                                              \
-       __asm__ ("ins %0.d[%2], %3.d[%4]"                                \
+       __asm__ ("dup %0.2d,%1.d[%2]"                                    \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_p8(a, b, c, d)                                      \
+#define vdupq_lane_p8(a, b)                                             \
   __extension__                                                         \
     ({                                                                  \
-       poly8x16_t c_ = (c);                                             \
-       poly8x16_t a_ = (a);                                             \
+       poly8x8_t a_ = (a);                                              \
        poly8x16_t result;                                               \
-       __asm__ ("ins %0.b[%2], %3.b[%4]"                                \
+       __asm__ ("dup %0.16b,%1.b[%2]"                                   \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_p16(a, b, c, d)                                     \
+#define vdupq_lane_p16(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       poly16x8_t c_ = (c);                                             \
-       poly16x8_t a_ = (a);                                             \
+       poly16x4_t a_ = (a);                                             \
        poly16x8_t result;                                               \
-       __asm__ ("ins %0.h[%2], %3.h[%4]"                                \
+       __asm__ ("dup %0.8h,%1.h[%2]"                                    \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_s8(a, b, c, d)                                      \
+#define vdupq_lane_s8(a, b)                                             \
   __extension__                                                         \
     ({                                                                  \
-       int8x16_t c_ = (c);                                              \
-       int8x16_t a_ = (a);                                              \
+       int8x8_t a_ = (a);                                               \
        int8x16_t result;                                                \
-       __asm__ ("ins %0.b[%2], %3.b[%4]"                                \
+       __asm__ ("dup %0.16b,%1.b[%2]"                                   \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_s16(a, b, c, d)                                     \
+#define vdupq_lane_s16(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t c_ = (c);                                              \
-       int16x8_t a_ = (a);                                              \
+       int16x4_t a_ = (a);                                              \
        int16x8_t result;                                                \
-       __asm__ ("ins %0.h[%2], %3.h[%4]"                                \
+       __asm__ ("dup %0.8h,%1.h[%2]"                                    \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_s32(a, b, c, d)                                     \
+#define vdupq_lane_s32(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t c_ = (c);                                              \
-       int32x4_t a_ = (a);                                              \
+       int32x2_t a_ = (a);                                              \
        int32x4_t result;                                                \
-       __asm__ ("ins %0.s[%2], %3.s[%4]"                                \
+       __asm__ ("dup %0.4s,%1.s[%2]"                                    \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_s64(a, b, c, d)                                     \
+#define vdupq_lane_s64(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       int64x2_t c_ = (c);                                              \
-       int64x2_t a_ = (a);                                              \
+       int64x1_t a_ = (a);                                              \
        int64x2_t result;                                                \
-       __asm__ ("ins %0.d[%2], %3.d[%4]"                                \
+       __asm__ ("dup %0.2d,%1.d[%2]"                                    \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_u8(a, b, c, d)                                      \
+#define vdupq_lane_u8(a, b)                                             \
   __extension__                                                         \
     ({                                                                  \
-       uint8x16_t c_ = (c);                                             \
-       uint8x16_t a_ = (a);                                             \
+       uint8x8_t a_ = (a);                                              \
        uint8x16_t result;                                               \
-       __asm__ ("ins %0.b[%2], %3.b[%4]"                                \
+       __asm__ ("dup %0.16b,%1.b[%2]"                                   \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_u16(a, b, c, d)                                     \
+#define vdupq_lane_u16(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
-       uint16x8_t a_ = (a);                                             \
+       uint16x4_t a_ = (a);                                             \
        uint16x8_t result;                                               \
-       __asm__ ("ins %0.h[%2], %3.h[%4]"                                \
+       __asm__ ("dup %0.8h,%1.h[%2]"                                    \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_u32(a, b, c, d)                                     \
+#define vdupq_lane_u32(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
-       uint32x4_t a_ = (a);                                             \
+       uint32x2_t a_ = (a);                                             \
        uint32x4_t result;                                               \
-       __asm__ ("ins %0.s[%2], %3.s[%4]"                                \
+       __asm__ ("dup %0.4s,%1.s[%2]"                                    \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcopyq_lane_u64(a, b, c, d)                                     \
+#define vdupq_lane_u64(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       uint64x2_t c_ = (c);                                             \
-       uint64x2_t a_ = (a);                                             \
+       uint64x1_t a_ = (a);                                             \
        uint64x2_t result;                                               \
-       __asm__ ("ins %0.d[%2], %3.d[%4]"                                \
+       __asm__ ("dup %0.2d,%1.d[%2]"                                    \
                 : "=w"(result)                                          \
-                : "0"(a_), "i"(b), "w"(c_), "i"(d)                      \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-/* vcvt_f16_f32 not supported */
-
-/* vcvt_f32_f16 not supported */
-
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vcvt_f32_f64 (float64x2_t a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vdupq_n_f32 (float32_t a)
 {
-  float32x2_t result;
-  __asm__ ("fcvtn %0.2s,%1.2d"
+  float32x4_t result;
+  __asm__ ("dup %0.4s, %w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vcvt_f32_s32 (int32x2_t a)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vdupq_n_f64 (float64_t a)
 {
-  float32x2_t result;
-  __asm__ ("scvtf %0.2s, %1.2s"
+  float64x2_t result;
+  __asm__ ("dup %0.2d, %x1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vcvt_f32_u32 (uint32x2_t a)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vdupq_n_p8 (uint32_t a)
 {
-  float32x2_t result;
-  __asm__ ("ucvtf %0.2s, %1.2s"
+  poly8x16_t result;
+  __asm__ ("dup %0.16b,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vcvt_f64_f32 (float32x2_t a)
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vdupq_n_p16 (uint32_t a)
 {
-  float64x2_t result;
-  __asm__ ("fcvtl %0.2d,%1.2s"
+  poly16x8_t result;
+  __asm__ ("dup %0.8h,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))
-vcvt_f64_s64 (uint64x1_t a)
-{
-  float64x1_t result;
-  __asm__ ("scvtf %d0, %d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))
-vcvt_f64_u64 (uint64x1_t a)
-{
-  float64x1_t result;
-  __asm__ ("ucvtf %d0, %d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-/* vcvt_high_f16_f32 not supported */
-
-/* vcvt_high_f32_f16 not supported */
-
-static float32x2_t vdup_n_f32 (float32_t);
-
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vcvt_high_f32_f64 (float32x2_t a, float64x2_t b)
-{
-  float32x4_t result = vcombine_f32 (a, vdup_n_f32 (0.0f));
-  __asm__ ("fcvtn2 %0.4s,%2.2d"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vcvt_high_f64_f32 (float32x4_t a)
-{
-  float64x2_t result;
-  __asm__ ("fcvtl2 %0.2d,%1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-#define vcvt_n_f32_s32(a, b)                                            \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t a_ = (a);                                              \
-       float32x2_t result;                                              \
-       __asm__ ("scvtf %0.2s, %1.2s, #%2"                               \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvt_n_f32_u32(a, b)                                            \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t a_ = (a);                                             \
-       float32x2_t result;                                              \
-       __asm__ ("ucvtf %0.2s, %1.2s, #%2"                               \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvt_n_s32_f32(a, b)                                            \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t a_ = (a);                                            \
-       int32x2_t result;                                                \
-       __asm__ ("fcvtzs %0.2s, %1.2s, #%2"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvt_n_u32_f32(a, b)                                            \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t a_ = (a);                                            \
-       uint32x2_t result;                                               \
-       __asm__ ("fcvtzu %0.2s, %1.2s, #%2"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vcvt_s32_f32 (float32x2_t a)
-{
-  int32x2_t result;
-  __asm__ ("fcvtzs %0.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcvt_u32_f32 (float32x2_t a)
-{
-  uint32x2_t result;
-  __asm__ ("fcvtzu %0.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vcvta_s32_f32 (float32x2_t a)
-{
-  int32x2_t result;
-  __asm__ ("fcvtas %0.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcvta_u32_f32 (float32x2_t a)
-{
-  uint32x2_t result;
-  __asm__ ("fcvtau %0.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtad_s64_f64 (float64_t a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vdupq_n_s8 (int32_t a)
 {
-  float64_t result;
-  __asm__ ("fcvtas %d0,%d1"
+  int8x16_t result;
+  __asm__ ("dup %0.16b,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtad_u64_f64 (float64_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vdupq_n_s16 (int32_t a)
 {
-  float64_t result;
-  __asm__ ("fcvtau %d0,%d1"
+  int16x8_t result;
+  __asm__ ("dup %0.8h,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vcvtaq_s32_f32 (float32x4_t a)
+vdupq_n_s32 (int32_t a)
 {
   int32x4_t result;
-  __asm__ ("fcvtas %0.4s, %1.4s"
+  __asm__ ("dup %0.4s,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vcvtaq_s64_f64 (float64x2_t a)
+vdupq_n_s64 (int64_t a)
 {
   int64x2_t result;
-  __asm__ ("fcvtas %0.2d, %1.2d"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcvtaq_u32_f32 (float32x4_t a)
-{
-  uint32x4_t result;
-  __asm__ ("fcvtau %0.4s, %1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcvtaq_u64_f64 (float64x2_t a)
-{
-  uint64x2_t result;
-  __asm__ ("fcvtau %0.2d, %1.2d"
+  __asm__ ("dup %0.2d,%x1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvtas_s64_f64 (float32_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vdupq_n_u8 (uint32_t a)
 {
-  float32_t result;
-  __asm__ ("fcvtas %s0,%s1"
+  uint8x16_t result;
+  __asm__ ("dup %0.16b,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvtas_u64_f64 (float32_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vdupq_n_u16 (uint32_t a)
 {
-  float32_t result;
-  __asm__ ("fcvtau %s0,%s1"
+  uint16x8_t result;
+  __asm__ ("dup %0.8h,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64_t __attribute__ ((__always_inline__))
-vcvtd_f64_s64 (int64_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vdupq_n_u32 (uint32_t a)
 {
-  int64_t result;
-  __asm__ ("scvtf %d0,%d1"
+  uint32x4_t result;
+  __asm__ ("dup %0.4s,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
-vcvtd_f64_u64 (uint64_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vdupq_n_u64 (uint64_t a)
 {
-  uint64_t result;
-  __asm__ ("ucvtf %d0,%d1"
+  uint64x2_t result;
+  __asm__ ("dup %0.2d,%x1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-#define vcvtd_n_f64_s64(a, b)                                           \
+#define vdups_lane_f32(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
-       int64_t a_ = (a);                                                \
-       int64_t result;                                                  \
-       __asm__ ("scvtf %d0,%d1,%2"                                      \
+       float32x4_t a_ = (a);                                            \
+       float32_t result;                                                \
+       __asm__ ("dup %s0, %1.s[%2]"                                     \
                 : "=w"(result)                                          \
                 : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcvtd_n_f64_u64(a, b)                                           \
+#define vext_f32(a, b, c)                                               \
   __extension__                                                         \
     ({                                                                  \
-       uint64_t a_ = (a);                                               \
-       uint64_t result;                                                 \
-       __asm__ ("ucvtf %d0,%d1,%2"                                      \
+       float32x2_t b_ = (b);                                            \
+       float32x2_t a_ = (a);                                            \
+       float32x2_t result;                                              \
+       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*4"                        \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcvtd_n_s64_f64(a, b)                                           \
+#define vext_f64(a, b, c)                                               \
   __extension__                                                         \
     ({                                                                  \
-       float64_t a_ = (a);                                              \
-       float64_t result;                                                \
-       __asm__ ("fcvtzs %d0,%d1,%2"                                     \
+       float64x1_t b_ = (b);                                            \
+       float64x1_t a_ = (a);                                            \
+       float64x1_t result;                                              \
+       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*8"                        \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vcvtd_n_u64_f64(a, b)                                           \
+#define vext_p8(a, b, c)                                                \
   __extension__                                                         \
     ({                                                                  \
-       float64_t a_ = (a);                                              \
-       float64_t result;                                                \
-       __asm__ ("fcvtzu %d0,%d1,%2"                                     \
+       poly8x8_t b_ = (b);                                              \
+       poly8x8_t a_ = (a);                                              \
+       poly8x8_t result;                                                \
+       __asm__ ("ext %0.8b,%1.8b,%2.8b,%3"                              \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtd_s64_f64 (float64_t a)
-{
-  float64_t result;
-  __asm__ ("fcvtzs %d0,%d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtd_u64_f64 (float64_t a)
-{
-  float64_t result;
-  __asm__ ("fcvtzu %d0,%d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vcvtm_s32_f32 (float32x2_t a)
-{
-  int32x2_t result;
-  __asm__ ("fcvtms %0.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcvtm_u32_f32 (float32x2_t a)
-{
-  uint32x2_t result;
-  __asm__ ("fcvtmu %0.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vext_p16(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x4_t b_ = (b);                                             \
+       poly16x4_t a_ = (a);                                             \
+       poly16x4_t result;                                               \
+       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*2"                        \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtmd_s64_f64 (float64_t a)
-{
-  float64_t result;
-  __asm__ ("fcvtms %d0,%d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vext_s8(a, b, c)                                                \
+  __extension__                                                         \
+    ({                                                                  \
+       int8x8_t b_ = (b);                                               \
+       int8x8_t a_ = (a);                                               \
+       int8x8_t result;                                                 \
+       __asm__ ("ext %0.8b,%1.8b,%2.8b,%3"                              \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtmd_u64_f64 (float64_t a)
-{
-  float64_t result;
-  __asm__ ("fcvtmu %d0,%d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vext_s16(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x4_t b_ = (b);                                              \
+       int16x4_t a_ = (a);                                              \
+       int16x4_t result;                                                \
+       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*2"                        \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vcvtmq_s32_f32 (float32x4_t a)
-{
-  int32x4_t result;
-  __asm__ ("fcvtms %0.4s, %1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vext_s32(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x2_t b_ = (b);                                              \
+       int32x2_t a_ = (a);                                              \
+       int32x2_t result;                                                \
+       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*4"                        \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vcvtmq_s64_f64 (float64x2_t a)
-{
-  int64x2_t result;
-  __asm__ ("fcvtms %0.2d, %1.2d"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vext_s64(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x1_t b_ = (b);                                              \
+       int64x1_t a_ = (a);                                              \
+       int64x1_t result;                                                \
+       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*8"                        \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcvtmq_u32_f32 (float32x4_t a)
-{
-  uint32x4_t result;
-  __asm__ ("fcvtmu %0.4s, %1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vext_u8(a, b, c)                                                \
+  __extension__                                                         \
+    ({                                                                  \
+       uint8x8_t b_ = (b);                                              \
+       uint8x8_t a_ = (a);                                              \
+       uint8x8_t result;                                                \
+       __asm__ ("ext %0.8b,%1.8b,%2.8b,%3"                              \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcvtmq_u64_f64 (float64x2_t a)
-{
-  uint64x2_t result;
-  __asm__ ("fcvtmu %0.2d, %1.2d"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vext_u16(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x4_t b_ = (b);                                             \
+       uint16x4_t a_ = (a);                                             \
+       uint16x4_t result;                                               \
+       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*2"                        \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvtms_s64_f64 (float32_t a)
-{
-  float32_t result;
-  __asm__ ("fcvtms %s0,%s1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vext_u32(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x2_t b_ = (b);                                             \
+       uint32x2_t a_ = (a);                                             \
+       uint32x2_t result;                                               \
+       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*4"                        \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvtms_u64_f64 (float32_t a)
-{
-  float32_t result;
-  __asm__ ("fcvtmu %s0,%s1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vext_u64(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x1_t b_ = (b);                                             \
+       uint64x1_t a_ = (a);                                             \
+       uint64x1_t result;                                               \
+       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*8"                        \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vcvtn_s32_f32 (float32x2_t a)
-{
-  int32x2_t result;
-  __asm__ ("fcvtns %0.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vextq_f32(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t b_ = (b);                                            \
+       float32x4_t a_ = (a);                                            \
+       float32x4_t result;                                              \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*4"                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcvtn_u32_f32 (float32x2_t a)
-{
-  uint32x2_t result;
-  __asm__ ("fcvtnu %0.2s, %1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vextq_f64(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x2_t b_ = (b);                                            \
+       float64x2_t a_ = (a);                                            \
+       float64x2_t result;                                              \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*8"                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtnd_s64_f64 (float64_t a)
-{
-  float64_t result;
-  __asm__ ("fcvtns %d0,%d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vextq_p8(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x16_t b_ = (b);                                             \
+       poly8x16_t a_ = (a);                                             \
+       poly8x16_t result;                                               \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3"                       \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtnd_u64_f64 (float64_t a)
-{
-  float64_t result;
-  __asm__ ("fcvtnu %d0,%d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vextq_p16(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x8_t b_ = (b);                                             \
+       poly16x8_t a_ = (a);                                             \
+       poly16x8_t result;                                               \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*2"                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vcvtnq_s32_f32 (float32x4_t a)
-{
-  int32x4_t result;
-  __asm__ ("fcvtns %0.4s, %1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vextq_s8(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       int8x16_t b_ = (b);                                              \
+       int8x16_t a_ = (a);                                              \
+       int8x16_t result;                                                \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3"                       \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vcvtnq_s64_f64 (float64x2_t a)
-{
-  int64x2_t result;
-  __asm__ ("fcvtns %0.2d, %1.2d"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vextq_s16(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t b_ = (b);                                              \
+       int16x8_t a_ = (a);                                              \
+       int16x8_t result;                                                \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*2"                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcvtnq_u32_f32 (float32x4_t a)
+#define vextq_s32(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*4"                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vextq_s64(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t b_ = (b);                                              \
+       int64x2_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*8"                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vextq_u8(a, b, c)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       uint8x16_t b_ = (b);                                             \
+       uint8x16_t a_ = (a);                                             \
+       uint8x16_t result;                                               \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3"                       \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vextq_u16(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t b_ = (b);                                             \
+       uint16x8_t a_ = (a);                                             \
+       uint16x8_t result;                                               \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*2"                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vextq_u32(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*4"                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vextq_u64(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t b_ = (b);                                             \
+       uint64x2_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*8"                     \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vfma_f32 (float32x2_t a, float32x2_t b, float32x2_t c)
 {
-  uint32x4_t result;
-  __asm__ ("fcvtnu %0.4s, %1.4s"
+  float32x2_t result;
+  __asm__ ("fmla %0.2s,%2.2s,%3.2s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcvtnq_u64_f64 (float64x2_t a)
+#define vfma_lane_f32(a, b, c, d)                                       \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x2_t c_ = (c);                                            \
+       float32x2_t b_ = (b);                                            \
+       float32x2_t a_ = (a);                                            \
+       float32x2_t result;                                              \
+       __asm__ ("fmla %0.2s,%2.2s,%3.s[%4]"                             \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vfmad_lane_f64(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x2_t b_ = (b);                                            \
+       float64_t a_ = (a);                                              \
+       float64_t result;                                                \
+       __asm__ ("fmla %d0,%d1,%2.d[%3]"                                 \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vfmaq_f32 (float32x4_t a, float32x4_t b, float32x4_t c)
 {
-  uint64x2_t result;
-  __asm__ ("fcvtnu %0.2d, %1.2d"
+  float32x4_t result;
+  __asm__ ("fmla %0.4s,%2.4s,%3.4s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvtns_s64_f64 (float32_t a)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vfmaq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)
 {
-  float32_t result;
-  __asm__ ("fcvtns %s0,%s1"
+  float64x2_t result;
+  __asm__ ("fmla %0.2d,%2.2d,%3.2d"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvtns_u64_f64 (float32_t a)
+#define vfmaq_lane_f32(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t c_ = (c);                                            \
+       float32x4_t b_ = (b);                                            \
+       float32x4_t a_ = (a);                                            \
+       float32x4_t result;                                              \
+       __asm__ ("fmla %0.4s,%2.4s,%3.s[%4]"                             \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vfmaq_lane_f64(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x2_t c_ = (c);                                            \
+       float64x2_t b_ = (b);                                            \
+       float64x2_t a_ = (a);                                            \
+       float64x2_t result;                                              \
+       __asm__ ("fmla %0.2d,%2.2d,%3.d[%4]"                             \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vfmas_lane_f32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t b_ = (b);                                            \
+       float32_t a_ = (a);                                              \
+       float32_t result;                                                \
+       __asm__ ("fmla %s0,%s1,%2.s[%3]"                                 \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vfma_n_f32 (float32x2_t a, float32x2_t b, float32_t c)
 {
-  float32_t result;
-  __asm__ ("fcvtnu %s0,%s1"
+  float32x2_t result;
+  __asm__ ("fmla %0.2s, %2.2s, %3.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vcvtp_s32_f32 (float32x2_t a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vfmaq_n_f32 (float32x4_t a, float32x4_t b, float32_t c)
 {
-  int32x2_t result;
-  __asm__ ("fcvtps %0.2s, %1.2s"
+  float32x4_t result;
+  __asm__ ("fmla %0.4s, %2.4s, %3.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcvtp_u32_f32 (float32x2_t a)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vfmaq_n_f64 (float64x2_t a, float64x2_t b, float64_t c)
 {
-  uint32x2_t result;
-  __asm__ ("fcvtpu %0.2s, %1.2s"
+  float64x2_t result;
+  __asm__ ("fmla %0.2d, %2.2d, %3.d[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtpd_s64_f64 (float64_t a)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vfms_f32 (float32x2_t a, float32x2_t b, float32x2_t c)
 {
-  float64_t result;
-  __asm__ ("fcvtps %d0,%d1"
+  float32x2_t result;
+  __asm__ ("fmls %0.2s,%2.2s,%3.2s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vcvtpd_u64_f64 (float64_t a)
+#define vfmsd_lane_f64(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x2_t b_ = (b);                                            \
+       float64_t a_ = (a);                                              \
+       float64_t result;                                                \
+       __asm__ ("fmls %d0,%d1,%2.d[%3]"                                 \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vfmsq_f32 (float32x4_t a, float32x4_t b, float32x4_t c)
 {
-  float64_t result;
-  __asm__ ("fcvtpu %d0,%d1"
+  float32x4_t result;
+  __asm__ ("fmls %0.4s,%2.4s,%3.4s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vcvtpq_s32_f32 (float32x4_t a)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vfmsq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)
 {
-  int32x4_t result;
-  __asm__ ("fcvtps %0.4s, %1.4s"
+  float64x2_t result;
+  __asm__ ("fmls %0.2d,%2.2d,%3.2d"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vcvtpq_s64_f64 (float64x2_t a)
+#define vfmss_lane_f32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t b_ = (b);                                            \
+       float32_t a_ = (a);                                              \
+       float32_t result;                                                \
+       __asm__ ("fmls %s0,%s1,%2.s[%3]"                                 \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vget_high_f32 (float32x4_t a)
 {
-  int64x2_t result;
-  __asm__ ("fcvtps %0.2d, %1.2d"
+  float32x2_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcvtpq_u32_f32 (float32x4_t a)
+__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))
+vget_high_f64 (float64x2_t a)
 {
-  uint32x4_t result;
-  __asm__ ("fcvtpu %0.4s, %1.4s"
+  float64x1_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcvtpq_u64_f64 (float64x2_t a)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vget_high_p8 (poly8x16_t a)
 {
-  uint64x2_t result;
-  __asm__ ("fcvtpu %0.2d, %1.2d"
+  poly8x8_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvtps_s64_f64 (float32_t a)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vget_high_p16 (poly16x8_t a)
 {
-  float32_t result;
-  __asm__ ("fcvtps %s0,%s1"
+  poly16x4_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvtps_u64_f64 (float32_t a)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vget_high_s8 (int8x16_t a)
 {
-  float32_t result;
-  __asm__ ("fcvtpu %s0,%s1"
+  int8x8_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vcvtq_f32_s32 (int32x4_t a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vget_high_s16 (int16x8_t a)
 {
-  float32x4_t result;
-  __asm__ ("scvtf %0.4s, %1.4s"
+  int16x4_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vcvtq_f32_u32 (uint32x4_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vget_high_s32 (int32x4_t a)
 {
-  float32x4_t result;
-  __asm__ ("ucvtf %0.4s, %1.4s"
+  int32x2_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vcvtq_f64_s64 (int64x2_t a)
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vget_high_s64 (int64x2_t a)
 {
-  float64x2_t result;
-  __asm__ ("scvtf %0.2d, %1.2d"
+  int64x1_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vcvtq_f64_u64 (uint64x2_t a)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vget_high_u8 (uint8x16_t a)
 {
-  float64x2_t result;
-  __asm__ ("ucvtf %0.2d, %1.2d"
+  uint8x8_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-#define vcvtq_n_f32_s32(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t a_ = (a);                                              \
-       float32x4_t result;                                              \
-       __asm__ ("scvtf %0.4s, %1.4s, #%2"                               \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvtq_n_f32_u32(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t a_ = (a);                                             \
-       float32x4_t result;                                              \
-       __asm__ ("ucvtf %0.4s, %1.4s, #%2"                               \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvtq_n_f64_s64(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x2_t a_ = (a);                                              \
-       float64x2_t result;                                              \
-       __asm__ ("scvtf %0.2d, %1.2d, #%2"                               \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvtq_n_f64_u64(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x2_t a_ = (a);                                             \
-       float64x2_t result;                                              \
-       __asm__ ("ucvtf %0.2d, %1.2d, #%2"                               \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvtq_n_s32_f32(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t a_ = (a);                                            \
-       int32x4_t result;                                                \
-       __asm__ ("fcvtzs %0.4s, %1.4s, #%2"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvtq_n_s64_f64(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t a_ = (a);                                            \
-       int64x2_t result;                                                \
-       __asm__ ("fcvtzs %0.2d, %1.2d, #%2"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvtq_n_u32_f32(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t a_ = (a);                                            \
-       uint32x4_t result;                                               \
-       __asm__ ("fcvtzu %0.4s, %1.4s, #%2"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvtq_n_u64_f64(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t a_ = (a);                                            \
-       uint64x2_t result;                                               \
-       __asm__ ("fcvtzu %0.2d, %1.2d, #%2"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vcvtq_s32_f32 (float32x4_t a)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vget_high_u16 (uint16x8_t a)
 {
-  int32x4_t result;
-  __asm__ ("fcvtzs %0.4s, %1.4s"
+  uint16x4_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vcvtq_s64_f64 (float64x2_t a)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vget_high_u32 (uint32x4_t a)
 {
-  int64x2_t result;
-  __asm__ ("fcvtzs %0.2d, %1.2d"
+  uint32x2_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcvtq_u32_f32 (float32x4_t a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vget_high_u64 (uint64x2_t a)
 {
-  uint32x4_t result;
-  __asm__ ("fcvtzu %0.4s, %1.4s"
+  uint64x1_t result;
+  __asm__ ("ins %0.d[0], %1.d[1]"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcvtq_u64_f64 (float64x2_t a)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vhsub_s8 (int8x8_t a, int8x8_t b)
 {
-  uint64x2_t result;
-  __asm__ ("fcvtzu %0.2d, %1.2d"
+  int8x8_t result;
+  __asm__ ("shsub %0.8b, %1.8b, %2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32_t __attribute__ ((__always_inline__))
-vcvts_f64_s32 (int32_t a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vhsub_s16 (int16x4_t a, int16x4_t b)
 {
-  int32_t result;
-  __asm__ ("scvtf %s0,%s1"
+  int16x4_t result;
+  __asm__ ("shsub %0.4h, %1.4h, %2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
-vcvts_f64_u32 (uint32_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vhsub_s32 (int32x2_t a, int32x2_t b)
 {
-  uint32_t result;
-  __asm__ ("ucvtf %s0,%s1"
+  int32x2_t result;
+  __asm__ ("shsub %0.2s, %1.2s, %2.2s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vcvts_n_f32_s32(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       int32_t a_ = (a);                                                \
-       int32_t result;                                                  \
-       __asm__ ("scvtf %s0,%s1,%2"                                      \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vhsub_u8 (uint8x8_t a, uint8x8_t b)
+{
+  uint8x8_t result;
+  __asm__ ("uhsub %0.8b, %1.8b, %2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vcvts_n_f32_u32(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32_t a_ = (a);                                               \
-       uint32_t result;                                                 \
-       __asm__ ("ucvtf %s0,%s1,%2"                                      \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvts_n_s32_f32(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       float32_t a_ = (a);                                              \
-       float32_t result;                                                \
-       __asm__ ("fcvtzs %s0,%s1,%2"                                     \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vcvts_n_u32_f32(a, b)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       float32_t a_ = (a);                                              \
-       float32_t result;                                                \
-       __asm__ ("fcvtzu %s0,%s1,%2"                                     \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvts_s64_f64 (float32_t a)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vhsub_u16 (uint16x4_t a, uint16x4_t b)
 {
-  float32_t result;
-  __asm__ ("fcvtzs %s0,%s1"
+  uint16x4_t result;
+  __asm__ ("uhsub %0.4h, %1.4h, %2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvts_u64_f64 (float32_t a)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vhsub_u32 (uint32x2_t a, uint32x2_t b)
 {
-  float32_t result;
-  __asm__ ("fcvtzu %s0,%s1"
+  uint32x2_t result;
+  __asm__ ("uhsub %0.2s, %1.2s, %2.2s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vcvtx_f32_f64 (float64x2_t a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vhsubq_s8 (int8x16_t a, int8x16_t b)
 {
-  float32x2_t result;
-  __asm__ ("fcvtxn %0.2s,%1.2d"
+  int8x16_t result;
+  __asm__ ("shsub %0.16b, %1.16b, %2.16b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vcvtx_high_f32_f64 (float64x2_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vhsubq_s16 (int16x8_t a, int16x8_t b)
 {
-  float32x4_t result;
-  __asm__ ("fcvtxn2 %0.4s,%1.2d"
+  int16x8_t result;
+  __asm__ ("shsub %0.8h, %1.8h, %2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vcvtxd_f32_f64 (float64_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vhsubq_s32 (int32x4_t a, int32x4_t b)
 {
-  float32_t result;
-  __asm__ ("fcvtxn %s0,%d1"
+  int32x4_t result;
+  __asm__ ("shsub %0.4s, %1.4s, %2.4s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vdup_lane_f32(a, b)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t a_ = (a);                                            \
-       float32x2_t result;                                              \
-       __asm__ ("dup %0.2s,%1.s[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_p8(a, b)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x8_t a_ = (a);                                              \
-       poly8x8_t result;                                                \
-       __asm__ ("dup %0.8b,%1.b[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_p16(a, b)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x4_t a_ = (a);                                             \
-       poly16x4_t result;                                               \
-       __asm__ ("dup %0.4h,%1.h[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_s8(a, b)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x8_t a_ = (a);                                               \
-       int8x8_t result;                                                 \
-       __asm__ ("dup %0.8b,%1.b[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_s16(a, b)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t a_ = (a);                                              \
-       int16x4_t result;                                                \
-       __asm__ ("dup %0.4h,%1.h[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_s32(a, b)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t a_ = (a);                                              \
-       int32x2_t result;                                                \
-       __asm__ ("dup %0.2s,%1.s[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_s64(a, b)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x1_t a_ = (a);                                              \
-       int64x1_t result;                                                \
-       __asm__ ("ins %0.d[0],%1.d[%2]"                                  \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_u8(a, b)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x8_t a_ = (a);                                              \
-       uint8x8_t result;                                                \
-       __asm__ ("dup %0.8b,%1.b[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_u16(a, b)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t a_ = (a);                                             \
-       uint16x4_t result;                                               \
-       __asm__ ("dup %0.4h,%1.h[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_u32(a, b)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t a_ = (a);                                             \
-       uint32x2_t result;                                               \
-       __asm__ ("dup %0.2s,%1.s[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdup_lane_u64(a, b)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x1_t a_ = (a);                                             \
-       uint64x1_t result;                                               \
-       __asm__ ("ins %0.d[0],%1.d[%2]"                                  \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vdup_n_f32 (float32_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vhsubq_u8 (uint8x16_t a, uint8x16_t b)
 {
-  float32x2_t result;
-  __asm__ ("dup %0.2s, %w1"
+  uint8x16_t result;
+  __asm__ ("uhsub %0.16b, %1.16b, %2.16b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vdup_n_p8 (uint32_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vhsubq_u16 (uint16x8_t a, uint16x8_t b)
 {
-  poly8x8_t result;
-  __asm__ ("dup %0.8b,%w1"
+  uint16x8_t result;
+  __asm__ ("uhsub %0.8h, %1.8h, %2.8h"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vdup_n_p16 (uint32_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vhsubq_u32 (uint32x4_t a, uint32x4_t b)
 {
-  poly16x4_t result;
-  __asm__ ("dup %0.4h,%w1"
+  uint32x4_t result;
+  __asm__ ("uhsub %0.4s, %1.4s, %2.4s"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vdup_n_s8 (int32_t a)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vld1_dup_f32 (const float32_t * a)
 {
-  int8x8_t result;
-  __asm__ ("dup %0.8b,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  float32x2_t result;
+  __asm__ ("ld1r {%0.2s}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))
+vld1_dup_f64 (const float64_t * a)
+{
+  float64x1_t result;
+  __asm__ ("ld1r {%0.1d}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vld1_dup_p8 (const poly8_t * a)
+{
+  poly8x8_t result;
+  __asm__ ("ld1r {%0.8b}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vld1_dup_p16 (const poly16_t * a)
+{
+  poly16x4_t result;
+  __asm__ ("ld1r {%0.4h}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vld1_dup_s8 (const int8_t * a)
+{
+  int8x8_t result;
+  __asm__ ("ld1r {%0.8b}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vdup_n_s16 (int32_t a)
+vld1_dup_s16 (const int16_t * a)
 {
   int16x4_t result;
-  __asm__ ("dup %0.4h,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.4h}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vdup_n_s32 (int32_t a)
+vld1_dup_s32 (const int32_t * a)
 {
   int32x2_t result;
-  __asm__ ("dup %0.2s,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.2s}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vdup_n_s64 (int64_t a)
+vld1_dup_s64 (const int64_t * a)
 {
   int64x1_t result;
-  __asm__ ("ins %0.d[0],%x1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.1d}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vdup_n_u8 (uint32_t a)
+vld1_dup_u8 (const uint8_t * a)
 {
   uint8x8_t result;
-  __asm__ ("dup %0.8b,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.8b}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vdup_n_u16 (uint32_t a)
+vld1_dup_u16 (const uint16_t * a)
 {
   uint16x4_t result;
-  __asm__ ("dup %0.4h,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.4h}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vdup_n_u32 (uint32_t a)
+vld1_dup_u32 (const uint32_t * a)
 {
   uint32x2_t result;
-  __asm__ ("dup %0.2s,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.2s}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vdup_n_u64 (uint64_t a)
+vld1_dup_u64 (const uint64_t * a)
 {
   uint64x1_t result;
-  __asm__ ("ins %0.d[0],%x1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.1d}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
-#define vdupd_lane_f64(a, b)                                            \
+#define vld1_lane_f32(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       float64x2_t a_ = (a);                                            \
-       float64_t result;                                                \
-       __asm__ ("dup %d0, %1.d[%2]"                                     \
+       float32x2_t b_ = (b);                                            \
+       const float32_t * a_ = (a);                                      \
+       float32x2_t result;                                              \
+       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_f32(a, b)                                            \
+#define vld1_lane_f64(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       float32x2_t a_ = (a);                                            \
-       float32x4_t result;                                              \
-       __asm__ ("dup %0.4s,%1.s[%2]"                                    \
+       float64x1_t b_ = (b);                                            \
+       const float64_t * a_ = (a);                                      \
+       float64x1_t result;                                              \
+       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_f64(a, b)                                            \
+#define vld1_lane_p8(a, b, c)                                           \
   __extension__                                                         \
     ({                                                                  \
-       float64x1_t a_ = (a);                                            \
-       float64x2_t result;                                              \
-       __asm__ ("dup %0.2d,%1.d[%2]"                                    \
+       poly8x8_t b_ = (b);                                              \
+       const poly8_t * a_ = (a);                                        \
+       poly8x8_t result;                                                \
+       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_p8(a, b)                                             \
+#define vld1_lane_p16(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       poly8x8_t a_ = (a);                                              \
-       poly8x16_t result;                                               \
-       __asm__ ("dup %0.16b,%1.b[%2]"                                   \
+       poly16x4_t b_ = (b);                                             \
+       const poly16_t * a_ = (a);                                       \
+       poly16x4_t result;                                               \
+       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_p16(a, b)                                            \
+#define vld1_lane_s8(a, b, c)                                           \
   __extension__                                                         \
     ({                                                                  \
-       poly16x4_t a_ = (a);                                             \
-       poly16x8_t result;                                               \
-       __asm__ ("dup %0.8h,%1.h[%2]"                                    \
+       int8x8_t b_ = (b);                                               \
+       const int8_t * a_ = (a);                                         \
+       int8x8_t result;                                                 \
+       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_s8(a, b)                                             \
+#define vld1_lane_s16(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       int8x8_t a_ = (a);                                               \
-       int8x16_t result;                                                \
-       __asm__ ("dup %0.16b,%1.b[%2]"                                   \
+       int16x4_t b_ = (b);                                              \
+       const int16_t * a_ = (a);                                        \
+       int16x4_t result;                                                \
+       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_s16(a, b)                                            \
+#define vld1_lane_s32(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       int16x4_t a_ = (a);                                              \
-       int16x8_t result;                                                \
-       __asm__ ("dup %0.8h,%1.h[%2]"                                    \
+       int32x2_t b_ = (b);                                              \
+       const int32_t * a_ = (a);                                        \
+       int32x2_t result;                                                \
+       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_s32(a, b)                                            \
+#define vld1_lane_s64(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       int32x2_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("dup %0.4s,%1.s[%2]"                                    \
+       int64x1_t b_ = (b);                                              \
+       const int64_t * a_ = (a);                                        \
+       int64x1_t result;                                                \
+       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_s64(a, b)                                            \
+#define vld1_lane_u8(a, b, c)                                           \
   __extension__                                                         \
     ({                                                                  \
-       int64x1_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("dup %0.2d,%1.d[%2]"                                    \
+       uint8x8_t b_ = (b);                                              \
+       const uint8_t * a_ = (a);                                        \
+       uint8x8_t result;                                                \
+       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_u8(a, b)                                             \
+#define vld1_lane_u16(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       uint8x8_t a_ = (a);                                              \
-       uint8x16_t result;                                               \
-       __asm__ ("dup %0.16b,%1.b[%2]"                                   \
+       uint16x4_t b_ = (b);                                             \
+       const uint16_t * a_ = (a);                                       \
+       uint16x4_t result;                                               \
+       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_u16(a, b)                                            \
+#define vld1_lane_u32(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       uint16x4_t a_ = (a);                                             \
-       uint16x8_t result;                                               \
-       __asm__ ("dup %0.8h,%1.h[%2]"                                    \
+       uint32x2_t b_ = (b);                                             \
+       const uint32_t * a_ = (a);                                       \
+       uint32x2_t result;                                               \
+       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vdupq_lane_u32(a, b)                                            \
+#define vld1_lane_u64(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       uint32x2_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("dup %0.4s,%1.s[%2]"                                    \
+       uint64x1_t b_ = (b);                                             \
+       const uint64_t * a_ = (a);                                       \
+       uint64x1_t result;                                               \
+       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vdupq_lane_u64(a, b)                                            \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x1_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("dup %0.2d,%1.d[%2]"                                    \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i" (c), "Utv"(*a_), "0"(b_)                          \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
 __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vdupq_n_f32 (float32_t a)
+vld1q_dup_f32 (const float32_t * a)
 {
   float32x4_t result;
-  __asm__ ("dup %0.4s, %w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.4s}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vdupq_n_f64 (float64_t a)
+vld1q_dup_f64 (const float64_t * a)
 {
   float64x2_t result;
-  __asm__ ("dup %0.2d, %x1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.2d}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vdupq_n_p8 (uint32_t a)
+vld1q_dup_p8 (const poly8_t * a)
 {
   poly8x16_t result;
-  __asm__ ("dup %0.16b,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.16b}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vdupq_n_p16 (uint32_t a)
+vld1q_dup_p16 (const poly16_t * a)
 {
   poly16x8_t result;
-  __asm__ ("dup %0.8h,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.8h}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vdupq_n_s8 (int32_t a)
+vld1q_dup_s8 (const int8_t * a)
 {
   int8x16_t result;
-  __asm__ ("dup %0.16b,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.16b}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vdupq_n_s16 (int32_t a)
+vld1q_dup_s16 (const int16_t * a)
 {
   int16x8_t result;
-  __asm__ ("dup %0.8h,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.8h}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vdupq_n_s32 (int32_t a)
+vld1q_dup_s32 (const int32_t * a)
 {
   int32x4_t result;
-  __asm__ ("dup %0.4s,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.4s}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vdupq_n_s64 (int64_t a)
+vld1q_dup_s64 (const int64_t * a)
 {
   int64x2_t result;
-  __asm__ ("dup %0.2d,%x1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.2d}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vdupq_n_u8 (uint32_t a)
+vld1q_dup_u8 (const uint8_t * a)
 {
   uint8x16_t result;
-  __asm__ ("dup %0.16b,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.16b}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vdupq_n_u16 (uint32_t a)
+vld1q_dup_u16 (const uint16_t * a)
 {
   uint16x8_t result;
-  __asm__ ("dup %0.8h,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.8h}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vdupq_n_u32 (uint32_t a)
+vld1q_dup_u32 (const uint32_t * a)
 {
   uint32x4_t result;
-  __asm__ ("dup %0.4s,%w1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.4s}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vdupq_n_u64 (uint64_t a)
+vld1q_dup_u64 (const uint64_t * a)
 {
   uint64x2_t result;
-  __asm__ ("dup %0.2d,%x1"
-           : "=w"(result)
-           : "r"(a)
-           : /* No clobbers */);
+  __asm__ ("ld1r {%0.2d}, %1"
+	   : "=w"(result)
+	   : "Utv"(*a)
+	   : /* No clobbers */);
   return result;
 }
 
-#define vdups_lane_f32(a, b)                                            \
+#define vld1q_lane_f32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       float32x4_t a_ = (a);                                            \
-       float32_t result;                                                \
-       __asm__ ("dup %s0, %1.s[%2]"                                     \
+       float32x4_t b_ = (b);                                            \
+       const float32_t * a_ = (a);                                      \
+       float32x4_t result;                                              \
+       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_f32(a, b, c)                                               \
+#define vld1q_lane_f64(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       float32x2_t b_ = (b);                                            \
-       float32x2_t a_ = (a);                                            \
-       float32x2_t result;                                              \
-       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*4"                        \
+       float64x2_t b_ = (b);                                            \
+       const float64_t * a_ = (a);                                      \
+       float64x2_t result;                                              \
+       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_f64(a, b, c)                                               \
+#define vld1q_lane_p8(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       float64x1_t b_ = (b);                                            \
-       float64x1_t a_ = (a);                                            \
-       float64x1_t result;                                              \
-       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*8"                        \
+       poly8x16_t b_ = (b);                                             \
+       const poly8_t * a_ = (a);                                        \
+       poly8x16_t result;                                               \
+       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_p8(a, b, c)                                                \
+#define vld1q_lane_p16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       poly8x8_t b_ = (b);                                              \
-       poly8x8_t a_ = (a);                                              \
-       poly8x8_t result;                                                \
-       __asm__ ("ext %0.8b,%1.8b,%2.8b,%3"                              \
+       poly16x8_t b_ = (b);                                             \
+       const poly16_t * a_ = (a);                                       \
+       poly16x8_t result;                                               \
+       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_p16(a, b, c)                                               \
+#define vld1q_lane_s8(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       poly16x4_t b_ = (b);                                             \
-       poly16x4_t a_ = (a);                                             \
-       poly16x4_t result;                                               \
-       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*2"                        \
+       int8x16_t b_ = (b);                                              \
+       const int8_t * a_ = (a);                                         \
+       int8x16_t result;                                                \
+       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_s8(a, b, c)                                                \
+#define vld1q_lane_s16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int8x8_t b_ = (b);                                               \
-       int8x8_t a_ = (a);                                               \
-       int8x8_t result;                                                 \
-       __asm__ ("ext %0.8b,%1.8b,%2.8b,%3"                              \
+       int16x8_t b_ = (b);                                              \
+       const int16_t * a_ = (a);                                        \
+       int16x8_t result;                                                \
+       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_s16(a, b, c)                                               \
+#define vld1q_lane_s32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int16x4_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int16x4_t result;                                                \
-       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*2"                        \
+       int32x4_t b_ = (b);                                              \
+       const int32_t * a_ = (a);                                        \
+       int32x4_t result;                                                \
+       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_s32(a, b, c)                                               \
+#define vld1q_lane_s64(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int32x2_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int32x2_t result;                                                \
-       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*4"                        \
+       int64x2_t b_ = (b);                                              \
+       const int64_t * a_ = (a);                                        \
+       int64x2_t result;                                                \
+       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_s64(a, b, c)                                               \
+#define vld1q_lane_u8(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       int64x1_t b_ = (b);                                              \
-       int64x1_t a_ = (a);                                              \
-       int64x1_t result;                                                \
-       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*8"                        \
+       uint8x16_t b_ = (b);                                             \
+       const uint8_t * a_ = (a);                                        \
+       uint8x16_t result;                                               \
+       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_u8(a, b, c)                                                \
+#define vld1q_lane_u16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       uint8x8_t b_ = (b);                                              \
-       uint8x8_t a_ = (a);                                              \
-       uint8x8_t result;                                                \
-       __asm__ ("ext %0.8b,%1.8b,%2.8b,%3"                              \
+       uint16x8_t b_ = (b);                                             \
+       const uint16_t * a_ = (a);                                       \
+       uint16x8_t result;                                               \
+       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_u16(a, b, c)                                               \
+#define vld1q_lane_u32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       uint16x4_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint16x4_t result;                                               \
-       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*2"                        \
+       uint32x4_t b_ = (b);                                             \
+       const uint32_t * a_ = (a);                                       \
+       uint32x4_t result;                                               \
+       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vext_u32(a, b, c)                                               \
+#define vld1q_lane_u64(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       uint32x2_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint32x2_t result;                                               \
-       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*4"                        \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vext_u64(a, b, c)                                               \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x1_t b_ = (b);                                             \
-       uint64x1_t a_ = (a);                                             \
-       uint64x1_t result;                                               \
-       __asm__ ("ext %0.8b, %1.8b, %2.8b, #%3*8"                        \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vextq_f32(a, b, c)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32x4_t a_ = (a);                                            \
-       float32x4_t result;                                              \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*4"                     \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vextq_f64(a, b, c)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64x2_t a_ = (a);                                            \
-       float64x2_t result;                                              \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*8"                     \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vextq_p8(a, b, c)                                               \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x16_t b_ = (b);                                             \
-       poly8x16_t a_ = (a);                                             \
-       poly8x16_t result;                                               \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3"                       \
+       uint64x2_t b_ = (b);                                             \
+       const uint64_t * a_ = (a);                                       \
+       uint64x2_t result;                                               \
+       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "i"(c), "Utv"(*a_), "0"(b_)                           \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vextq_p16(a, b, c)                                              \
+#define vmla_lane_f32(a, b, c, d)                                       \
   __extension__                                                         \
     ({                                                                  \
-       poly16x8_t b_ = (b);                                             \
-       poly16x8_t a_ = (a);                                             \
-       poly16x8_t result;                                               \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*2"                     \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+       float32x2_t c_ = (c);                                            \
+       float32x2_t b_ = (b);                                            \
+       float32x2_t a_ = (a);                                            \
+       float32x2_t result;                                              \
+       float32x2_t t1;                                                  \
+       __asm__ ("fmul %1.2s, %3.2s, %4.s[%5]; fadd %0.2s, %0.2s, %1.2s" \
+                : "=w"(result), "=w"(t1)                                \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vextq_s8(a, b, c)                                               \
+#define vmla_lane_s16(a, b, c, d)                                       \
   __extension__                                                         \
     ({                                                                  \
-       int8x16_t b_ = (b);                                              \
-       int8x16_t a_ = (a);                                              \
-       int8x16_t result;                                                \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3"                       \
+       int16x4_t c_ = (c);                                              \
+       int16x4_t b_ = (b);                                              \
+       int16x4_t a_ = (a);                                              \
+       int16x4_t result;                                                \
+       __asm__ ("mla %0.4h, %2.4h, %3.h[%4]"                            \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vextq_s16(a, b, c)                                              \
+#define vmla_lane_s32(a, b, c, d)                                       \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16x8_t a_ = (a);                                              \
-       int16x8_t result;                                                \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*2"                     \
+       int32x2_t c_ = (c);                                              \
+       int32x2_t b_ = (b);                                              \
+       int32x2_t a_ = (a);                                              \
+       int32x2_t result;                                                \
+       __asm__ ("mla %0.2s, %2.2s, %3.s[%4]"                            \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vextq_s32(a, b, c)                                              \
+#define vmla_lane_u16(a, b, c, d)                                       \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*4"                     \
+       uint16x4_t c_ = (c);                                             \
+       uint16x4_t b_ = (b);                                             \
+       uint16x4_t a_ = (a);                                             \
+       uint16x4_t result;                                               \
+       __asm__ ("mla %0.4h, %2.4h, %3.h[%4]"                            \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vextq_s64(a, b, c)                                              \
+#define vmla_lane_u32(a, b, c, d)                                       \
   __extension__                                                         \
     ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       int64x2_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*8"                     \
+       uint32x2_t c_ = (c);                                             \
+       uint32x2_t b_ = (b);                                             \
+       uint32x2_t a_ = (a);                                             \
+       uint32x2_t result;                                               \
+       __asm__ ("mla %0.2s, %2.2s, %3.s[%4]"                            \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vextq_u8(a, b, c)                                               \
+#define vmla_laneq_s16(a, b, c, d)                                      \
   __extension__                                                         \
     ({                                                                  \
-       uint8x16_t b_ = (b);                                             \
-       uint8x16_t a_ = (a);                                             \
-       uint8x16_t result;                                               \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3"                       \
+       int16x8_t c_ = (c);                                              \
+       int16x4_t b_ = (b);                                              \
+       int16x4_t a_ = (a);                                              \
+       int16x4_t result;                                                \
+       __asm__ ("mla %0.4h, %2.4h, %3.h[%4]"                            \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vextq_u16(a, b, c)                                              \
+#define vmla_laneq_s32(a, b, c, d)                                      \
   __extension__                                                         \
     ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16x8_t a_ = (a);                                             \
-       uint16x8_t result;                                               \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*2"                     \
+       int32x4_t c_ = (c);                                              \
+       int32x2_t b_ = (b);                                              \
+       int32x2_t a_ = (a);                                              \
+       int32x2_t result;                                                \
+       __asm__ ("mla %0.2s, %2.2s, %3.s[%4]"                            \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vextq_u32(a, b, c)                                              \
+#define vmla_laneq_u16(a, b, c, d)                                      \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*4"                     \
+       uint16x8_t c_ = (c);                                             \
+       uint16x4_t b_ = (b);                                             \
+       uint16x4_t a_ = (a);                                             \
+       uint16x4_t result;                                               \
+       __asm__ ("mla %0.4h, %2.4h, %3.h[%4]"                            \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vextq_u64(a, b, c)                                              \
+#define vmla_laneq_u32(a, b, c, d)                                      \
   __extension__                                                         \
     ({                                                                  \
-       uint64x2_t b_ = (b);                                             \
-       uint64x2_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("ext %0.16b, %1.16b, %2.16b, #%3*8"                     \
+       uint32x4_t c_ = (c);                                             \
+       uint32x2_t b_ = (b);                                             \
+       uint32x2_t a_ = (a);                                             \
+       uint32x2_t result;                                               \
+       __asm__ ("mla %0.2s, %2.2s, %3.s[%4]"                            \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
 __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vfma_f32 (float32x2_t a, float32x2_t b, float32x2_t c)
+vmla_n_f32 (float32x2_t a, float32x2_t b, float32_t c)
 {
   float32x2_t result;
-  __asm__ ("fmla %0.2s,%2.2s,%3.2s"
-           : "=w"(result)
+  float32x2_t t1;
+  __asm__ ("fmul %1.2s, %3.2s, %4.s[0]; fadd %0.2s, %0.2s, %1.2s"
+           : "=w"(result), "=w"(t1)
            : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-#define vfma_lane_f32(a, b, c, d)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t c_ = (c);                                            \
-       float32x2_t b_ = (b);                                            \
-       float32x2_t a_ = (a);                                            \
-       float32x2_t result;                                              \
-       __asm__ ("fmla %0.2s,%2.2s,%3.s[%4]"                             \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vmla_n_s16 (int16x4_t a, int16x4_t b, int16_t c)
+{
+  int16x4_t result;
+  __asm__ ("mla %0.4h,%2.4h,%3.h[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vfmad_lane_f64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64_t a_ = (a);                                              \
-       float64_t result;                                                \
-       __asm__ ("fmla %d0,%d1,%2.d[%3]"                                 \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vmla_n_s32 (int32x2_t a, int32x2_t b, int32_t c)
+{
+  int32x2_t result;
+  __asm__ ("mla %0.2s,%2.2s,%3.s[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
+  return result;
+}
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vfmaq_f32 (float32x4_t a, float32x4_t b, float32x4_t c)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vmla_n_u16 (uint16x4_t a, uint16x4_t b, uint16_t c)
 {
-  float32x4_t result;
-  __asm__ ("fmla %0.4s,%2.4s,%3.4s"
+  uint16x4_t result;
+  __asm__ ("mla %0.4h,%2.4h,%3.h[0]"
            : "=w"(result)
            : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vfmaq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vmla_n_u32 (uint32x2_t a, uint32x2_t b, uint32_t c)
 {
-  float64x2_t result;
-  __asm__ ("fmla %0.2d,%2.2d,%3.2d"
+  uint32x2_t result;
+  __asm__ ("mla %0.2s,%2.2s,%3.s[0]"
            : "=w"(result)
            : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-#define vfmaq_lane_f32(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t c_ = (c);                                            \
-       float32x4_t b_ = (b);                                            \
-       float32x4_t a_ = (a);                                            \
-       float32x4_t result;                                              \
-       __asm__ ("fmla %0.4s,%2.4s,%3.s[%4]"                             \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vfmaq_lane_f64(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t c_ = (c);                                            \
-       float64x2_t b_ = (b);                                            \
-       float64x2_t a_ = (a);                                            \
-       float64x2_t result;                                              \
-       __asm__ ("fmla %0.2d,%2.2d,%3.d[%4]"                             \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vfmas_lane_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32_t a_ = (a);                                              \
-       float32_t result;                                                \
-       __asm__ ("fmla %s0,%s1,%2.s[%3]"                                 \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vfma_n_f32 (float32x2_t a, float32x2_t b, float32_t c)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vmla_s8 (int8x8_t a, int8x8_t b, int8x8_t c)
 {
-  float32x2_t result;
-  __asm__ ("fmla %0.2s, %2.2s, %3.s[0]"
+  int8x8_t result;
+  __asm__ ("mla %0.8b, %2.8b, %3.8b"
            : "=w"(result)
            : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vfmaq_n_f32 (float32x4_t a, float32x4_t b, float32_t c)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vmla_s16 (int16x4_t a, int16x4_t b, int16x4_t c)
 {
-  float32x4_t result;
-  __asm__ ("fmla %0.4s, %2.4s, %3.s[0]"
+  int16x4_t result;
+  __asm__ ("mla %0.4h, %2.4h, %3.4h"
            : "=w"(result)
            : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vfmaq_n_f64 (float64x2_t a, float64x2_t b, float64_t c)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vmla_s32 (int32x2_t a, int32x2_t b, int32x2_t c)
 {
-  float64x2_t result;
-  __asm__ ("fmla %0.2d, %2.2d, %3.d[0]"
+  int32x2_t result;
+  __asm__ ("mla %0.2s, %2.2s, %3.2s"
            : "=w"(result)
            : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vfms_f32 (float32x2_t a, float32x2_t b, float32x2_t c)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vmla_u8 (uint8x8_t a, uint8x8_t b, uint8x8_t c)
 {
-  float32x2_t result;
-  __asm__ ("fmls %0.2s,%2.2s,%3.2s"
+  uint8x8_t result;
+  __asm__ ("mla %0.8b, %2.8b, %3.8b"
            : "=w"(result)
            : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-#define vfmsd_lane_f64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64_t a_ = (a);                                              \
-       float64_t result;                                                \
-       __asm__ ("fmls %d0,%d1,%2.d[%3]"                                 \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vfmsq_f32 (float32x4_t a, float32x4_t b, float32x4_t c)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vmla_u16 (uint16x4_t a, uint16x4_t b, uint16x4_t c)
 {
-  float32x4_t result;
-  __asm__ ("fmls %0.4s,%2.4s,%3.4s"
+  uint16x4_t result;
+  __asm__ ("mla %0.4h, %2.4h, %3.4h"
            : "=w"(result)
            : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vfmsq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vmla_u32 (uint32x2_t a, uint32x2_t b, uint32x2_t c)
 {
-  float64x2_t result;
-  __asm__ ("fmls %0.2d,%2.2d,%3.2d"
+  uint32x2_t result;
+  __asm__ ("mla %0.2s, %2.2s, %3.2s"
            : "=w"(result)
            : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-#define vfmss_lane_f32(a, b, c)                                         \
+#define vmlal_high_lane_s16(a, b, c, d)                                 \
   __extension__                                                         \
     ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32_t a_ = (a);                                              \
-       float32_t result;                                                \
-       __asm__ ("fmls %s0,%s1,%2.s[%3]"                                 \
+       int16x8_t c_ = (c);                                              \
+       int16x8_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("smlal2 %0.4s, %2.8h, %3.h[%4]"                         \
                 : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vget_high_f32 (float32x4_t a)
+#define vmlal_high_lane_s32(a, b, c, d)                                 \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t c_ = (c);                                              \
+       int32x4_t b_ = (b);                                              \
+       int64x2_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("smlal2 %0.2d, %2.4s, %3.s[%4]"                         \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_high_lane_u16(a, b, c, d)                                 \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t c_ = (c);                                             \
+       uint16x8_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("umlal2 %0.4s, %2.8h, %3.h[%4]"                         \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_high_lane_u32(a, b, c, d)                                 \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t c_ = (c);                                             \
+       uint32x4_t b_ = (b);                                             \
+       uint64x2_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("umlal2 %0.2d, %2.4s, %3.s[%4]"                         \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_high_laneq_s16(a, b, c, d)                                \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t c_ = (c);                                              \
+       int16x8_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("smlal2 %0.4s, %2.8h, %3.h[%4]"                         \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_high_laneq_s32(a, b, c, d)                                \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t c_ = (c);                                              \
+       int32x4_t b_ = (b);                                              \
+       int64x2_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("smlal2 %0.2d, %2.4s, %3.s[%4]"                         \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_high_laneq_u16(a, b, c, d)                                \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t c_ = (c);                                             \
+       uint16x8_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("umlal2 %0.4s, %2.8h, %3.h[%4]"                         \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_high_laneq_u32(a, b, c, d)                                \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t c_ = (c);                                             \
+       uint32x4_t b_ = (b);                                             \
+       uint64x2_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("umlal2 %0.2d, %2.4s, %3.s[%4]"                         \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlal_high_n_s16 (int32x4_t a, int16x8_t b, int16_t c)
 {
-  float32x2_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  int32x4_t result;
+  __asm__ ("smlal2 %0.4s,%2.8h,%3.h[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))
-vget_high_f64 (float64x2_t a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmlal_high_n_s32 (int64x2_t a, int32x4_t b, int32_t c)
 {
-  float64x1_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  int64x2_t result;
+  __asm__ ("smlal2 %0.2d,%2.4s,%3.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vget_high_p8 (poly8x16_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlal_high_n_u16 (uint32x4_t a, uint16x8_t b, uint16_t c)
 {
-  poly8x8_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  uint32x4_t result;
+  __asm__ ("umlal2 %0.4s,%2.8h,%3.h[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vget_high_p16 (poly16x8_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmlal_high_n_u32 (uint64x2_t a, uint32x4_t b, uint32_t c)
 {
-  poly16x4_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  uint64x2_t result;
+  __asm__ ("umlal2 %0.2d,%2.4s,%3.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vget_high_s8 (int8x16_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmlal_high_s8 (int16x8_t a, int8x16_t b, int8x16_t c)
 {
-  int8x8_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  int16x8_t result;
+  __asm__ ("smlal2 %0.8h,%2.16b,%3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vget_high_s16 (int16x8_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlal_high_s16 (int32x4_t a, int16x8_t b, int16x8_t c)
 {
-  int16x4_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  int32x4_t result;
+  __asm__ ("smlal2 %0.4s,%2.8h,%3.8h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vget_high_s32 (int32x4_t a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmlal_high_s32 (int64x2_t a, int32x4_t b, int32x4_t c)
 {
-  int32x2_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  int64x2_t result;
+  __asm__ ("smlal2 %0.2d,%2.4s,%3.4s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vget_high_s64 (int64x2_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmlal_high_u8 (uint16x8_t a, uint8x16_t b, uint8x16_t c)
 {
-  int64x1_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  uint16x8_t result;
+  __asm__ ("umlal2 %0.8h,%2.16b,%3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vget_high_u8 (uint8x16_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlal_high_u16 (uint32x4_t a, uint16x8_t b, uint16x8_t c)
 {
-  uint8x8_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  uint32x4_t result;
+  __asm__ ("umlal2 %0.4s,%2.8h,%3.8h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vget_high_u16 (uint16x8_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmlal_high_u32 (uint64x2_t a, uint32x4_t b, uint32x4_t c)
 {
-  uint16x4_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
+  uint64x2_t result;
+  __asm__ ("umlal2 %0.2d,%2.4s,%3.4s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vget_high_u32 (uint32x4_t a)
-{
-  uint32x2_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vmlal_lane_s16(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x4_t c_ = (c);                                              \
+       int16x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("smlal %0.4s,%2.4h,%3.h[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vget_high_u64 (uint64x2_t a)
-{
-  uint64x1_t result;
-  __asm__ ("ins %0.d[0], %1.d[1]"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vmlal_lane_s32(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x2_t c_ = (c);                                              \
+       int32x2_t b_ = (b);                                              \
+       int64x2_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("smlal %0.2d,%2.2s,%3.s[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-#define vget_lane_f64(a, b)                                             \
+#define vmlal_lane_u16(a, b, c, d)                                      \
   __extension__                                                         \
     ({                                                                  \
-       float64x1_t a_ = (a);                                            \
-       float64_t result;                                                \
-       __asm__ ("umov %x0, %1.d[%2]"                                    \
-                : "=r"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
+       uint16x4_t c_ = (c);                                             \
+       uint16x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("umlal %0.4s,%2.4h,%3.h[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vget_low_f32 (float32x4_t a)
+#define vmlal_lane_u32(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x2_t c_ = (c);                                             \
+       uint32x2_t b_ = (b);                                             \
+       uint64x2_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("umlal %0.2d, %2.2s, %3.s[%4]"                          \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_laneq_s16(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t c_ = (c);                                              \
+       int16x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("smlal %0.4s, %2.4h, %3.h[%4]"                          \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_laneq_s32(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t c_ = (c);                                              \
+       int32x2_t b_ = (b);                                              \
+       int64x2_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("smlal %0.2d, %2.2s, %3.s[%4]"                          \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_laneq_u16(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t c_ = (c);                                             \
+       uint16x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("umlal %0.4s, %2.4h, %3.h[%4]"                          \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlal_laneq_u32(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t c_ = (c);                                             \
+       uint32x2_t b_ = (b);                                             \
+       uint64x2_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("umlal %0.2d, %2.2s, %3.s[%4]"                          \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlal_n_s16 (int32x4_t a, int16x4_t b, int16_t c)
 {
-  float32x2_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  int32x4_t result;
+  __asm__ ("smlal %0.4s,%2.4h,%3.h[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))
-vget_low_f64 (float64x2_t a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmlal_n_s32 (int64x2_t a, int32x2_t b, int32_t c)
 {
-  float64x1_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  int64x2_t result;
+  __asm__ ("smlal %0.2d,%2.2s,%3.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vget_low_p8 (poly8x16_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlal_n_u16 (uint32x4_t a, uint16x4_t b, uint16_t c)
 {
-  poly8x8_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  uint32x4_t result;
+  __asm__ ("umlal %0.4s,%2.4h,%3.h[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vget_low_p16 (poly16x8_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmlal_n_u32 (uint64x2_t a, uint32x2_t b, uint32_t c)
 {
-  poly16x4_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  uint64x2_t result;
+  __asm__ ("umlal %0.2d,%2.2s,%3.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vget_low_s8 (int8x16_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmlal_s8 (int16x8_t a, int8x8_t b, int8x8_t c)
 {
-  int8x8_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  int16x8_t result;
+  __asm__ ("smlal %0.8h,%2.8b,%3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vget_low_s16 (int16x8_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlal_s16 (int32x4_t a, int16x4_t b, int16x4_t c)
 {
-  int16x4_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  int32x4_t result;
+  __asm__ ("smlal %0.4s,%2.4h,%3.4h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vget_low_s32 (int32x4_t a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmlal_s32 (int64x2_t a, int32x2_t b, int32x2_t c)
 {
-  int32x2_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  int64x2_t result;
+  __asm__ ("smlal %0.2d,%2.2s,%3.2s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vget_low_s64 (int64x2_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmlal_u8 (uint16x8_t a, uint8x8_t b, uint8x8_t c)
 {
-  int64x1_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  uint16x8_t result;
+  __asm__ ("umlal %0.8h,%2.8b,%3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vget_low_u8 (uint8x16_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlal_u16 (uint32x4_t a, uint16x4_t b, uint16x4_t c)
 {
-  uint8x8_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  uint32x4_t result;
+  __asm__ ("umlal %0.4s,%2.4h,%3.4h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vget_low_u16 (uint16x8_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmlal_u32 (uint64x2_t a, uint32x2_t b, uint32x2_t c)
 {
-  uint16x4_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
+  uint64x2_t result;
+  __asm__ ("umlal %0.2d,%2.2s,%3.2s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vget_low_u32 (uint32x4_t a)
-{
-  uint32x2_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vmlaq_lane_f32(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t c_ = (c);                                            \
+       float32x4_t b_ = (b);                                            \
+       float32x4_t a_ = (a);                                            \
+       float32x4_t result;                                              \
+       float32x4_t t1;                                                  \
+       __asm__ ("fmul %1.4s, %3.4s, %4.s[%5]; fadd %0.4s, %0.4s, %1.4s" \
+                : "=w"(result), "=w"(t1)                                \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vget_low_u64 (uint64x2_t a)
-{
-  uint64x1_t result;
-  __asm__ ("ins %0.d[0], %1.d[0]"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vmlaq_lane_s16(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t c_ = (c);                                              \
+       int16x8_t b_ = (b);                                              \
+       int16x8_t a_ = (a);                                              \
+       int16x8_t result;                                                \
+       __asm__ ("mla %0.8h, %2.8h, %3.h[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vhsub_s8 (int8x8_t a, int8x8_t b)
+#define vmlaq_lane_s32(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t c_ = (c);                                              \
+       int32x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("mla %0.4s, %2.4s, %3.s[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlaq_lane_u16(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t c_ = (c);                                             \
+       uint16x8_t b_ = (b);                                             \
+       uint16x8_t a_ = (a);                                             \
+       uint16x8_t result;                                               \
+       __asm__ ("mla %0.8h, %2.8h, %3.h[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlaq_lane_u32(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t c_ = (c);                                             \
+       uint32x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("mla %0.4s, %2.4s, %3.s[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlaq_laneq_s16(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t c_ = (c);                                              \
+       int16x8_t b_ = (b);                                              \
+       int16x8_t a_ = (a);                                              \
+       int16x8_t result;                                                \
+       __asm__ ("mla %0.8h, %2.8h, %3.h[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlaq_laneq_s32(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t c_ = (c);                                              \
+       int32x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("mla %0.4s, %2.4s, %3.s[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlaq_laneq_u16(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t c_ = (c);                                             \
+       uint16x8_t b_ = (b);                                             \
+       uint16x8_t a_ = (a);                                             \
+       uint16x8_t result;                                               \
+       __asm__ ("mla %0.8h, %2.8h, %3.h[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlaq_laneq_u32(a, b, c, d)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t c_ = (c);                                             \
+       uint32x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("mla %0.4s, %2.4s, %3.s[%4]"                            \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vmlaq_n_f32 (float32x4_t a, float32x4_t b, float32_t c)
 {
-  int8x8_t result;
-  __asm__ ("shsub %0.8b, %1.8b, %2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  float32x4_t result;
+  float32x4_t t1;
+  __asm__ ("fmul %1.4s, %3.4s, %4.s[0]; fadd %0.4s, %0.4s, %1.4s"
+           : "=w"(result), "=w"(t1)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vhsub_s16 (int16x4_t a, int16x4_t b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vmlaq_n_f64 (float64x2_t a, float64x2_t b, float64_t c)
 {
-  int16x4_t result;
-  __asm__ ("shsub %0.4h, %1.4h, %2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  float64x2_t result;
+  float64x2_t t1;
+  __asm__ ("fmul %1.2d, %3.2d, %4.d[0]; fadd %0.2d, %0.2d, %1.2d"
+           : "=w"(result), "=w"(t1)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vhsub_s32 (int32x2_t a, int32x2_t b)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmlaq_n_s16 (int16x8_t a, int16x8_t b, int16_t c)
 {
-  int32x2_t result;
-  __asm__ ("shsub %0.2s, %1.2s, %2.2s"
+  int16x8_t result;
+  __asm__ ("mla %0.8h,%2.8h,%3.h[0]"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vhsub_u8 (uint8x8_t a, uint8x8_t b)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlaq_n_s32 (int32x4_t a, int32x4_t b, int32_t c)
 {
-  uint8x8_t result;
-  __asm__ ("uhsub %0.8b, %1.8b, %2.8b"
+  int32x4_t result;
+  __asm__ ("mla %0.4s,%2.4s,%3.s[0]"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vhsub_u16 (uint16x4_t a, uint16x4_t b)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmlaq_n_u16 (uint16x8_t a, uint16x8_t b, uint16_t c)
 {
-  uint16x4_t result;
-  __asm__ ("uhsub %0.4h, %1.4h, %2.4h"
+  uint16x8_t result;
+  __asm__ ("mla %0.8h,%2.8h,%3.h[0]"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vhsub_u32 (uint32x2_t a, uint32x2_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlaq_n_u32 (uint32x4_t a, uint32x4_t b, uint32_t c)
 {
-  uint32x2_t result;
-  __asm__ ("uhsub %0.2s, %1.2s, %2.2s"
+  uint32x4_t result;
+  __asm__ ("mla %0.4s,%2.4s,%3.s[0]"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vhsubq_s8 (int8x16_t a, int8x16_t b)
+vmlaq_s8 (int8x16_t a, int8x16_t b, int8x16_t c)
 {
   int8x16_t result;
-  __asm__ ("shsub %0.16b, %1.16b, %2.16b"
+  __asm__ ("mla %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vhsubq_s16 (int16x8_t a, int16x8_t b)
+vmlaq_s16 (int16x8_t a, int16x8_t b, int16x8_t c)
 {
   int16x8_t result;
-  __asm__ ("shsub %0.8h, %1.8h, %2.8h"
+  __asm__ ("mla %0.8h, %2.8h, %3.8h"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vhsubq_s32 (int32x4_t a, int32x4_t b)
+vmlaq_s32 (int32x4_t a, int32x4_t b, int32x4_t c)
 {
   int32x4_t result;
-  __asm__ ("shsub %0.4s, %1.4s, %2.4s"
+  __asm__ ("mla %0.4s, %2.4s, %3.4s"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vhsubq_u8 (uint8x16_t a, uint8x16_t b)
+vmlaq_u8 (uint8x16_t a, uint8x16_t b, uint8x16_t c)
 {
   uint8x16_t result;
-  __asm__ ("uhsub %0.16b, %1.16b, %2.16b"
+  __asm__ ("mla %0.16b, %2.16b, %3.16b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vhsubq_u16 (uint16x8_t a, uint16x8_t b)
+vmlaq_u16 (uint16x8_t a, uint16x8_t b, uint16x8_t c)
 {
   uint16x8_t result;
-  __asm__ ("uhsub %0.8h, %1.8h, %2.8h"
+  __asm__ ("mla %0.8h, %2.8h, %3.8h"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vhsubq_u32 (uint32x4_t a, uint32x4_t b)
+vmlaq_u32 (uint32x4_t a, uint32x4_t b, uint32x4_t c)
 {
   uint32x4_t result;
-  __asm__ ("uhsub %0.4s, %1.4s, %2.4s"
+  __asm__ ("mla %0.4s, %2.4s, %3.4s"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vld1_dup_f32 (const float32_t * a)
-{
-  float32x2_t result;
-  __asm__ ("ld1r {%0.2s}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))
-vld1_dup_f64 (const float64_t * a)
+#define vmls_lane_f32(a, b, c, d)                                       \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x2_t c_ = (c);                                            \
+       float32x2_t b_ = (b);                                            \
+       float32x2_t a_ = (a);                                            \
+       float32x2_t result;                                              \
+       float32x2_t t1;                                                  \
+       __asm__ ("fmul %1.2s, %3.2s, %4.s[%5]; fsub %0.2s, %0.2s, %1.2s" \
+                : "=w"(result), "=w"(t1)                                \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmls_lane_s16(a, b, c, d)                                       \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x4_t c_ = (c);                                              \
+       int16x4_t b_ = (b);                                              \
+       int16x4_t a_ = (a);                                              \
+       int16x4_t result;                                                \
+       __asm__ ("mls %0.4h,%2.4h,%3.h[%4]"                              \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmls_lane_s32(a, b, c, d)                                       \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x2_t c_ = (c);                                              \
+       int32x2_t b_ = (b);                                              \
+       int32x2_t a_ = (a);                                              \
+       int32x2_t result;                                                \
+       __asm__ ("mls %0.2s,%2.2s,%3.s[%4]"                              \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmls_lane_u16(a, b, c, d)                                       \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x4_t c_ = (c);                                             \
+       uint16x4_t b_ = (b);                                             \
+       uint16x4_t a_ = (a);                                             \
+       uint16x4_t result;                                               \
+       __asm__ ("mls %0.4h,%2.4h,%3.h[%4]"                              \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmls_lane_u32(a, b, c, d)                                       \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x2_t c_ = (c);                                             \
+       uint32x2_t b_ = (b);                                             \
+       uint32x2_t a_ = (a);                                             \
+       uint32x2_t result;                                               \
+       __asm__ ("mls %0.2s,%2.2s,%3.s[%4]"                              \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vmls_n_f32 (float32x2_t a, float32x2_t b, float32_t c)
 {
-  float64x1_t result;
-  __asm__ ("ld1r {%0.1d}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  float32x2_t result;
+  float32x2_t t1;
+  __asm__ ("fmul %1.2s, %3.2s, %4.s[0]; fsub %0.2s, %0.2s, %1.2s"
+           : "=w"(result), "=w"(t1)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vld1_dup_p8 (const poly8_t * a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vmls_n_s16 (int16x4_t a, int16x4_t b, int16_t c)
 {
-  poly8x8_t result;
-  __asm__ ("ld1r {%0.8b}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  int16x4_t result;
+  __asm__ ("mls %0.4h, %2.4h, %3.h[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vld1_dup_p16 (const poly16_t * a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vmls_n_s32 (int32x2_t a, int32x2_t b, int32_t c)
 {
-  poly16x4_t result;
-  __asm__ ("ld1r {%0.4h}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  int32x2_t result;
+  __asm__ ("mls %0.2s, %2.2s, %3.s[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vmls_n_u16 (uint16x4_t a, uint16x4_t b, uint16_t c)
+{
+  uint16x4_t result;
+  __asm__ ("mls %0.4h, %2.4h, %3.h[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vmls_n_u32 (uint32x2_t a, uint32x2_t b, uint32_t c)
+{
+  uint32x2_t result;
+  __asm__ ("mls %0.2s, %2.2s, %3.s[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vld1_dup_s8 (const int8_t * a)
+vmls_s8 (int8x8_t a, int8x8_t b, int8x8_t c)
 {
   int8x8_t result;
-  __asm__ ("ld1r {%0.8b}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("mls %0.8b,%2.8b,%3.8b"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vld1_dup_s16 (const int16_t * a)
+vmls_s16 (int16x4_t a, int16x4_t b, int16x4_t c)
 {
   int16x4_t result;
-  __asm__ ("ld1r {%0.4h}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("mls %0.4h,%2.4h,%3.4h"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vld1_dup_s32 (const int32_t * a)
+vmls_s32 (int32x2_t a, int32x2_t b, int32x2_t c)
 {
   int32x2_t result;
-  __asm__ ("ld1r {%0.2s}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vld1_dup_s64 (const int64_t * a)
-{
-  int64x1_t result;
-  __asm__ ("ld1r {%0.1d}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("mls %0.2s,%2.2s,%3.2s"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vld1_dup_u8 (const uint8_t * a)
+vmls_u8 (uint8x8_t a, uint8x8_t b, uint8x8_t c)
 {
   uint8x8_t result;
-  __asm__ ("ld1r {%0.8b}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("mls %0.8b,%2.8b,%3.8b"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vld1_dup_u16 (const uint16_t * a)
+vmls_u16 (uint16x4_t a, uint16x4_t b, uint16x4_t c)
 {
   uint16x4_t result;
-  __asm__ ("ld1r {%0.4h}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("mls %0.4h,%2.4h,%3.4h"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vld1_dup_u32 (const uint32_t * a)
+vmls_u32 (uint32x2_t a, uint32x2_t b, uint32x2_t c)
 {
   uint32x2_t result;
-  __asm__ ("ld1r {%0.2s}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vld1_dup_u64 (const uint64_t * a)
-{
-  uint64x1_t result;
-  __asm__ ("ld1r {%0.1d}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("mls %0.2s,%2.2s,%3.2s"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
-#define vld1_lane_f32(a, b, c)                                          \
+#define vmlsl_high_lane_s16(a, b, c, d)                                 \
   __extension__                                                         \
     ({                                                                  \
-       float32x2_t b_ = (b);                                            \
-       const float32_t * a_ = (a);                                      \
-       float32x2_t result;                                              \
-       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
+       int16x8_t c_ = (c);                                              \
+       int16x8_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("smlsl2 %0.4s, %2.8h, %3.h[%4]"                         \
                 : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1_lane_f64(a, b, c)                                          \
+#define vmlsl_high_lane_s32(a, b, c, d)                                 \
   __extension__                                                         \
     ({                                                                  \
-       float64x1_t b_ = (b);                                            \
-       const float64_t * a_ = (a);                                      \
-       float64x1_t result;                                              \
-       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
+       int32x4_t c_ = (c);                                              \
+       int32x4_t b_ = (b);                                              \
+       int64x2_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("smlsl2 %0.2d, %2.4s, %3.s[%4]"                         \
                 : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1_lane_p8(a, b, c)                                           \
+#define vmlsl_high_lane_u16(a, b, c, d)                                 \
   __extension__                                                         \
     ({                                                                  \
-       poly8x8_t b_ = (b);                                              \
-       const poly8_t * a_ = (a);                                        \
-       poly8x8_t result;                                                \
-       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
+       uint16x8_t c_ = (c);                                             \
+       uint16x8_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("umlsl2 %0.4s, %2.8h, %3.h[%4]"                         \
                 : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1_lane_p16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x4_t b_ = (b);                                             \
-       const poly16_t * a_ = (a);                                       \
-       poly16x4_t result;                                               \
-       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
-                : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vld1_lane_s8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x8_t b_ = (b);                                               \
-       const int8_t * a_ = (a);                                         \
-       int8x8_t result;                                                 \
-       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
-                : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vld1_lane_s16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t b_ = (b);                                              \
-       const int16_t * a_ = (a);                                        \
-       int16x4_t result;                                                \
-       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
-                : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vld1_lane_s32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t b_ = (b);                                              \
-       const int32_t * a_ = (a);                                        \
-       int32x2_t result;                                                \
-       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
-                : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vld1_lane_s64(a, b, c)                                          \
+#define vmlsl_high_lane_u32(a, b, c, d)                                 \
   __extension__                                                         \
     ({                                                                  \
-       int64x1_t b_ = (b);                                              \
-       const int64_t * a_ = (a);                                        \
-       int64x1_t result;                                                \
-       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
+       uint32x4_t c_ = (c);                                             \
+       uint32x4_t b_ = (b);                                             \
+       uint64x2_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("umlsl2 %0.2d, %2.4s, %3.s[%4]"                         \
                 : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1_lane_u8(a, b, c)                                           \
+#define vmlsl_high_laneq_s16(a, b, c, d)                                \
   __extension__                                                         \
     ({                                                                  \
-       uint8x8_t b_ = (b);                                              \
-       const uint8_t * a_ = (a);                                        \
-       uint8x8_t result;                                                \
-       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
+       int16x8_t c_ = (c);                                              \
+       int16x8_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("smlsl2 %0.4s, %2.8h, %3.h[%4]"                         \
                 : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1_lane_u16(a, b, c)                                          \
+#define vmlsl_high_laneq_s32(a, b, c, d)                                \
   __extension__                                                         \
     ({                                                                  \
-       uint16x4_t b_ = (b);                                             \
-       const uint16_t * a_ = (a);                                       \
-       uint16x4_t result;                                               \
-       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
+       int32x4_t c_ = (c);                                              \
+       int32x4_t b_ = (b);                                              \
+       int64x2_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("smlsl2 %0.2d, %2.4s, %3.s[%4]"                         \
                 : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1_lane_u32(a, b, c)                                          \
+#define vmlsl_high_laneq_u16(a, b, c, d)                                \
   __extension__                                                         \
     ({                                                                  \
-       uint32x2_t b_ = (b);                                             \
-       const uint32_t * a_ = (a);                                       \
-       uint32x2_t result;                                               \
-       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
+       uint16x8_t c_ = (c);                                             \
+       uint16x8_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("umlsl2 %0.4s, %2.8h, %3.h[%4]"                         \
                 : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1_lane_u64(a, b, c)                                          \
+#define vmlsl_high_laneq_u32(a, b, c, d)                                \
   __extension__                                                         \
     ({                                                                  \
-       uint64x1_t b_ = (b);                                             \
-       const uint64_t * a_ = (a);                                       \
-       uint64x1_t result;                                               \
-       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
+       uint32x4_t c_ = (c);                                             \
+       uint32x4_t b_ = (b);                                             \
+       uint64x2_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("umlsl2 %0.2d, %2.4s, %3.s[%4]"                         \
                 : "=w"(result)                                          \
-                : "i" (c), "Utv"(*a_), "0"(b_)                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vld1q_dup_f32 (const float32_t * a)
-{
-  float32x4_t result;
-  __asm__ ("ld1r {%0.4s}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vld1q_dup_f64 (const float64_t * a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlsl_high_n_s16 (int32x4_t a, int16x8_t b, int16_t c)
 {
-  float64x2_t result;
-  __asm__ ("ld1r {%0.2d}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  int32x4_t result;
+  __asm__ ("smlsl2 %0.4s, %2.8h, %3.h[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vld1q_dup_p8 (const poly8_t * a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmlsl_high_n_s32 (int64x2_t a, int32x4_t b, int32_t c)
 {
-  poly8x16_t result;
-  __asm__ ("ld1r {%0.16b}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  int64x2_t result;
+  __asm__ ("smlsl2 %0.2d, %2.4s, %3.s[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vld1q_dup_p16 (const poly16_t * a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlsl_high_n_u16 (uint32x4_t a, uint16x8_t b, uint16_t c)
 {
-  poly16x8_t result;
-  __asm__ ("ld1r {%0.8h}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  uint32x4_t result;
+  __asm__ ("umlsl2 %0.4s, %2.8h, %3.h[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vld1q_dup_s8 (const int8_t * a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmlsl_high_n_u32 (uint64x2_t a, uint32x4_t b, uint32_t c)
 {
-  int8x16_t result;
-  __asm__ ("ld1r {%0.16b}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  uint64x2_t result;
+  __asm__ ("umlsl2 %0.2d, %2.4s, %3.s[0]"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vld1q_dup_s16 (const int16_t * a)
+vmlsl_high_s8 (int16x8_t a, int8x16_t b, int8x16_t c)
 {
   int16x8_t result;
-  __asm__ ("ld1r {%0.8h}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("smlsl2 %0.8h,%2.16b,%3.16b"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vld1q_dup_s32 (const int32_t * a)
+vmlsl_high_s16 (int32x4_t a, int16x8_t b, int16x8_t c)
 {
   int32x4_t result;
-  __asm__ ("ld1r {%0.4s}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("smlsl2 %0.4s,%2.8h,%3.8h"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vld1q_dup_s64 (const int64_t * a)
+vmlsl_high_s32 (int64x2_t a, int32x4_t b, int32x4_t c)
 {
   int64x2_t result;
-  __asm__ ("ld1r {%0.2d}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vld1q_dup_u8 (const uint8_t * a)
-{
-  uint8x16_t result;
-  __asm__ ("ld1r {%0.16b}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("smlsl2 %0.2d,%2.4s,%3.4s"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vld1q_dup_u16 (const uint16_t * a)
+vmlsl_high_u8 (uint16x8_t a, uint8x16_t b, uint8x16_t c)
 {
   uint16x8_t result;
-  __asm__ ("ld1r {%0.8h}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("umlsl2 %0.8h,%2.16b,%3.16b"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vld1q_dup_u32 (const uint32_t * a)
+vmlsl_high_u16 (uint32x4_t a, uint16x8_t b, uint16x8_t c)
 {
   uint32x4_t result;
-  __asm__ ("ld1r {%0.4s}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("umlsl2 %0.4s,%2.8h,%3.8h"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vld1q_dup_u64 (const uint64_t * a)
+vmlsl_high_u32 (uint64x2_t a, uint32x4_t b, uint32x4_t c)
 {
   uint64x2_t result;
-  __asm__ ("ld1r {%0.2d}, %1"
-	   : "=w"(result)
-	   : "Utv"(*a)
-	   : /* No clobbers */);
+  __asm__ ("umlsl2 %0.2d,%2.4s,%3.4s"
+           : "=w"(result)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
   return result;
 }
 
-#define vld1q_lane_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       const float32_t * a_ = (a);                                      \
-       float32x4_t result;                                              \
-       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
-                : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vld1q_lane_f64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       const float64_t * a_ = (a);                                      \
-       float64x2_t result;                                              \
-       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
-                : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vld1q_lane_p8(a, b, c)                                          \
+#define vmlsl_lane_s16(a, b, c, d)                                      \
   __extension__                                                         \
     ({                                                                  \
-       poly8x16_t b_ = (b);                                             \
-       const poly8_t * a_ = (a);                                        \
-       poly8x16_t result;                                               \
-       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
+       int16x4_t c_ = (c);                                              \
+       int16x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("smlsl %0.4s, %2.4h, %3.h[%4]"                          \
                 : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1q_lane_p16(a, b, c)                                         \
+#define vmlsl_lane_s32(a, b, c, d)                                      \
   __extension__                                                         \
     ({                                                                  \
-       poly16x8_t b_ = (b);                                             \
-       const poly16_t * a_ = (a);                                       \
-       poly16x8_t result;                                               \
-       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
+       int32x2_t c_ = (c);                                              \
+       int32x2_t b_ = (b);                                              \
+       int64x2_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("smlsl %0.2d, %2.2s, %3.s[%4]"                          \
                 : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1q_lane_s8(a, b, c)                                          \
+#define vmlsl_lane_u16(a, b, c, d)                                      \
   __extension__                                                         \
     ({                                                                  \
-       int8x16_t b_ = (b);                                              \
-       const int8_t * a_ = (a);                                         \
-       int8x16_t result;                                                \
-       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
+       uint16x4_t c_ = (c);                                             \
+       uint16x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("umlsl %0.4s, %2.4h, %3.h[%4]"                          \
                 : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1q_lane_s16(a, b, c)                                         \
+#define vmlsl_lane_u32(a, b, c, d)                                      \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       const int16_t * a_ = (a);                                        \
-       int16x8_t result;                                                \
-       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
+       uint32x2_t c_ = (c);                                             \
+       uint32x2_t b_ = (b);                                             \
+       uint64x2_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("umlsl %0.2d, %2.2s, %3.s[%4]"                          \
                 : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1q_lane_s32(a, b, c)                                         \
+#define vmlsl_laneq_s16(a, b, c, d)                                     \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       const int32_t * a_ = (a);                                        \
+       int16x8_t c_ = (c);                                              \
+       int16x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
        int32x4_t result;                                                \
-       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
+       __asm__ ("smlsl %0.4s, %2.4h, %3.h[%4]"                          \
                 : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1q_lane_s64(a, b, c)                                         \
+#define vmlsl_laneq_s32(a, b, c, d)                                     \
   __extension__                                                         \
     ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       const int64_t * a_ = (a);                                        \
+       int32x4_t c_ = (c);                                              \
+       int32x2_t b_ = (b);                                              \
+       int64x2_t a_ = (a);                                              \
        int64x2_t result;                                                \
-       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
-                : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vld1q_lane_u8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x16_t b_ = (b);                                             \
-       const uint8_t * a_ = (a);                                        \
-       uint8x16_t result;                                               \
-       __asm__ ("ld1 {%0.b}[%1], %2"                                    \
-                : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vld1q_lane_u16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       const uint16_t * a_ = (a);                                       \
-       uint16x8_t result;                                               \
-       __asm__ ("ld1 {%0.h}[%1], %2"                                    \
+       __asm__ ("smlsl %0.2d, %2.2s, %3.s[%4]"                          \
                 : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1q_lane_u32(a, b, c)                                         \
+#define vmlsl_laneq_u16(a, b, c, d)                                     \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       const uint32_t * a_ = (a);                                       \
+       uint16x8_t c_ = (c);                                             \
+       uint16x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
        uint32x4_t result;                                               \
-       __asm__ ("ld1 {%0.s}[%1], %2"                                    \
+       __asm__ ("umlsl %0.4s, %2.4h, %3.h[%4]"                          \
                 : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vld1q_lane_u64(a, b, c)                                         \
+#define vmlsl_laneq_u32(a, b, c, d)                                     \
   __extension__                                                         \
     ({                                                                  \
-       uint64x2_t b_ = (b);                                             \
-       const uint64_t * a_ = (a);                                       \
+       uint32x4_t c_ = (c);                                             \
+       uint32x2_t b_ = (b);                                             \
+       uint64x2_t a_ = (a);                                             \
        uint64x2_t result;                                               \
-       __asm__ ("ld1 {%0.d}[%1], %2"                                    \
+       __asm__ ("umlsl %0.2d, %2.2s, %3.s[%4]"                          \
                 : "=w"(result)                                          \
-                : "i"(c), "Utv"(*a_), "0"(b_)                           \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vmaxnm_f32 (float32x2_t a, float32x2_t b)
-{
-  float32x2_t result;
-  __asm__ ("fmaxnm %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vmaxnmq_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlsl_n_s16 (int32x4_t a, int16x4_t b, int16_t c)
 {
-  float32x4_t result;
-  __asm__ ("fmaxnm %0.4s,%1.4s,%2.4s"
+  int32x4_t result;
+  __asm__ ("smlsl %0.4s, %2.4h, %3.h[0]"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vmaxnmq_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmlsl_n_s32 (int64x2_t a, int32x2_t b, int32_t c)
 {
-  float64x2_t result;
-  __asm__ ("fmaxnm %0.2d,%1.2d,%2.2d"
+  int64x2_t result;
+  __asm__ ("smlsl %0.2d, %2.2s, %3.s[0]"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vmaxnmvq_f32 (float32x4_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlsl_n_u16 (uint32x4_t a, uint16x4_t b, uint16_t c)
 {
-  float32_t result;
-  __asm__ ("fmaxnmv %s0,%1.4s"
+  uint32x4_t result;
+  __asm__ ("umlsl %0.4s, %2.4h, %3.h[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8_t __attribute__ ((__always_inline__))
-vmaxv_s8 (int8x8_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmlsl_n_u32 (uint64x2_t a, uint32x2_t b, uint32_t c)
 {
-  int8_t result;
-  __asm__ ("smaxv %b0,%1.8b"
+  uint64x2_t result;
+  __asm__ ("umlsl %0.2d, %2.2s, %3.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16_t __attribute__ ((__always_inline__))
-vmaxv_s16 (int16x4_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmlsl_s8 (int16x8_t a, int8x8_t b, int8x8_t c)
 {
-  int16_t result;
-  __asm__ ("smaxv %h0,%1.4h"
+  int16x8_t result;
+  __asm__ ("smlsl %0.8h, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
-vmaxv_u8 (uint8x8_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlsl_s16 (int32x4_t a, int16x4_t b, int16x4_t c)
 {
-  uint8_t result;
-  __asm__ ("umaxv %b0,%1.8b"
+  int32x4_t result;
+  __asm__ ("smlsl %0.4s, %2.4h, %3.4h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
-vmaxv_u16 (uint16x4_t a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmlsl_s32 (int64x2_t a, int32x2_t b, int32x2_t c)
 {
-  uint16_t result;
-  __asm__ ("umaxv %h0,%1.4h"
+  int64x2_t result;
+  __asm__ ("smlsl %0.2d, %2.2s, %3.2s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vmaxvq_f32 (float32x4_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmlsl_u8 (uint16x8_t a, uint8x8_t b, uint8x8_t c)
 {
-  float32_t result;
-  __asm__ ("fmaxv %s0,%1.4s"
+  uint16x8_t result;
+  __asm__ ("umlsl %0.8h, %2.8b, %3.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8_t __attribute__ ((__always_inline__))
-vmaxvq_s8 (int8x16_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlsl_u16 (uint32x4_t a, uint16x4_t b, uint16x4_t c)
 {
-  int8_t result;
-  __asm__ ("smaxv %b0,%1.16b"
+  uint32x4_t result;
+  __asm__ ("umlsl %0.4s, %2.4h, %3.4h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16_t __attribute__ ((__always_inline__))
-vmaxvq_s16 (int16x8_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmlsl_u32 (uint64x2_t a, uint32x2_t b, uint32x2_t c)
 {
-  int16_t result;
-  __asm__ ("smaxv %h0,%1.8h"
+  uint64x2_t result;
+  __asm__ ("umlsl %0.2d, %2.2s, %3.2s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32_t __attribute__ ((__always_inline__))
-vmaxvq_s32 (int32x4_t a)
-{
-  int32_t result;
-  __asm__ ("smaxv %s0,%1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vmlsq_lane_f32(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t c_ = (c);                                            \
+       float32x4_t b_ = (b);                                            \
+       float32x4_t a_ = (a);                                            \
+       float32x4_t result;                                              \
+       float32x4_t t1;                                                  \
+       __asm__ ("fmul %1.4s, %3.4s, %4.s[%5]; fsub %0.4s, %0.4s, %1.4s" \
+                : "=w"(result), "=w"(t1)                                \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
-vmaxvq_u8 (uint8x16_t a)
+#define vmlsq_lane_s16(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t c_ = (c);                                              \
+       int16x8_t b_ = (b);                                              \
+       int16x8_t a_ = (a);                                              \
+       int16x8_t result;                                                \
+       __asm__ ("mls %0.8h,%2.8h,%3.h[%4]"                              \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlsq_lane_s32(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t c_ = (c);                                              \
+       int32x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("mls %0.4s,%2.4s,%3.s[%4]"                              \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlsq_lane_u16(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t c_ = (c);                                             \
+       uint16x8_t b_ = (b);                                             \
+       uint16x8_t a_ = (a);                                             \
+       uint16x8_t result;                                               \
+       __asm__ ("mls %0.8h,%2.8h,%3.h[%4]"                              \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlsq_lane_u32(a, b, c, d)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t c_ = (c);                                             \
+       uint32x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("mls %0.4s,%2.4s,%3.s[%4]"                              \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmlsq_laneq_f32(__a, __b, __c, __d)				\
+  __extension__								\
+    ({									\
+       float32x4_t __c_ = (__c);					\
+       float32x4_t __b_ = (__b);					\
+       float32x4_t __a_ = (__a);					\
+       float32x4_t __result;						\
+       float32x4_t __t1;						\
+       __asm__ ("fmul %1.4s, %3.4s, %4.s[%5]; fsub %0.4s, %0.4s, %1.4s"	\
+                : "=w"(__result), "=w"(__t1)				\
+                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
+                : /* No clobbers */);					\
+       __result;							\
+     })
+
+#define vmlsq_laneq_s16(__a, __b, __c, __d)				\
+  __extension__								\
+    ({									\
+       int16x8_t __c_ = (__c);						\
+       int16x8_t __b_ = (__b);						\
+       int16x8_t __a_ = (__a);						\
+       int16x8_t __result;						\
+       __asm__ ("mls %0.8h, %2.8h, %3.h[%4]"				\
+                : "=w"(__result)					\
+                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
+                : /* No clobbers */);					\
+       __result;							\
+     })
+
+#define vmlsq_laneq_s32(__a, __b, __c, __d)				\
+  __extension__								\
+    ({									\
+       int32x4_t __c_ = (__c);						\
+       int32x4_t __b_ = (__b);						\
+       int32x4_t __a_ = (__a);						\
+       int32x4_t __result;						\
+       __asm__ ("mls %0.4s, %2.4s, %3.s[%4]"				\
+                : "=w"(__result)					\
+                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
+                : /* No clobbers */);					\
+       __result;							\
+     })
+
+#define vmlsq_laneq_u16(__a, __b, __c, __d)				\
+  __extension__								\
+    ({									\
+       uint16x8_t __c_ = (__c);						\
+       uint16x8_t __b_ = (__b);						\
+       uint16x8_t __a_ = (__a);						\
+       uint16x8_t __result;						\
+       __asm__ ("mls %0.8h, %2.8h, %3.h[%4]"				\
+                : "=w"(__result)					\
+                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
+                : /* No clobbers */);					\
+       __result;							\
+     })
+
+#define vmlsq_laneq_u32(__a, __b, __c, __d)				\
+  __extension__								\
+    ({									\
+       uint32x4_t __c_ = (__c);						\
+       uint32x4_t __b_ = (__b);						\
+       uint32x4_t __a_ = (__a);						\
+       uint32x4_t __result;						\
+       __asm__ ("mls %0.4s, %2.4s, %3.s[%4]"				\
+                : "=w"(__result)					\
+                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
+                : /* No clobbers */);					\
+       __result;							\
+     })
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vmlsq_n_f32 (float32x4_t a, float32x4_t b, float32_t c)
 {
-  uint8_t result;
-  __asm__ ("umaxv %b0,%1.16b"
-           : "=w"(result)
-           : "w"(a)
+  float32x4_t result;
+  float32x4_t t1;
+  __asm__ ("fmul %1.4s, %3.4s, %4.s[0]; fsub %0.4s, %0.4s, %1.4s"
+           : "=w"(result), "=w"(t1)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
-vmaxvq_u16 (uint16x8_t a)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vmlsq_n_f64 (float64x2_t a, float64x2_t b, float64_t c)
 {
-  uint16_t result;
-  __asm__ ("umaxv %h0,%1.8h"
+  float64x2_t result;
+  float64x2_t t1;
+  __asm__ ("fmul %1.2d, %3.2d, %4.d[0]; fsub %0.2d, %0.2d, %1.2d"
+           : "=w"(result), "=w"(t1)
+           : "0"(a), "w"(b), "w"(c)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmlsq_n_s16 (int16x8_t a, int16x8_t b, int16_t c)
+{
+  int16x8_t result;
+  __asm__ ("mls %0.8h, %2.8h, %3.h[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
-vmaxvq_u32 (uint32x4_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlsq_n_s32 (int32x4_t a, int32x4_t b, int32_t c)
 {
-  uint32_t result;
-  __asm__ ("umaxv %s0,%1.4s"
+  int32x4_t result;
+  __asm__ ("mls %0.4s, %2.4s, %3.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vminnmvq_f32 (float32x4_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmlsq_n_u16 (uint16x8_t a, uint16x8_t b, uint16_t c)
 {
-  float32_t result;
-  __asm__ ("fminnmv %s0,%1.4s"
+  uint16x8_t result;
+  __asm__ ("mls %0.8h, %2.8h, %3.h[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8_t __attribute__ ((__always_inline__))
-vminv_s8 (int8x8_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlsq_n_u32 (uint32x4_t a, uint32x4_t b, uint32_t c)
 {
-  int8_t result;
-  __asm__ ("sminv %b0,%1.8b"
+  uint32x4_t result;
+  __asm__ ("mls %0.4s, %2.4s, %3.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16_t __attribute__ ((__always_inline__))
-vminv_s16 (int16x4_t a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vmlsq_s8 (int8x16_t a, int8x16_t b, int8x16_t c)
 {
-  int16_t result;
-  __asm__ ("sminv %h0,%1.4h"
+  int8x16_t result;
+  __asm__ ("mls %0.16b,%2.16b,%3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
-vminv_u8 (uint8x8_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmlsq_s16 (int16x8_t a, int16x8_t b, int16x8_t c)
 {
-  uint8_t result;
-  __asm__ ("uminv %b0,%1.8b"
+  int16x8_t result;
+  __asm__ ("mls %0.8h,%2.8h,%3.8h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
-vminv_u16 (uint16x4_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmlsq_s32 (int32x4_t a, int32x4_t b, int32x4_t c)
 {
-  uint16_t result;
-  __asm__ ("uminv %h0,%1.4h"
+  int32x4_t result;
+  __asm__ ("mls %0.4s,%2.4s,%3.4s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vminvq_f32 (float32x4_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vmlsq_u8 (uint8x16_t a, uint8x16_t b, uint8x16_t c)
 {
-  float32_t result;
-  __asm__ ("fminv %s0,%1.4s"
+  uint8x16_t result;
+  __asm__ ("mls %0.16b,%2.16b,%3.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8_t __attribute__ ((__always_inline__))
-vminvq_s8 (int8x16_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmlsq_u16 (uint16x8_t a, uint16x8_t b, uint16x8_t c)
 {
-  int8_t result;
-  __asm__ ("sminv %b0,%1.16b"
+  uint16x8_t result;
+  __asm__ ("mls %0.8h,%2.8h,%3.8h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16_t __attribute__ ((__always_inline__))
-vminvq_s16 (int16x8_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmlsq_u32 (uint32x4_t a, uint32x4_t b, uint32x4_t c)
 {
-  int16_t result;
-  __asm__ ("sminv %h0,%1.8h"
+  uint32x4_t result;
+  __asm__ ("mls %0.4s,%2.4s,%3.4s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32_t __attribute__ ((__always_inline__))
-vminvq_s32 (int32x4_t a)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vmov_n_f32 (float32_t a)
 {
-  int32_t result;
-  __asm__ ("sminv %s0,%1.4s"
+  float32x2_t result;
+  __asm__ ("dup %0.2s, %w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
-vminvq_u8 (uint8x16_t a)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vmov_n_p8 (uint32_t a)
 {
-  uint8_t result;
-  __asm__ ("uminv %b0,%1.16b"
+  poly8x8_t result;
+  __asm__ ("dup %0.8b,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
-vminvq_u16 (uint16x8_t a)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vmov_n_p16 (uint32_t a)
 {
-  uint16_t result;
-  __asm__ ("uminv %h0,%1.8h"
+  poly16x4_t result;
+  __asm__ ("dup %0.4h,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
-vminvq_u32 (uint32x4_t a)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vmov_n_s8 (int32_t a)
 {
-  uint32_t result;
-  __asm__ ("uminv %s0,%1.4s"
+  int8x8_t result;
+  __asm__ ("dup %0.8b,%w1"
            : "=w"(result)
-           : "w"(a)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-#define vmla_lane_f32(a, b, c, d)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t c_ = (c);                                            \
-       float32x2_t b_ = (b);                                            \
-       float32x2_t a_ = (a);                                            \
-       float32x2_t result;                                              \
-       float32x2_t t1;                                                  \
-       __asm__ ("fmul %1.2s, %3.2s, %4.s[%5]; fadd %0.2s, %0.2s, %1.2s" \
-                : "=w"(result), "=w"(t1)                                \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmla_lane_s16(a, b, c, d)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t c_ = (c);                                              \
-       int16x4_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int16x4_t result;                                                \
-       __asm__ ("mla %0.4h, %2.4h, %3.h[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmla_lane_s32(a, b, c, d)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t c_ = (c);                                              \
-       int32x2_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int32x2_t result;                                                \
-       __asm__ ("mla %0.2s, %2.2s, %3.s[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmla_lane_u16(a, b, c, d)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t c_ = (c);                                             \
-       uint16x4_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint16x4_t result;                                               \
-       __asm__ ("mla %0.4h, %2.4h, %3.h[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmla_lane_u32(a, b, c, d)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t c_ = (c);                                             \
-       uint32x2_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint32x2_t result;                                               \
-       __asm__ ("mla %0.2s, %2.2s, %3.s[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmla_laneq_s16(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t c_ = (c);                                              \
-       int16x4_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int16x4_t result;                                                \
-       __asm__ ("mla %0.4h, %2.4h, %3.h[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmla_laneq_s32(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t c_ = (c);                                              \
-       int32x2_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int32x2_t result;                                                \
-       __asm__ ("mla %0.2s, %2.2s, %3.s[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmla_laneq_u16(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
-       uint16x4_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint16x4_t result;                                               \
-       __asm__ ("mla %0.4h, %2.4h, %3.h[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmla_laneq_u32(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
-       uint32x2_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint32x2_t result;                                               \
-       __asm__ ("mla %0.2s, %2.2s, %3.s[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vmov_n_s16 (int32_t a)
+{
+  int16x4_t result;
+  __asm__ ("dup %0.4h,%w1"
+           : "=w"(result)
+           : "r"(a)
+           : /* No clobbers */);
+  return result;
+}
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vmla_n_f32 (float32x2_t a, float32x2_t b, float32_t c)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vmov_n_s32 (int32_t a)
 {
-  float32x2_t result;
-  float32x2_t t1;
-  __asm__ ("fmul %1.2s, %3.2s, %4.s[0]; fadd %0.2s, %0.2s, %1.2s"
-           : "=w"(result), "=w"(t1)
-           : "0"(a), "w"(b), "w"(c)
+  int32x2_t result;
+  __asm__ ("dup %0.2s,%w1"
+           : "=w"(result)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vmla_n_s16 (int16x4_t a, int16x4_t b, int16_t c)
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vmov_n_s64 (int64_t a)
 {
-  int16x4_t result;
-  __asm__ ("mla %0.4h,%2.4h,%3.h[0]"
+  int64x1_t result;
+  __asm__ ("ins %0.d[0],%x1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vmla_n_s32 (int32x2_t a, int32x2_t b, int32_t c)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vmov_n_u8 (uint32_t a)
 {
-  int32x2_t result;
-  __asm__ ("mla %0.2s,%2.2s,%3.s[0]"
+  uint8x8_t result;
+  __asm__ ("dup %0.8b,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vmla_n_u16 (uint16x4_t a, uint16x4_t b, uint16_t c)
+vmov_n_u16 (uint32_t a)
 {
   uint16x4_t result;
-  __asm__ ("mla %0.4h,%2.4h,%3.h[0]"
+  __asm__ ("dup %0.4h,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vmla_n_u32 (uint32x2_t a, uint32x2_t b, uint32_t c)
+vmov_n_u32 (uint32_t a)
 {
   uint32x2_t result;
-  __asm__ ("mla %0.2s,%2.2s,%3.s[0]"
+  __asm__ ("dup %0.2s,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vmla_s8 (int8x8_t a, int8x8_t b, int8x8_t c)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vmov_n_u64 (uint64_t a)
 {
-  int8x8_t result;
-  __asm__ ("mla %0.8b, %2.8b, %3.8b"
+  uint64x1_t result;
+  __asm__ ("ins %0.d[0],%x1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vmla_s16 (int16x4_t a, int16x4_t b, int16x4_t c)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmovl_high_s8 (int8x16_t a)
 {
-  int16x4_t result;
-  __asm__ ("mla %0.4h, %2.4h, %3.4h"
+  int16x8_t result;
+  __asm__ ("sshll2 %0.8h,%1.16b,#0"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vmla_s32 (int32x2_t a, int32x2_t b, int32x2_t c)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmovl_high_s16 (int16x8_t a)
 {
-  int32x2_t result;
-  __asm__ ("mla %0.2s, %2.2s, %3.2s"
+  int32x4_t result;
+  __asm__ ("sshll2 %0.4s,%1.8h,#0"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vmla_u8 (uint8x8_t a, uint8x8_t b, uint8x8_t c)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmovl_high_s32 (int32x4_t a)
 {
-  uint8x8_t result;
-  __asm__ ("mla %0.8b, %2.8b, %3.8b"
+  int64x2_t result;
+  __asm__ ("sshll2 %0.2d,%1.4s,#0"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vmla_u16 (uint16x4_t a, uint16x4_t b, uint16x4_t c)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmovl_high_u8 (uint8x16_t a)
 {
-  uint16x4_t result;
-  __asm__ ("mla %0.4h, %2.4h, %3.4h"
+  uint16x8_t result;
+  __asm__ ("ushll2 %0.8h,%1.16b,#0"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vmla_u32 (uint32x2_t a, uint32x2_t b, uint32x2_t c)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmovl_high_u16 (uint16x8_t a)
 {
-  uint32x2_t result;
-  __asm__ ("mla %0.2s, %2.2s, %3.2s"
+  uint32x4_t result;
+  __asm__ ("ushll2 %0.4s,%1.8h,#0"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-#define vmlal_high_lane_s16(a, b, c, d)                                 \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t c_ = (c);                                              \
-       int16x8_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smlal2 %0.4s, %2.8h, %3.h[%4]"                         \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_high_lane_s32(a, b, c, d)                                 \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t c_ = (c);                                              \
-       int32x4_t b_ = (b);                                              \
-       int64x2_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smlal2 %0.2d, %2.4s, %3.s[%4]"                         \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_high_lane_u16(a, b, c, d)                                 \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
-       uint16x8_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("umlal2 %0.4s, %2.8h, %3.h[%4]"                         \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_high_lane_u32(a, b, c, d)                                 \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
-       uint32x4_t b_ = (b);                                             \
-       uint64x2_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umlal2 %0.2d, %2.4s, %3.s[%4]"                         \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_high_laneq_s16(a, b, c, d)                                \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t c_ = (c);                                              \
-       int16x8_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smlal2 %0.4s, %2.8h, %3.h[%4]"                         \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_high_laneq_s32(a, b, c, d)                                \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t c_ = (c);                                              \
-       int32x4_t b_ = (b);                                              \
-       int64x2_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smlal2 %0.2d, %2.4s, %3.s[%4]"                         \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_high_laneq_u16(a, b, c, d)                                \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
-       uint16x8_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("umlal2 %0.4s, %2.8h, %3.h[%4]"                         \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmovl_high_u32 (uint32x4_t a)
+{
+  uint64x2_t result;
+  __asm__ ("ushll2 %0.2d,%1.4s,#0"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmlal_high_laneq_u32(a, b, c, d)                                \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
-       uint32x4_t b_ = (b);                                             \
-       uint64x2_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umlal2 %0.2d, %2.4s, %3.s[%4]"                         \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmovl_s8 (int8x8_t a)
+{
+  int16x8_t result;
+  __asm__ ("sshll %0.8h,%1.8b,#0"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlal_high_n_s16 (int32x4_t a, int16x8_t b, int16_t c)
+vmovl_s16 (int16x4_t a)
 {
   int32x4_t result;
-  __asm__ ("smlal2 %0.4s,%2.8h,%3.h[0]"
+  __asm__ ("sshll %0.4s,%1.4h,#0"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmlal_high_n_s32 (int64x2_t a, int32x4_t b, int32_t c)
+vmovl_s32 (int32x2_t a)
 {
   int64x2_t result;
-  __asm__ ("smlal2 %0.2d,%2.4s,%3.s[0]"
+  __asm__ ("sshll %0.2d,%1.2s,#0"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmovl_u8 (uint8x8_t a)
+{
+  uint16x8_t result;
+  __asm__ ("ushll %0.8h,%1.8b,#0"
+           : "=w"(result)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlal_high_n_u16 (uint32x4_t a, uint16x8_t b, uint16_t c)
+vmovl_u16 (uint16x4_t a)
 {
   uint32x4_t result;
-  __asm__ ("umlal2 %0.4s,%2.8h,%3.h[0]"
+  __asm__ ("ushll %0.4s,%1.4h,#0"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmlal_high_n_u32 (uint64x2_t a, uint32x4_t b, uint32_t c)
+vmovl_u32 (uint32x2_t a)
 {
   uint64x2_t result;
-  __asm__ ("umlal2 %0.2d,%2.4s,%3.s[0]"
+  __asm__ ("ushll %0.2d,%1.2s,#0"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vmovn_high_s16 (int8x8_t a, int16x8_t b)
+{
+  int8x16_t result = vcombine_s8 (a, vcreate_s8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("xtn2 %0.16b,%1.8h"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmlal_high_s8 (int16x8_t a, int8x16_t b, int8x16_t c)
+vmovn_high_s32 (int16x4_t a, int32x4_t b)
 {
-  int16x8_t result;
-  __asm__ ("smlal2 %0.8h,%2.16b,%3.16b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+  int16x8_t result = vcombine_s16 (a, vcreate_s16 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("xtn2 %0.8h,%1.4s"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlal_high_s16 (int32x4_t a, int16x8_t b, int16x8_t c)
+vmovn_high_s64 (int32x2_t a, int64x2_t b)
 {
-  int32x4_t result;
-  __asm__ ("smlal2 %0.4s,%2.8h,%3.8h"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+  int32x4_t result = vcombine_s32 (a, vcreate_s32 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("xtn2 %0.4s,%1.2d"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmlal_high_s32 (int64x2_t a, int32x4_t b, int32x4_t c)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vmovn_high_u16 (uint8x8_t a, uint16x8_t b)
 {
-  int64x2_t result;
-  __asm__ ("smlal2 %0.2d,%2.4s,%3.4s"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("xtn2 %0.16b,%1.8h"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmlal_high_u8 (uint16x8_t a, uint8x16_t b, uint8x16_t c)
+vmovn_high_u32 (uint16x4_t a, uint32x4_t b)
 {
-  uint16x8_t result;
-  __asm__ ("umlal2 %0.8h,%2.16b,%3.16b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("xtn2 %0.8h,%1.4s"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlal_high_u16 (uint32x4_t a, uint16x8_t b, uint16x8_t c)
+vmovn_high_u64 (uint32x2_t a, uint64x2_t b)
 {
-  uint32x4_t result;
-  __asm__ ("umlal2 %0.4s,%2.8h,%3.8h"
+  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("xtn2 %0.4s,%1.2d"
+           : "+w"(result)
+           : "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vmovn_s16 (int16x8_t a)
+{
+  int8x8_t result;
+  __asm__ ("xtn %0.8b,%1.8h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmlal_high_u32 (uint64x2_t a, uint32x4_t b, uint32x4_t c)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vmovn_s32 (int32x4_t a)
 {
-  uint64x2_t result;
-  __asm__ ("umlal2 %0.2d,%2.4s,%3.4s"
+  int16x4_t result;
+  __asm__ ("xtn %0.4h,%1.4s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-#define vmlal_lane_s16(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t c_ = (c);                                              \
-       int16x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smlal %0.4s,%2.4h,%3.h[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vmovn_s64 (int64x2_t a)
+{
+  int32x2_t result;
+  __asm__ ("xtn %0.2s,%1.2d"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmlal_lane_s32(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t c_ = (c);                                              \
-       int32x2_t b_ = (b);                                              \
-       int64x2_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smlal %0.2d,%2.2s,%3.s[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_lane_u16(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t c_ = (c);                                             \
-       uint16x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("umlal %0.4s,%2.4h,%3.h[%4]"                            \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_lane_u32(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t c_ = (c);                                             \
-       uint32x2_t b_ = (b);                                             \
-       uint64x2_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umlal %0.2d, %2.2s, %3.s[%4]"                          \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_laneq_s16(a, b, c, d)                                     \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t c_ = (c);                                              \
-       int16x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smlal %0.4s, %2.4h, %3.h[%4]"                          \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlal_laneq_s32(a, b, c, d)                                     \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t c_ = (c);                                              \
-       int32x2_t b_ = (b);                                              \
-       int64x2_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smlal %0.2d, %2.2s, %3.s[%4]"                          \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vmovn_u16 (uint16x8_t a)
+{
+  uint8x8_t result;
+  __asm__ ("xtn %0.8b,%1.8h"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmlal_laneq_u16(a, b, c, d)                                     \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
-       uint16x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("umlal %0.4s, %2.4h, %3.h[%4]"                          \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vmovn_u32 (uint32x4_t a)
+{
+  uint16x4_t result;
+  __asm__ ("xtn %0.4h,%1.4s"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmlal_laneq_u32(a, b, c, d)                                     \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
-       uint32x2_t b_ = (b);                                             \
-       uint64x2_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umlal %0.2d, %2.2s, %3.s[%4]"                          \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vmovn_u64 (uint64x2_t a)
+{
+  uint32x2_t result;
+  __asm__ ("xtn %0.2s,%1.2d"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlal_n_s16 (int32x4_t a, int16x4_t b, int16_t c)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vmovq_n_f32 (float32_t a)
 {
-  int32x4_t result;
-  __asm__ ("smlal %0.4s,%2.4h,%3.h[0]"
+  float32x4_t result;
+  __asm__ ("dup %0.4s, %w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmlal_n_s32 (int64x2_t a, int32x2_t b, int32_t c)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vmovq_n_f64 (float64_t a)
 {
-  int64x2_t result;
-  __asm__ ("smlal %0.2d,%2.2s,%3.s[0]"
+  return (float64x2_t) {a, a};
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vmovq_n_p8 (uint32_t a)
+{
+  poly8x16_t result;
+  __asm__ ("dup %0.16b,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlal_n_u16 (uint32x4_t a, uint16x4_t b, uint16_t c)
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vmovq_n_p16 (uint32_t a)
 {
-  uint32x4_t result;
-  __asm__ ("umlal %0.4s,%2.4h,%3.h[0]"
+  poly16x8_t result;
+  __asm__ ("dup %0.8h,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmlal_n_u32 (uint64x2_t a, uint32x2_t b, uint32_t c)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vmovq_n_s8 (int32_t a)
 {
-  uint64x2_t result;
-  __asm__ ("umlal %0.2d,%2.2s,%3.s[0]"
+  int8x16_t result;
+  __asm__ ("dup %0.16b,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmlal_s8 (int16x8_t a, int8x8_t b, int8x8_t c)
+vmovq_n_s16 (int32_t a)
 {
   int16x8_t result;
-  __asm__ ("smlal %0.8h,%2.8b,%3.8b"
+  __asm__ ("dup %0.8h,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlal_s16 (int32x4_t a, int16x4_t b, int16x4_t c)
+vmovq_n_s32 (int32_t a)
 {
   int32x4_t result;
-  __asm__ ("smlal %0.4s,%2.4h,%3.4h"
+  __asm__ ("dup %0.4s,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmlal_s32 (int64x2_t a, int32x2_t b, int32x2_t c)
+vmovq_n_s64 (int64_t a)
 {
   int64x2_t result;
-  __asm__ ("smlal %0.2d,%2.2s,%3.2s"
+  __asm__ ("dup %0.2d,%x1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vmovq_n_u8 (uint32_t a)
+{
+  uint8x16_t result;
+  __asm__ ("dup %0.16b,%w1"
+           : "=w"(result)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmlal_u8 (uint16x8_t a, uint8x8_t b, uint8x8_t c)
+vmovq_n_u16 (uint32_t a)
 {
   uint16x8_t result;
-  __asm__ ("umlal %0.8h,%2.8b,%3.8b"
+  __asm__ ("dup %0.8h,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlal_u16 (uint32x4_t a, uint16x4_t b, uint16x4_t c)
+vmovq_n_u32 (uint32_t a)
 {
   uint32x4_t result;
-  __asm__ ("umlal %0.4s,%2.4h,%3.4h"
+  __asm__ ("dup %0.4s,%w1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmlal_u32 (uint64x2_t a, uint32x2_t b, uint32x2_t c)
+vmovq_n_u64 (uint64_t a)
 {
   uint64x2_t result;
-  __asm__ ("umlal %0.2d,%2.2s,%3.2s"
+  __asm__ ("dup %0.2d,%x1"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "r"(a)
            : /* No clobbers */);
   return result;
 }
 
-#define vmlaq_lane_f32(a, b, c, d)                                      \
+#define vmul_lane_f32(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       float32x4_t c_ = (c);                                            \
-       float32x4_t b_ = (b);                                            \
-       float32x4_t a_ = (a);                                            \
-       float32x4_t result;                                              \
-       float32x4_t t1;                                                  \
-       __asm__ ("fmul %1.4s, %3.4s, %4.s[%5]; fadd %0.4s, %0.4s, %1.4s" \
-                : "=w"(result), "=w"(t1)                                \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+       float32x2_t b_ = (b);                                            \
+       float32x2_t a_ = (a);                                            \
+       float32x2_t result;                                              \
+       __asm__ ("fmul %0.2s,%1.2s,%2.s[%3]"                             \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlaq_lane_s16(a, b, c, d)                                      \
+#define vmul_lane_s16(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t c_ = (c);                                              \
-       int16x8_t b_ = (b);                                              \
-       int16x8_t a_ = (a);                                              \
-       int16x8_t result;                                                \
-       __asm__ ("mla %0.8h, %2.8h, %3.h[%4]"                            \
+       int16x4_t b_ = (b);                                              \
+       int16x4_t a_ = (a);                                              \
+       int16x4_t result;                                                \
+       __asm__ ("mul %0.4h,%1.4h,%2.h[%3]"                              \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlaq_lane_s32(a, b, c, d)                                      \
+#define vmul_lane_s32(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t c_ = (c);                                              \
-       int32x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("mla %0.4s, %2.4s, %3.s[%4]"                            \
+       int32x2_t b_ = (b);                                              \
+       int32x2_t a_ = (a);                                              \
+       int32x2_t result;                                                \
+       __asm__ ("mul %0.2s,%1.2s,%2.s[%3]"                              \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlaq_lane_u16(a, b, c, d)                                      \
+#define vmul_lane_u16(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
-       uint16x8_t b_ = (b);                                             \
-       uint16x8_t a_ = (a);                                             \
-       uint16x8_t result;                                               \
-       __asm__ ("mla %0.8h, %2.8h, %3.h[%4]"                            \
+       uint16x4_t b_ = (b);                                             \
+       uint16x4_t a_ = (a);                                             \
+       uint16x4_t result;                                               \
+       __asm__ ("mul %0.4h,%1.4h,%2.h[%3]"                              \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlaq_lane_u32(a, b, c, d)                                      \
+#define vmul_lane_u32(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
-       uint32x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("mla %0.4s, %2.4s, %3.s[%4]"                            \
+       uint32x2_t b_ = (b);                                             \
+       uint32x2_t a_ = (a);                                             \
+       uint32x2_t result;                                               \
+       __asm__ ("mul %0.2s, %1.2s, %2.s[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlaq_laneq_s16(a, b, c, d)                                     \
+#define vmul_laneq_f32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t b_ = (b);                                            \
+       float32x2_t a_ = (a);                                            \
+       float32x2_t result;                                              \
+       __asm__ ("fmul %0.2s, %1.2s, %2.s[%3]"                           \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmul_laneq_s16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t c_ = (c);                                              \
        int16x8_t b_ = (b);                                              \
-       int16x8_t a_ = (a);                                              \
-       int16x8_t result;                                                \
-       __asm__ ("mla %0.8h, %2.8h, %3.h[%4]"                            \
+       int16x4_t a_ = (a);                                              \
+       int16x4_t result;                                                \
+       __asm__ ("mul %0.4h, %1.4h, %2.h[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlaq_laneq_s32(a, b, c, d)                                     \
+#define vmul_laneq_s32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t c_ = (c);                                              \
        int32x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("mla %0.4s, %2.4s, %3.s[%4]"                            \
+       int32x2_t a_ = (a);                                              \
+       int32x2_t result;                                                \
+       __asm__ ("mul %0.2s, %1.2s, %2.s[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlaq_laneq_u16(a, b, c, d)                                     \
+#define vmul_laneq_u16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
        uint16x8_t b_ = (b);                                             \
-       uint16x8_t a_ = (a);                                             \
-       uint16x8_t result;                                               \
-       __asm__ ("mla %0.8h, %2.8h, %3.h[%4]"                            \
+       uint16x4_t a_ = (a);                                             \
+       uint16x4_t result;                                               \
+       __asm__ ("mul %0.4h, %1.4h, %2.h[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlaq_laneq_u32(a, b, c, d)                                     \
+#define vmul_laneq_u32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
        uint32x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("mla %0.4s, %2.4s, %3.s[%4]"                            \
+       uint32x2_t a_ = (a);                                             \
+       uint32x2_t result;                                               \
+       __asm__ ("mul %0.2s, %1.2s, %2.s[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vmlaq_n_f32 (float32x4_t a, float32x4_t b, float32_t c)
-{
-  float32x4_t result;
-  float32x4_t t1;
-  __asm__ ("fmul %1.4s, %3.4s, %4.s[0]; fadd %0.4s, %0.4s, %1.4s"
-           : "=w"(result), "=w"(t1)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vmlaq_n_f64 (float64x2_t a, float64x2_t b, float64_t c)
-{
-  float64x2_t result;
-  float64x2_t t1;
-  __asm__ ("fmul %1.2d, %3.2d, %4.d[0]; fadd %0.2d, %0.2d, %1.2d"
-           : "=w"(result), "=w"(t1)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmlaq_n_s16 (int16x8_t a, int16x8_t b, int16_t c)
-{
-  int16x8_t result;
-  __asm__ ("mla %0.8h,%2.8h,%3.h[0]"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlaq_n_s32 (int32x4_t a, int32x4_t b, int32_t c)
-{
-  int32x4_t result;
-  __asm__ ("mla %0.4s,%2.4s,%3.s[0]"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmlaq_n_u16 (uint16x8_t a, uint16x8_t b, uint16_t c)
-{
-  uint16x8_t result;
-  __asm__ ("mla %0.8h,%2.8h,%3.h[0]"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlaq_n_u32 (uint32x4_t a, uint32x4_t b, uint32_t c)
-{
-  uint32x4_t result;
-  __asm__ ("mla %0.4s,%2.4s,%3.s[0]"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vmlaq_s8 (int8x16_t a, int8x16_t b, int8x16_t c)
-{
-  int8x16_t result;
-  __asm__ ("mla %0.16b, %2.16b, %3.16b"
-           : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmlaq_s16 (int16x8_t a, int16x8_t b, int16x8_t c)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vmul_n_f32 (float32x2_t a, float32_t b)
 {
-  int16x8_t result;
-  __asm__ ("mla %0.8h, %2.8h, %3.8h"
+  float32x2_t result;
+  __asm__ ("fmul %0.2s,%1.2s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlaq_s32 (int32x4_t a, int32x4_t b, int32x4_t c)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vmul_n_s16 (int16x4_t a, int16_t b)
 {
-  int32x4_t result;
-  __asm__ ("mla %0.4s, %2.4s, %3.4s"
+  int16x4_t result;
+  __asm__ ("mul %0.4h,%1.4h,%2.h[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vmlaq_u8 (uint8x16_t a, uint8x16_t b, uint8x16_t c)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vmul_n_s32 (int32x2_t a, int32_t b)
 {
-  uint8x16_t result;
-  __asm__ ("mla %0.16b, %2.16b, %3.16b"
+  int32x2_t result;
+  __asm__ ("mul %0.2s,%1.2s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmlaq_u16 (uint16x8_t a, uint16x8_t b, uint16x8_t c)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vmul_n_u16 (uint16x4_t a, uint16_t b)
 {
-  uint16x8_t result;
-  __asm__ ("mla %0.8h, %2.8h, %3.8h"
+  uint16x4_t result;
+  __asm__ ("mul %0.4h,%1.4h,%2.h[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlaq_u32 (uint32x4_t a, uint32x4_t b, uint32x4_t c)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vmul_n_u32 (uint32x2_t a, uint32_t b)
 {
-  uint32x4_t result;
-  __asm__ ("mla %0.4s, %2.4s, %3.4s"
+  uint32x2_t result;
+  __asm__ ("mul %0.2s,%1.2s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmls_lane_f32(a, b, c, d)                                       \
+#define vmuld_lane_f64(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       float32x2_t c_ = (c);                                            \
-       float32x2_t b_ = (b);                                            \
-       float32x2_t a_ = (a);                                            \
-       float32x2_t result;                                              \
-       float32x2_t t1;                                                  \
-       __asm__ ("fmul %1.2s, %3.2s, %4.s[%5]; fsub %0.2s, %0.2s, %1.2s" \
-                : "=w"(result), "=w"(t1)                                \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+       float64x2_t b_ = (b);                                            \
+       float64_t a_ = (a);                                              \
+       float64_t result;                                                \
+       __asm__ ("fmul %d0,%d1,%2.d[%3]"                                 \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmls_lane_s16(a, b, c, d)                                       \
+#define vmull_high_lane_s16(a, b, c)                                    \
   __extension__                                                         \
     ({                                                                  \
-       int16x4_t c_ = (c);                                              \
-       int16x4_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int16x4_t result;                                                \
-       __asm__ ("mls %0.4h,%2.4h,%3.h[%4]"                              \
+       int16x8_t b_ = (b);                                              \
+       int16x8_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("smull2 %0.4s, %1.8h, %2.h[%3]"                         \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmls_lane_s32(a, b, c, d)                                       \
+#define vmull_high_lane_s32(a, b, c)                                    \
   __extension__                                                         \
     ({                                                                  \
-       int32x2_t c_ = (c);                                              \
-       int32x2_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int32x2_t result;                                                \
-       __asm__ ("mls %0.2s,%2.2s,%3.s[%4]"                              \
+       int32x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("smull2 %0.2d, %1.4s, %2.s[%3]"                         \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmls_lane_u16(a, b, c, d)                                       \
+#define vmull_high_lane_u16(a, b, c)                                    \
   __extension__                                                         \
     ({                                                                  \
-       uint16x4_t c_ = (c);                                             \
-       uint16x4_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint16x4_t result;                                               \
-       __asm__ ("mls %0.4h,%2.4h,%3.h[%4]"                              \
+       uint16x8_t b_ = (b);                                             \
+       uint16x8_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("umull2 %0.4s, %1.8h, %2.h[%3]"                         \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmls_lane_u32(a, b, c, d)                                       \
+#define vmull_high_lane_u32(a, b, c)                                    \
   __extension__                                                         \
     ({                                                                  \
-       uint32x2_t c_ = (c);                                             \
-       uint32x2_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint32x2_t result;                                               \
-       __asm__ ("mls %0.2s,%2.2s,%3.s[%4]"                              \
+       uint32x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("umull2 %0.2d, %1.4s, %2.s[%3]"                         \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vmls_n_f32 (float32x2_t a, float32x2_t b, float32_t c)
+#define vmull_high_laneq_s16(a, b, c)                                   \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t b_ = (b);                                              \
+       int16x8_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("smull2 %0.4s, %1.8h, %2.h[%3]"                         \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmull_high_laneq_s32(a, b, c)                                   \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int64x2_t result;                                                \
+       __asm__ ("smull2 %0.2d, %1.4s, %2.s[%3]"                         \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmull_high_laneq_u16(a, b, c)                                   \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t b_ = (b);                                             \
+       uint16x8_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("umull2 %0.4s, %1.8h, %2.h[%3]"                         \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmull_high_laneq_u32(a, b, c)                                   \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint64x2_t result;                                               \
+       __asm__ ("umull2 %0.2d, %1.4s, %2.s[%3]"                         \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmull_high_n_s16 (int16x8_t a, int16_t b)
 {
-  float32x2_t result;
-  float32x2_t t1;
-  __asm__ ("fmul %1.2s, %3.2s, %4.s[0]; fsub %0.2s, %0.2s, %1.2s"
-           : "=w"(result), "=w"(t1)
-           : "0"(a), "w"(b), "w"(c)
+  int32x4_t result;
+  __asm__ ("smull2 %0.4s,%1.8h,%2.h[0]"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vmls_n_s16 (int16x4_t a, int16x4_t b, int16_t c)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmull_high_n_s32 (int32x4_t a, int32_t b)
 {
-  int16x4_t result;
-  __asm__ ("mls %0.4h, %2.4h, %3.h[0]"
+  int64x2_t result;
+  __asm__ ("smull2 %0.2d,%1.4s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vmls_n_s32 (int32x2_t a, int32x2_t b, int32_t c)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmull_high_n_u16 (uint16x8_t a, uint16_t b)
 {
-  int32x2_t result;
-  __asm__ ("mls %0.2s, %2.2s, %3.s[0]"
+  uint32x4_t result;
+  __asm__ ("umull2 %0.4s,%1.8h,%2.h[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vmls_n_u16 (uint16x4_t a, uint16x4_t b, uint16_t c)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmull_high_n_u32 (uint32x4_t a, uint32_t b)
 {
-  uint16x4_t result;
-  __asm__ ("mls %0.4h, %2.4h, %3.h[0]"
+  uint64x2_t result;
+  __asm__ ("umull2 %0.2d,%1.4s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vmls_n_u32 (uint32x2_t a, uint32x2_t b, uint32_t c)
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vmull_high_p8 (poly8x16_t a, poly8x16_t b)
 {
-  uint32x2_t result;
-  __asm__ ("mls %0.2s, %2.2s, %3.s[0]"
+  poly16x8_t result;
+  __asm__ ("pmull2 %0.8h,%1.16b,%2.16b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vmls_s8 (int8x8_t a, int8x8_t b, int8x8_t c)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmull_high_s8 (int8x16_t a, int8x16_t b)
 {
-  int8x8_t result;
-  __asm__ ("mls %0.8b,%2.8b,%3.8b"
+  int16x8_t result;
+  __asm__ ("smull2 %0.8h,%1.16b,%2.16b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vmls_s16 (int16x4_t a, int16x4_t b, int16x4_t c)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmull_high_s16 (int16x8_t a, int16x8_t b)
 {
-  int16x4_t result;
-  __asm__ ("mls %0.4h,%2.4h,%3.4h"
+  int32x4_t result;
+  __asm__ ("smull2 %0.4s,%1.8h,%2.8h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vmls_s32 (int32x2_t a, int32x2_t b, int32x2_t c)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vmull_high_s32 (int32x4_t a, int32x4_t b)
 {
-  int32x2_t result;
-  __asm__ ("mls %0.2s,%2.2s,%3.2s"
+  int64x2_t result;
+  __asm__ ("smull2 %0.2d,%1.4s,%2.4s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vmls_u8 (uint8x8_t a, uint8x8_t b, uint8x8_t c)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmull_high_u8 (uint8x16_t a, uint8x16_t b)
 {
-  uint8x8_t result;
-  __asm__ ("mls %0.8b,%2.8b,%3.8b"
+  uint16x8_t result;
+  __asm__ ("umull2 %0.8h,%1.16b,%2.16b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vmls_u16 (uint16x4_t a, uint16x4_t b, uint16x4_t c)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmull_high_u16 (uint16x8_t a, uint16x8_t b)
 {
-  uint16x4_t result;
-  __asm__ ("mls %0.4h,%2.4h,%3.4h"
+  uint32x4_t result;
+  __asm__ ("umull2 %0.4s,%1.8h,%2.8h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vmls_u32 (uint32x2_t a, uint32x2_t b, uint32x2_t c)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vmull_high_u32 (uint32x4_t a, uint32x4_t b)
 {
-  uint32x2_t result;
-  __asm__ ("mls %0.2s,%2.2s,%3.2s"
+  uint64x2_t result;
+  __asm__ ("umull2 %0.2d,%1.4s,%2.4s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmlsl_high_lane_s16(a, b, c, d)                                 \
+#define vmull_lane_s16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t c_ = (c);                                              \
-       int16x8_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
+       int16x4_t b_ = (b);                                              \
+       int16x4_t a_ = (a);                                              \
        int32x4_t result;                                                \
-       __asm__ ("smlsl2 %0.4s, %2.8h, %3.h[%4]"                         \
+       __asm__ ("smull %0.4s,%1.4h,%2.h[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_high_lane_s32(a, b, c, d)                                 \
+#define vmull_lane_s32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t c_ = (c);                                              \
-       int32x4_t b_ = (b);                                              \
-       int64x2_t a_ = (a);                                              \
+       int32x2_t b_ = (b);                                              \
+       int32x2_t a_ = (a);                                              \
        int64x2_t result;                                                \
-       __asm__ ("smlsl2 %0.2d, %2.4s, %3.s[%4]"                         \
+       __asm__ ("smull %0.2d,%1.2s,%2.s[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_high_lane_u16(a, b, c, d)                                 \
+#define vmull_lane_u16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
-       uint16x8_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
+       uint16x4_t b_ = (b);                                             \
+       uint16x4_t a_ = (a);                                             \
        uint32x4_t result;                                               \
-       __asm__ ("umlsl2 %0.4s, %2.8h, %3.h[%4]"                         \
+       __asm__ ("umull %0.4s,%1.4h,%2.h[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_high_lane_u32(a, b, c, d)                                 \
+#define vmull_lane_u32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
-       uint32x4_t b_ = (b);                                             \
-       uint64x2_t a_ = (a);                                             \
+       uint32x2_t b_ = (b);                                             \
+       uint32x2_t a_ = (a);                                             \
        uint64x2_t result;                                               \
-       __asm__ ("umlsl2 %0.2d, %2.4s, %3.s[%4]"                         \
+       __asm__ ("umull %0.2d, %1.2s, %2.s[%3]"                          \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_high_laneq_s16(a, b, c, d)                                \
+#define vmull_laneq_s16(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t c_ = (c);                                              \
        int16x8_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
+       int16x4_t a_ = (a);                                              \
        int32x4_t result;                                                \
-       __asm__ ("smlsl2 %0.4s, %2.8h, %3.h[%4]"                         \
+       __asm__ ("smull %0.4s, %1.4h, %2.h[%3]"                          \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_high_laneq_s32(a, b, c, d)                                \
+#define vmull_laneq_s32(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t c_ = (c);                                              \
        int32x4_t b_ = (b);                                              \
-       int64x2_t a_ = (a);                                              \
+       int32x2_t a_ = (a);                                              \
        int64x2_t result;                                                \
-       __asm__ ("smlsl2 %0.2d, %2.4s, %3.s[%4]"                         \
+       __asm__ ("smull %0.2d, %1.2s, %2.s[%3]"                          \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_high_laneq_u16(a, b, c, d)                                \
+#define vmull_laneq_u16(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
        uint16x8_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
+       uint16x4_t a_ = (a);                                             \
        uint32x4_t result;                                               \
-       __asm__ ("umlsl2 %0.4s, %2.8h, %3.h[%4]"                         \
+       __asm__ ("umull %0.4s, %1.4h, %2.h[%3]"                          \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_high_laneq_u32(a, b, c, d)                                \
+#define vmull_laneq_u32(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
        uint32x4_t b_ = (b);                                             \
-       uint64x2_t a_ = (a);                                             \
+       uint32x2_t a_ = (a);                                             \
        uint64x2_t result;                                               \
-       __asm__ ("umlsl2 %0.2d, %2.4s, %3.s[%4]"                         \
+       __asm__ ("umull %0.2d, %1.2s, %2.s[%3]"                          \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlsl_high_n_s16 (int32x4_t a, int16x8_t b, int16_t c)
+vmull_n_s16 (int16x4_t a, int16_t b)
 {
   int32x4_t result;
-  __asm__ ("smlsl2 %0.4s, %2.8h, %3.h[0]"
+  __asm__ ("smull %0.4s,%1.4h,%2.h[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmlsl_high_n_s32 (int64x2_t a, int32x4_t b, int32_t c)
+vmull_n_s32 (int32x2_t a, int32_t b)
 {
   int64x2_t result;
-  __asm__ ("smlsl2 %0.2d, %2.4s, %3.s[0]"
+  __asm__ ("smull %0.2d,%1.2s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlsl_high_n_u16 (uint32x4_t a, uint16x8_t b, uint16_t c)
+vmull_n_u16 (uint16x4_t a, uint16_t b)
 {
   uint32x4_t result;
-  __asm__ ("umlsl2 %0.4s, %2.8h, %3.h[0]"
+  __asm__ ("umull %0.4s,%1.4h,%2.h[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmlsl_high_n_u32 (uint64x2_t a, uint32x4_t b, uint32_t c)
+vmull_n_u32 (uint32x2_t a, uint32_t b)
 {
   uint64x2_t result;
-  __asm__ ("umlsl2 %0.2d, %2.4s, %3.s[0]"
+  __asm__ ("umull %0.2d,%1.2s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vmull_p8 (poly8x8_t a, poly8x8_t b)
+{
+  poly16x8_t result;
+  __asm__ ("pmull %0.8h, %1.8b, %2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmlsl_high_s8 (int16x8_t a, int8x16_t b, int8x16_t c)
+vmull_s8 (int8x8_t a, int8x8_t b)
 {
   int16x8_t result;
-  __asm__ ("smlsl2 %0.8h,%2.16b,%3.16b"
+  __asm__ ("smull %0.8h, %1.8b, %2.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlsl_high_s16 (int32x4_t a, int16x8_t b, int16x8_t c)
+vmull_s16 (int16x4_t a, int16x4_t b)
 {
   int32x4_t result;
-  __asm__ ("smlsl2 %0.4s,%2.8h,%3.8h"
+  __asm__ ("smull %0.4s, %1.4h, %2.4h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmlsl_high_s32 (int64x2_t a, int32x4_t b, int32x4_t c)
+vmull_s32 (int32x2_t a, int32x2_t b)
 {
   int64x2_t result;
-  __asm__ ("smlsl2 %0.2d,%2.4s,%3.4s"
+  __asm__ ("smull %0.2d, %1.2s, %2.2s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmlsl_high_u8 (uint16x8_t a, uint8x16_t b, uint8x16_t c)
+vmull_u8 (uint8x8_t a, uint8x8_t b)
 {
   uint16x8_t result;
-  __asm__ ("umlsl2 %0.8h,%2.16b,%3.16b"
+  __asm__ ("umull %0.8h, %1.8b, %2.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlsl_high_u16 (uint32x4_t a, uint16x8_t b, uint16x8_t c)
+vmull_u16 (uint16x4_t a, uint16x4_t b)
 {
   uint32x4_t result;
-  __asm__ ("umlsl2 %0.4s,%2.8h,%3.8h"
+  __asm__ ("umull %0.4s, %1.4h, %2.4h"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmlsl_high_u32 (uint64x2_t a, uint32x4_t b, uint32x4_t c)
+vmull_u32 (uint32x2_t a, uint32x2_t b)
 {
   uint64x2_t result;
-  __asm__ ("umlsl2 %0.2d,%2.4s,%3.4s"
+  __asm__ ("umull %0.2d, %1.2s, %2.2s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmlsl_lane_s16(a, b, c, d)                                      \
+#define vmulq_lane_f32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x2_t b_ = (b);                                            \
+       float32x4_t a_ = (a);                                            \
+       float32x4_t result;                                              \
+       __asm__ ("fmul %0.4s, %1.4s, %2.s[%3]"                           \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmulq_lane_f64(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x1_t b_ = (b);                                            \
+       float64x2_t a_ = (a);                                            \
+       float64x2_t result;                                              \
+       __asm__ ("fmul %0.2d,%1.2d,%2.d[%3]"                             \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmulq_lane_s16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int16x4_t c_ = (c);                                              \
        int16x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smlsl %0.4s, %2.4h, %3.h[%4]"                          \
+       int16x8_t a_ = (a);                                              \
+       int16x8_t result;                                                \
+       __asm__ ("mul %0.8h,%1.8h,%2.h[%3]"                              \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_lane_s32(a, b, c, d)                                      \
+#define vmulq_lane_s32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int32x2_t c_ = (c);                                              \
        int32x2_t b_ = (b);                                              \
-       int64x2_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smlsl %0.2d, %2.2s, %3.s[%4]"                          \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("mul %0.4s,%1.4s,%2.s[%3]"                              \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_lane_u16(a, b, c, d)                                      \
+#define vmulq_lane_u16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       uint16x4_t c_ = (c);                                             \
        uint16x4_t b_ = (b);                                             \
+       uint16x8_t a_ = (a);                                             \
+       uint16x8_t result;                                               \
+       __asm__ ("mul %0.8h,%1.8h,%2.h[%3]"                              \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vmulq_lane_u32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x2_t b_ = (b);                                             \
        uint32x4_t a_ = (a);                                             \
        uint32x4_t result;                                               \
-       __asm__ ("umlsl %0.4s, %2.4h, %3.h[%4]"                          \
+       __asm__ ("mul %0.4s, %1.4s, %2.s[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_lane_u32(a, b, c, d)                                      \
+#define vmulq_laneq_f32(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       uint32x2_t c_ = (c);                                             \
-       uint32x2_t b_ = (b);                                             \
-       uint64x2_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umlsl %0.2d, %2.2s, %3.s[%4]"                          \
+       float32x4_t b_ = (b);                                            \
+       float32x4_t a_ = (a);                                            \
+       float32x4_t result;                                              \
+       __asm__ ("fmul %0.4s, %1.4s, %2.s[%3]"                           \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_laneq_s16(a, b, c, d)                                     \
+#define vmulq_laneq_f64(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t c_ = (c);                                              \
-       int16x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smlsl %0.4s, %2.4h, %3.h[%4]"                          \
+       float64x2_t b_ = (b);                                            \
+       float64x2_t a_ = (a);                                            \
+       float64x2_t result;                                              \
+       __asm__ ("fmul %0.2d,%1.2d,%2.d[%3]"                             \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_laneq_s32(a, b, c, d)                                     \
+#define vmulq_laneq_s16(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t c_ = (c);                                              \
-       int32x2_t b_ = (b);                                              \
-       int64x2_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smlsl %0.2d, %2.2s, %3.s[%4]"                          \
+       int16x8_t b_ = (b);                                              \
+       int16x8_t a_ = (a);                                              \
+       int16x8_t result;                                                \
+       __asm__ ("mul %0.8h, %1.8h, %2.h[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_laneq_u16(a, b, c, d)                                     \
+#define vmulq_laneq_s32(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
-       uint16x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("umlsl %0.4s, %2.4h, %3.h[%4]"                          \
+       int32x4_t b_ = (b);                                              \
+       int32x4_t a_ = (a);                                              \
+       int32x4_t result;                                                \
+       __asm__ ("mul %0.4s, %1.4s, %2.s[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsl_laneq_u32(a, b, c, d)                                     \
+#define vmulq_laneq_u16(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
-       uint32x2_t b_ = (b);                                             \
-       uint64x2_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umlsl %0.2d, %2.2s, %3.s[%4]"                          \
+       uint16x8_t b_ = (b);                                             \
+       uint16x8_t a_ = (a);                                             \
+       uint16x8_t result;                                               \
+       __asm__ ("mul %0.8h, %1.8h, %2.h[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlsl_n_s16 (int32x4_t a, int16x4_t b, int16_t c)
+#define vmulq_laneq_u32(a, b, c)                                        \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t b_ = (b);                                             \
+       uint32x4_t a_ = (a);                                             \
+       uint32x4_t result;                                               \
+       __asm__ ("mul %0.4s, %1.4s, %2.s[%3]"                            \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vmulq_n_f32 (float32x4_t a, float32_t b)
 {
-  int32x4_t result;
-  __asm__ ("smlsl %0.4s, %2.4h, %3.h[0]"
+  float32x4_t result;
+  __asm__ ("fmul %0.4s,%1.4s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmlsl_n_s32 (int64x2_t a, int32x2_t b, int32_t c)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vmulq_n_f64 (float64x2_t a, float64_t b)
 {
-  int64x2_t result;
-  __asm__ ("smlsl %0.2d, %2.2s, %3.s[0]"
+  float64x2_t result;
+  __asm__ ("fmul %0.2d,%1.2d,%2.d[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlsl_n_u16 (uint32x4_t a, uint16x4_t b, uint16_t c)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmulq_n_s16 (int16x8_t a, int16_t b)
 {
-  uint32x4_t result;
-  __asm__ ("umlsl %0.4s, %2.4h, %3.h[0]"
+  int16x8_t result;
+  __asm__ ("mul %0.8h,%1.8h,%2.h[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmlsl_n_u32 (uint64x2_t a, uint32x2_t b, uint32_t c)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmulq_n_s32 (int32x4_t a, int32_t b)
 {
-  uint64x2_t result;
-  __asm__ ("umlsl %0.2d, %2.2s, %3.s[0]"
+  int32x4_t result;
+  __asm__ ("mul %0.4s,%1.4s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmlsl_s8 (int16x8_t a, int8x8_t b, int8x8_t c)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmulq_n_u16 (uint16x8_t a, uint16_t b)
 {
-  int16x8_t result;
-  __asm__ ("smlsl %0.8h, %2.8b, %3.8b"
+  uint16x8_t result;
+  __asm__ ("mul %0.8h,%1.8h,%2.h[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlsl_s16 (int32x4_t a, int16x4_t b, int16x4_t c)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmulq_n_u32 (uint32x4_t a, uint32_t b)
 {
-  int32x4_t result;
-  __asm__ ("smlsl %0.4s, %2.4h, %3.4h"
+  uint32x4_t result;
+  __asm__ ("mul %0.4s,%1.4s,%2.s[0]"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmlsl_s32 (int64x2_t a, int32x2_t b, int32x2_t c)
+#define vmuls_lane_f32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t b_ = (b);                                            \
+       float32_t a_ = (a);                                              \
+       float32_t result;                                                \
+       __asm__ ("fmul %s0,%s1,%2.s[%3]"                                 \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vmulx_f32 (float32x2_t a, float32x2_t b)
 {
-  int64x2_t result;
-  __asm__ ("smlsl %0.2d, %2.2s, %3.2s"
+  float32x2_t result;
+  __asm__ ("fmulx %0.2s,%1.2s,%2.2s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmlsl_u8 (uint16x8_t a, uint8x8_t b, uint8x8_t c)
+#define vmulx_lane_f32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t b_ = (b);                                            \
+       float32x2_t a_ = (a);                                            \
+       float32x2_t result;                                              \
+       __asm__ ("fmulx %0.2s,%1.2s,%2.s[%3]"                            \
+                : "=w"(result)                                          \
+                : "w"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vmulxd_f64 (float64_t a, float64_t b)
 {
-  uint16x8_t result;
-  __asm__ ("umlsl %0.8h, %2.8b, %3.8b"
+  float64_t result;
+  __asm__ ("fmulx %d0, %d1, %d2"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlsl_u16 (uint32x4_t a, uint16x4_t b, uint16x4_t c)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vmulxq_f32 (float32x4_t a, float32x4_t b)
 {
-  uint32x4_t result;
-  __asm__ ("umlsl %0.4s, %2.4h, %3.4h"
+  float32x4_t result;
+  __asm__ ("fmulx %0.4s,%1.4s,%2.4s"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmlsl_u32 (uint64x2_t a, uint32x2_t b, uint32x2_t c)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vmulxq_f64 (float64x2_t a, float64x2_t b)
 {
-  uint64x2_t result;
-  __asm__ ("umlsl %0.2d, %2.2s, %3.2s"
+  float64x2_t result;
+  __asm__ ("fmulx %0.2d,%1.2d,%2.2d"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmlsq_lane_f32(a, b, c, d)                                      \
+#define vmulxq_lane_f32(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       float32x4_t c_ = (c);                                            \
        float32x4_t b_ = (b);                                            \
        float32x4_t a_ = (a);                                            \
        float32x4_t result;                                              \
-       float32x4_t t1;                                                  \
-       __asm__ ("fmul %1.4s, %3.4s, %4.s[%5]; fsub %0.4s, %0.4s, %1.4s" \
-                : "=w"(result), "=w"(t1)                                \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlsq_lane_s16(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t c_ = (c);                                              \
-       int16x8_t b_ = (b);                                              \
-       int16x8_t a_ = (a);                                              \
-       int16x8_t result;                                                \
-       __asm__ ("mls %0.8h,%2.8h,%3.h[%4]"                              \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlsq_lane_s32(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t c_ = (c);                                              \
-       int32x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("mls %0.4s,%2.4s,%3.s[%4]"                              \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmlsq_lane_u16(a, b, c, d)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t c_ = (c);                                             \
-       uint16x8_t b_ = (b);                                             \
-       uint16x8_t a_ = (a);                                             \
-       uint16x8_t result;                                               \
-       __asm__ ("mls %0.8h,%2.8h,%3.h[%4]"                              \
+       __asm__ ("fmulx %0.4s,%1.4s,%2.s[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsq_lane_u32(a, b, c, d)                                      \
+#define vmulxq_lane_f64(a, b, c)                                        \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t c_ = (c);                                             \
-       uint32x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("mls %0.4s,%2.4s,%3.s[%4]"                              \
+       float64x2_t b_ = (b);                                            \
+       float64x2_t a_ = (a);                                            \
+       float64x2_t result;                                              \
+       __asm__ ("fmulx %0.2d,%1.2d,%2.d[%3]"                            \
                 : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "w"(c_), "i"(d)                     \
+                : "w"(a_), "w"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vmlsq_laneq_f32(__a, __b, __c, __d)				\
-  __extension__								\
-    ({									\
-       float32x4_t __c_ = (__c);					\
-       float32x4_t __b_ = (__b);					\
-       float32x4_t __a_ = (__a);					\
-       float32x4_t __result;						\
-       float32x4_t __t1;						\
-       __asm__ ("fmul %1.4s, %3.4s, %4.s[%5]; fsub %0.4s, %0.4s, %1.4s"	\
-                : "=w"(__result), "=w"(__t1)				\
-                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
-                : /* No clobbers */);					\
-       __result;							\
-     })
-
-#define vmlsq_laneq_s16(__a, __b, __c, __d)				\
-  __extension__								\
-    ({									\
-       int16x8_t __c_ = (__c);						\
-       int16x8_t __b_ = (__b);						\
-       int16x8_t __a_ = (__a);						\
-       int16x8_t __result;						\
-       __asm__ ("mls %0.8h, %2.8h, %3.h[%4]"				\
-                : "=w"(__result)					\
-                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
-                : /* No clobbers */);					\
-       __result;							\
-     })
-
-#define vmlsq_laneq_s32(__a, __b, __c, __d)				\
-  __extension__								\
-    ({									\
-       int32x4_t __c_ = (__c);						\
-       int32x4_t __b_ = (__b);						\
-       int32x4_t __a_ = (__a);						\
-       int32x4_t __result;						\
-       __asm__ ("mls %0.4s, %2.4s, %3.s[%4]"				\
-                : "=w"(__result)					\
-                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
-                : /* No clobbers */);					\
-       __result;							\
-     })
-
-#define vmlsq_laneq_u16(__a, __b, __c, __d)				\
-  __extension__								\
-    ({									\
-       uint16x8_t __c_ = (__c);						\
-       uint16x8_t __b_ = (__b);						\
-       uint16x8_t __a_ = (__a);						\
-       uint16x8_t __result;						\
-       __asm__ ("mls %0.8h, %2.8h, %3.h[%4]"				\
-                : "=w"(__result)					\
-                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
-                : /* No clobbers */);					\
-       __result;							\
-     })
-
-#define vmlsq_laneq_u32(__a, __b, __c, __d)				\
-  __extension__								\
-    ({									\
-       uint32x4_t __c_ = (__c);						\
-       uint32x4_t __b_ = (__b);						\
-       uint32x4_t __a_ = (__a);						\
-       uint32x4_t __result;						\
-       __asm__ ("mls %0.4s, %2.4s, %3.s[%4]"				\
-                : "=w"(__result)					\
-                : "0"(__a_), "w"(__b_), "w"(__c_), "i"(__d)		\
-                : /* No clobbers */);					\
-       __result;							\
-     })
-
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vmlsq_n_f32 (float32x4_t a, float32x4_t b, float32_t c)
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vmulxs_f32 (float32_t a, float32_t b)
 {
-  float32x4_t result;
-  float32x4_t t1;
-  __asm__ ("fmul %1.4s, %3.4s, %4.s[0]; fsub %0.4s, %0.4s, %1.4s"
-           : "=w"(result), "=w"(t1)
-           : "0"(a), "w"(b), "w"(c)
+  float32_t result;
+  __asm__ ("fmulx %s0, %s1, %s2"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vmlsq_n_f64 (float64x2_t a, float64x2_t b, float64_t c)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vmvn_p8 (poly8x8_t a)
 {
-  float64x2_t result;
-  float64x2_t t1;
-  __asm__ ("fmul %1.2d, %3.2d, %4.d[0]; fsub %0.2d, %0.2d, %1.2d"
-           : "=w"(result), "=w"(t1)
-           : "0"(a), "w"(b), "w"(c)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmlsq_n_s16 (int16x8_t a, int16x8_t b, int16_t c)
-{
-  int16x8_t result;
-  __asm__ ("mls %0.8h, %2.8h, %3.h[0]"
+  poly8x8_t result;
+  __asm__ ("mvn %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlsq_n_s32 (int32x4_t a, int32x4_t b, int32_t c)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vmvn_s8 (int8x8_t a)
 {
-  int32x4_t result;
-  __asm__ ("mls %0.4s, %2.4s, %3.s[0]"
+  int8x8_t result;
+  __asm__ ("mvn %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmlsq_n_u16 (uint16x8_t a, uint16x8_t b, uint16_t c)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vmvn_s16 (int16x4_t a)
 {
-  uint16x8_t result;
-  __asm__ ("mls %0.8h, %2.8h, %3.h[0]"
+  int16x4_t result;
+  __asm__ ("mvn %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlsq_n_u32 (uint32x4_t a, uint32x4_t b, uint32_t c)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vmvn_s32 (int32x2_t a)
 {
-  uint32x4_t result;
-  __asm__ ("mls %0.4s, %2.4s, %3.s[0]"
+  int32x2_t result;
+  __asm__ ("mvn %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vmlsq_s8 (int8x16_t a, int8x16_t b, int8x16_t c)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vmvn_u8 (uint8x8_t a)
 {
-  int8x16_t result;
-  __asm__ ("mls %0.16b,%2.16b,%3.16b"
+  uint8x8_t result;
+  __asm__ ("mvn %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmlsq_s16 (int16x8_t a, int16x8_t b, int16x8_t c)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vmvn_u16 (uint16x4_t a)
 {
-  int16x8_t result;
-  __asm__ ("mls %0.8h,%2.8h,%3.8h"
+  uint16x4_t result;
+  __asm__ ("mvn %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmlsq_s32 (int32x4_t a, int32x4_t b, int32x4_t c)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vmvn_u32 (uint32x2_t a)
 {
-  int32x4_t result;
-  __asm__ ("mls %0.4s,%2.4s,%3.4s"
+  uint32x2_t result;
+  __asm__ ("mvn %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vmlsq_u8 (uint8x16_t a, uint8x16_t b, uint8x16_t c)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vmvnq_p8 (poly8x16_t a)
 {
-  uint8x16_t result;
-  __asm__ ("mls %0.16b,%2.16b,%3.16b"
+  poly8x16_t result;
+  __asm__ ("mvn %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmlsq_u16 (uint16x8_t a, uint16x8_t b, uint16x8_t c)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vmvnq_s8 (int8x16_t a)
 {
-  uint16x8_t result;
-  __asm__ ("mls %0.8h,%2.8h,%3.8h"
+  int8x16_t result;
+  __asm__ ("mvn %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmlsq_u32 (uint32x4_t a, uint32x4_t b, uint32x4_t c)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vmvnq_s16 (int16x8_t a)
 {
-  uint32x4_t result;
-  __asm__ ("mls %0.4s,%2.4s,%3.4s"
+  int16x8_t result;
+  __asm__ ("mvn %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b), "w"(c)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vmov_n_f32 (float32_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vmvnq_s32 (int32x4_t a)
 {
-  float32x2_t result;
-  __asm__ ("dup %0.2s, %w1"
+  int32x4_t result;
+  __asm__ ("mvn %0.16b,%1.16b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vmov_n_p8 (uint32_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vmvnq_u8 (uint8x16_t a)
 {
-  poly8x8_t result;
-  __asm__ ("dup %0.8b,%w1"
+  uint8x16_t result;
+  __asm__ ("mvn %0.16b,%1.16b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vmov_n_p16 (uint32_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmvnq_u16 (uint16x8_t a)
 {
-  poly16x4_t result;
-  __asm__ ("dup %0.4h,%w1"
+  uint16x8_t result;
+  __asm__ ("mvn %0.16b,%1.16b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vmov_n_s8 (int32_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmvnq_u32 (uint32x4_t a)
 {
-  int8x8_t result;
-  __asm__ ("dup %0.8b,%w1"
+  uint32x4_t result;
+  __asm__ ("mvn %0.16b,%1.16b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vmov_n_s16 (int32_t a)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vneg_f32 (float32x2_t a)
 {
-  int16x4_t result;
-  __asm__ ("dup %0.4h,%w1"
+  float32x2_t result;
+  __asm__ ("fneg %0.2s,%1.2s"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vmov_n_s32 (int32_t a)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vneg_s8 (int8x8_t a)
 {
-  int32x2_t result;
-  __asm__ ("dup %0.2s,%w1"
+  int8x8_t result;
+  __asm__ ("neg %0.8b,%1.8b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vmov_n_s64 (int64_t a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vneg_s16 (int16x4_t a)
 {
-  int64x1_t result;
-  __asm__ ("ins %0.d[0],%x1"
+  int16x4_t result;
+  __asm__ ("neg %0.4h,%1.4h"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vmov_n_u8 (uint32_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vneg_s32 (int32x2_t a)
 {
-  uint8x8_t result;
-  __asm__ ("dup %0.8b,%w1"
+  int32x2_t result;
+  __asm__ ("neg %0.2s,%1.2s"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vmov_n_u16 (uint32_t a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vnegq_f32 (float32x4_t a)
 {
-  uint16x4_t result;
-  __asm__ ("dup %0.4h,%w1"
+  float32x4_t result;
+  __asm__ ("fneg %0.4s,%1.4s"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vmov_n_u32 (uint32_t a)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vnegq_f64 (float64x2_t a)
 {
-  uint32x2_t result;
-  __asm__ ("dup %0.2s,%w1"
+  float64x2_t result;
+  __asm__ ("fneg %0.2d,%1.2d"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vmov_n_u64 (uint64_t a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vnegq_s8 (int8x16_t a)
 {
-  uint64x1_t result;
-  __asm__ ("ins %0.d[0],%x1"
+  int8x16_t result;
+  __asm__ ("neg %0.16b,%1.16b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmovl_high_s8 (int8x16_t a)
+vnegq_s16 (int16x8_t a)
 {
   int16x8_t result;
-  __asm__ ("sshll2 %0.8h,%1.16b,#0"
+  __asm__ ("neg %0.8h,%1.8h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -11259,10 +11177,10 @@ vmovl_high_s8 (int8x16_t a)
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmovl_high_s16 (int16x8_t a)
+vnegq_s32 (int32x4_t a)
 {
   int32x4_t result;
-  __asm__ ("sshll2 %0.4s,%1.8h,#0"
+  __asm__ ("neg %0.4s,%1.4s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -11270,186 +11188,203 @@ vmovl_high_s16 (int16x8_t a)
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmovl_high_s32 (int32x4_t a)
+vnegq_s64 (int64x2_t a)
 {
   int64x2_t result;
-  __asm__ ("sshll2 %0.2d,%1.4s,#0"
+  __asm__ ("neg %0.2d,%1.2d"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmovl_high_u8 (uint8x16_t a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vpadal_s8 (int16x4_t a, int8x8_t b)
 {
-  uint16x8_t result;
-  __asm__ ("ushll2 %0.8h,%1.16b,#0"
+  int16x4_t result;
+  __asm__ ("sadalp %0.4h,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmovl_high_u16 (uint16x8_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vpadal_s16 (int32x2_t a, int16x4_t b)
 {
-  uint32x4_t result;
-  __asm__ ("ushll2 %0.4s,%1.8h,#0"
+  int32x2_t result;
+  __asm__ ("sadalp %0.2s,%2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmovl_high_u32 (uint32x4_t a)
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vpadal_s32 (int64x1_t a, int32x2_t b)
 {
-  uint64x2_t result;
-  __asm__ ("ushll2 %0.2d,%1.4s,#0"
+  int64x1_t result;
+  __asm__ ("sadalp %0.1d,%2.2s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmovl_s8 (int8x8_t a)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vpadal_u8 (uint16x4_t a, uint8x8_t b)
+{
+  uint16x4_t result;
+  __asm__ ("uadalp %0.4h,%2.8b"
+           : "=w"(result)
+           : "0"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vpadal_u16 (uint32x2_t a, uint16x4_t b)
+{
+  uint32x2_t result;
+  __asm__ ("uadalp %0.2s,%2.4h"
+           : "=w"(result)
+           : "0"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vpadal_u32 (uint64x1_t a, uint32x2_t b)
+{
+  uint64x1_t result;
+  __asm__ ("uadalp %0.1d,%2.2s"
+           : "=w"(result)
+           : "0"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vpadalq_s8 (int16x8_t a, int8x16_t b)
 {
   int16x8_t result;
-  __asm__ ("sshll %0.8h,%1.8b,#0"
+  __asm__ ("sadalp %0.8h,%2.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmovl_s16 (int16x4_t a)
+vpadalq_s16 (int32x4_t a, int16x8_t b)
 {
   int32x4_t result;
-  __asm__ ("sshll %0.4s,%1.4h,#0"
+  __asm__ ("sadalp %0.4s,%2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmovl_s32 (int32x2_t a)
+vpadalq_s32 (int64x2_t a, int32x4_t b)
 {
   int64x2_t result;
-  __asm__ ("sshll %0.2d,%1.2s,#0"
+  __asm__ ("sadalp %0.2d,%2.4s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmovl_u8 (uint8x8_t a)
+vpadalq_u8 (uint16x8_t a, uint8x16_t b)
 {
   uint16x8_t result;
-  __asm__ ("ushll %0.8h,%1.8b,#0"
+  __asm__ ("uadalp %0.8h,%2.16b"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmovl_u16 (uint16x4_t a)
+vpadalq_u16 (uint32x4_t a, uint16x8_t b)
 {
   uint32x4_t result;
-  __asm__ ("ushll %0.4s,%1.4h,#0"
+  __asm__ ("uadalp %0.4s,%2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmovl_u32 (uint32x2_t a)
+vpadalq_u32 (uint64x2_t a, uint32x4_t b)
 {
   uint64x2_t result;
-  __asm__ ("ushll %0.2d,%1.2s,#0"
+  __asm__ ("uadalp %0.2d,%2.4s"
            : "=w"(result)
-           : "w"(a)
+           : "0"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vmovn_high_s16 (int8x8_t a, int16x8_t b)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vpadd_f32 (float32x2_t a, float32x2_t b)
 {
-  int8x16_t result = vcombine_s8 (a, vcreate_s8 (UINT64_C (0x0)));
-  __asm__ ("xtn2 %0.16b,%1.8h"
-           : "+w"(result)
-           : "w"(b)
+  float32x2_t result;
+  __asm__ ("faddp %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmovn_high_s32 (int16x4_t a, int32x4_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vpadd_s8 (int8x8_t __a, int8x8_t __b)
 {
-  int16x8_t result = vcombine_s16 (a, vcreate_s16 (UINT64_C (0x0)));
-  __asm__ ("xtn2 %0.8h,%1.4s"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_addpv8qi (__a, __b);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmovn_high_s64 (int32x2_t a, int64x2_t b)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vpadd_s16 (int16x4_t __a, int16x4_t __b)
 {
-  int32x4_t result = vcombine_s32 (a, vcreate_s32 (UINT64_C (0x0)));
-  __asm__ ("xtn2 %0.4s,%1.2d"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_addpv4hi (__a, __b);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vmovn_high_u16 (uint8x8_t a, uint16x8_t b)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vpadd_s32 (int32x2_t __a, int32x2_t __b)
 {
-  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (UINT64_C (0x0)));
-  __asm__ ("xtn2 %0.16b,%1.8h"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_addpv2si (__a, __b);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmovn_high_u32 (uint16x4_t a, uint32x4_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vpadd_u8 (uint8x8_t __a, uint8x8_t __b)
 {
-  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (UINT64_C (0x0)));
-  __asm__ ("xtn2 %0.8h,%1.4s"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x8_t) __builtin_aarch64_addpv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmovn_high_u64 (uint32x2_t a, uint64x2_t b)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vpadd_u16 (uint16x4_t __a, uint16x4_t __b)
 {
-  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (UINT64_C (0x0)));
-  __asm__ ("xtn2 %0.4s,%1.2d"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint16x4_t) __builtin_aarch64_addpv4hi ((int16x4_t) __a,
+						  (int16x4_t) __b);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vmovn_s16 (int16x8_t a)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vpadd_u32 (uint32x2_t __a, uint32x2_t __b)
 {
-  int8x8_t result;
-  __asm__ ("xtn %0.8b,%1.8h"
+  return (uint32x2_t) __builtin_aarch64_addpv2si ((int32x2_t) __a,
+						  (int32x2_t) __b);
+}
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vpaddd_f64 (float64x2_t a)
+{
+  float64_t result;
+  __asm__ ("faddp %d0,%1.2d"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -11457,10 +11392,10 @@ vmovn_s16 (int16x8_t a)
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vmovn_s32 (int32x4_t a)
+vpaddl_s8 (int8x8_t a)
 {
   int16x4_t result;
-  __asm__ ("xtn %0.4h,%1.4s"
+  __asm__ ("saddlp %0.4h,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -11468,21 +11403,21 @@ vmovn_s32 (int32x4_t a)
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vmovn_s64 (int64x2_t a)
+vpaddl_s16 (int16x4_t a)
 {
   int32x2_t result;
-  __asm__ ("xtn %0.2s,%1.2d"
+  __asm__ ("saddlp %0.2s,%1.4h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vmovn_u16 (uint16x8_t a)
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vpaddl_s32 (int32x2_t a)
 {
-  uint8x8_t result;
-  __asm__ ("xtn %0.8b,%1.8h"
+  int64x1_t result;
+  __asm__ ("saddlp %0.1d,%1.2s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -11490,10 +11425,10 @@ vmovn_u16 (uint16x8_t a)
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vmovn_u32 (uint32x4_t a)
+vpaddl_u8 (uint8x8_t a)
 {
   uint16x4_t result;
-  __asm__ ("xtn %0.4h,%1.4s"
+  __asm__ ("uaddlp %0.4h,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -11501,278 +11436,230 @@ vmovn_u32 (uint32x4_t a)
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vmovn_u64 (uint64x2_t a)
+vpaddl_u16 (uint16x4_t a)
 {
   uint32x2_t result;
-  __asm__ ("xtn %0.2s,%1.2d"
+  __asm__ ("uaddlp %0.2s,%1.4h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vmovq_n_f32 (float32_t a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vpaddl_u32 (uint32x2_t a)
 {
-  float32x4_t result;
-  __asm__ ("dup %0.4s, %w1"
+  uint64x1_t result;
+  __asm__ ("uaddlp %0.1d,%1.2s"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vmovq_n_f64 (float64_t a)
-{
-  return (float64x2_t) {a, a};
-}
-
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vmovq_n_p8 (uint32_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vpaddlq_s8 (int8x16_t a)
 {
-  poly8x16_t result;
-  __asm__ ("dup %0.16b,%w1"
+  int16x8_t result;
+  __asm__ ("saddlp %0.8h,%1.16b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vmovq_n_p16 (uint32_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vpaddlq_s16 (int16x8_t a)
 {
-  poly16x8_t result;
-  __asm__ ("dup %0.8h,%w1"
+  int32x4_t result;
+  __asm__ ("saddlp %0.4s,%1.8h"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vmovq_n_s8 (int32_t a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vpaddlq_s32 (int32x4_t a)
 {
-  int8x16_t result;
-  __asm__ ("dup %0.16b,%w1"
+  int64x2_t result;
+  __asm__ ("saddlp %0.2d,%1.4s"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmovq_n_s16 (int32_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vpaddlq_u8 (uint8x16_t a)
 {
-  int16x8_t result;
-  __asm__ ("dup %0.8h,%w1"
+  uint16x8_t result;
+  __asm__ ("uaddlp %0.8h,%1.16b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmovq_n_s32 (int32_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vpaddlq_u16 (uint16x8_t a)
 {
-  int32x4_t result;
-  __asm__ ("dup %0.4s,%w1"
+  uint32x4_t result;
+  __asm__ ("uaddlp %0.4s,%1.8h"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmovq_n_s64 (int64_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vpaddlq_u32 (uint32x4_t a)
 {
-  int64x2_t result;
-  __asm__ ("dup %0.2d,%x1"
+  uint64x2_t result;
+  __asm__ ("uaddlp %0.2d,%1.4s"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vmovq_n_u8 (uint32_t a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vpaddq_f32 (float32x4_t a, float32x4_t b)
 {
-  uint8x16_t result;
-  __asm__ ("dup %0.16b,%w1"
+  float32x4_t result;
+  __asm__ ("faddp %0.4s,%1.4s,%2.4s"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmovq_n_u16 (uint32_t a)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vpaddq_f64 (float64x2_t a, float64x2_t b)
 {
-  uint16x8_t result;
-  __asm__ ("dup %0.8h,%w1"
+  float64x2_t result;
+  __asm__ ("faddp %0.2d,%1.2d,%2.2d"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmovq_n_u32 (uint32_t a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vpaddq_s8 (int8x16_t a, int8x16_t b)
 {
-  uint32x4_t result;
-  __asm__ ("dup %0.4s,%w1"
+  int8x16_t result;
+  __asm__ ("addp %0.16b,%1.16b,%2.16b"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmovq_n_u64 (uint64_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vpaddq_s16 (int16x8_t a, int16x8_t b)
 {
-  uint64x2_t result;
-  __asm__ ("dup %0.2d,%x1"
+  int16x8_t result;
+  __asm__ ("addp %0.8h,%1.8h,%2.8h"
            : "=w"(result)
-           : "r"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmul_lane_f32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t b_ = (b);                                            \
-       float32x2_t a_ = (a);                                            \
-       float32x2_t result;                                              \
-       __asm__ ("fmul %0.2s,%1.2s,%2.s[%3]"                             \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmul_lane_s16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int16x4_t result;                                                \
-       __asm__ ("mul %0.4h,%1.4h,%2.h[%3]"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmul_lane_s32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int32x2_t result;                                                \
-       __asm__ ("mul %0.2s,%1.2s,%2.s[%3]"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmul_lane_u16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint16x4_t result;                                               \
-       __asm__ ("mul %0.4h,%1.4h,%2.h[%3]"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vpaddq_s32 (int32x4_t a, int32x4_t b)
+{
+  int32x4_t result;
+  __asm__ ("addp %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmul_lane_u32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint32x2_t result;                                               \
-       __asm__ ("mul %0.2s, %1.2s, %2.s[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vpaddq_s64 (int64x2_t a, int64x2_t b)
+{
+  int64x2_t result;
+  __asm__ ("addp %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmul_laneq_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32x2_t a_ = (a);                                            \
-       float32x2_t result;                                              \
-       __asm__ ("fmul %0.2s, %1.2s, %2.s[%3]"                           \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vpaddq_u8 (uint8x16_t a, uint8x16_t b)
+{
+  uint8x16_t result;
+  __asm__ ("addp %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmul_laneq_s16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int16x4_t result;                                                \
-       __asm__ ("mul %0.4h, %1.4h, %2.h[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vpaddq_u16 (uint16x8_t a, uint16x8_t b)
+{
+  uint16x8_t result;
+  __asm__ ("addp %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmul_laneq_s32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int32x2_t result;                                                \
-       __asm__ ("mul %0.2s, %1.2s, %2.s[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vpaddq_u32 (uint32x4_t a, uint32x4_t b)
+{
+  uint32x4_t result;
+  __asm__ ("addp %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmul_laneq_u16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint16x4_t result;                                               \
-       __asm__ ("mul %0.4h, %1.4h, %2.h[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vpaddq_u64 (uint64x2_t a, uint64x2_t b)
+{
+  uint64x2_t result;
+  __asm__ ("addp %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmul_laneq_u32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint32x2_t result;                                               \
-       __asm__ ("mul %0.2s, %1.2s, %2.s[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vpadds_f32 (float32x2_t a)
+{
+  float32_t result;
+  __asm__ ("faddp %s0,%1.2s"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
 
 __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vmul_n_f32 (float32x2_t a, float32_t b)
+vpmax_f32 (float32x2_t a, float32x2_t b)
 {
   float32x2_t result;
-  __asm__ ("fmul %0.2s,%1.2s,%2.s[0]"
+  __asm__ ("fmaxp %0.2s, %1.2s, %2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vpmax_s8 (int8x8_t a, int8x8_t b)
+{
+  int8x8_t result;
+  __asm__ ("smaxp %0.8b, %1.8b, %2.8b"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -11780,10 +11667,10 @@ vmul_n_f32 (float32x2_t a, float32_t b)
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vmul_n_s16 (int16x4_t a, int16_t b)
+vpmax_s16 (int16x4_t a, int16x4_t b)
 {
   int16x4_t result;
-  __asm__ ("mul %0.4h,%1.4h,%2.h[0]"
+  __asm__ ("smaxp %0.4h, %1.4h, %2.4h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -11791,204 +11678,131 @@ vmul_n_s16 (int16x4_t a, int16_t b)
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vmul_n_s32 (int32x2_t a, int32_t b)
+vpmax_s32 (int32x2_t a, int32x2_t b)
 {
   int32x2_t result;
-  __asm__ ("mul %0.2s,%1.2s,%2.s[0]"
+  __asm__ ("smaxp %0.2s, %1.2s, %2.2s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vmul_n_u16 (uint16x4_t a, uint16_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vpmax_u8 (uint8x8_t a, uint8x8_t b)
 {
-  uint16x4_t result;
-  __asm__ ("mul %0.4h,%1.4h,%2.h[0]"
+  uint8x8_t result;
+  __asm__ ("umaxp %0.8b, %1.8b, %2.8b"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vmul_n_u32 (uint32x2_t a, uint32_t b)
-{
-  uint32x2_t result;
-  __asm__ ("mul %0.2s,%1.2s,%2.s[0]"
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vpmax_u16 (uint16x4_t a, uint16x4_t b)
+{
+  uint16x4_t result;
+  __asm__ ("umaxp %0.4h, %1.4h, %2.4h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmuld_lane_f64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64_t a_ = (a);                                              \
-       float64_t result;                                                \
-       __asm__ ("fmul %d0,%d1,%2.d[%3]"                                 \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmull_high_lane_s16(a, b, c)                                    \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16x8_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smull2 %0.4s, %1.8h, %2.h[%3]"                         \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmull_high_lane_s32(a, b, c)                                    \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smull2 %0.2d, %1.4s, %2.s[%3]"                         \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmull_high_lane_u16(a, b, c)                                    \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16x8_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("umull2 %0.4s, %1.8h, %2.h[%3]"                         \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmull_high_lane_u32(a, b, c)                                    \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umull2 %0.2d, %1.4s, %2.s[%3]"                         \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmull_high_laneq_s16(a, b, c)                                   \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16x8_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smull2 %0.4s, %1.8h, %2.h[%3]"                         \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmull_high_laneq_s32(a, b, c)                                   \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smull2 %0.2d, %1.4s, %2.s[%3]"                         \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmull_high_laneq_u16(a, b, c)                                   \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16x8_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("umull2 %0.4s, %1.8h, %2.h[%3]"                         \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vpmax_u32 (uint32x2_t a, uint32x2_t b)
+{
+  uint32x2_t result;
+  __asm__ ("umaxp %0.2s, %1.2s, %2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmull_high_laneq_u32(a, b, c)                                   \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umull2 %0.2d, %1.4s, %2.s[%3]"                         \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vpmaxnm_f32 (float32x2_t a, float32x2_t b)
+{
+  float32x2_t result;
+  __asm__ ("fmaxnmp %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmull_high_n_s16 (int16x8_t a, int16_t b)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vpmaxnmq_f32 (float32x4_t a, float32x4_t b)
 {
-  int32x4_t result;
-  __asm__ ("smull2 %0.4s,%1.8h,%2.h[0]"
+  float32x4_t result;
+  __asm__ ("fmaxnmp %0.4s,%1.4s,%2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmull_high_n_s32 (int32x4_t a, int32_t b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vpmaxnmq_f64 (float64x2_t a, float64x2_t b)
 {
-  int64x2_t result;
-  __asm__ ("smull2 %0.2d,%1.4s,%2.s[0]"
+  float64x2_t result;
+  __asm__ ("fmaxnmp %0.2d,%1.2d,%2.2d"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmull_high_n_u16 (uint16x8_t a, uint16_t b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vpmaxnmqd_f64 (float64x2_t a)
 {
-  uint32x4_t result;
-  __asm__ ("umull2 %0.4s,%1.8h,%2.h[0]"
+  float64_t result;
+  __asm__ ("fmaxnmp %d0,%1.2d"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vpmaxnms_f32 (float32x2_t a)
+{
+  float32_t result;
+  __asm__ ("fmaxnmp %s0,%1.2s"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vpmaxq_f32 (float32x4_t a, float32x4_t b)
+{
+  float32x4_t result;
+  __asm__ ("fmaxp %0.4s, %1.4s, %2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmull_high_n_u32 (uint32x4_t a, uint32_t b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vpmaxq_f64 (float64x2_t a, float64x2_t b)
 {
-  uint64x2_t result;
-  __asm__ ("umull2 %0.2d,%1.4s,%2.s[0]"
+  float64x2_t result;
+  __asm__ ("fmaxp %0.2d, %1.2d, %2.2d"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vmull_high_p8 (poly8x16_t a, poly8x16_t b)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vpmaxq_s8 (int8x16_t a, int8x16_t b)
 {
-  poly16x8_t result;
-  __asm__ ("pmull2 %0.8h,%1.16b,%2.16b"
+  int8x16_t result;
+  __asm__ ("smaxp %0.16b, %1.16b, %2.16b"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -11996,10 +11810,10 @@ vmull_high_p8 (poly8x16_t a, poly8x16_t
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmull_high_s8 (int8x16_t a, int8x16_t b)
+vpmaxq_s16 (int16x8_t a, int16x8_t b)
 {
   int16x8_t result;
-  __asm__ ("smull2 %0.8h,%1.16b,%2.16b"
+  __asm__ ("smaxp %0.8h, %1.8h, %2.8h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -12007,21 +11821,21 @@ vmull_high_s8 (int8x16_t a, int8x16_t b)
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmull_high_s16 (int16x8_t a, int16x8_t b)
+vpmaxq_s32 (int32x4_t a, int32x4_t b)
 {
   int32x4_t result;
-  __asm__ ("smull2 %0.4s,%1.8h,%2.8h"
+  __asm__ ("smaxp %0.4s, %1.4s, %2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmull_high_s32 (int32x4_t a, int32x4_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vpmaxq_u8 (uint8x16_t a, uint8x16_t b)
 {
-  int64x2_t result;
-  __asm__ ("smull2 %0.2d,%1.4s,%2.4s"
+  uint8x16_t result;
+  __asm__ ("umaxp %0.16b, %1.16b, %2.16b"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -12029,10 +11843,10 @@ vmull_high_s32 (int32x4_t a, int32x4_t b
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmull_high_u8 (uint8x16_t a, uint8x16_t b)
+vpmaxq_u16 (uint16x8_t a, uint16x8_t b)
 {
   uint16x8_t result;
-  __asm__ ("umull2 %0.8h,%1.16b,%2.16b"
+  __asm__ ("umaxp %0.8h, %1.8h, %2.8h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -12040,1032 +11854,1039 @@ vmull_high_u8 (uint8x16_t a, uint8x16_t
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmull_high_u16 (uint16x8_t a, uint16x8_t b)
+vpmaxq_u32 (uint32x4_t a, uint32x4_t b)
 {
   uint32x4_t result;
-  __asm__ ("umull2 %0.4s,%1.8h,%2.8h"
+  __asm__ ("umaxp %0.4s, %1.4s, %2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmull_high_u32 (uint32x4_t a, uint32x4_t b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vpmaxqd_f64 (float64x2_t a)
 {
-  uint64x2_t result;
-  __asm__ ("umull2 %0.2d,%1.4s,%2.4s"
+  float64_t result;
+  __asm__ ("fmaxp %d0,%1.2d"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-#define vmull_lane_s16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smull %0.4s,%1.4h,%2.h[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmull_lane_s32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smull %0.2d,%1.2s,%2.s[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmull_lane_u16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("umull %0.4s,%1.4h,%2.h[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vpmaxs_f32 (float32x2_t a)
+{
+  float32_t result;
+  __asm__ ("fmaxp %s0,%1.2s"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmull_lane_u32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umull %0.2d, %1.2s, %2.s[%3]"                          \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vpmin_f32 (float32x2_t a, float32x2_t b)
+{
+  float32x2_t result;
+  __asm__ ("fminp %0.2s, %1.2s, %2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmull_laneq_s16(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("smull %0.4s, %1.4h, %2.h[%3]"                          \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vpmin_s8 (int8x8_t a, int8x8_t b)
+{
+  int8x8_t result;
+  __asm__ ("sminp %0.8b, %1.8b, %2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmull_laneq_s32(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int64x2_t result;                                                \
-       __asm__ ("smull %0.2d, %1.2s, %2.s[%3]"                          \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vpmin_s16 (int16x4_t a, int16x4_t b)
+{
+  int16x4_t result;
+  __asm__ ("sminp %0.4h, %1.4h, %2.4h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmull_laneq_u16(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("umull %0.4s, %1.4h, %2.h[%3]"                          \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vpmin_s32 (int32x2_t a, int32x2_t b)
+{
+  int32x2_t result;
+  __asm__ ("sminp %0.2s, %1.2s, %2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vmull_laneq_u32(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint64x2_t result;                                               \
-       __asm__ ("umull %0.2d, %1.2s, %2.s[%3]"                          \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vpmin_u8 (uint8x8_t a, uint8x8_t b)
+{
+  uint8x8_t result;
+  __asm__ ("uminp %0.8b, %1.8b, %2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmull_n_s16 (int16x4_t a, int16_t b)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vpmin_u16 (uint16x4_t a, uint16x4_t b)
 {
-  int32x4_t result;
-  __asm__ ("smull %0.4s,%1.4h,%2.h[0]"
+  uint16x4_t result;
+  __asm__ ("uminp %0.4h, %1.4h, %2.4h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmull_n_s32 (int32x2_t a, int32_t b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vpmin_u32 (uint32x2_t a, uint32x2_t b)
 {
-  int64x2_t result;
-  __asm__ ("smull %0.2d,%1.2s,%2.s[0]"
+  uint32x2_t result;
+  __asm__ ("uminp %0.2s, %1.2s, %2.2s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmull_n_u16 (uint16x4_t a, uint16_t b)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vpminnm_f32 (float32x2_t a, float32x2_t b)
 {
-  uint32x4_t result;
-  __asm__ ("umull %0.4s,%1.4h,%2.h[0]"
+  float32x2_t result;
+  __asm__ ("fminnmp %0.2s,%1.2s,%2.2s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmull_n_u32 (uint32x2_t a, uint32_t b)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vpminnmq_f32 (float32x4_t a, float32x4_t b)
 {
-  uint64x2_t result;
-  __asm__ ("umull %0.2d,%1.2s,%2.s[0]"
+  float32x4_t result;
+  __asm__ ("fminnmp %0.4s,%1.4s,%2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vmull_p8 (poly8x8_t a, poly8x8_t b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vpminnmq_f64 (float64x2_t a, float64x2_t b)
 {
-  poly16x8_t result;
-  __asm__ ("pmull %0.8h, %1.8b, %2.8b"
+  float64x2_t result;
+  __asm__ ("fminnmp %0.2d,%1.2d,%2.2d"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmull_s8 (int8x8_t a, int8x8_t b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vpminnmqd_f64 (float64x2_t a)
 {
-  int16x8_t result;
-  __asm__ ("smull %0.8h, %1.8b, %2.8b"
+  float64_t result;
+  __asm__ ("fminnmp %d0,%1.2d"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vpminnms_f32 (float32x2_t a)
+{
+  float32_t result;
+  __asm__ ("fminnmp %s0,%1.2s"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vpminq_f32 (float32x4_t a, float32x4_t b)
+{
+  float32x4_t result;
+  __asm__ ("fminp %0.4s, %1.4s, %2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmull_s16 (int16x4_t a, int16x4_t b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vpminq_f64 (float64x2_t a, float64x2_t b)
 {
-  int32x4_t result;
-  __asm__ ("smull %0.4s, %1.4h, %2.4h"
+  float64x2_t result;
+  __asm__ ("fminp %0.2d, %1.2d, %2.2d"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vmull_s32 (int32x2_t a, int32x2_t b)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vpminq_s8 (int8x16_t a, int8x16_t b)
 {
-  int64x2_t result;
-  __asm__ ("smull %0.2d, %1.2s, %2.2s"
+  int8x16_t result;
+  __asm__ ("sminp %0.16b, %1.16b, %2.16b"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmull_u8 (uint8x8_t a, uint8x8_t b)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vpminq_s16 (int16x8_t a, int16x8_t b)
 {
-  uint16x8_t result;
-  __asm__ ("umull %0.8h, %1.8b, %2.8b"
+  int16x8_t result;
+  __asm__ ("sminp %0.8h, %1.8h, %2.8h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmull_u16 (uint16x4_t a, uint16x4_t b)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vpminq_s32 (int32x4_t a, int32x4_t b)
 {
-  uint32x4_t result;
-  __asm__ ("umull %0.4s, %1.4h, %2.4h"
+  int32x4_t result;
+  __asm__ ("sminp %0.4s, %1.4s, %2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vmull_u32 (uint32x2_t a, uint32x2_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vpminq_u8 (uint8x16_t a, uint8x16_t b)
 {
-  uint64x2_t result;
-  __asm__ ("umull %0.2d, %1.2s, %2.2s"
+  uint8x16_t result;
+  __asm__ ("uminp %0.16b, %1.16b, %2.16b"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmulq_lane_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t b_ = (b);                                            \
-       float32x4_t a_ = (a);                                            \
-       float32x4_t result;                                              \
-       __asm__ ("fmul %0.4s, %1.4s, %2.s[%3]"                           \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_lane_f64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x1_t b_ = (b);                                            \
-       float64x2_t a_ = (a);                                            \
-       float64x2_t result;                                              \
-       __asm__ ("fmul %0.2d,%1.2d,%2.d[%3]"                             \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_lane_s16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t b_ = (b);                                              \
-       int16x8_t a_ = (a);                                              \
-       int16x8_t result;                                                \
-       __asm__ ("mul %0.8h,%1.8h,%2.h[%3]"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_lane_s32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("mul %0.4s,%1.4s,%2.s[%3]"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_lane_u16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t b_ = (b);                                             \
-       uint16x8_t a_ = (a);                                             \
-       uint16x8_t result;                                               \
-       __asm__ ("mul %0.8h,%1.8h,%2.h[%3]"                              \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_lane_u32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("mul %0.4s, %1.4s, %2.s[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_laneq_f32(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32x4_t a_ = (a);                                            \
-       float32x4_t result;                                              \
-       __asm__ ("fmul %0.4s, %1.4s, %2.s[%3]"                           \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_laneq_f64(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64x2_t a_ = (a);                                            \
-       float64x2_t result;                                              \
-       __asm__ ("fmul %0.2d,%1.2d,%2.d[%3]"                             \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_laneq_s16(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16x8_t a_ = (a);                                              \
-       int16x8_t result;                                                \
-       __asm__ ("mul %0.8h, %1.8h, %2.h[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_laneq_s32(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32x4_t a_ = (a);                                              \
-       int32x4_t result;                                                \
-       __asm__ ("mul %0.4s, %1.4s, %2.s[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_laneq_u16(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16x8_t a_ = (a);                                             \
-       uint16x8_t result;                                               \
-       __asm__ ("mul %0.8h, %1.8h, %2.h[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulq_laneq_u32(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32x4_t a_ = (a);                                             \
-       uint32x4_t result;                                               \
-       __asm__ ("mul %0.4s, %1.4s, %2.s[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vmulq_n_f32 (float32x4_t a, float32_t b)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vpminq_u16 (uint16x8_t a, uint16x8_t b)
 {
-  float32x4_t result;
-  __asm__ ("fmul %0.4s,%1.4s,%2.s[0]"
+  uint16x8_t result;
+  __asm__ ("uminp %0.8h, %1.8h, %2.8h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vmulq_n_f64 (float64x2_t a, float64_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vpminq_u32 (uint32x4_t a, uint32x4_t b)
 {
-  float64x2_t result;
-  __asm__ ("fmul %0.2d,%1.2d,%2.d[0]"
+  uint32x4_t result;
+  __asm__ ("uminp %0.4s, %1.4s, %2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmulq_n_s16 (int16x8_t a, int16_t b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vpminqd_f64 (float64x2_t a)
 {
-  int16x8_t result;
-  __asm__ ("mul %0.8h,%1.8h,%2.h[0]"
+  float64_t result;
+  __asm__ ("fminp %d0,%1.2d"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmulq_n_s32 (int32x4_t a, int32_t b)
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vpmins_f32 (float32x2_t a)
 {
-  int32x4_t result;
-  __asm__ ("mul %0.4s,%1.4s,%2.s[0]"
+  float32_t result;
+  __asm__ ("fminp %s0,%1.2s"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmulq_n_u16 (uint16x8_t a, uint16_t b)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vqdmulh_n_s16 (int16x4_t a, int16_t b)
 {
-  uint16x8_t result;
-  __asm__ ("mul %0.8h,%1.8h,%2.h[0]"
+  int16x4_t result;
+  __asm__ ("sqdmulh %0.4h,%1.4h,%2.h[0]"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a), "x"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmulq_n_u32 (uint32x4_t a, uint32_t b)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vqdmulh_n_s32 (int32x2_t a, int32_t b)
 {
-  uint32x4_t result;
-  __asm__ ("mul %0.4s,%1.4s,%2.s[0]"
+  int32x2_t result;
+  __asm__ ("sqdmulh %0.2s,%1.2s,%2.s[0]"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmuls_lane_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32_t a_ = (a);                                              \
-       float32_t result;                                                \
-       __asm__ ("fmul %s0,%s1,%2.s[%3]"                                 \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vmulx_f32 (float32x2_t a, float32x2_t b)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vqdmulhq_n_s16 (int16x8_t a, int16_t b)
 {
-  float32x2_t result;
-  __asm__ ("fmulx %0.2s,%1.2s,%2.2s"
+  int16x8_t result;
+  __asm__ ("sqdmulh %0.8h,%1.8h,%2.h[0]"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a), "x"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmulx_lane_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32x2_t a_ = (a);                                            \
-       float32x2_t result;                                              \
-       __asm__ ("fmulx %0.2s,%1.2s,%2.s[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vmulxd_f64 (float64_t a, float64_t b)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vqdmulhq_n_s32 (int32x4_t a, int32_t b)
 {
-  float64_t result;
-  __asm__ ("fmulx %d0, %d1, %d2"
+  int32x4_t result;
+  __asm__ ("sqdmulh %0.4s,%1.4s,%2.s[0]"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vmulxq_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vqmovn_high_s16 (int8x8_t a, int16x8_t b)
 {
-  float32x4_t result;
-  __asm__ ("fmulx %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vmulxq_f64 (float64x2_t a, float64x2_t b)
-{
-  float64x2_t result;
-  __asm__ ("fmulx %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  int8x16_t result = vcombine_s8 (a, vcreate_s8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("sqxtn2 %0.16b, %1.8h"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vmulxq_lane_f32(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32x4_t a_ = (a);                                            \
-       float32x4_t result;                                              \
-       __asm__ ("fmulx %0.4s,%1.4s,%2.s[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vmulxq_lane_f64(a, b, c)                                        \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64x2_t a_ = (a);                                            \
-       float64x2_t result;                                              \
-       __asm__ ("fmulx %0.2d,%1.2d,%2.d[%3]"                            \
-                : "=w"(result)                                          \
-                : "w"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vmulxs_f32 (float32_t a, float32_t b)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vqmovn_high_s32 (int16x4_t a, int32x4_t b)
 {
-  float32_t result;
-  __asm__ ("fmulx %s0, %s1, %s2"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  int16x8_t result = vcombine_s16 (a, vcreate_s16 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("sqxtn2 %0.8h, %1.4s"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vmvn_p8 (poly8x8_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vqmovn_high_s64 (int32x2_t a, int64x2_t b)
 {
-  poly8x8_t result;
-  __asm__ ("mvn %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  int32x4_t result = vcombine_s32 (a, vcreate_s32 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("sqxtn2 %0.4s, %1.2d"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vmvn_s8 (int8x8_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqmovn_high_u16 (uint8x8_t a, uint16x8_t b)
 {
-  int8x8_t result;
-  __asm__ ("mvn %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("uqxtn2 %0.16b, %1.8h"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vmvn_s16 (int16x4_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vqmovn_high_u32 (uint16x4_t a, uint32x4_t b)
 {
-  int16x4_t result;
-  __asm__ ("mvn %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("uqxtn2 %0.8h, %1.4s"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vmvn_s32 (int32x2_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vqmovn_high_u64 (uint32x2_t a, uint64x2_t b)
 {
-  int32x2_t result;
-  __asm__ ("mvn %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("uqxtn2 %0.4s, %1.2d"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vmvn_u8 (uint8x8_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqmovun_high_s16 (uint8x8_t a, int16x8_t b)
 {
-  uint8x8_t result;
-  __asm__ ("mvn %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("sqxtun2 %0.16b, %1.8h"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vmvn_u16 (uint16x4_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vqmovun_high_s32 (uint16x4_t a, int32x4_t b)
 {
-  uint16x4_t result;
-  __asm__ ("mvn %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("sqxtun2 %0.8h, %1.4s"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vmvn_u32 (uint32x2_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vqmovun_high_s64 (uint32x2_t a, int64x2_t b)
 {
-  uint32x2_t result;
-  __asm__ ("mvn %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("sqxtun2 %0.4s, %1.2d"
+           : "+w"(result)
+           : "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vmvnq_p8 (poly8x16_t a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vqrdmulh_n_s16 (int16x4_t a, int16_t b)
 {
-  poly8x16_t result;
-  __asm__ ("mvn %0.16b,%1.16b"
+  int16x4_t result;
+  __asm__ ("sqrdmulh %0.4h,%1.4h,%2.h[0]"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vmvnq_s8 (int8x16_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vqrdmulh_n_s32 (int32x2_t a, int32_t b)
 {
-  int8x16_t result;
-  __asm__ ("mvn %0.16b,%1.16b"
+  int32x2_t result;
+  __asm__ ("sqrdmulh %0.2s,%1.2s,%2.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vmvnq_s16 (int16x8_t a)
+vqrdmulhq_n_s16 (int16x8_t a, int16_t b)
 {
   int16x8_t result;
-  __asm__ ("mvn %0.16b,%1.16b"
+  __asm__ ("sqrdmulh %0.8h,%1.8h,%2.h[0]"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vmvnq_s32 (int32x4_t a)
+vqrdmulhq_n_s32 (int32x4_t a, int32_t b)
 {
   int32x4_t result;
-  __asm__ ("mvn %0.16b,%1.16b"
+  __asm__ ("sqrdmulh %0.4s,%1.4s,%2.s[0]"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vmvnq_u8 (uint8x16_t a)
-{
-  uint8x16_t result;
-  __asm__ ("mvn %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vqrshrn_high_n_s16(a, b, c)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t b_ = (b);                                              \
+       int8x8_t a_ = (a);                                               \
+       int8x16_t result = vcombine_s8                                   \
+                            (a_, vcreate_s8                             \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("sqrshrn2 %0.16b, %1.8h, #%2"                           \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmvnq_u16 (uint16x8_t a)
-{
-  uint16x8_t result;
-  __asm__ ("mvn %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vqrshrn_high_n_s32(a, b, c)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t b_ = (b);                                              \
+       int16x4_t a_ = (a);                                              \
+       int16x8_t result = vcombine_s16                                  \
+                            (a_, vcreate_s16                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("sqrshrn2 %0.8h, %1.4s, #%2"                            \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmvnq_u32 (uint32x4_t a)
-{
-  uint32x4_t result;
-  __asm__ ("mvn %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vqrshrn_high_n_s64(a, b, c)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t b_ = (b);                                              \
+       int32x2_t a_ = (a);                                              \
+       int32x4_t result = vcombine_s32                                  \
+                            (a_, vcreate_s32                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("sqrshrn2 %0.4s, %1.2d, #%2"                            \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vneg_f32 (float32x2_t a)
-{
-  float32x2_t result;
-  __asm__ ("fneg %0.2s,%1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vqrshrn_high_n_u16(a, b, c)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t b_ = (b);                                             \
+       uint8x8_t a_ = (a);                                              \
+       uint8x16_t result = vcombine_u8                                  \
+                             (a_, vcreate_u8                            \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("uqrshrn2 %0.16b, %1.8h, #%2"                           \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vneg_s8 (int8x8_t a)
-{
-  int8x8_t result;
-  __asm__ ("neg %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vqrshrn_high_n_u32(a, b, c)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t b_ = (b);                                             \
+       uint16x4_t a_ = (a);                                             \
+       uint16x8_t result = vcombine_u16                                 \
+                             (a_, vcreate_u16                           \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("uqrshrn2 %0.8h, %1.4s, #%2"                            \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vneg_s16 (int16x4_t a)
+#define vqrshrn_high_n_u64(a, b, c)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t b_ = (b);                                             \
+       uint32x2_t a_ = (a);                                             \
+       uint32x4_t result = vcombine_u32                                 \
+                             (a_, vcreate_u32                           \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("uqrshrn2 %0.4s, %1.2d, #%2"                            \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqrshrun_high_n_s16(a, b, c)                                    \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t b_ = (b);                                              \
+       uint8x8_t a_ = (a);                                              \
+       uint8x16_t result = vcombine_u8                                  \
+                             (a_, vcreate_u8                            \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("sqrshrun2 %0.16b, %1.8h, #%2"                          \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqrshrun_high_n_s32(a, b, c)                                    \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t b_ = (b);                                              \
+       uint16x4_t a_ = (a);                                             \
+       uint16x8_t result = vcombine_u16                                 \
+                             (a_, vcreate_u16                           \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("sqrshrun2 %0.8h, %1.4s, #%2"                           \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqrshrun_high_n_s64(a, b, c)                                    \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t b_ = (b);                                              \
+       uint32x2_t a_ = (a);                                             \
+       uint32x4_t result = vcombine_u32                                 \
+                             (a_, vcreate_u32                           \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("sqrshrun2 %0.4s, %1.2d, #%2"                           \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqshrn_high_n_s16(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t b_ = (b);                                              \
+       int8x8_t a_ = (a);                                               \
+       int8x16_t result = vcombine_s8                                   \
+                            (a_, vcreate_s8                             \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("sqshrn2 %0.16b, %1.8h, #%2"                            \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqshrn_high_n_s32(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t b_ = (b);                                              \
+       int16x4_t a_ = (a);                                              \
+       int16x8_t result = vcombine_s16                                  \
+                            (a_, vcreate_s16                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("sqshrn2 %0.8h, %1.4s, #%2"                             \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqshrn_high_n_s64(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t b_ = (b);                                              \
+       int32x2_t a_ = (a);                                              \
+       int32x4_t result = vcombine_s32                                  \
+                            (a_, vcreate_s32                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("sqshrn2 %0.4s, %1.2d, #%2"                             \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqshrn_high_n_u16(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t b_ = (b);                                             \
+       uint8x8_t a_ = (a);                                              \
+       uint8x16_t result = vcombine_u8                                  \
+                             (a_, vcreate_u8                            \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("uqshrn2 %0.16b, %1.8h, #%2"                            \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqshrn_high_n_u32(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t b_ = (b);                                             \
+       uint16x4_t a_ = (a);                                             \
+       uint16x8_t result = vcombine_u16                                 \
+                             (a_, vcreate_u16                           \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("uqshrn2 %0.8h, %1.4s, #%2"                             \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqshrn_high_n_u64(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t b_ = (b);                                             \
+       uint32x2_t a_ = (a);                                             \
+       uint32x4_t result = vcombine_u32                                 \
+                             (a_, vcreate_u32                           \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("uqshrn2 %0.4s, %1.2d, #%2"                             \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqshrun_high_n_s16(a, b, c)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t b_ = (b);                                              \
+       uint8x8_t a_ = (a);                                              \
+       uint8x16_t result = vcombine_u8                                  \
+                             (a_, vcreate_u8                            \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("sqshrun2 %0.16b, %1.8h, #%2"                           \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqshrun_high_n_s32(a, b, c)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t b_ = (b);                                              \
+       uint16x4_t a_ = (a);                                             \
+       uint16x8_t result = vcombine_u16                                 \
+                             (a_, vcreate_u16                           \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("sqshrun2 %0.8h, %1.4s, #%2"                            \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vqshrun_high_n_s64(a, b, c)                                     \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t b_ = (b);                                              \
+       uint32x2_t a_ = (a);                                             \
+       uint32x4_t result = vcombine_u32                                 \
+                             (a_, vcreate_u32                           \
+                                    (__AARCH64_UINT64_C (0x0)));        \
+       __asm__ ("sqshrun2 %0.4s, %1.2d, #%2"                            \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vrbit_s8 (int8x8_t a)
 {
-  int16x4_t result;
-  __asm__ ("neg %0.4h,%1.4h"
+  int8x8_t result;
+  __asm__ ("rbit %0.8b,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vneg_s32 (int32x2_t a)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vrbit_u8 (uint8x8_t a)
 {
-  int32x2_t result;
-  __asm__ ("neg %0.2s,%1.2s"
+  uint8x8_t result;
+  __asm__ ("rbit %0.8b,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vnegq_f32 (float32x4_t a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vrbitq_s8 (int8x16_t a)
 {
-  float32x4_t result;
-  __asm__ ("fneg %0.4s,%1.4s"
+  int8x16_t result;
+  __asm__ ("rbit %0.16b,%1.16b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vnegq_f64 (float64x2_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vrbitq_u8 (uint8x16_t a)
 {
-  float64x2_t result;
-  __asm__ ("fneg %0.2d,%1.2d"
+  uint8x16_t result;
+  __asm__ ("rbit %0.16b,%1.16b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vnegq_s8 (int8x16_t a)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vrecpe_u32 (uint32x2_t a)
 {
-  int8x16_t result;
-  __asm__ ("neg %0.16b,%1.16b"
+  uint32x2_t result;
+  __asm__ ("urecpe %0.2s,%1.2s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vnegq_s16 (int16x8_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vrecpeq_u32 (uint32x4_t a)
 {
-  int16x8_t result;
-  __asm__ ("neg %0.8h,%1.8h"
+  uint32x4_t result;
+  __asm__ ("urecpe %0.4s,%1.4s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vnegq_s32 (int32x4_t a)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vrev16_p8 (poly8x8_t a)
 {
-  int32x4_t result;
-  __asm__ ("neg %0.4s,%1.4s"
+  poly8x8_t result;
+  __asm__ ("rev16 %0.8b,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vnegq_s64 (int64x2_t a)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vrev16_s8 (int8x8_t a)
 {
-  int64x2_t result;
-  __asm__ ("neg %0.2d,%1.2d"
+  int8x8_t result;
+  __asm__ ("rev16 %0.8b,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vpadal_s8 (int16x4_t a, int8x8_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vrev16_u8 (uint8x8_t a)
 {
-  int16x4_t result;
-  __asm__ ("sadalp %0.4h,%2.8b"
+  uint8x8_t result;
+  __asm__ ("rev16 %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vpadal_s16 (int32x2_t a, int16x4_t b)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vrev16q_p8 (poly8x16_t a)
 {
-  int32x2_t result;
-  __asm__ ("sadalp %0.2s,%2.4h"
+  poly8x16_t result;
+  __asm__ ("rev16 %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vpadal_s32 (int64x1_t a, int32x2_t b)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vrev16q_s8 (int8x16_t a)
 {
-  int64x1_t result;
-  __asm__ ("sadalp %0.1d,%2.2s"
+  int8x16_t result;
+  __asm__ ("rev16 %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vpadal_u8 (uint16x4_t a, uint8x8_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vrev16q_u8 (uint8x16_t a)
 {
-  uint16x4_t result;
-  __asm__ ("uadalp %0.4h,%2.8b"
+  uint8x16_t result;
+  __asm__ ("rev16 %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vpadal_u16 (uint32x2_t a, uint16x4_t b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vrev32_p8 (poly8x8_t a)
 {
-  uint32x2_t result;
-  __asm__ ("uadalp %0.2s,%2.4h"
+  poly8x8_t result;
+  __asm__ ("rev32 %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vpadal_u32 (uint64x1_t a, uint32x2_t b)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vrev32_p16 (poly16x4_t a)
 {
-  uint64x1_t result;
-  __asm__ ("uadalp %0.1d,%2.2s"
+  poly16x4_t result;
+  __asm__ ("rev32 %0.4h,%1.4h"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vpadalq_s8 (int16x8_t a, int8x16_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vrev32_s8 (int8x8_t a)
 {
-  int16x8_t result;
-  __asm__ ("sadalp %0.8h,%2.16b"
+  int8x8_t result;
+  __asm__ ("rev32 %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vpadalq_s16 (int32x4_t a, int16x8_t b)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vrev32_s16 (int16x4_t a)
 {
-  int32x4_t result;
-  __asm__ ("sadalp %0.4s,%2.8h"
+  int16x4_t result;
+  __asm__ ("rev32 %0.4h,%1.4h"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vpadalq_s32 (int64x2_t a, int32x4_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vrev32_u8 (uint8x8_t a)
 {
-  int64x2_t result;
-  __asm__ ("sadalp %0.2d,%2.4s"
+  uint8x8_t result;
+  __asm__ ("rev32 %0.8b,%1.8b"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vpadalq_u8 (uint16x8_t a, uint8x16_t b)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vrev32_u16 (uint16x4_t a)
 {
-  uint16x8_t result;
-  __asm__ ("uadalp %0.8h,%2.16b"
+  uint16x4_t result;
+  __asm__ ("rev32 %0.4h,%1.4h"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vpadalq_u16 (uint32x4_t a, uint16x8_t b)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vrev32q_p8 (poly8x16_t a)
 {
-  uint32x4_t result;
-  __asm__ ("uadalp %0.4s,%2.8h"
+  poly8x16_t result;
+  __asm__ ("rev32 %0.16b,%1.16b"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vpadalq_u32 (uint64x2_t a, uint32x4_t b)
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vrev32q_p16 (poly16x8_t a)
 {
-  uint64x2_t result;
-  __asm__ ("uadalp %0.2d,%2.4s"
+  poly16x8_t result;
+  __asm__ ("rev32 %0.8h,%1.8h"
            : "=w"(result)
-           : "0"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vpadd_f32 (float32x2_t a, float32x2_t b)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vrev32q_s8 (int8x16_t a)
 {
-  float32x2_t result;
-  __asm__ ("faddp %0.2s,%1.2s,%2.2s"
+  int8x16_t result;
+  __asm__ ("rev32 %0.16b,%1.16b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vpadd_s8 (int8x8_t __a, int8x8_t __b)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vrev32q_s16 (int16x8_t a)
 {
-  return __builtin_aarch64_addpv8qi (__a, __b);
+  int16x8_t result;
+  __asm__ ("rev32 %0.8h,%1.8h"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vpadd_s16 (int16x4_t __a, int16x4_t __b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vrev32q_u8 (uint8x16_t a)
 {
-  return __builtin_aarch64_addpv4hi (__a, __b);
+  uint8x16_t result;
+  __asm__ ("rev32 %0.16b,%1.16b"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vpadd_s32 (int32x2_t __a, int32x2_t __b)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vrev32q_u16 (uint16x8_t a)
 {
-  return __builtin_aarch64_addpv2si (__a, __b);
+  uint16x8_t result;
+  __asm__ ("rev32 %0.8h,%1.8h"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vpadd_u8 (uint8x8_t __a, uint8x8_t __b)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrev64_f32 (float32x2_t a)
 {
-  return (uint8x8_t) __builtin_aarch64_addpv8qi ((int8x8_t) __a,
-						 (int8x8_t) __b);
+  float32x2_t result;
+  __asm__ ("rev64 %0.2s,%1.2s"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vpadd_u16 (uint16x4_t __a, uint16x4_t __b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vrev64_p8 (poly8x8_t a)
 {
-  return (uint16x4_t) __builtin_aarch64_addpv4hi ((int16x4_t) __a,
-						  (int16x4_t) __b);
+  poly8x8_t result;
+  __asm__ ("rev64 %0.8b,%1.8b"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vpadd_u32 (uint32x2_t __a, uint32x2_t __b)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vrev64_p16 (poly16x4_t a)
 {
-  return (uint32x2_t) __builtin_aarch64_addpv2si ((int32x2_t) __a,
-						  (int32x2_t) __b);
+  poly16x4_t result;
+  __asm__ ("rev64 %0.4h,%1.4h"
+           : "=w"(result)
+           : "w"(a)
+           : /* No clobbers */);
+  return result;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vpaddd_f64 (float64x2_t a)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vrev64_s8 (int8x8_t a)
 {
-  float64_t result;
-  __asm__ ("faddp %d0,%1.2d"
+  int8x8_t result;
+  __asm__ ("rev64 %0.8b,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -13073,10 +12894,10 @@ vpaddd_f64 (float64x2_t a)
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vpaddl_s8 (int8x8_t a)
+vrev64_s16 (int16x4_t a)
 {
   int16x4_t result;
-  __asm__ ("saddlp %0.4h,%1.8b"
+  __asm__ ("rev64 %0.4h,%1.4h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -13084,21 +12905,21 @@ vpaddl_s8 (int8x8_t a)
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vpaddl_s16 (int16x4_t a)
+vrev64_s32 (int32x2_t a)
 {
   int32x2_t result;
-  __asm__ ("saddlp %0.2s,%1.4h"
+  __asm__ ("rev64 %0.2s,%1.2s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vpaddl_s32 (int32x2_t a)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vrev64_u8 (uint8x8_t a)
 {
-  int64x1_t result;
-  __asm__ ("saddlp %0.1d,%1.2s"
+  uint8x8_t result;
+  __asm__ ("rev64 %0.8b,%1.8b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -13106,10 +12927,10 @@ vpaddl_s32 (int32x2_t a)
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vpaddl_u8 (uint8x8_t a)
+vrev64_u16 (uint16x4_t a)
 {
   uint16x4_t result;
-  __asm__ ("uaddlp %0.4h,%1.8b"
+  __asm__ ("rev64 %0.4h,%1.4h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -13117,208 +12938,359 @@ vpaddl_u8 (uint8x8_t a)
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vpaddl_u16 (uint16x4_t a)
+vrev64_u32 (uint32x2_t a)
 {
   uint32x2_t result;
-  __asm__ ("uaddlp %0.2s,%1.4h"
+  __asm__ ("rev64 %0.2s,%1.2s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vpaddl_u32 (uint32x2_t a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrev64q_f32 (float32x4_t a)
 {
-  uint64x1_t result;
-  __asm__ ("uaddlp %0.1d,%1.2s"
+  float32x4_t result;
+  __asm__ ("rev64 %0.4s,%1.4s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vpaddlq_s8 (int8x16_t a)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vrev64q_p8 (poly8x16_t a)
 {
-  int16x8_t result;
-  __asm__ ("saddlp %0.8h,%1.16b"
+  poly8x16_t result;
+  __asm__ ("rev64 %0.16b,%1.16b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vpaddlq_s16 (int16x8_t a)
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vrev64q_p16 (poly16x8_t a)
 {
-  int32x4_t result;
-  __asm__ ("saddlp %0.4s,%1.8h"
+  poly16x8_t result;
+  __asm__ ("rev64 %0.8h,%1.8h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vpaddlq_s32 (int32x4_t a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vrev64q_s8 (int8x16_t a)
 {
-  int64x2_t result;
-  __asm__ ("saddlp %0.2d,%1.4s"
+  int8x16_t result;
+  __asm__ ("rev64 %0.16b,%1.16b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vpaddlq_u8 (uint8x16_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vrev64q_s16 (int16x8_t a)
 {
-  uint16x8_t result;
-  __asm__ ("uaddlp %0.8h,%1.16b"
+  int16x8_t result;
+  __asm__ ("rev64 %0.8h,%1.8h"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vpaddlq_u16 (uint16x8_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vrev64q_s32 (int32x4_t a)
 {
-  uint32x4_t result;
-  __asm__ ("uaddlp %0.4s,%1.8h"
+  int32x4_t result;
+  __asm__ ("rev64 %0.4s,%1.4s"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vpaddlq_u32 (uint32x4_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vrev64q_u8 (uint8x16_t a)
 {
-  uint64x2_t result;
-  __asm__ ("uaddlp %0.2d,%1.4s"
+  uint8x16_t result;
+  __asm__ ("rev64 %0.16b,%1.16b"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vpaddq_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vrev64q_u16 (uint16x8_t a)
 {
-  float32x4_t result;
-  __asm__ ("faddp %0.4s,%1.4s,%2.4s"
+  uint16x8_t result;
+  __asm__ ("rev64 %0.8h,%1.8h"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vpaddq_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vrev64q_u32 (uint32x4_t a)
 {
-  float64x2_t result;
-  __asm__ ("faddp %0.2d,%1.2d,%2.2d"
+  uint32x4_t result;
+  __asm__ ("rev64 %0.4s,%1.4s"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vpaddq_s8 (int8x16_t a, int8x16_t b)
+#define vrshrn_high_n_s16(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t b_ = (b);                                              \
+       int8x8_t a_ = (a);                                               \
+       int8x16_t result = vcombine_s8                                   \
+                            (a_, vcreate_s8                             \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("rshrn2 %0.16b,%1.8h,#%2"                               \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_high_n_s32(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t b_ = (b);                                              \
+       int16x4_t a_ = (a);                                              \
+       int16x8_t result = vcombine_s16                                  \
+                            (a_, vcreate_s16                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("rshrn2 %0.8h,%1.4s,#%2"                                \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_high_n_s64(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t b_ = (b);                                              \
+       int32x2_t a_ = (a);                                              \
+       int32x4_t result = vcombine_s32                                  \
+                            (a_, vcreate_s32                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("rshrn2 %0.4s,%1.2d,#%2"                                \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_high_n_u16(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t b_ = (b);                                             \
+       uint8x8_t a_ = (a);                                              \
+       uint8x16_t result = vcombine_u8                                  \
+                            (a_, vcreate_u8                             \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("rshrn2 %0.16b,%1.8h,#%2"                               \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_high_n_u32(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t b_ = (b);                                             \
+       uint16x4_t a_ = (a);                                             \
+       uint16x8_t result = vcombine_u16                                 \
+                            (a_, vcreate_u16                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("rshrn2 %0.8h,%1.4s,#%2"                                \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_high_n_u64(a, b, c)                                      \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t b_ = (b);                                             \
+       uint32x2_t a_ = (a);                                             \
+       uint32x4_t result = vcombine_u32                                 \
+                            (a_, vcreate_u32                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("rshrn2 %0.4s,%1.2d,#%2"                                \
+                : "+w"(result)                                          \
+                : "w"(b_), "i"(c)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_n_s16(a, b)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t a_ = (a);                                              \
+       int8x8_t result;                                                 \
+       __asm__ ("rshrn %0.8b,%1.8h,%2"                                  \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_n_s32(a, b)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t a_ = (a);                                              \
+       int16x4_t result;                                                \
+       __asm__ ("rshrn %0.4h,%1.4s,%2"                                  \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_n_s64(a, b)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t a_ = (a);                                              \
+       int32x2_t result;                                                \
+       __asm__ ("rshrn %0.2s,%1.2d,%2"                                  \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_n_u16(a, b)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t a_ = (a);                                             \
+       uint8x8_t result;                                                \
+       __asm__ ("rshrn %0.8b,%1.8h,%2"                                  \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_n_u32(a, b)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t a_ = (a);                                             \
+       uint16x4_t result;                                               \
+       __asm__ ("rshrn %0.4h,%1.4s,%2"                                  \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vrshrn_n_u64(a, b)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t a_ = (a);                                             \
+       uint32x2_t result;                                               \
+       __asm__ ("rshrn %0.2s,%1.2d,%2"                                  \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrsqrte_f32 (float32x2_t a)
 {
-  int8x16_t result;
-  __asm__ ("addp %0.16b,%1.16b,%2.16b"
+  float32x2_t result;
+  __asm__ ("frsqrte %0.2s,%1.2s"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vpaddq_s16 (int16x8_t a, int16x8_t b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrsqrte_f64 (float64x2_t a)
 {
-  int16x8_t result;
-  __asm__ ("addp %0.8h,%1.8h,%2.8h"
+  float64x2_t result;
+  __asm__ ("frsqrte %0.2d,%1.2d"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vpaddq_s32 (int32x4_t a, int32x4_t b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vrsqrte_u32 (uint32x2_t a)
 {
-  int32x4_t result;
-  __asm__ ("addp %0.4s,%1.4s,%2.4s"
+  uint32x2_t result;
+  __asm__ ("ursqrte %0.2s,%1.2s"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vpaddq_s64 (int64x2_t a, int64x2_t b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vrsqrted_f64 (float64_t a)
 {
-  int64x2_t result;
-  __asm__ ("addp %0.2d,%1.2d,%2.2d"
+  float64_t result;
+  __asm__ ("frsqrte %d0,%d1"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vpaddq_u8 (uint8x16_t a, uint8x16_t b)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrsqrteq_f32 (float32x4_t a)
 {
-  uint8x16_t result;
-  __asm__ ("addp %0.16b,%1.16b,%2.16b"
+  float32x4_t result;
+  __asm__ ("frsqrte %0.4s,%1.4s"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vpaddq_u16 (uint16x8_t a, uint16x8_t b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrsqrteq_f64 (float64x2_t a)
 {
-  uint16x8_t result;
-  __asm__ ("addp %0.8h,%1.8h,%2.8h"
+  float64x2_t result;
+  __asm__ ("frsqrte %0.2d,%1.2d"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vpaddq_u32 (uint32x4_t a, uint32x4_t b)
+vrsqrteq_u32 (uint32x4_t a)
 {
   uint32x4_t result;
-  __asm__ ("addp %0.4s,%1.4s,%2.4s"
+  __asm__ ("ursqrte %0.4s,%1.4s"
            : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vpaddq_u64 (uint64x2_t a, uint64x2_t b)
-{
-  uint64x2_t result;
-  __asm__ ("addp %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(a)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vpadds_f32 (float32x2_t a)
+vrsqrtes_f32 (float32_t a)
 {
   float32_t result;
-  __asm__ ("faddp %s0,%1.2s"
+  __asm__ ("frsqrte %s0,%s1"
            : "=w"(result)
            : "w"(a)
            : /* No clobbers */);
@@ -13326,87 +13298,21 @@ vpadds_f32 (float32x2_t a)
 }
 
 __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vpmax_f32 (float32x2_t a, float32x2_t b)
+vrsqrts_f32 (float32x2_t a, float32x2_t b)
 {
   float32x2_t result;
-  __asm__ ("fmaxp %0.2s, %1.2s, %2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vpmax_s8 (int8x8_t a, int8x8_t b)
-{
-  int8x8_t result;
-  __asm__ ("smaxp %0.8b, %1.8b, %2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vpmax_s16 (int16x4_t a, int16x4_t b)
-{
-  int16x4_t result;
-  __asm__ ("smaxp %0.4h, %1.4h, %2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vpmax_s32 (int32x2_t a, int32x2_t b)
-{
-  int32x2_t result;
-  __asm__ ("smaxp %0.2s, %1.2s, %2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vpmax_u8 (uint8x8_t a, uint8x8_t b)
-{
-  uint8x8_t result;
-  __asm__ ("umaxp %0.8b, %1.8b, %2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vpmax_u16 (uint16x4_t a, uint16x4_t b)
-{
-  uint16x4_t result;
-  __asm__ ("umaxp %0.4h, %1.4h, %2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vpmax_u32 (uint32x2_t a, uint32x2_t b)
-{
-  uint32x2_t result;
-  __asm__ ("umaxp %0.2s, %1.2s, %2.2s"
+  __asm__ ("frsqrts %0.2s,%1.2s,%2.2s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vpmaxnm_f32 (float32x2_t a, float32x2_t b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vrsqrtsd_f64 (float64_t a, float64_t b)
 {
-  float32x2_t result;
-  __asm__ ("fmaxnmp %0.2s,%1.2s,%2.2s"
+  float64_t result;
+  __asm__ ("frsqrts %d0,%d1,%d2"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -13414,10 +13320,10 @@ vpmaxnm_f32 (float32x2_t a, float32x2_t
 }
 
 __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vpmaxnmq_f32 (float32x4_t a, float32x4_t b)
+vrsqrtsq_f32 (float32x4_t a, float32x4_t b)
 {
   float32x4_t result;
-  __asm__ ("fmaxnmp %0.4s,%1.4s,%2.4s"
+  __asm__ ("frsqrts %0.4s,%1.4s,%2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -13425,43 +13331,21 @@ vpmaxnmq_f32 (float32x4_t a, float32x4_t
 }
 
 __extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vpmaxnmq_f64 (float64x2_t a, float64x2_t b)
+vrsqrtsq_f64 (float64x2_t a, float64x2_t b)
 {
   float64x2_t result;
-  __asm__ ("fmaxnmp %0.2d,%1.2d,%2.2d"
+  __asm__ ("frsqrts %0.2d,%1.2d,%2.2d"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vpmaxnmqd_f64 (float64x2_t a)
-{
-  float64_t result;
-  __asm__ ("fmaxnmp %d0,%1.2d"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
 __extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vpmaxnms_f32 (float32x2_t a)
+vrsqrtss_f32 (float32_t a, float32_t b)
 {
   float32_t result;
-  __asm__ ("fmaxnmp %s0,%1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vpmaxq_f32 (float32x4_t a, float32x4_t b)
-{
-  float32x4_t result;
-  __asm__ ("fmaxp %0.4s, %1.4s, %2.4s"
+  __asm__ ("frsqrts %s0,%s1,%s2"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -13469,10 +13353,10 @@ vpmaxq_f32 (float32x4_t a, float32x4_t b
 }
 
 __extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vpmaxq_f64 (float64x2_t a, float64x2_t b)
+vrsrtsq_f64 (float64x2_t a, float64x2_t b)
 {
   float64x2_t result;
-  __asm__ ("fmaxp %0.2d, %1.2d, %2.2d"
+  __asm__ ("frsqrts %0.2d,%1.2d,%2.2d"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -13480,109 +13364,76 @@ vpmaxq_f64 (float64x2_t a, float64x2_t b
 }
 
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vpmaxq_s8 (int8x16_t a, int8x16_t b)
+vrsubhn_high_s16 (int8x8_t a, int16x8_t b, int16x8_t c)
 {
-  int8x16_t result;
-  __asm__ ("smaxp %0.16b, %1.16b, %2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  int8x16_t result = vcombine_s8 (a, vcreate_s8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("rsubhn2 %0.16b, %1.8h, %2.8h"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vpmaxq_s16 (int16x8_t a, int16x8_t b)
+vrsubhn_high_s32 (int16x4_t a, int32x4_t b, int32x4_t c)
 {
-  int16x8_t result;
-  __asm__ ("smaxp %0.8h, %1.8h, %2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  int16x8_t result = vcombine_s16 (a, vcreate_s16 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("rsubhn2 %0.8h, %1.4s, %2.4s"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vpmaxq_s32 (int32x4_t a, int32x4_t b)
+vrsubhn_high_s64 (int32x2_t a, int64x2_t b, int64x2_t c)
 {
-  int32x4_t result;
-  __asm__ ("smaxp %0.4s, %1.4s, %2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  int32x4_t result = vcombine_s32 (a, vcreate_s32 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("rsubhn2 %0.4s, %1.2d, %2.2d"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vpmaxq_u8 (uint8x16_t a, uint8x16_t b)
+vrsubhn_high_u16 (uint8x8_t a, uint16x8_t b, uint16x8_t c)
 {
-  uint8x16_t result;
-  __asm__ ("umaxp %0.16b, %1.16b, %2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("rsubhn2 %0.16b, %1.8h, %2.8h"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vpmaxq_u16 (uint16x8_t a, uint16x8_t b)
+vrsubhn_high_u32 (uint16x4_t a, uint32x4_t b, uint32x4_t c)
 {
-  uint16x8_t result;
-  __asm__ ("umaxp %0.8h, %1.8h, %2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("rsubhn2 %0.8h, %1.4s, %2.4s"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vpmaxq_u32 (uint32x4_t a, uint32x4_t b)
-{
-  uint32x4_t result;
-  __asm__ ("umaxp %0.4s, %1.4s, %2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vpmaxqd_f64 (float64x2_t a)
-{
-  float64_t result;
-  __asm__ ("fmaxp %d0,%1.2d"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vpmaxs_f32 (float32x2_t a)
-{
-  float32_t result;
-  __asm__ ("fmaxp %s0,%1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vpmin_f32 (float32x2_t a, float32x2_t b)
+vrsubhn_high_u64 (uint32x2_t a, uint64x2_t b, uint64x2_t c)
 {
-  float32x2_t result;
-  __asm__ ("fminp %0.2s, %1.2s, %2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("rsubhn2 %0.4s, %1.2d, %2.2d"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vpmin_s8 (int8x8_t a, int8x8_t b)
+vrsubhn_s16 (int16x8_t a, int16x8_t b)
 {
   int8x8_t result;
-  __asm__ ("sminp %0.8b, %1.8b, %2.8b"
+  __asm__ ("rsubhn %0.8b, %1.8h, %2.8h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -13590,10 +13441,10 @@ vpmin_s8 (int8x8_t a, int8x8_t b)
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vpmin_s16 (int16x4_t a, int16x4_t b)
+vrsubhn_s32 (int32x4_t a, int32x4_t b)
 {
   int16x4_t result;
-  __asm__ ("sminp %0.4h, %1.4h, %2.4h"
+  __asm__ ("rsubhn %0.4h, %1.4s, %2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -13601,10 +13452,10 @@ vpmin_s16 (int16x4_t a, int16x4_t b)
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vpmin_s32 (int32x2_t a, int32x2_t b)
+vrsubhn_s64 (int64x2_t a, int64x2_t b)
 {
   int32x2_t result;
-  __asm__ ("sminp %0.2s, %1.2s, %2.2s"
+  __asm__ ("rsubhn %0.2s, %1.2d, %2.2d"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -13612,10 +13463,10 @@ vpmin_s32 (int32x2_t a, int32x2_t b)
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vpmin_u8 (uint8x8_t a, uint8x8_t b)
+vrsubhn_u16 (uint16x8_t a, uint16x8_t b)
 {
   uint8x8_t result;
-  __asm__ ("uminp %0.8b, %1.8b, %2.8b"
+  __asm__ ("rsubhn %0.8b, %1.8h, %2.8h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -13623,10 +13474,10 @@ vpmin_u8 (uint8x8_t a, uint8x8_t b)
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vpmin_u16 (uint16x4_t a, uint16x4_t b)
+vrsubhn_u32 (uint32x4_t a, uint32x4_t b)
 {
   uint16x4_t result;
-  __asm__ ("uminp %0.4h, %1.4h, %2.4h"
+  __asm__ ("rsubhn %0.4h, %1.4s, %2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -13634,1639 +13485,1589 @@ vpmin_u16 (uint16x4_t a, uint16x4_t b)
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vpmin_u32 (uint32x2_t a, uint32x2_t b)
+vrsubhn_u64 (uint64x2_t a, uint64x2_t b)
 {
   uint32x2_t result;
-  __asm__ ("uminp %0.2s, %1.2s, %2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vpminnm_f32 (float32x2_t a, float32x2_t b)
-{
-  float32x2_t result;
-  __asm__ ("fminnmp %0.2s,%1.2s,%2.2s"
+  __asm__ ("rsubhn %0.2s, %1.2d, %2.2d"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vpminnmq_f32 (float32x4_t a, float32x4_t b)
-{
-  float32x4_t result;
-  __asm__ ("fminnmp %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vpminnmq_f64 (float64x2_t a, float64x2_t b)
-{
-  float64x2_t result;
-  __asm__ ("fminnmp %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_f32(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x2_t b_ = (b);                                            \
+       float32_t a_ = (a);                                              \
+       float32x2_t result;                                              \
+       __asm__ ("ins %0.s[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vpminnmqd_f64 (float64x2_t a)
-{
-  float64_t result;
-  __asm__ ("fminnmp %d0,%1.2d"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_f64(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x1_t b_ = (b);                                            \
+       float64_t a_ = (a);                                              \
+       float64x1_t result;                                              \
+       __asm__ ("ins %0.d[%3], %x1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vpminnms_f32 (float32x2_t a)
-{
-  float32_t result;
-  __asm__ ("fminnmp %s0,%1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_p8(a, b, c)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x8_t b_ = (b);                                              \
+       poly8_t a_ = (a);                                                \
+       poly8x8_t result;                                                \
+       __asm__ ("ins %0.b[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vpminq_f32 (float32x4_t a, float32x4_t b)
-{
-  float32x4_t result;
-  __asm__ ("fminp %0.4s, %1.4s, %2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_p16(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x4_t b_ = (b);                                             \
+       poly16_t a_ = (a);                                               \
+       poly16x4_t result;                                               \
+       __asm__ ("ins %0.h[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vpminq_f64 (float64x2_t a, float64x2_t b)
-{
-  float64x2_t result;
-  __asm__ ("fminp %0.2d, %1.2d, %2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_s8(a, b, c)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       int8x8_t b_ = (b);                                               \
+       int8_t a_ = (a);                                                 \
+       int8x8_t result;                                                 \
+       __asm__ ("ins %0.b[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vpminq_s8 (int8x16_t a, int8x16_t b)
-{
-  int8x16_t result;
-  __asm__ ("sminp %0.16b, %1.16b, %2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_s16(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x4_t b_ = (b);                                              \
+       int16_t a_ = (a);                                                \
+       int16x4_t result;                                                \
+       __asm__ ("ins %0.h[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vpminq_s16 (int16x8_t a, int16x8_t b)
-{
-  int16x8_t result;
-  __asm__ ("sminp %0.8h, %1.8h, %2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_s32(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x2_t b_ = (b);                                              \
+       int32_t a_ = (a);                                                \
+       int32x2_t result;                                                \
+       __asm__ ("ins %0.s[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vpminq_s32 (int32x4_t a, int32x4_t b)
-{
-  int32x4_t result;
-  __asm__ ("sminp %0.4s, %1.4s, %2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_s64(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x1_t b_ = (b);                                              \
+       int64_t a_ = (a);                                                \
+       int64x1_t result;                                                \
+       __asm__ ("ins %0.d[%3], %x1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vpminq_u8 (uint8x16_t a, uint8x16_t b)
-{
-  uint8x16_t result;
-  __asm__ ("uminp %0.16b, %1.16b, %2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_u8(a, b, c)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       uint8x8_t b_ = (b);                                              \
+       uint8_t a_ = (a);                                                \
+       uint8x8_t result;                                                \
+       __asm__ ("ins %0.b[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vpminq_u16 (uint16x8_t a, uint16x8_t b)
-{
-  uint16x8_t result;
-  __asm__ ("uminp %0.8h, %1.8h, %2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_u16(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x4_t b_ = (b);                                             \
+       uint16_t a_ = (a);                                               \
+       uint16x4_t result;                                               \
+       __asm__ ("ins %0.h[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vpminq_u32 (uint32x4_t a, uint32x4_t b)
-{
-  uint32x4_t result;
-  __asm__ ("uminp %0.4s, %1.4s, %2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vset_lane_u32(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x2_t b_ = (b);                                             \
+       uint32_t a_ = (a);                                               \
+       uint32x2_t result;                                               \
+       __asm__ ("ins %0.s[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vpminqd_f64 (float64x2_t a)
-{
-  float64_t result;
-  __asm__ ("fminp %d0,%1.2d"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vpmins_f32 (float32x2_t a)
-{
-  float32_t result;
-  __asm__ ("fminp %s0,%1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vqdmulh_n_s16 (int16x4_t a, int16_t b)
-{
-  int16x4_t result;
-  __asm__ ("sqdmulh %0.4h,%1.4h,%2.h[0]"
-           : "=w"(result)
-           : "w"(a), "x"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vqdmulh_n_s32 (int32x2_t a, int32_t b)
-{
-  int32x2_t result;
-  __asm__ ("sqdmulh %0.2s,%1.2s,%2.s[0]"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vqdmulhq_n_s16 (int16x8_t a, int16_t b)
-{
-  int16x8_t result;
-  __asm__ ("sqdmulh %0.8h,%1.8h,%2.h[0]"
-           : "=w"(result)
-           : "w"(a), "x"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vqdmulhq_n_s32 (int32x4_t a, int32_t b)
-{
-  int32x4_t result;
-  __asm__ ("sqdmulh %0.4s,%1.4s,%2.s[0]"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vqmovn_high_s16 (int8x8_t a, int16x8_t b)
-{
-  int8x16_t result = vcombine_s8 (a, vcreate_s8 (UINT64_C (0x0)));
-  __asm__ ("sqxtn2 %0.16b, %1.8h"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vqmovn_high_s32 (int16x4_t a, int32x4_t b)
-{
-  int16x8_t result = vcombine_s16 (a, vcreate_s16 (UINT64_C (0x0)));
-  __asm__ ("sqxtn2 %0.8h, %1.4s"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vqmovn_high_s64 (int32x2_t a, int64x2_t b)
-{
-  int32x4_t result = vcombine_s32 (a, vcreate_s32 (UINT64_C (0x0)));
-  __asm__ ("sqxtn2 %0.4s, %1.2d"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqmovn_high_u16 (uint8x8_t a, uint16x8_t b)
-{
-  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (UINT64_C (0x0)));
-  __asm__ ("uqxtn2 %0.16b, %1.8h"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vqmovn_high_u32 (uint16x4_t a, uint32x4_t b)
-{
-  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (UINT64_C (0x0)));
-  __asm__ ("uqxtn2 %0.8h, %1.4s"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vqmovn_high_u64 (uint32x2_t a, uint64x2_t b)
-{
-  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (UINT64_C (0x0)));
-  __asm__ ("uqxtn2 %0.4s, %1.2d"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqmovun_high_s16 (uint8x8_t a, int16x8_t b)
-{
-  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (UINT64_C (0x0)));
-  __asm__ ("sqxtun2 %0.16b, %1.8h"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vqmovun_high_s32 (uint16x4_t a, int32x4_t b)
-{
-  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (UINT64_C (0x0)));
-  __asm__ ("sqxtun2 %0.8h, %1.4s"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vqmovun_high_s64 (uint32x2_t a, int64x2_t b)
-{
-  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (UINT64_C (0x0)));
-  __asm__ ("sqxtun2 %0.4s, %1.2d"
-           : "+w"(result)
-           : "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vqrdmulh_n_s16 (int16x4_t a, int16_t b)
-{
-  int16x4_t result;
-  __asm__ ("sqrdmulh %0.4h,%1.4h,%2.h[0]"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vqrdmulh_n_s32 (int32x2_t a, int32_t b)
-{
-  int32x2_t result;
-  __asm__ ("sqrdmulh %0.2s,%1.2s,%2.s[0]"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vqrdmulhq_n_s16 (int16x8_t a, int16_t b)
-{
-  int16x8_t result;
-  __asm__ ("sqrdmulh %0.8h,%1.8h,%2.h[0]"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vqrdmulhq_n_s32 (int32x4_t a, int32_t b)
-{
-  int32x4_t result;
-  __asm__ ("sqrdmulh %0.4s,%1.4s,%2.s[0]"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
-
-#define vqrshrn_high_n_s16(a, b, c)                                     \
+#define vset_lane_u64(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int8x8_t a_ = (a);                                               \
-       int8x16_t result = vcombine_s8                                   \
-                            (a_, vcreate_s8 (UINT64_C (0x0)));          \
-       __asm__ ("sqrshrn2 %0.16b, %1.8h, #%2"                           \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       uint64x1_t b_ = (b);                                             \
+       uint64_t a_ = (a);                                               \
+       uint64x1_t result;                                               \
+       __asm__ ("ins %0.d[%3], %x1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqrshrn_high_n_s32(a, b, c)                                     \
+#define vsetq_lane_f32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int16x8_t result = vcombine_s16                                  \
-                            (a_, vcreate_s16 (UINT64_C (0x0)));         \
-       __asm__ ("sqrshrn2 %0.8h, %1.4s, #%2"                            \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       float32x4_t b_ = (b);                                            \
+       float32_t a_ = (a);                                              \
+       float32x4_t result;                                              \
+       __asm__ ("ins %0.s[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqrshrn_high_n_s64(a, b, c)                                     \
+#define vsetq_lane_f64(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int32x4_t result = vcombine_s32                                  \
-                            (a_, vcreate_s32 (UINT64_C (0x0)));         \
-       __asm__ ("sqrshrn2 %0.4s, %1.2d, #%2"                            \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       float64x2_t b_ = (b);                                            \
+       float64_t a_ = (a);                                              \
+       float64x2_t result;                                              \
+       __asm__ ("ins %0.d[%3], %x1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqrshrn_high_n_u16(a, b, c)                                     \
+#define vsetq_lane_p8(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint8x8_t a_ = (a);                                              \
-       uint8x16_t result = vcombine_u8                                  \
-                             (a_, vcreate_u8 (UINT64_C (0x0)));         \
-       __asm__ ("uqrshrn2 %0.16b, %1.8h, #%2"                           \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       poly8x16_t b_ = (b);                                             \
+       poly8_t a_ = (a);                                                \
+       poly8x16_t result;                                               \
+       __asm__ ("ins %0.b[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqrshrn_high_n_u32(a, b, c)                                     \
+#define vsetq_lane_p16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint16x8_t result = vcombine_u16                                 \
-                             (a_, vcreate_u16 (UINT64_C (0x0)));        \
-       __asm__ ("uqrshrn2 %0.8h, %1.4s, #%2"                            \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       poly16x8_t b_ = (b);                                             \
+       poly16_t a_ = (a);                                               \
+       poly16x8_t result;                                               \
+       __asm__ ("ins %0.h[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqrshrn_high_n_u64(a, b, c)                                     \
+#define vsetq_lane_s8(a, b, c)                                          \
   __extension__                                                         \
     ({                                                                  \
-       uint64x2_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint32x4_t result = vcombine_u32                                 \
-                             (a_, vcreate_u32 (UINT64_C (0x0)));        \
-       __asm__ ("uqrshrn2 %0.4s, %1.2d, #%2"                            \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       int8x16_t b_ = (b);                                              \
+       int8_t a_ = (a);                                                 \
+       int8x16_t result;                                                \
+       __asm__ ("ins %0.b[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqrshrun_high_n_s16(a, b, c)                                    \
+#define vsetq_lane_s16(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
        int16x8_t b_ = (b);                                              \
-       uint8x8_t a_ = (a);                                              \
-       uint8x16_t result = vcombine_u8                                  \
-                             (a_, vcreate_u8 (UINT64_C (0x0)));         \
-       __asm__ ("sqrshrun2 %0.16b, %1.8h, #%2"                          \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       int16_t a_ = (a);                                                \
+       int16x8_t result;                                                \
+       __asm__ ("ins %0.h[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqrshrun_high_n_s32(a, b, c)                                    \
+#define vsetq_lane_s32(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
        int32x4_t b_ = (b);                                              \
-       uint16x4_t a_ = (a);                                             \
-       uint16x8_t result = vcombine_u16                                 \
-                             (a_, vcreate_u16 (UINT64_C (0x0)));        \
-       __asm__ ("sqrshrun2 %0.8h, %1.4s, #%2"                           \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       int32_t a_ = (a);                                                \
+       int32x4_t result;                                                \
+       __asm__ ("ins %0.s[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqrshrun_high_n_s64(a, b, c)                                    \
+#define vsetq_lane_s64(a, b, c)                                         \
   __extension__                                                         \
     ({                                                                  \
        int64x2_t b_ = (b);                                              \
-       uint32x2_t a_ = (a);                                             \
-       uint32x4_t result = vcombine_u32                                 \
-                             (a_, vcreate_u32 (UINT64_C (0x0)));        \
-       __asm__ ("sqrshrun2 %0.4s, %1.2d, #%2"                           \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       int64_t a_ = (a);                                                \
+       int64x2_t result;                                                \
+       __asm__ ("ins %0.d[%3], %x1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqshrn_high_n_s16(a, b, c)                                      \
+#define vsetq_lane_u8(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       uint8x16_t b_ = (b);                                             \
+       uint8_t a_ = (a);                                                \
+       uint8x16_t result;                                               \
+       __asm__ ("ins %0.b[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vsetq_lane_u16(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t b_ = (b);                                             \
+       uint16_t a_ = (a);                                               \
+       uint16x8_t result;                                               \
+       __asm__ ("ins %0.h[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vsetq_lane_u32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t b_ = (b);                                             \
+       uint32_t a_ = (a);                                               \
+       uint32x4_t result;                                               \
+       __asm__ ("ins %0.s[%3], %w1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vsetq_lane_u64(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t b_ = (b);                                             \
+       uint64_t a_ = (a);                                               \
+       uint64x2_t result;                                               \
+       __asm__ ("ins %0.d[%3], %x1"                                     \
+                : "=w"(result)                                          \
+                : "r"(a_), "0"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
+
+#define vshrn_high_n_s16(a, b, c)                                       \
   __extension__                                                         \
     ({                                                                  \
        int16x8_t b_ = (b);                                              \
        int8x8_t a_ = (a);                                               \
        int8x16_t result = vcombine_s8                                   \
-                            (a_, vcreate_s8 (UINT64_C (0x0)));          \
-       __asm__ ("sqshrn2 %0.16b, %1.8h, #%2"                            \
+                            (a_, vcreate_s8                             \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("shrn2 %0.16b,%1.8h,#%2"                                \
                 : "+w"(result)                                          \
                 : "w"(b_), "i"(c)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqshrn_high_n_s32(a, b, c)                                      \
+#define vshrn_high_n_s32(a, b, c)                                       \
   __extension__                                                         \
     ({                                                                  \
        int32x4_t b_ = (b);                                              \
        int16x4_t a_ = (a);                                              \
        int16x8_t result = vcombine_s16                                  \
-                            (a_, vcreate_s16 (UINT64_C (0x0)));         \
-       __asm__ ("sqshrn2 %0.8h, %1.4s, #%2"                             \
+                            (a_, vcreate_s16                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("shrn2 %0.8h,%1.4s,#%2"                                 \
                 : "+w"(result)                                          \
                 : "w"(b_), "i"(c)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqshrn_high_n_s64(a, b, c)                                      \
+#define vshrn_high_n_s64(a, b, c)                                       \
   __extension__                                                         \
     ({                                                                  \
        int64x2_t b_ = (b);                                              \
        int32x2_t a_ = (a);                                              \
        int32x4_t result = vcombine_s32                                  \
-                            (a_, vcreate_s32 (UINT64_C (0x0)));         \
-       __asm__ ("sqshrn2 %0.4s, %1.2d, #%2"                             \
+                            (a_, vcreate_s32                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("shrn2 %0.4s,%1.2d,#%2"                                 \
                 : "+w"(result)                                          \
                 : "w"(b_), "i"(c)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqshrn_high_n_u16(a, b, c)                                      \
+#define vshrn_high_n_u16(a, b, c)                                       \
   __extension__                                                         \
     ({                                                                  \
        uint16x8_t b_ = (b);                                             \
        uint8x8_t a_ = (a);                                              \
        uint8x16_t result = vcombine_u8                                  \
-                             (a_, vcreate_u8 (UINT64_C (0x0)));         \
-       __asm__ ("uqshrn2 %0.16b, %1.8h, #%2"                            \
+                            (a_, vcreate_u8                             \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("shrn2 %0.16b,%1.8h,#%2"                                \
                 : "+w"(result)                                          \
                 : "w"(b_), "i"(c)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqshrn_high_n_u32(a, b, c)                                      \
+#define vshrn_high_n_u32(a, b, c)                                       \
   __extension__                                                         \
     ({                                                                  \
        uint32x4_t b_ = (b);                                             \
        uint16x4_t a_ = (a);                                             \
        uint16x8_t result = vcombine_u16                                 \
-                             (a_, vcreate_u16 (UINT64_C (0x0)));        \
-       __asm__ ("uqshrn2 %0.8h, %1.4s, #%2"                             \
+                            (a_, vcreate_u16                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("shrn2 %0.8h,%1.4s,#%2"                                 \
                 : "+w"(result)                                          \
                 : "w"(b_), "i"(c)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqshrn_high_n_u64(a, b, c)                                      \
+#define vshrn_high_n_u64(a, b, c)                                       \
   __extension__                                                         \
     ({                                                                  \
        uint64x2_t b_ = (b);                                             \
        uint32x2_t a_ = (a);                                             \
        uint32x4_t result = vcombine_u32                                 \
-                             (a_, vcreate_u32 (UINT64_C (0x0)));        \
-       __asm__ ("uqshrn2 %0.4s, %1.2d, #%2"                             \
+                            (a_, vcreate_u32                            \
+                                   (__AARCH64_UINT64_C (0x0)));         \
+       __asm__ ("shrn2 %0.4s,%1.2d,#%2"                                 \
                 : "+w"(result)                                          \
                 : "w"(b_), "i"(c)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqshrun_high_n_s16(a, b, c)                                     \
+#define vshrn_n_s16(a, b)                                               \
   __extension__                                                         \
     ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       uint8x8_t a_ = (a);                                              \
-       uint8x16_t result = vcombine_u8                                  \
-                             (a_, vcreate_u8 (UINT64_C (0x0)));         \
-       __asm__ ("sqshrun2 %0.16b, %1.8h, #%2"                           \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       int16x8_t a_ = (a);                                              \
+       int8x8_t result;                                                 \
+       __asm__ ("shrn %0.8b,%1.8h,%2"                                   \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqshrun_high_n_s32(a, b, c)                                     \
+#define vshrn_n_s32(a, b)                                               \
   __extension__                                                         \
     ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       uint16x4_t a_ = (a);                                             \
-       uint16x8_t result = vcombine_u16                                 \
-                             (a_, vcreate_u16 (UINT64_C (0x0)));        \
-       __asm__ ("sqshrun2 %0.8h, %1.4s, #%2"                            \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       int32x4_t a_ = (a);                                              \
+       int16x4_t result;                                                \
+       __asm__ ("shrn %0.4h,%1.4s,%2"                                   \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-#define vqshrun_high_n_s64(a, b, c)                                     \
+#define vshrn_n_s64(a, b)                                               \
   __extension__                                                         \
     ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       uint32x2_t a_ = (a);                                             \
-       uint32x4_t result = vcombine_u32                                 \
-                             (a_, vcreate_u32 (UINT64_C (0x0)));        \
-       __asm__ ("sqshrun2 %0.4s, %1.2d, #%2"                            \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
+       int64x2_t a_ = (a);                                              \
+       int32x2_t result;                                                \
+       __asm__ ("shrn %0.2s,%1.2d,%2"                                   \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
                 : /* No clobbers */);                                   \
        result;                                                          \
      })
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vrbit_s8 (int8x8_t a)
-{
-  int8x8_t result;
-  __asm__ ("rbit %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vrbit_u8 (uint8x8_t a)
-{
-  uint8x8_t result;
-  __asm__ ("rbit %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vshrn_n_u16(a, b)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t a_ = (a);                                             \
+       uint8x8_t result;                                                \
+       __asm__ ("shrn %0.8b,%1.8h,%2"                                   \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vrbitq_s8 (int8x16_t a)
-{
-  int8x16_t result;
-  __asm__ ("rbit %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vshrn_n_u32(a, b)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t a_ = (a);                                             \
+       uint16x4_t result;                                               \
+       __asm__ ("shrn %0.4h,%1.4s,%2"                                   \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vrbitq_u8 (uint8x16_t a)
-{
-  uint8x16_t result;
-  __asm__ ("rbit %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vshrn_n_u64(a, b)                                               \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t a_ = (a);                                             \
+       uint32x2_t result;                                               \
+       __asm__ ("shrn %0.2s,%1.2d,%2"                                   \
+                : "=w"(result)                                          \
+                : "w"(a_), "i"(b)                                       \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrecpe_f32 (float32x2_t a)
-{
-  float32x2_t result;
-  __asm__ ("frecpe %0.2s,%1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vrecpe_u32 (uint32x2_t a)
-{
-  uint32x2_t result;
-  __asm__ ("urecpe %0.2s,%1.2s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vsli_n_p8(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x8_t b_ = (b);                                              \
+       poly8x8_t a_ = (a);                                              \
+       poly8x8_t result;                                                \
+       __asm__ ("sli %0.8b,%2.8b,%3"                                    \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vrecped_f64 (float64_t a)
-{
-  float64_t result;
-  __asm__ ("frecpe %d0,%d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vsli_n_p16(a, b, c)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x4_t b_ = (b);                                             \
+       poly16x4_t a_ = (a);                                             \
+       poly16x4_t result;                                               \
+       __asm__ ("sli %0.4h,%2.4h,%3"                                    \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrecpeq_f32 (float32x4_t a)
-{
-  float32x4_t result;
-  __asm__ ("frecpe %0.4s,%1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vsliq_n_p8(a, b, c)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x16_t b_ = (b);                                             \
+       poly8x16_t a_ = (a);                                             \
+       poly8x16_t result;                                               \
+       __asm__ ("sli %0.16b,%2.16b,%3"                                  \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrecpeq_f64 (float64x2_t a)
-{
-  float64x2_t result;
-  __asm__ ("frecpe %0.2d,%1.2d"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vsliq_n_p16(a, b, c)                                            \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x8_t b_ = (b);                                             \
+       poly16x8_t a_ = (a);                                             \
+       poly16x8_t result;                                               \
+       __asm__ ("sli %0.8h,%2.8h,%3"                                    \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vrecpeq_u32 (uint32x4_t a)
-{
-  uint32x4_t result;
-  __asm__ ("urecpe %0.4s,%1.4s"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vsri_n_p8(a, b, c)                                              \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x8_t b_ = (b);                                              \
+       poly8x8_t a_ = (a);                                              \
+       poly8x8_t result;                                                \
+       __asm__ ("sri %0.8b,%2.8b,%3"                                    \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vrecpes_f32 (float32_t a)
-{
-  float32_t result;
-  __asm__ ("frecpe %s0,%s1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vsri_n_p16(a, b, c)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x4_t b_ = (b);                                             \
+       poly16x4_t a_ = (a);                                             \
+       poly16x4_t result;                                               \
+       __asm__ ("sri %0.4h,%2.4h,%3"                                    \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrecps_f32 (float32x2_t a, float32x2_t b)
-{
-  float32x2_t result;
-  __asm__ ("frecps %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vsriq_n_p8(a, b, c)                                             \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x16_t b_ = (b);                                             \
+       poly8x16_t a_ = (a);                                             \
+       poly8x16_t result;                                               \
+       __asm__ ("sri %0.16b,%2.16b,%3"                                  \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vrecpsd_f64 (float64_t a, float64_t b)
-{
-  float64_t result;
-  __asm__ ("frecps %d0,%d1,%d2"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vsriq_n_p16(a, b, c)                                            \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x8_t b_ = (b);                                             \
+       poly16x8_t a_ = (a);                                             \
+       poly16x8_t result;                                               \
+       __asm__ ("sri %0.8h,%2.8h,%3"                                    \
+                : "=w"(result)                                          \
+                : "0"(a_), "w"(b_), "i"(c)                              \
+                : /* No clobbers */);                                   \
+       result;                                                          \
+     })
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrecpsq_f32 (float32x4_t a, float32x4_t b)
-{
-  float32x4_t result;
-  __asm__ ("frecps %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vst1_lane_f32(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x2_t b_ = (b);                                            \
+       float32_t * a_ = (a);                                            \
+       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrecpsq_f64 (float64x2_t a, float64x2_t b)
-{
-  float64x2_t result;
-  __asm__ ("frecps %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vst1_lane_f64(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x1_t b_ = (b);                                            \
+       float64_t * a_ = (a);                                            \
+       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vrecpss_f32 (float32_t a, float32_t b)
-{
-  float32_t result;
-  __asm__ ("frecps %s0,%s1,%s2"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
-}
+#define vst1_lane_p8(a, b, c)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x8_t b_ = (b);                                              \
+       poly8_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vrecpxd_f64 (float64_t a)
-{
-  float64_t result;
-  __asm__ ("frecpe %d0,%d1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vst1_lane_p16(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x4_t b_ = (b);                                             \
+       poly16_t * a_ = (a);                                             \
+       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vrecpxs_f32 (float32_t a)
-{
-  float32_t result;
-  __asm__ ("frecpe %s0,%s1"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
+#define vst1_lane_s8(a, b, c)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       int8x8_t b_ = (b);                                               \
+       int8_t * a_ = (a);                                               \
+       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vrev16_p8 (poly8x8_t a)
+#define vst1_lane_s16(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x4_t b_ = (b);                                              \
+       int16_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1_lane_s32(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x2_t b_ = (b);                                              \
+       int32_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1_lane_s64(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x1_t b_ = (b);                                              \
+       int64_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1_lane_u8(a, b, c)                                           \
+  __extension__                                                         \
+    ({                                                                  \
+       uint8x8_t b_ = (b);                                              \
+       uint8_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1_lane_u16(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x4_t b_ = (b);                                             \
+       uint16_t * a_ = (a);                                             \
+       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1_lane_u32(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x2_t b_ = (b);                                             \
+       uint32_t * a_ = (a);                                             \
+       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1_lane_u64(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x1_t b_ = (b);                                             \
+       uint64_t * a_ = (a);                                             \
+       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+
+#define vst1q_lane_f32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float32x4_t b_ = (b);                                            \
+       float32_t * a_ = (a);                                            \
+       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_f64(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       float64x2_t b_ = (b);                                            \
+       float64_t * a_ = (a);                                            \
+       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_p8(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       poly8x16_t b_ = (b);                                             \
+       poly8_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_p16(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       poly16x8_t b_ = (b);                                             \
+       poly16_t * a_ = (a);                                             \
+       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_s8(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       int8x16_t b_ = (b);                                              \
+       int8_t * a_ = (a);                                               \
+       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_s16(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       int16x8_t b_ = (b);                                              \
+       int16_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_s32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       int32x4_t b_ = (b);                                              \
+       int32_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_s64(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       int64x2_t b_ = (b);                                              \
+       int64_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_u8(a, b, c)                                          \
+  __extension__                                                         \
+    ({                                                                  \
+       uint8x16_t b_ = (b);                                             \
+       uint8_t * a_ = (a);                                              \
+       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_u16(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       uint16x8_t b_ = (b);                                             \
+       uint16_t * a_ = (a);                                             \
+       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_u32(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       uint32x4_t b_ = (b);                                             \
+       uint32_t * a_ = (a);                                             \
+       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+#define vst1q_lane_u64(a, b, c)                                         \
+  __extension__                                                         \
+    ({                                                                  \
+       uint64x2_t b_ = (b);                                             \
+       uint64_t * a_ = (a);                                             \
+       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
+                :                                                       \
+                : "r"(a_), "w"(b_), "i"(c)                              \
+                : "memory");                                            \
+     })
+
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vsubhn_high_s16 (int8x8_t a, int16x8_t b, int16x8_t c)
 {
-  poly8x8_t result;
-  __asm__ ("rev16 %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  int8x16_t result = vcombine_s8 (a, vcreate_s8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("subhn2 %0.16b, %1.8h, %2.8h"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vrev16_s8 (int8x8_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vsubhn_high_s32 (int16x4_t a, int32x4_t b, int32x4_t c)
 {
-  int8x8_t result;
-  __asm__ ("rev16 %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  int16x8_t result = vcombine_s16 (a, vcreate_s16 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("subhn2 %0.8h, %1.4s, %2.4s"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vrev16_u8 (uint8x8_t a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vsubhn_high_s64 (int32x2_t a, int64x2_t b, int64x2_t c)
 {
-  uint8x8_t result;
-  __asm__ ("rev16 %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  int32x4_t result = vcombine_s32 (a, vcreate_s32 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("subhn2 %0.4s, %1.2d, %2.2d"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vrev16q_p8 (poly8x16_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vsubhn_high_u16 (uint8x8_t a, uint16x8_t b, uint16x8_t c)
 {
-  poly8x16_t result;
-  __asm__ ("rev16 %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
+  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("subhn2 %0.16b, %1.8h, %2.8h"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vrev16q_s8 (int8x16_t a)
-{
-  int8x16_t result;
-  __asm__ ("rev16 %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vrev16q_u8 (uint8x16_t a)
-{
-  uint8x16_t result;
-  __asm__ ("rev16 %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vrev32_p8 (poly8x8_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vsubhn_high_u32 (uint16x4_t a, uint32x4_t b, uint32x4_t c)
 {
-  poly8x8_t result;
-  __asm__ ("rev32 %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
+  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("subhn2 %0.8h, %1.4s, %2.4s"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vrev32_p16 (poly16x4_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsubhn_high_u64 (uint32x2_t a, uint64x2_t b, uint64x2_t c)
 {
-  poly16x4_t result;
-  __asm__ ("rev32 %0.4h,%1.4h"
-           : "=w"(result)
-           : "w"(a)
+  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("subhn2 %0.4s, %1.2d, %2.2d"
+           : "+w"(result)
+           : "w"(b), "w"(c)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vrev32_s8 (int8x8_t a)
+vsubhn_s16 (int16x8_t a, int16x8_t b)
 {
   int8x8_t result;
-  __asm__ ("rev32 %0.8b,%1.8b"
+  __asm__ ("subhn %0.8b, %1.8h, %2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vrev32_s16 (int16x4_t a)
+vsubhn_s32 (int32x4_t a, int32x4_t b)
 {
   int16x4_t result;
-  __asm__ ("rev32 %0.4h,%1.4h"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vrev32_u8 (uint8x8_t a)
-{
-  uint8x8_t result;
-  __asm__ ("rev32 %0.8b,%1.8b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vrev32_u16 (uint16x4_t a)
-{
-  uint16x4_t result;
-  __asm__ ("rev32 %0.4h,%1.4h"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vrev32q_p8 (poly8x16_t a)
-{
-  poly8x16_t result;
-  __asm__ ("rev32 %0.16b,%1.16b"
-           : "=w"(result)
-           : "w"(a)
-           : /* No clobbers */);
-  return result;
-}
-
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vrev32q_p16 (poly16x8_t a)
-{
-  poly16x8_t result;
-  __asm__ ("rev32 %0.8h,%1.8h"
+  __asm__ ("subhn %0.4h, %1.4s, %2.4s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vrev32q_s8 (int8x16_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vsubhn_s64 (int64x2_t a, int64x2_t b)
 {
-  int8x16_t result;
-  __asm__ ("rev32 %0.16b,%1.16b"
+  int32x2_t result;
+  __asm__ ("subhn %0.2s, %1.2d, %2.2d"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vrev32q_s16 (int16x8_t a)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vsubhn_u16 (uint16x8_t a, uint16x8_t b)
 {
-  int16x8_t result;
-  __asm__ ("rev32 %0.8h,%1.8h"
+  uint8x8_t result;
+  __asm__ ("subhn %0.8b, %1.8h, %2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vrev32q_u8 (uint8x16_t a)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vsubhn_u32 (uint32x4_t a, uint32x4_t b)
 {
-  uint8x16_t result;
-  __asm__ ("rev32 %0.16b,%1.16b"
+  uint16x4_t result;
+  __asm__ ("subhn %0.4h, %1.4s, %2.4s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vrev32q_u16 (uint16x8_t a)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vsubhn_u64 (uint64x2_t a, uint64x2_t b)
 {
-  uint16x8_t result;
-  __asm__ ("rev32 %0.8h,%1.8h"
+  uint32x2_t result;
+  __asm__ ("subhn %0.2s, %1.2d, %2.2d"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrev64_f32 (float32x2_t a)
+vtrn1_f32 (float32x2_t a, float32x2_t b)
 {
   float32x2_t result;
-  __asm__ ("rev64 %0.2s,%1.2s"
+  __asm__ ("trn1 %0.2s,%1.2s,%2.2s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vrev64_p8 (poly8x8_t a)
+vtrn1_p8 (poly8x8_t a, poly8x8_t b)
 {
   poly8x8_t result;
-  __asm__ ("rev64 %0.8b,%1.8b"
+  __asm__ ("trn1 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vrev64_p16 (poly16x4_t a)
+vtrn1_p16 (poly16x4_t a, poly16x4_t b)
 {
   poly16x4_t result;
-  __asm__ ("rev64 %0.4h,%1.4h"
+  __asm__ ("trn1 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vrev64_s8 (int8x8_t a)
+vtrn1_s8 (int8x8_t a, int8x8_t b)
 {
   int8x8_t result;
-  __asm__ ("rev64 %0.8b,%1.8b"
+  __asm__ ("trn1 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vrev64_s16 (int16x4_t a)
+vtrn1_s16 (int16x4_t a, int16x4_t b)
 {
   int16x4_t result;
-  __asm__ ("rev64 %0.4h,%1.4h"
+  __asm__ ("trn1 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vrev64_s32 (int32x2_t a)
+vtrn1_s32 (int32x2_t a, int32x2_t b)
 {
   int32x2_t result;
-  __asm__ ("rev64 %0.2s,%1.2s"
+  __asm__ ("trn1 %0.2s,%1.2s,%2.2s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vrev64_u8 (uint8x8_t a)
+vtrn1_u8 (uint8x8_t a, uint8x8_t b)
 {
   uint8x8_t result;
-  __asm__ ("rev64 %0.8b,%1.8b"
+  __asm__ ("trn1 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vrev64_u16 (uint16x4_t a)
+vtrn1_u16 (uint16x4_t a, uint16x4_t b)
 {
   uint16x4_t result;
-  __asm__ ("rev64 %0.4h,%1.4h"
+  __asm__ ("trn1 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vrev64_u32 (uint32x2_t a)
+vtrn1_u32 (uint32x2_t a, uint32x2_t b)
 {
   uint32x2_t result;
-  __asm__ ("rev64 %0.2s,%1.2s"
+  __asm__ ("trn1 %0.2s,%1.2s,%2.2s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrev64q_f32 (float32x4_t a)
+vtrn1q_f32 (float32x4_t a, float32x4_t b)
 {
   float32x4_t result;
-  __asm__ ("rev64 %0.4s,%1.4s"
+  __asm__ ("trn1 %0.4s,%1.4s,%2.4s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vtrn1q_f64 (float64x2_t a, float64x2_t b)
+{
+  float64x2_t result;
+  __asm__ ("trn1 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vrev64q_p8 (poly8x16_t a)
+vtrn1q_p8 (poly8x16_t a, poly8x16_t b)
 {
   poly8x16_t result;
-  __asm__ ("rev64 %0.16b,%1.16b"
+  __asm__ ("trn1 %0.16b,%1.16b,%2.16b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vrev64q_p16 (poly16x8_t a)
+vtrn1q_p16 (poly16x8_t a, poly16x8_t b)
 {
   poly16x8_t result;
-  __asm__ ("rev64 %0.8h,%1.8h"
+  __asm__ ("trn1 %0.8h,%1.8h,%2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vrev64q_s8 (int8x16_t a)
+vtrn1q_s8 (int8x16_t a, int8x16_t b)
 {
   int8x16_t result;
-  __asm__ ("rev64 %0.16b,%1.16b"
+  __asm__ ("trn1 %0.16b,%1.16b,%2.16b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vrev64q_s16 (int16x8_t a)
+vtrn1q_s16 (int16x8_t a, int16x8_t b)
 {
   int16x8_t result;
-  __asm__ ("rev64 %0.8h,%1.8h"
+  __asm__ ("trn1 %0.8h,%1.8h,%2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vrev64q_s32 (int32x4_t a)
+vtrn1q_s32 (int32x4_t a, int32x4_t b)
 {
   int32x4_t result;
-  __asm__ ("rev64 %0.4s,%1.4s"
+  __asm__ ("trn1 %0.4s,%1.4s,%2.4s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vtrn1q_s64 (int64x2_t a, int64x2_t b)
+{
+  int64x2_t result;
+  __asm__ ("trn1 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vrev64q_u8 (uint8x16_t a)
+vtrn1q_u8 (uint8x16_t a, uint8x16_t b)
 {
   uint8x16_t result;
-  __asm__ ("rev64 %0.16b,%1.16b"
+  __asm__ ("trn1 %0.16b,%1.16b,%2.16b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vrev64q_u16 (uint16x8_t a)
+vtrn1q_u16 (uint16x8_t a, uint16x8_t b)
 {
   uint16x8_t result;
-  __asm__ ("rev64 %0.8h,%1.8h"
+  __asm__ ("trn1 %0.8h,%1.8h,%2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vrev64q_u32 (uint32x4_t a)
+vtrn1q_u32 (uint32x4_t a, uint32x4_t b)
 {
   uint32x4_t result;
-  __asm__ ("rev64 %0.4s,%1.4s"
+  __asm__ ("trn1 %0.4s,%1.4s,%2.4s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrnd_f32 (float32x2_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vtrn1q_u64 (uint64x2_t a, uint64x2_t b)
 {
-  float32x2_t result;
-  __asm__ ("frintz %0.2s,%1.2s"
+  uint64x2_t result;
+  __asm__ ("trn1 %0.2d,%1.2d,%2.2d"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrnda_f32 (float32x2_t a)
+vtrn2_f32 (float32x2_t a, float32x2_t b)
 {
   float32x2_t result;
-  __asm__ ("frinta %0.2s,%1.2s"
+  __asm__ ("trn2 %0.2s,%1.2s,%2.2s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrndm_f32 (float32x2_t a)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vtrn2_p8 (poly8x8_t a, poly8x8_t b)
 {
-  float32x2_t result;
-  __asm__ ("frintm %0.2s,%1.2s"
+  poly8x8_t result;
+  __asm__ ("trn2 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrndn_f32 (float32x2_t a)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vtrn2_p16 (poly16x4_t a, poly16x4_t b)
 {
-  float32x2_t result;
-  __asm__ ("frintn %0.2s,%1.2s"
+  poly16x4_t result;
+  __asm__ ("trn2 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrndp_f32 (float32x2_t a)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vtrn2_s8 (int8x8_t a, int8x8_t b)
 {
-  float32x2_t result;
-  __asm__ ("frintp %0.2s,%1.2s"
+  int8x8_t result;
+  __asm__ ("trn2 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrndq_f32 (float32x4_t a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vtrn2_s16 (int16x4_t a, int16x4_t b)
 {
-  float32x4_t result;
-  __asm__ ("frintz %0.4s,%1.4s"
+  int16x4_t result;
+  __asm__ ("trn2 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrndq_f64 (float64x2_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vtrn2_s32 (int32x2_t a, int32x2_t b)
 {
-  float64x2_t result;
-  __asm__ ("frintz %0.2d,%1.2d"
+  int32x2_t result;
+  __asm__ ("trn2 %0.2s,%1.2s,%2.2s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrndqa_f32 (float32x4_t a)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vtrn2_u8 (uint8x8_t a, uint8x8_t b)
 {
-  float32x4_t result;
-  __asm__ ("frinta %0.4s,%1.4s"
+  uint8x8_t result;
+  __asm__ ("trn2 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrndqa_f64 (float64x2_t a)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vtrn2_u16 (uint16x4_t a, uint16x4_t b)
 {
-  float64x2_t result;
-  __asm__ ("frinta %0.2d,%1.2d"
+  uint16x4_t result;
+  __asm__ ("trn2 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vtrn2_u32 (uint32x2_t a, uint32x2_t b)
+{
+  uint32x2_t result;
+  __asm__ ("trn2 %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrndqm_f32 (float32x4_t a)
+vtrn2q_f32 (float32x4_t a, float32x4_t b)
 {
   float32x4_t result;
-  __asm__ ("frintm %0.4s,%1.4s"
+  __asm__ ("trn2 %0.4s,%1.4s,%2.4s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrndqm_f64 (float64x2_t a)
+vtrn2q_f64 (float64x2_t a, float64x2_t b)
 {
   float64x2_t result;
-  __asm__ ("frintm %0.2d,%1.2d"
+  __asm__ ("trn2 %0.2d,%1.2d,%2.2d"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrndqn_f32 (float32x4_t a)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vtrn2q_p8 (poly8x16_t a, poly8x16_t b)
 {
-  float32x4_t result;
-  __asm__ ("frintn %0.4s,%1.4s"
+  poly8x16_t result;
+  __asm__ ("trn2 %0.16b,%1.16b,%2.16b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrndqn_f64 (float64x2_t a)
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vtrn2q_p16 (poly16x8_t a, poly16x8_t b)
 {
-  float64x2_t result;
-  __asm__ ("frintn %0.2d,%1.2d"
+  poly16x8_t result;
+  __asm__ ("trn2 %0.8h,%1.8h,%2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrndqp_f32 (float32x4_t a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vtrn2q_s8 (int8x16_t a, int8x16_t b)
 {
-  float32x4_t result;
-  __asm__ ("frintp %0.4s,%1.4s"
+  int8x16_t result;
+  __asm__ ("trn2 %0.16b,%1.16b,%2.16b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrndqp_f64 (float64x2_t a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vtrn2q_s16 (int16x8_t a, int16x8_t b)
 {
-  float64x2_t result;
-  __asm__ ("frintp %0.2d,%1.2d"
+  int16x8_t result;
+  __asm__ ("trn2 %0.8h,%1.8h,%2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vrshrn_high_n_s16(a, b, c)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int8x8_t a_ = (a);                                               \
-       int8x16_t result = vcombine_s8                                   \
-                            (a_, vcreate_s8 (UINT64_C (0x0)));          \
-       __asm__ ("rshrn2 %0.16b,%1.8h,#%2"                               \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vrshrn_high_n_s32(a, b, c)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int16x8_t result = vcombine_s16                                  \
-                            (a_, vcreate_s16 (UINT64_C (0x0)));         \
-       __asm__ ("rshrn2 %0.8h,%1.4s,#%2"                                \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vtrn2q_s32 (int32x4_t a, int32x4_t b)
+{
+  int32x4_t result;
+  __asm__ ("trn2 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vrshrn_high_n_s64(a, b, c)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int32x4_t result = vcombine_s32                                  \
-                            (a_, vcreate_s32 (UINT64_C (0x0)));         \
-       __asm__ ("rshrn2 %0.4s,%1.2d,#%2"                                \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vtrn2q_s64 (int64x2_t a, int64x2_t b)
+{
+  int64x2_t result;
+  __asm__ ("trn2 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vrshrn_high_n_u16(a, b, c)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint8x8_t a_ = (a);                                              \
-       uint8x16_t result = vcombine_u8                                  \
-                            (a_, vcreate_u8 (UINT64_C (0x0)));          \
-       __asm__ ("rshrn2 %0.16b,%1.8h,#%2"                               \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vtrn2q_u8 (uint8x16_t a, uint8x16_t b)
+{
+  uint8x16_t result;
+  __asm__ ("trn2 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vrshrn_high_n_u32(a, b, c)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint16x8_t result = vcombine_u16                                 \
-                            (a_, vcreate_u16 (UINT64_C (0x0)));         \
-       __asm__ ("rshrn2 %0.8h,%1.4s,#%2"                                \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vtrn2q_u16 (uint16x8_t a, uint16x8_t b)
+{
+  uint16x8_t result;
+  __asm__ ("trn2 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vrshrn_high_n_u64(a, b, c)                                      \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x2_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint32x4_t result = vcombine_u32                                 \
-                            (a_, vcreate_u32 (UINT64_C (0x0)));         \
-       __asm__ ("rshrn2 %0.4s,%1.2d,#%2"                                \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vtrn2q_u32 (uint32x4_t a, uint32x4_t b)
+{
+  uint32x4_t result;
+  __asm__ ("trn2 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vrshrn_n_s16(a, b)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t a_ = (a);                                              \
-       int8x8_t result;                                                 \
-       __asm__ ("rshrn %0.8b,%1.8h,%2"                                  \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vrshrn_n_s32(a, b)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t a_ = (a);                                              \
-       int16x4_t result;                                                \
-       __asm__ ("rshrn %0.4h,%1.4s,%2"                                  \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vrshrn_n_s64(a, b)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x2_t a_ = (a);                                              \
-       int32x2_t result;                                                \
-       __asm__ ("rshrn %0.2s,%1.2d,%2"                                  \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vrshrn_n_u16(a, b)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t a_ = (a);                                             \
-       uint8x8_t result;                                                \
-       __asm__ ("rshrn %0.8b,%1.8h,%2"                                  \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vtrn2q_u64 (uint64x2_t a, uint64x2_t b)
+{
+  uint64x2_t result;
+  __asm__ ("trn2 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vrshrn_n_u32(a, b)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t a_ = (a);                                             \
-       uint16x4_t result;                                               \
-       __asm__ ("rshrn %0.4h,%1.4s,%2"                                  \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vtst_p8 (poly8x8_t a, poly8x8_t b)
+{
+  uint8x8_t result;
+  __asm__ ("cmtst %0.8b, %1.8b, %2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vrshrn_n_u64(a, b)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x2_t a_ = (a);                                             \
-       uint32x2_t result;                                               \
-       __asm__ ("rshrn %0.2s,%1.2d,%2"                                  \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vtst_p16 (poly16x4_t a, poly16x4_t b)
+{
+  uint16x4_t result;
+  __asm__ ("cmtst %0.4h, %1.4h, %2.4h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrsqrte_f32 (float32x2_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vtstq_p8 (poly8x16_t a, poly8x16_t b)
 {
-  float32x2_t result;
-  __asm__ ("frsqrte %0.2s,%1.2s"
+  uint8x16_t result;
+  __asm__ ("cmtst %0.16b, %1.16b, %2.16b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrsqrte_f64 (float64x2_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vtstq_p16 (poly16x8_t a, poly16x8_t b)
 {
-  float64x2_t result;
-  __asm__ ("frsqrte %0.2d,%1.2d"
+  uint16x8_t result;
+  __asm__ ("cmtst %0.8h, %1.8h, %2.8h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vuzp1_f32 (float32x2_t a, float32x2_t b)
+{
+  float32x2_t result;
+  __asm__ ("uzp1 %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vrsqrte_u32 (uint32x2_t a)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vuzp1_p8 (poly8x8_t a, poly8x8_t b)
 {
-  uint32x2_t result;
-  __asm__ ("ursqrte %0.2s,%1.2s"
+  poly8x8_t result;
+  __asm__ ("uzp1 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vrsqrted_f64 (float64_t a)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vuzp1_p16 (poly16x4_t a, poly16x4_t b)
 {
-  float64_t result;
-  __asm__ ("frsqrte %d0,%d1"
+  poly16x4_t result;
+  __asm__ ("uzp1 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrsqrteq_f32 (float32x4_t a)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vuzp1_s8 (int8x8_t a, int8x8_t b)
 {
-  float32x4_t result;
-  __asm__ ("frsqrte %0.4s,%1.4s"
+  int8x8_t result;
+  __asm__ ("uzp1 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrsqrteq_f64 (float64x2_t a)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vuzp1_s16 (int16x4_t a, int16x4_t b)
 {
-  float64x2_t result;
-  __asm__ ("frsqrte %0.2d,%1.2d"
+  int16x4_t result;
+  __asm__ ("uzp1 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vrsqrteq_u32 (uint32x4_t a)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vuzp1_s32 (int32x2_t a, int32x2_t b)
 {
-  uint32x4_t result;
-  __asm__ ("ursqrte %0.4s,%1.4s"
+  int32x2_t result;
+  __asm__ ("uzp1 %0.2s,%1.2s,%2.2s"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vrsqrtes_f32 (float32_t a)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vuzp1_u8 (uint8x8_t a, uint8x8_t b)
 {
-  float32_t result;
-  __asm__ ("frsqrte %s0,%s1"
+  uint8x8_t result;
+  __asm__ ("uzp1 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
-           : "w"(a)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vrsqrts_f32 (float32x2_t a, float32x2_t b)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vuzp1_u16 (uint16x4_t a, uint16x4_t b)
 {
-  float32x2_t result;
-  __asm__ ("frsqrts %0.2s,%1.2s,%2.2s"
+  uint16x4_t result;
+  __asm__ ("uzp1 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vrsqrtsd_f64 (float64_t a, float64_t b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vuzp1_u32 (uint32x2_t a, uint32x2_t b)
 {
-  float64_t result;
-  __asm__ ("frsqrts %d0,%d1,%d2"
+  uint32x2_t result;
+  __asm__ ("uzp1 %0.2s,%1.2s,%2.2s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -15274,10 +15075,10 @@ vrsqrtsd_f64 (float64_t a, float64_t b)
 }
 
 __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vrsqrtsq_f32 (float32x4_t a, float32x4_t b)
+vuzp1q_f32 (float32x4_t a, float32x4_t b)
 {
   float32x4_t result;
-  __asm__ ("frsqrts %0.4s,%1.4s,%2.4s"
+  __asm__ ("uzp1 %0.4s,%1.4s,%2.4s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -15285,32 +15086,32 @@ vrsqrtsq_f32 (float32x4_t a, float32x4_t
 }
 
 __extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrsqrtsq_f64 (float64x2_t a, float64x2_t b)
+vuzp1q_f64 (float64x2_t a, float64x2_t b)
 {
   float64x2_t result;
-  __asm__ ("frsqrts %0.2d,%1.2d,%2.2d"
+  __asm__ ("uzp1 %0.2d,%1.2d,%2.2d"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vrsqrtss_f32 (float32_t a, float32_t b)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vuzp1q_p8 (poly8x16_t a, poly8x16_t b)
 {
-  float32_t result;
-  __asm__ ("frsqrts %s0,%s1,%s2"
+  poly8x16_t result;
+  __asm__ ("uzp1 %0.16b,%1.16b,%2.16b"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vrsrtsq_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vuzp1q_p16 (poly16x8_t a, poly16x8_t b)
 {
-  float64x2_t result;
-  __asm__ ("frsqrts %0.2d,%1.2d,%2.2d"
+  poly16x8_t result;
+  __asm__ ("uzp1 %0.8h,%1.8h,%2.8h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -15318,76 +15119,131 @@ vrsrtsq_f64 (float64x2_t a, float64x2_t
 }
 
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vrsubhn_high_s16 (int8x8_t a, int16x8_t b, int16x8_t c)
+vuzp1q_s8 (int8x16_t a, int8x16_t b)
 {
-  int8x16_t result = vcombine_s8 (a, vcreate_s8 (UINT64_C (0x0)));
-  __asm__ ("rsubhn2 %0.16b, %1.8h, %2.8h"
-           : "+w"(result)
-           : "w"(b), "w"(c)
+  int8x16_t result;
+  __asm__ ("uzp1 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vrsubhn_high_s32 (int16x4_t a, int32x4_t b, int32x4_t c)
+vuzp1q_s16 (int16x8_t a, int16x8_t b)
 {
-  int16x8_t result = vcombine_s16 (a, vcreate_s16 (UINT64_C (0x0)));
-  __asm__ ("rsubhn2 %0.8h, %1.4s, %2.4s"
-           : "+w"(result)
-           : "w"(b), "w"(c)
+  int16x8_t result;
+  __asm__ ("uzp1 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vrsubhn_high_s64 (int32x2_t a, int64x2_t b, int64x2_t c)
+vuzp1q_s32 (int32x4_t a, int32x4_t b)
 {
-  int32x4_t result = vcombine_s32 (a, vcreate_s32 (UINT64_C (0x0)));
-  __asm__ ("rsubhn2 %0.4s, %1.2d, %2.2d"
-           : "+w"(result)
-           : "w"(b), "w"(c)
+  int32x4_t result;
+  __asm__ ("uzp1 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vrsubhn_high_u16 (uint8x8_t a, uint16x8_t b, uint16x8_t c)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vuzp1q_s64 (int64x2_t a, int64x2_t b)
 {
-  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (UINT64_C (0x0)));
-  __asm__ ("rsubhn2 %0.16b, %1.8h, %2.8h"
-           : "+w"(result)
-           : "w"(b), "w"(c)
+  int64x2_t result;
+  __asm__ ("uzp1 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vuzp1q_u8 (uint8x16_t a, uint8x16_t b)
+{
+  uint8x16_t result;
+  __asm__ ("uzp1 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vrsubhn_high_u32 (uint16x4_t a, uint32x4_t b, uint32x4_t c)
+vuzp1q_u16 (uint16x8_t a, uint16x8_t b)
 {
-  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (UINT64_C (0x0)));
-  __asm__ ("rsubhn2 %0.8h, %1.4s, %2.4s"
-           : "+w"(result)
-           : "w"(b), "w"(c)
+  uint16x8_t result;
+  __asm__ ("uzp1 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vrsubhn_high_u64 (uint32x2_t a, uint64x2_t b, uint64x2_t c)
+vuzp1q_u32 (uint32x4_t a, uint32x4_t b)
 {
-  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (UINT64_C (0x0)));
-  __asm__ ("rsubhn2 %0.4s, %1.2d, %2.2d"
-           : "+w"(result)
-           : "w"(b), "w"(c)
+  uint32x4_t result;
+  __asm__ ("uzp1 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vuzp1q_u64 (uint64x2_t a, uint64x2_t b)
+{
+  uint64x2_t result;
+  __asm__ ("uzp1 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vuzp2_f32 (float32x2_t a, float32x2_t b)
+{
+  float32x2_t result;
+  __asm__ ("uzp2 %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vuzp2_p8 (poly8x8_t a, poly8x8_t b)
+{
+  poly8x8_t result;
+  __asm__ ("uzp2 %0.8b,%1.8b,%2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vuzp2_p16 (poly16x4_t a, poly16x4_t b)
+{
+  poly16x4_t result;
+  __asm__ ("uzp2 %0.4h,%1.4h,%2.4h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vrsubhn_s16 (int16x8_t a, int16x8_t b)
+vuzp2_s8 (int8x8_t a, int8x8_t b)
 {
   int8x8_t result;
-  __asm__ ("rsubhn %0.8b, %1.8h, %2.8h"
+  __asm__ ("uzp2 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -15395,10 +15251,10 @@ vrsubhn_s16 (int16x8_t a, int16x8_t b)
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vrsubhn_s32 (int32x4_t a, int32x4_t b)
+vuzp2_s16 (int16x4_t a, int16x4_t b)
 {
   int16x4_t result;
-  __asm__ ("rsubhn %0.4h, %1.4s, %2.4s"
+  __asm__ ("uzp2 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -15406,10 +15262,10 @@ vrsubhn_s32 (int32x4_t a, int32x4_t b)
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vrsubhn_s64 (int64x2_t a, int64x2_t b)
+vuzp2_s32 (int32x2_t a, int32x2_t b)
 {
   int32x2_t result;
-  __asm__ ("rsubhn %0.2s, %1.2d, %2.2d"
+  __asm__ ("uzp2 %0.2s,%1.2s,%2.2s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -15417,10 +15273,10 @@ vrsubhn_s64 (int64x2_t a, int64x2_t b)
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vrsubhn_u16 (uint16x8_t a, uint16x8_t b)
+vuzp2_u8 (uint8x8_t a, uint8x8_t b)
 {
   uint8x8_t result;
-  __asm__ ("rsubhn %0.8b, %1.8h, %2.8h"
+  __asm__ ("uzp2 %0.8b,%1.8b,%2.8b"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -15428,10 +15284,10 @@ vrsubhn_u16 (uint16x8_t a, uint16x8_t b)
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vrsubhn_u32 (uint32x4_t a, uint32x4_t b)
+vuzp2_u16 (uint16x4_t a, uint16x4_t b)
 {
   uint16x4_t result;
-  __asm__ ("rsubhn %0.4h, %1.4s, %2.4s"
+  __asm__ ("uzp2 %0.4h,%1.4h,%2.4h"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
@@ -15439,4573 +15295,4499 @@ vrsubhn_u32 (uint32x4_t a, uint32x4_t b)
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vrsubhn_u64 (uint64x2_t a, uint64x2_t b)
+vuzp2_u32 (uint32x2_t a, uint32x2_t b)
 {
   uint32x2_t result;
-  __asm__ ("rsubhn %0.2s, %1.2d, %2.2d"
+  __asm__ ("uzp2 %0.2s,%1.2s,%2.2s"
            : "=w"(result)
            : "w"(a), "w"(b)
            : /* No clobbers */);
   return result;
 }
 
-#define vset_lane_f32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t b_ = (b);                                            \
-       float32_t a_ = (a);                                              \
-       float32x2_t result;                                              \
-       __asm__ ("ins %0.s[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
-
-#define vset_lane_f64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x1_t b_ = (b);                                            \
-       float64_t a_ = (a);                                              \
-       float64x1_t result;                                              \
-       __asm__ ("ins %0.d[%3], %x1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vuzp2q_f32 (float32x4_t a, float32x4_t b)
+{
+  float32x4_t result;
+  __asm__ ("uzp2 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vset_lane_p8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x8_t b_ = (b);                                              \
-       poly8_t a_ = (a);                                                \
-       poly8x8_t result;                                                \
-       __asm__ ("ins %0.b[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vuzp2q_f64 (float64x2_t a, float64x2_t b)
+{
+  float64x2_t result;
+  __asm__ ("uzp2 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vset_lane_p16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x4_t b_ = (b);                                             \
-       poly16_t a_ = (a);                                               \
-       poly16x4_t result;                                               \
-       __asm__ ("ins %0.h[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vuzp2q_p8 (poly8x16_t a, poly8x16_t b)
+{
+  poly8x16_t result;
+  __asm__ ("uzp2 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vset_lane_s8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x8_t b_ = (b);                                               \
-       int8_t a_ = (a);                                                 \
-       int8x8_t result;                                                 \
-       __asm__ ("ins %0.b[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vuzp2q_p16 (poly16x8_t a, poly16x8_t b)
+{
+  poly16x8_t result;
+  __asm__ ("uzp2 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vset_lane_s16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t b_ = (b);                                              \
-       int16_t a_ = (a);                                                \
-       int16x4_t result;                                                \
-       __asm__ ("ins %0.h[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vuzp2q_s8 (int8x16_t a, int8x16_t b)
+{
+  int8x16_t result;
+  __asm__ ("uzp2 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vset_lane_s32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t b_ = (b);                                              \
-       int32_t a_ = (a);                                                \
-       int32x2_t result;                                                \
-       __asm__ ("ins %0.s[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vuzp2q_s16 (int16x8_t a, int16x8_t b)
+{
+  int16x8_t result;
+  __asm__ ("uzp2 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vset_lane_s64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x1_t b_ = (b);                                              \
-       int64_t a_ = (a);                                                \
-       int64x1_t result;                                                \
-       __asm__ ("ins %0.d[%3], %x1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vuzp2q_s32 (int32x4_t a, int32x4_t b)
+{
+  int32x4_t result;
+  __asm__ ("uzp2 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vuzp2q_s64 (int64x2_t a, int64x2_t b)
+{
+  int64x2_t result;
+  __asm__ ("uzp2 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vuzp2q_u8 (uint8x16_t a, uint8x16_t b)
+{
+  uint8x16_t result;
+  __asm__ ("uzp2 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vuzp2q_u16 (uint16x8_t a, uint16x8_t b)
+{
+  uint16x8_t result;
+  __asm__ ("uzp2 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vuzp2q_u32 (uint32x4_t a, uint32x4_t b)
+{
+  uint32x4_t result;
+  __asm__ ("uzp2 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vuzp2q_u64 (uint64x2_t a, uint64x2_t b)
+{
+  uint64x2_t result;
+  __asm__ ("uzp2 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vzip1_f32 (float32x2_t a, float32x2_t b)
+{
+  float32x2_t result;
+  __asm__ ("zip1 %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vzip1_p8 (poly8x8_t a, poly8x8_t b)
+{
+  poly8x8_t result;
+  __asm__ ("zip1 %0.8b,%1.8b,%2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vzip1_p16 (poly16x4_t a, poly16x4_t b)
+{
+  poly16x4_t result;
+  __asm__ ("zip1 %0.4h,%1.4h,%2.4h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vzip1_s8 (int8x8_t a, int8x8_t b)
+{
+  int8x8_t result;
+  __asm__ ("zip1 %0.8b,%1.8b,%2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vzip1_s16 (int16x4_t a, int16x4_t b)
+{
+  int16x4_t result;
+  __asm__ ("zip1 %0.4h,%1.4h,%2.4h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vzip1_s32 (int32x2_t a, int32x2_t b)
+{
+  int32x2_t result;
+  __asm__ ("zip1 %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vzip1_u8 (uint8x8_t a, uint8x8_t b)
+{
+  uint8x8_t result;
+  __asm__ ("zip1 %0.8b,%1.8b,%2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vzip1_u16 (uint16x4_t a, uint16x4_t b)
+{
+  uint16x4_t result;
+  __asm__ ("zip1 %0.4h,%1.4h,%2.4h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vzip1_u32 (uint32x2_t a, uint32x2_t b)
+{
+  uint32x2_t result;
+  __asm__ ("zip1 %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vzip1q_f32 (float32x4_t a, float32x4_t b)
+{
+  float32x4_t result;
+  __asm__ ("zip1 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vzip1q_f64 (float64x2_t a, float64x2_t b)
+{
+  float64x2_t result;
+  __asm__ ("zip1 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vzip1q_p8 (poly8x16_t a, poly8x16_t b)
+{
+  poly8x16_t result;
+  __asm__ ("zip1 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vzip1q_p16 (poly16x8_t a, poly16x8_t b)
+{
+  poly16x8_t result;
+  __asm__ ("zip1 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vzip1q_s8 (int8x16_t a, int8x16_t b)
+{
+  int8x16_t result;
+  __asm__ ("zip1 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vzip1q_s16 (int16x8_t a, int16x8_t b)
+{
+  int16x8_t result;
+  __asm__ ("zip1 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vzip1q_s32 (int32x4_t a, int32x4_t b)
+{
+  int32x4_t result;
+  __asm__ ("zip1 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vzip1q_s64 (int64x2_t a, int64x2_t b)
+{
+  int64x2_t result;
+  __asm__ ("zip1 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vzip1q_u8 (uint8x16_t a, uint8x16_t b)
+{
+  uint8x16_t result;
+  __asm__ ("zip1 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vzip1q_u16 (uint16x8_t a, uint16x8_t b)
+{
+  uint16x8_t result;
+  __asm__ ("zip1 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vzip1q_u32 (uint32x4_t a, uint32x4_t b)
+{
+  uint32x4_t result;
+  __asm__ ("zip1 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vzip1q_u64 (uint64x2_t a, uint64x2_t b)
+{
+  uint64x2_t result;
+  __asm__ ("zip1 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vzip2_f32 (float32x2_t a, float32x2_t b)
+{
+  float32x2_t result;
+  __asm__ ("zip2 %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vzip2_p8 (poly8x8_t a, poly8x8_t b)
+{
+  poly8x8_t result;
+  __asm__ ("zip2 %0.8b,%1.8b,%2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vzip2_p16 (poly16x4_t a, poly16x4_t b)
+{
+  poly16x4_t result;
+  __asm__ ("zip2 %0.4h,%1.4h,%2.4h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vzip2_s8 (int8x8_t a, int8x8_t b)
+{
+  int8x8_t result;
+  __asm__ ("zip2 %0.8b,%1.8b,%2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vzip2_s16 (int16x4_t a, int16x4_t b)
+{
+  int16x4_t result;
+  __asm__ ("zip2 %0.4h,%1.4h,%2.4h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vzip2_s32 (int32x2_t a, int32x2_t b)
+{
+  int32x2_t result;
+  __asm__ ("zip2 %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vzip2_u8 (uint8x8_t a, uint8x8_t b)
+{
+  uint8x8_t result;
+  __asm__ ("zip2 %0.8b,%1.8b,%2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vzip2_u16 (uint16x4_t a, uint16x4_t b)
+{
+  uint16x4_t result;
+  __asm__ ("zip2 %0.4h,%1.4h,%2.4h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vzip2_u32 (uint32x2_t a, uint32x2_t b)
+{
+  uint32x2_t result;
+  __asm__ ("zip2 %0.2s,%1.2s,%2.2s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vzip2q_f32 (float32x4_t a, float32x4_t b)
+{
+  float32x4_t result;
+  __asm__ ("zip2 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vzip2q_f64 (float64x2_t a, float64x2_t b)
+{
+  float64x2_t result;
+  __asm__ ("zip2 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vzip2q_p8 (poly8x16_t a, poly8x16_t b)
+{
+  poly8x16_t result;
+  __asm__ ("zip2 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vzip2q_p16 (poly16x8_t a, poly16x8_t b)
+{
+  poly16x8_t result;
+  __asm__ ("zip2 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vzip2q_s8 (int8x16_t a, int8x16_t b)
+{
+  int8x16_t result;
+  __asm__ ("zip2 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vzip2q_s16 (int16x8_t a, int16x8_t b)
+{
+  int16x8_t result;
+  __asm__ ("zip2 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vzip2q_s32 (int32x4_t a, int32x4_t b)
+{
+  int32x4_t result;
+  __asm__ ("zip2 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vzip2q_s64 (int64x2_t a, int64x2_t b)
+{
+  int64x2_t result;
+  __asm__ ("zip2 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vzip2q_u8 (uint8x16_t a, uint8x16_t b)
+{
+  uint8x16_t result;
+  __asm__ ("zip2 %0.16b,%1.16b,%2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vzip2q_u16 (uint16x8_t a, uint16x8_t b)
+{
+  uint16x8_t result;
+  __asm__ ("zip2 %0.8h,%1.8h,%2.8h"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vzip2q_u32 (uint32x4_t a, uint32x4_t b)
+{
+  uint32x4_t result;
+  __asm__ ("zip2 %0.4s,%1.4s,%2.4s"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vzip2q_u64 (uint64x2_t a, uint64x2_t b)
+{
+  uint64x2_t result;
+  __asm__ ("zip2 %0.2d,%1.2d,%2.2d"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
+
+/* End of temporary inline asm implementations.  */
+
+/* Start of temporary inline asm for vldn, vstn and friends.  */
+
+/* Create struct element types for duplicating loads.
+
+   Create 2 element structures of:
+
+   +------+----+----+----+----+
+   |      | 8  | 16 | 32 | 64 |
+   +------+----+----+----+----+
+   |int   | Y  | Y  | N  | N  |
+   +------+----+----+----+----+
+   |uint  | Y  | Y  | N  | N  |
+   +------+----+----+----+----+
+   |float | -  | -  | N  | N  |
+   +------+----+----+----+----+
+   |poly  | Y  | Y  | -  | -  |
+   +------+----+----+----+----+
+
+   Create 3 element structures of:
+
+   +------+----+----+----+----+
+   |      | 8  | 16 | 32 | 64 |
+   +------+----+----+----+----+
+   |int   | Y  | Y  | Y  | Y  |
+   +------+----+----+----+----+
+   |uint  | Y  | Y  | Y  | Y  |
+   +------+----+----+----+----+
+   |float | -  | -  | Y  | Y  |
+   +------+----+----+----+----+
+   |poly  | Y  | Y  | -  | -  |
+   +------+----+----+----+----+
+
+   Create 4 element structures of:
+
+   +------+----+----+----+----+
+   |      | 8  | 16 | 32 | 64 |
+   +------+----+----+----+----+
+   |int   | Y  | N  | N  | Y  |
+   +------+----+----+----+----+
+   |uint  | Y  | N  | N  | Y  |
+   +------+----+----+----+----+
+   |float | -  | -  | N  | Y  |
+   +------+----+----+----+----+
+   |poly  | Y  | N  | -  | -  |
+   +------+----+----+----+----+
+
+  This is required for casting memory reference.  */
+#define __STRUCTN(t, sz, nelem)			\
+  typedef struct t ## sz ## x ## nelem ## _t {	\
+    t ## sz ## _t val[nelem];			\
+  }  t ## sz ## x ## nelem ## _t;
+
+/* 2-element structs.  */
+__STRUCTN (int, 8, 2)
+__STRUCTN (int, 16, 2)
+__STRUCTN (uint, 8, 2)
+__STRUCTN (uint, 16, 2)
+__STRUCTN (poly, 8, 2)
+__STRUCTN (poly, 16, 2)
+/* 3-element structs.  */
+__STRUCTN (int, 8, 3)
+__STRUCTN (int, 16, 3)
+__STRUCTN (int, 32, 3)
+__STRUCTN (int, 64, 3)
+__STRUCTN (uint, 8, 3)
+__STRUCTN (uint, 16, 3)
+__STRUCTN (uint, 32, 3)
+__STRUCTN (uint, 64, 3)
+__STRUCTN (float, 32, 3)
+__STRUCTN (float, 64, 3)
+__STRUCTN (poly, 8, 3)
+__STRUCTN (poly, 16, 3)
+/* 4-element structs.  */
+__STRUCTN (int, 8, 4)
+__STRUCTN (int, 64, 4)
+__STRUCTN (uint, 8, 4)
+__STRUCTN (uint, 64, 4)
+__STRUCTN (poly, 8, 4)
+__STRUCTN (float, 64, 4)
+#undef __STRUCTN
+
+#define __LD2R_FUNC(rettype, structtype, ptrtype,			\
+		    regsuffix, funcsuffix, Q)				\
+  __extension__ static __inline rettype					\
+  __attribute__ ((__always_inline__)) 					\
+  vld2 ## Q ## _dup_ ## funcsuffix (const ptrtype *ptr)			\
+  {									\
+    rettype result;							\
+    __asm__ ("ld2r {v16." #regsuffix ", v17." #regsuffix "}, %1\n\t"	\
+	     "st1 {v16." #regsuffix ", v17." #regsuffix "}, %0\n\t"	\
+	     : "=Q"(result)						\
+	     : "Q"(*(const structtype *)ptr)				\
+	     : "memory", "v16", "v17");					\
+    return result;							\
+  }
+
+__LD2R_FUNC (float32x2x2_t, float32x2_t, float32_t, 2s, f32,)
+__LD2R_FUNC (float64x1x2_t, float64x2_t, float64_t, 1d, f64,)
+__LD2R_FUNC (poly8x8x2_t, poly8x2_t, poly8_t, 8b, p8,)
+__LD2R_FUNC (poly16x4x2_t, poly16x2_t, poly16_t, 4h, p16,)
+__LD2R_FUNC (int8x8x2_t, int8x2_t, int8_t, 8b, s8,)
+__LD2R_FUNC (int16x4x2_t, int16x2_t, int16_t, 4h, s16,)
+__LD2R_FUNC (int32x2x2_t, int32x2_t, int32_t, 2s, s32,)
+__LD2R_FUNC (int64x1x2_t, int64x2_t, int64_t, 1d, s64,)
+__LD2R_FUNC (uint8x8x2_t, uint8x2_t, uint8_t, 8b, u8,)
+__LD2R_FUNC (uint16x4x2_t, uint16x2_t, uint16_t, 4h, u16,)
+__LD2R_FUNC (uint32x2x2_t, uint32x2_t, uint32_t, 2s, u32,)
+__LD2R_FUNC (uint64x1x2_t, uint64x2_t, uint64_t, 1d, u64,)
+__LD2R_FUNC (float32x4x2_t, float32x2_t, float32_t, 4s, f32, q)
+__LD2R_FUNC (float64x2x2_t, float64x2_t, float64_t, 2d, f64, q)
+__LD2R_FUNC (poly8x16x2_t, poly8x2_t, poly8_t, 16b, p8, q)
+__LD2R_FUNC (poly16x8x2_t, poly16x2_t, poly16_t, 8h, p16, q)
+__LD2R_FUNC (int8x16x2_t, int8x2_t, int8_t, 16b, s8, q)
+__LD2R_FUNC (int16x8x2_t, int16x2_t, int16_t, 8h, s16, q)
+__LD2R_FUNC (int32x4x2_t, int32x2_t, int32_t, 4s, s32, q)
+__LD2R_FUNC (int64x2x2_t, int64x2_t, int64_t, 2d, s64, q)
+__LD2R_FUNC (uint8x16x2_t, uint8x2_t, uint8_t, 16b, u8, q)
+__LD2R_FUNC (uint16x8x2_t, uint16x2_t, uint16_t, 8h, u16, q)
+__LD2R_FUNC (uint32x4x2_t, uint32x2_t, uint32_t, 4s, u32, q)
+__LD2R_FUNC (uint64x2x2_t, uint64x2_t, uint64_t, 2d, u64, q)
+
+#define __LD2_LANE_FUNC(rettype, ptrtype, regsuffix,			\
+			lnsuffix, funcsuffix, Q)			\
+  __extension__ static __inline rettype					\
+  __attribute__ ((__always_inline__))					\
+  vld2 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
+				     rettype b, const int c)		\
+  {									\
+    rettype result;							\
+    __asm__ ("ld1 {v16." #regsuffix ", v17." #regsuffix "}, %1\n\t"	\
+	     "ld2 {v16." #lnsuffix ", v17." #lnsuffix "}[%3], %2\n\t"	\
+	     "st1 {v16." #regsuffix ", v17." #regsuffix "}, %0\n\t"	\
+	     : "=Q"(result)						\
+	     : "Q"(b), "Q"(*(const rettype *)ptr), "i"(c)		\
+	     : "memory", "v16", "v17");					\
+    return result;							\
+  }
+
+__LD2_LANE_FUNC (int8x8x2_t, uint8_t, 8b, b, s8,)
+__LD2_LANE_FUNC (float32x2x2_t, float32_t, 2s, s, f32,)
+__LD2_LANE_FUNC (float64x1x2_t, float64_t, 1d, d, f64,)
+__LD2_LANE_FUNC (poly8x8x2_t, poly8_t, 8b, b, p8,)
+__LD2_LANE_FUNC (poly16x4x2_t, poly16_t, 4h, h, p16,)
+__LD2_LANE_FUNC (int16x4x2_t, int16_t, 4h, h, s16,)
+__LD2_LANE_FUNC (int32x2x2_t, int32_t, 2s, s, s32,)
+__LD2_LANE_FUNC (int64x1x2_t, int64_t, 1d, d, s64,)
+__LD2_LANE_FUNC (uint8x8x2_t, uint8_t, 8b, b, u8,)
+__LD2_LANE_FUNC (uint16x4x2_t, uint16_t, 4h, h, u16,)
+__LD2_LANE_FUNC (uint32x2x2_t, uint32_t, 2s, s, u32,)
+__LD2_LANE_FUNC (uint64x1x2_t, uint64_t, 1d, d, u64,)
+__LD2_LANE_FUNC (float32x4x2_t, float32_t, 4s, s, f32, q)
+__LD2_LANE_FUNC (float64x2x2_t, float64_t, 2d, d, f64, q)
+__LD2_LANE_FUNC (poly8x16x2_t, poly8_t, 16b, b, p8, q)
+__LD2_LANE_FUNC (poly16x8x2_t, poly16_t, 8h, h, p16, q)
+__LD2_LANE_FUNC (int8x16x2_t, int8_t, 16b, b, s8, q)
+__LD2_LANE_FUNC (int16x8x2_t, int16_t, 8h, h, s16, q)
+__LD2_LANE_FUNC (int32x4x2_t, int32_t, 4s, s, s32, q)
+__LD2_LANE_FUNC (int64x2x2_t, int64_t, 2d, d, s64, q)
+__LD2_LANE_FUNC (uint8x16x2_t, uint8_t, 16b, b, u8, q)
+__LD2_LANE_FUNC (uint16x8x2_t, uint16_t, 8h, h, u16, q)
+__LD2_LANE_FUNC (uint32x4x2_t, uint32_t, 4s, s, u32, q)
+__LD2_LANE_FUNC (uint64x2x2_t, uint64_t, 2d, d, u64, q)
+
+#define __LD3R_FUNC(rettype, structtype, ptrtype,			\
+		    regsuffix, funcsuffix, Q)				\
+  __extension__ static __inline rettype					\
+  __attribute__ ((__always_inline__))					\
+  vld3 ## Q ## _dup_ ## funcsuffix (const ptrtype *ptr)			\
+  {									\
+    rettype result;							\
+    __asm__ ("ld3r {v16." #regsuffix " - v18." #regsuffix "}, %1\n\t"	\
+	     "st1 {v16." #regsuffix " - v18." #regsuffix "}, %0\n\t"	\
+	     : "=Q"(result)						\
+	     : "Q"(*(const structtype *)ptr)				\
+	     : "memory", "v16", "v17", "v18");				\
+    return result;							\
+  }
+
+__LD3R_FUNC (float32x2x3_t, float32x3_t, float32_t, 2s, f32,)
+__LD3R_FUNC (float64x1x3_t, float64x3_t, float64_t, 1d, f64,)
+__LD3R_FUNC (poly8x8x3_t, poly8x3_t, poly8_t, 8b, p8,)
+__LD3R_FUNC (poly16x4x3_t, poly16x3_t, poly16_t, 4h, p16,)
+__LD3R_FUNC (int8x8x3_t, int8x3_t, int8_t, 8b, s8,)
+__LD3R_FUNC (int16x4x3_t, int16x3_t, int16_t, 4h, s16,)
+__LD3R_FUNC (int32x2x3_t, int32x3_t, int32_t, 2s, s32,)
+__LD3R_FUNC (int64x1x3_t, int64x3_t, int64_t, 1d, s64,)
+__LD3R_FUNC (uint8x8x3_t, uint8x3_t, uint8_t, 8b, u8,)
+__LD3R_FUNC (uint16x4x3_t, uint16x3_t, uint16_t, 4h, u16,)
+__LD3R_FUNC (uint32x2x3_t, uint32x3_t, uint32_t, 2s, u32,)
+__LD3R_FUNC (uint64x1x3_t, uint64x3_t, uint64_t, 1d, u64,)
+__LD3R_FUNC (float32x4x3_t, float32x3_t, float32_t, 4s, f32, q)
+__LD3R_FUNC (float64x2x3_t, float64x3_t, float64_t, 2d, f64, q)
+__LD3R_FUNC (poly8x16x3_t, poly8x3_t, poly8_t, 16b, p8, q)
+__LD3R_FUNC (poly16x8x3_t, poly16x3_t, poly16_t, 8h, p16, q)
+__LD3R_FUNC (int8x16x3_t, int8x3_t, int8_t, 16b, s8, q)
+__LD3R_FUNC (int16x8x3_t, int16x3_t, int16_t, 8h, s16, q)
+__LD3R_FUNC (int32x4x3_t, int32x3_t, int32_t, 4s, s32, q)
+__LD3R_FUNC (int64x2x3_t, int64x3_t, int64_t, 2d, s64, q)
+__LD3R_FUNC (uint8x16x3_t, uint8x3_t, uint8_t, 16b, u8, q)
+__LD3R_FUNC (uint16x8x3_t, uint16x3_t, uint16_t, 8h, u16, q)
+__LD3R_FUNC (uint32x4x3_t, uint32x3_t, uint32_t, 4s, u32, q)
+__LD3R_FUNC (uint64x2x3_t, uint64x3_t, uint64_t, 2d, u64, q)
+
+#define __LD3_LANE_FUNC(rettype, ptrtype, regsuffix,			\
+			lnsuffix, funcsuffix, Q)			\
+  __extension__ static __inline rettype					\
+  __attribute__ ((__always_inline__))					\
+  vld3 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
+				     rettype b, const int c)		\
+  {									\
+    rettype result;							\
+    __asm__ ("ld1 {v16." #regsuffix " - v18." #regsuffix "}, %1\n\t"	\
+	     "ld3 {v16." #lnsuffix " - v18." #lnsuffix "}[%3], %2\n\t"	\
+	     "st1 {v16." #regsuffix " - v18." #regsuffix "}, %0\n\t"	\
+	     : "=Q"(result)						\
+	     : "Q"(b), "Q"(*(const rettype *)ptr), "i"(c)		\
+	     : "memory", "v16", "v17", "v18");				\
+    return result;							\
+  }
+
+__LD3_LANE_FUNC (int8x8x3_t, uint8_t, 8b, b, s8,)
+__LD3_LANE_FUNC (float32x2x3_t, float32_t, 2s, s, f32,)
+__LD3_LANE_FUNC (float64x1x3_t, float64_t, 1d, d, f64,)
+__LD3_LANE_FUNC (poly8x8x3_t, poly8_t, 8b, b, p8,)
+__LD3_LANE_FUNC (poly16x4x3_t, poly16_t, 4h, h, p16,)
+__LD3_LANE_FUNC (int16x4x3_t, int16_t, 4h, h, s16,)
+__LD3_LANE_FUNC (int32x2x3_t, int32_t, 2s, s, s32,)
+__LD3_LANE_FUNC (int64x1x3_t, int64_t, 1d, d, s64,)
+__LD3_LANE_FUNC (uint8x8x3_t, uint8_t, 8b, b, u8,)
+__LD3_LANE_FUNC (uint16x4x3_t, uint16_t, 4h, h, u16,)
+__LD3_LANE_FUNC (uint32x2x3_t, uint32_t, 2s, s, u32,)
+__LD3_LANE_FUNC (uint64x1x3_t, uint64_t, 1d, d, u64,)
+__LD3_LANE_FUNC (float32x4x3_t, float32_t, 4s, s, f32, q)
+__LD3_LANE_FUNC (float64x2x3_t, float64_t, 2d, d, f64, q)
+__LD3_LANE_FUNC (poly8x16x3_t, poly8_t, 16b, b, p8, q)
+__LD3_LANE_FUNC (poly16x8x3_t, poly16_t, 8h, h, p16, q)
+__LD3_LANE_FUNC (int8x16x3_t, int8_t, 16b, b, s8, q)
+__LD3_LANE_FUNC (int16x8x3_t, int16_t, 8h, h, s16, q)
+__LD3_LANE_FUNC (int32x4x3_t, int32_t, 4s, s, s32, q)
+__LD3_LANE_FUNC (int64x2x3_t, int64_t, 2d, d, s64, q)
+__LD3_LANE_FUNC (uint8x16x3_t, uint8_t, 16b, b, u8, q)
+__LD3_LANE_FUNC (uint16x8x3_t, uint16_t, 8h, h, u16, q)
+__LD3_LANE_FUNC (uint32x4x3_t, uint32_t, 4s, s, u32, q)
+__LD3_LANE_FUNC (uint64x2x3_t, uint64_t, 2d, d, u64, q)
+
+#define __LD4R_FUNC(rettype, structtype, ptrtype,			\
+		    regsuffix, funcsuffix, Q)				\
+  __extension__ static __inline rettype					\
+  __attribute__ ((__always_inline__))					\
+  vld4 ## Q ## _dup_ ## funcsuffix (const ptrtype *ptr)			\
+  {									\
+    rettype result;							\
+    __asm__ ("ld4r {v16." #regsuffix " - v19." #regsuffix "}, %1\n\t"	\
+	     "st1 {v16." #regsuffix " - v19." #regsuffix "}, %0\n\t"	\
+	     : "=Q"(result)						\
+	     : "Q"(*(const structtype *)ptr)				\
+	     : "memory", "v16", "v17", "v18", "v19");			\
+    return result;							\
+  }
 
-#define vset_lane_u8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x8_t b_ = (b);                                              \
-       uint8_t a_ = (a);                                                \
-       uint8x8_t result;                                                \
-       __asm__ ("ins %0.b[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__LD4R_FUNC (float32x2x4_t, float32x4_t, float32_t, 2s, f32,)
+__LD4R_FUNC (float64x1x4_t, float64x4_t, float64_t, 1d, f64,)
+__LD4R_FUNC (poly8x8x4_t, poly8x4_t, poly8_t, 8b, p8,)
+__LD4R_FUNC (poly16x4x4_t, poly16x4_t, poly16_t, 4h, p16,)
+__LD4R_FUNC (int8x8x4_t, int8x4_t, int8_t, 8b, s8,)
+__LD4R_FUNC (int16x4x4_t, int16x4_t, int16_t, 4h, s16,)
+__LD4R_FUNC (int32x2x4_t, int32x4_t, int32_t, 2s, s32,)
+__LD4R_FUNC (int64x1x4_t, int64x4_t, int64_t, 1d, s64,)
+__LD4R_FUNC (uint8x8x4_t, uint8x4_t, uint8_t, 8b, u8,)
+__LD4R_FUNC (uint16x4x4_t, uint16x4_t, uint16_t, 4h, u16,)
+__LD4R_FUNC (uint32x2x4_t, uint32x4_t, uint32_t, 2s, u32,)
+__LD4R_FUNC (uint64x1x4_t, uint64x4_t, uint64_t, 1d, u64,)
+__LD4R_FUNC (float32x4x4_t, float32x4_t, float32_t, 4s, f32, q)
+__LD4R_FUNC (float64x2x4_t, float64x4_t, float64_t, 2d, f64, q)
+__LD4R_FUNC (poly8x16x4_t, poly8x4_t, poly8_t, 16b, p8, q)
+__LD4R_FUNC (poly16x8x4_t, poly16x4_t, poly16_t, 8h, p16, q)
+__LD4R_FUNC (int8x16x4_t, int8x4_t, int8_t, 16b, s8, q)
+__LD4R_FUNC (int16x8x4_t, int16x4_t, int16_t, 8h, s16, q)
+__LD4R_FUNC (int32x4x4_t, int32x4_t, int32_t, 4s, s32, q)
+__LD4R_FUNC (int64x2x4_t, int64x4_t, int64_t, 2d, s64, q)
+__LD4R_FUNC (uint8x16x4_t, uint8x4_t, uint8_t, 16b, u8, q)
+__LD4R_FUNC (uint16x8x4_t, uint16x4_t, uint16_t, 8h, u16, q)
+__LD4R_FUNC (uint32x4x4_t, uint32x4_t, uint32_t, 4s, u32, q)
+__LD4R_FUNC (uint64x2x4_t, uint64x4_t, uint64_t, 2d, u64, q)
 
-#define vset_lane_u16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t b_ = (b);                                             \
-       uint16_t a_ = (a);                                               \
-       uint16x4_t result;                                               \
-       __asm__ ("ins %0.h[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+#define __LD4_LANE_FUNC(rettype, ptrtype, regsuffix,			\
+			lnsuffix, funcsuffix, Q)			\
+  __extension__ static __inline rettype					\
+  __attribute__ ((__always_inline__))					\
+  vld4 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
+				     rettype b, const int c)		\
+  {									\
+    rettype result;							\
+    __asm__ ("ld1 {v16." #regsuffix " - v19." #regsuffix "}, %1\n\t"	\
+	     "ld4 {v16." #lnsuffix " - v19." #lnsuffix "}[%3], %2\n\t"	\
+	     "st1 {v16." #regsuffix " - v19." #regsuffix "}, %0\n\t"	\
+	     : "=Q"(result)						\
+	     : "Q"(b), "Q"(*(const rettype *)ptr), "i"(c)		\
+	     : "memory", "v16", "v17", "v18", "v19");			\
+    return result;							\
+  }
 
-#define vset_lane_u32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t b_ = (b);                                             \
-       uint32_t a_ = (a);                                               \
-       uint32x2_t result;                                               \
-       __asm__ ("ins %0.s[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__LD4_LANE_FUNC (int8x8x4_t, uint8_t, 8b, b, s8,)
+__LD4_LANE_FUNC (float32x2x4_t, float32_t, 2s, s, f32,)
+__LD4_LANE_FUNC (float64x1x4_t, float64_t, 1d, d, f64,)
+__LD4_LANE_FUNC (poly8x8x4_t, poly8_t, 8b, b, p8,)
+__LD4_LANE_FUNC (poly16x4x4_t, poly16_t, 4h, h, p16,)
+__LD4_LANE_FUNC (int16x4x4_t, int16_t, 4h, h, s16,)
+__LD4_LANE_FUNC (int32x2x4_t, int32_t, 2s, s, s32,)
+__LD4_LANE_FUNC (int64x1x4_t, int64_t, 1d, d, s64,)
+__LD4_LANE_FUNC (uint8x8x4_t, uint8_t, 8b, b, u8,)
+__LD4_LANE_FUNC (uint16x4x4_t, uint16_t, 4h, h, u16,)
+__LD4_LANE_FUNC (uint32x2x4_t, uint32_t, 2s, s, u32,)
+__LD4_LANE_FUNC (uint64x1x4_t, uint64_t, 1d, d, u64,)
+__LD4_LANE_FUNC (float32x4x4_t, float32_t, 4s, s, f32, q)
+__LD4_LANE_FUNC (float64x2x4_t, float64_t, 2d, d, f64, q)
+__LD4_LANE_FUNC (poly8x16x4_t, poly8_t, 16b, b, p8, q)
+__LD4_LANE_FUNC (poly16x8x4_t, poly16_t, 8h, h, p16, q)
+__LD4_LANE_FUNC (int8x16x4_t, int8_t, 16b, b, s8, q)
+__LD4_LANE_FUNC (int16x8x4_t, int16_t, 8h, h, s16, q)
+__LD4_LANE_FUNC (int32x4x4_t, int32_t, 4s, s, s32, q)
+__LD4_LANE_FUNC (int64x2x4_t, int64_t, 2d, d, s64, q)
+__LD4_LANE_FUNC (uint8x16x4_t, uint8_t, 16b, b, u8, q)
+__LD4_LANE_FUNC (uint16x8x4_t, uint16_t, 8h, h, u16, q)
+__LD4_LANE_FUNC (uint32x4x4_t, uint32_t, 4s, s, u32, q)
+__LD4_LANE_FUNC (uint64x2x4_t, uint64_t, 2d, d, u64, q)
 
-#define vset_lane_u64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x1_t b_ = (b);                                             \
-       uint64_t a_ = (a);                                               \
-       uint64x1_t result;                                               \
-       __asm__ ("ins %0.d[%3], %x1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+#define __ST2_LANE_FUNC(intype, ptrtype, regsuffix,			\
+			lnsuffix, funcsuffix, Q)			\
+  __extension__ static __inline void					\
+  __attribute__ ((__always_inline__))					\
+  vst2 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
+				     intype b, const int c)		\
+  {									\
+    __asm__ ("ld1 {v16." #regsuffix ", v17." #regsuffix "}, %1\n\t"	\
+	     "st2 {v16." #lnsuffix ", v17." #lnsuffix "}[%2], %0\n\t"	\
+	     : "=Q"(*(intype *) ptr)					\
+	     : "Q"(b), "i"(c)						\
+	     : "memory", "v16", "v17");					\
+  }
 
-#define vsetq_lane_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32_t a_ = (a);                                              \
-       float32x4_t result;                                              \
-       __asm__ ("ins %0.s[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__ST2_LANE_FUNC (int8x8x2_t, int8_t, 8b, b, s8,)
+__ST2_LANE_FUNC (float32x2x2_t, float32_t, 2s, s, f32,)
+__ST2_LANE_FUNC (float64x1x2_t, float64_t, 1d, d, f64,)
+__ST2_LANE_FUNC (poly8x8x2_t, poly8_t, 8b, b, p8,)
+__ST2_LANE_FUNC (poly16x4x2_t, poly16_t, 4h, h, p16,)
+__ST2_LANE_FUNC (int16x4x2_t, int16_t, 4h, h, s16,)
+__ST2_LANE_FUNC (int32x2x2_t, int32_t, 2s, s, s32,)
+__ST2_LANE_FUNC (int64x1x2_t, int64_t, 1d, d, s64,)
+__ST2_LANE_FUNC (uint8x8x2_t, uint8_t, 8b, b, u8,)
+__ST2_LANE_FUNC (uint16x4x2_t, uint16_t, 4h, h, u16,)
+__ST2_LANE_FUNC (uint32x2x2_t, uint32_t, 2s, s, u32,)
+__ST2_LANE_FUNC (uint64x1x2_t, uint64_t, 1d, d, u64,)
+__ST2_LANE_FUNC (float32x4x2_t, float32_t, 4s, s, f32, q)
+__ST2_LANE_FUNC (float64x2x2_t, float64_t, 2d, d, f64, q)
+__ST2_LANE_FUNC (poly8x16x2_t, poly8_t, 16b, b, p8, q)
+__ST2_LANE_FUNC (poly16x8x2_t, poly16_t, 8h, h, p16, q)
+__ST2_LANE_FUNC (int8x16x2_t, int8_t, 16b, b, s8, q)
+__ST2_LANE_FUNC (int16x8x2_t, int16_t, 8h, h, s16, q)
+__ST2_LANE_FUNC (int32x4x2_t, int32_t, 4s, s, s32, q)
+__ST2_LANE_FUNC (int64x2x2_t, int64_t, 2d, d, s64, q)
+__ST2_LANE_FUNC (uint8x16x2_t, uint8_t, 16b, b, u8, q)
+__ST2_LANE_FUNC (uint16x8x2_t, uint16_t, 8h, h, u16, q)
+__ST2_LANE_FUNC (uint32x4x2_t, uint32_t, 4s, s, u32, q)
+__ST2_LANE_FUNC (uint64x2x2_t, uint64_t, 2d, d, u64, q)
+
+#define __ST3_LANE_FUNC(intype, ptrtype, regsuffix,			\
+			lnsuffix, funcsuffix, Q)			\
+  __extension__ static __inline void					\
+  __attribute__ ((__always_inline__))					\
+  vst3 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
+				     intype b, const int c)		\
+  {									\
+    __asm__ ("ld1 {v16." #regsuffix " - v18." #regsuffix "}, %1\n\t"	\
+	     "st3 {v16." #lnsuffix " - v18." #lnsuffix "}[%2], %0\n\t"	\
+	     : "=Q"(*(intype *) ptr)					\
+	     : "Q"(b), "i"(c)						\
+	     : "memory", "v16", "v17", "v18");				\
+  }
+
+__ST3_LANE_FUNC (int8x8x3_t, int8_t, 8b, b, s8,)
+__ST3_LANE_FUNC (float32x2x3_t, float32_t, 2s, s, f32,)
+__ST3_LANE_FUNC (float64x1x3_t, float64_t, 1d, d, f64,)
+__ST3_LANE_FUNC (poly8x8x3_t, poly8_t, 8b, b, p8,)
+__ST3_LANE_FUNC (poly16x4x3_t, poly16_t, 4h, h, p16,)
+__ST3_LANE_FUNC (int16x4x3_t, int16_t, 4h, h, s16,)
+__ST3_LANE_FUNC (int32x2x3_t, int32_t, 2s, s, s32,)
+__ST3_LANE_FUNC (int64x1x3_t, int64_t, 1d, d, s64,)
+__ST3_LANE_FUNC (uint8x8x3_t, uint8_t, 8b, b, u8,)
+__ST3_LANE_FUNC (uint16x4x3_t, uint16_t, 4h, h, u16,)
+__ST3_LANE_FUNC (uint32x2x3_t, uint32_t, 2s, s, u32,)
+__ST3_LANE_FUNC (uint64x1x3_t, uint64_t, 1d, d, u64,)
+__ST3_LANE_FUNC (float32x4x3_t, float32_t, 4s, s, f32, q)
+__ST3_LANE_FUNC (float64x2x3_t, float64_t, 2d, d, f64, q)
+__ST3_LANE_FUNC (poly8x16x3_t, poly8_t, 16b, b, p8, q)
+__ST3_LANE_FUNC (poly16x8x3_t, poly16_t, 8h, h, p16, q)
+__ST3_LANE_FUNC (int8x16x3_t, int8_t, 16b, b, s8, q)
+__ST3_LANE_FUNC (int16x8x3_t, int16_t, 8h, h, s16, q)
+__ST3_LANE_FUNC (int32x4x3_t, int32_t, 4s, s, s32, q)
+__ST3_LANE_FUNC (int64x2x3_t, int64_t, 2d, d, s64, q)
+__ST3_LANE_FUNC (uint8x16x3_t, uint8_t, 16b, b, u8, q)
+__ST3_LANE_FUNC (uint16x8x3_t, uint16_t, 8h, h, u16, q)
+__ST3_LANE_FUNC (uint32x4x3_t, uint32_t, 4s, s, u32, q)
+__ST3_LANE_FUNC (uint64x2x3_t, uint64_t, 2d, d, u64, q)
+
+#define __ST4_LANE_FUNC(intype, ptrtype, regsuffix,			\
+			lnsuffix, funcsuffix, Q)			\
+  __extension__ static __inline void					\
+  __attribute__ ((__always_inline__))					\
+  vst4 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
+				     intype b, const int c)		\
+  {									\
+    __asm__ ("ld1 {v16." #regsuffix " - v19." #regsuffix "}, %1\n\t"	\
+	     "st4 {v16." #lnsuffix " - v19." #lnsuffix "}[%2], %0\n\t"	\
+	     : "=Q"(*(intype *) ptr)					\
+	     : "Q"(b), "i"(c)						\
+	     : "memory", "v16", "v17", "v18", "v19");			\
+  }
+
+__ST4_LANE_FUNC (int8x8x4_t, int8_t, 8b, b, s8,)
+__ST4_LANE_FUNC (float32x2x4_t, float32_t, 2s, s, f32,)
+__ST4_LANE_FUNC (float64x1x4_t, float64_t, 1d, d, f64,)
+__ST4_LANE_FUNC (poly8x8x4_t, poly8_t, 8b, b, p8,)
+__ST4_LANE_FUNC (poly16x4x4_t, poly16_t, 4h, h, p16,)
+__ST4_LANE_FUNC (int16x4x4_t, int16_t, 4h, h, s16,)
+__ST4_LANE_FUNC (int32x2x4_t, int32_t, 2s, s, s32,)
+__ST4_LANE_FUNC (int64x1x4_t, int64_t, 1d, d, s64,)
+__ST4_LANE_FUNC (uint8x8x4_t, uint8_t, 8b, b, u8,)
+__ST4_LANE_FUNC (uint16x4x4_t, uint16_t, 4h, h, u16,)
+__ST4_LANE_FUNC (uint32x2x4_t, uint32_t, 2s, s, u32,)
+__ST4_LANE_FUNC (uint64x1x4_t, uint64_t, 1d, d, u64,)
+__ST4_LANE_FUNC (float32x4x4_t, float32_t, 4s, s, f32, q)
+__ST4_LANE_FUNC (float64x2x4_t, float64_t, 2d, d, f64, q)
+__ST4_LANE_FUNC (poly8x16x4_t, poly8_t, 16b, b, p8, q)
+__ST4_LANE_FUNC (poly16x8x4_t, poly16_t, 8h, h, p16, q)
+__ST4_LANE_FUNC (int8x16x4_t, int8_t, 16b, b, s8, q)
+__ST4_LANE_FUNC (int16x8x4_t, int16_t, 8h, h, s16, q)
+__ST4_LANE_FUNC (int32x4x4_t, int32_t, 4s, s, s32, q)
+__ST4_LANE_FUNC (int64x2x4_t, int64_t, 2d, d, s64, q)
+__ST4_LANE_FUNC (uint8x16x4_t, uint8_t, 16b, b, u8, q)
+__ST4_LANE_FUNC (uint16x8x4_t, uint16_t, 8h, h, u16, q)
+__ST4_LANE_FUNC (uint32x4x4_t, uint32_t, 4s, s, u32, q)
+__ST4_LANE_FUNC (uint64x2x4_t, uint64_t, 2d, d, u64, q)
+
+__extension__ static __inline int64_t __attribute__ ((__always_inline__))
+vaddlv_s32 (int32x2_t a)
+{
+  int64_t result;
+  __asm__ ("saddlp %0.1d, %1.2s" : "=w"(result) : "w"(a) : );
+  return result;
+}
+
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vaddlv_u32 (uint32x2_t a)
+{
+  uint64_t result;
+  __asm__ ("uaddlp %0.1d, %1.2s" : "=w"(result) : "w"(a) : );
+  return result;
+}
 
-#define vsetq_lane_f64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64_t a_ = (a);                                              \
-       float64x2_t result;                                              \
-       __asm__ ("ins %0.d[%3], %x1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vpaddd_s64 (int64x2_t __a)
+{
+  return __builtin_aarch64_addpdi (__a);
+}
 
-#define vsetq_lane_p8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x16_t b_ = (b);                                             \
-       poly8_t a_ = (a);                                                \
-       poly8x16_t result;                                               \
-       __asm__ ("ins %0.b[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vqdmulh_laneq_s16 (int16x4_t __a, int16x8_t __b, const int __c)
+{
+  return __builtin_aarch64_sqdmulh_laneqv4hi (__a, __b, __c);
+}
 
-#define vsetq_lane_p16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x8_t b_ = (b);                                             \
-       poly16_t a_ = (a);                                               \
-       poly16x8_t result;                                               \
-       __asm__ ("ins %0.h[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vqdmulh_laneq_s32 (int32x2_t __a, int32x4_t __b, const int __c)
+{
+  return __builtin_aarch64_sqdmulh_laneqv2si (__a, __b, __c);
+}
 
-#define vsetq_lane_s8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x16_t b_ = (b);                                              \
-       int8_t a_ = (a);                                                 \
-       int8x16_t result;                                                \
-       __asm__ ("ins %0.b[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vqdmulhq_laneq_s16 (int16x8_t __a, int16x8_t __b, const int __c)
+{
+  return __builtin_aarch64_sqdmulh_laneqv8hi (__a, __b, __c);
+}
 
-#define vsetq_lane_s16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16_t a_ = (a);                                                \
-       int16x8_t result;                                                \
-       __asm__ ("ins %0.h[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vqdmulhq_laneq_s32 (int32x4_t __a, int32x4_t __b, const int __c)
+{
+  return __builtin_aarch64_sqdmulh_laneqv4si (__a, __b, __c);
+}
 
-#define vsetq_lane_s32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32_t a_ = (a);                                                \
-       int32x4_t result;                                                \
-       __asm__ ("ins %0.s[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vqrdmulh_laneq_s16 (int16x4_t __a, int16x8_t __b, const int __c)
+{
+  return  __builtin_aarch64_sqrdmulh_laneqv4hi (__a, __b, __c);
+}
 
-#define vsetq_lane_s64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       int64_t a_ = (a);                                                \
-       int64x2_t result;                                                \
-       __asm__ ("ins %0.d[%3], %x1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vqrdmulh_laneq_s32 (int32x2_t __a, int32x4_t __b, const int __c)
+{
+  return __builtin_aarch64_sqrdmulh_laneqv2si (__a, __b, __c);
+}
 
-#define vsetq_lane_u8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x16_t b_ = (b);                                             \
-       uint8_t a_ = (a);                                                \
-       uint8x16_t result;                                               \
-       __asm__ ("ins %0.b[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vqrdmulhq_laneq_s16 (int16x8_t __a, int16x8_t __b, const int __c)
+{
+  return __builtin_aarch64_sqrdmulh_laneqv8hi (__a, __b, __c);
+}
 
-#define vsetq_lane_u16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16_t a_ = (a);                                               \
-       uint16x8_t result;                                               \
-       __asm__ ("ins %0.h[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vqrdmulhq_laneq_s32 (int32x4_t __a, int32x4_t __b, const int __c)
+{
+  return __builtin_aarch64_sqrdmulh_laneqv4si (__a, __b, __c);
+}
 
-#define vsetq_lane_u32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32_t a_ = (a);                                               \
-       uint32x4_t result;                                               \
-       __asm__ ("ins %0.s[%3], %w1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+/* Table intrinsics.  */
 
-#define vsetq_lane_u64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x2_t b_ = (b);                                             \
-       uint64_t a_ = (a);                                               \
-       uint64x2_t result;                                               \
-       __asm__ ("ins %0.d[%3], %x1"                                     \
-                : "=w"(result)                                          \
-                : "r"(a_), "0"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vqtbl1_p8 (poly8x16_t a, uint8x8_t b)
+{
+  poly8x8_t result;
+  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vshrn_high_n_s16(a, b, c)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int8x8_t a_ = (a);                                               \
-       int8x16_t result = vcombine_s8                                   \
-                            (a_, vcreate_s8 (UINT64_C (0x0)));          \
-       __asm__ ("shrn2 %0.16b,%1.8h,#%2"                                \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vqtbl1_s8 (int8x16_t a, int8x8_t b)
+{
+  int8x8_t result;
+  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vshrn_high_n_s32(a, b, c)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int16x4_t a_ = (a);                                              \
-       int16x8_t result = vcombine_s16                                  \
-                            (a_, vcreate_s16 (UINT64_C (0x0)));         \
-       __asm__ ("shrn2 %0.8h,%1.4s,#%2"                                 \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vqtbl1_u8 (uint8x16_t a, uint8x8_t b)
+{
+  uint8x8_t result;
+  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vshrn_high_n_s64(a, b, c)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       int32x2_t a_ = (a);                                              \
-       int32x4_t result = vcombine_s32                                  \
-                            (a_, vcreate_s32 (UINT64_C (0x0)));         \
-       __asm__ ("shrn2 %0.4s,%1.2d,#%2"                                 \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vqtbl1q_p8 (poly8x16_t a, uint8x16_t b)
+{
+  poly8x16_t result;
+  __asm__ ("tbl %0.16b, {%1.16b}, %2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vshrn_high_n_u16(a, b, c)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint8x8_t a_ = (a);                                              \
-       uint8x16_t result = vcombine_u8                                  \
-                            (a_, vcreate_u8 (UINT64_C (0x0)));          \
-       __asm__ ("shrn2 %0.16b,%1.8h,#%2"                                \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vqtbl1q_s8 (int8x16_t a, int8x16_t b)
+{
+  int8x16_t result;
+  __asm__ ("tbl %0.16b, {%1.16b}, %2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vshrn_high_n_u32(a, b, c)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint16x4_t a_ = (a);                                             \
-       uint16x8_t result = vcombine_u16                                 \
-                            (a_, vcreate_u16 (UINT64_C (0x0)));         \
-       __asm__ ("shrn2 %0.8h,%1.4s,#%2"                                 \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqtbl1q_u8 (uint8x16_t a, uint8x16_t b)
+{
+  uint8x16_t result;
+  __asm__ ("tbl %0.16b, {%1.16b}, %2.16b"
+           : "=w"(result)
+           : "w"(a), "w"(b)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vshrn_high_n_u64(a, b, c)                                       \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x2_t b_ = (b);                                             \
-       uint32x2_t a_ = (a);                                             \
-       uint32x4_t result = vcombine_u32                                 \
-                            (a_, vcreate_u32 (UINT64_C (0x0)));         \
-       __asm__ ("shrn2 %0.4s,%1.2d,#%2"                                 \
-                : "+w"(result)                                          \
-                : "w"(b_), "i"(c)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vqtbl2_s8 (int8x16x2_t tab, int8x8_t idx)
+{
+  int8x8_t result;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbl %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vshrn_n_s16(a, b)                                               \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t a_ = (a);                                              \
-       int8x8_t result;                                                 \
-       __asm__ ("shrn %0.8b,%1.8h,%2"                                   \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vqtbl2_u8 (uint8x16x2_t tab, uint8x8_t idx)
+{
+  uint8x8_t result;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbl %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vshrn_n_s32(a, b)                                               \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t a_ = (a);                                              \
-       int16x4_t result;                                                \
-       __asm__ ("shrn %0.4h,%1.4s,%2"                                   \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vqtbl2_p8 (poly8x16x2_t tab, uint8x8_t idx)
+{
+  poly8x8_t result;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbl %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vshrn_n_s64(a, b)                                               \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x2_t a_ = (a);                                              \
-       int32x2_t result;                                                \
-       __asm__ ("shrn %0.2s,%1.2d,%2"                                   \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vqtbl2q_s8 (int8x16x2_t tab, int8x16_t idx)
+{
+  int8x16_t result;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbl %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vshrn_n_u16(a, b)                                               \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t a_ = (a);                                             \
-       uint8x8_t result;                                                \
-       __asm__ ("shrn %0.8b,%1.8h,%2"                                   \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqtbl2q_u8 (uint8x16x2_t tab, uint8x16_t idx)
+{
+  uint8x16_t result;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbl %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vshrn_n_u32(a, b)                                               \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t a_ = (a);                                             \
-       uint16x4_t result;                                               \
-       __asm__ ("shrn %0.4h,%1.4s,%2"                                   \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vqtbl2q_p8 (poly8x16x2_t tab, uint8x16_t idx)
+{
+  poly8x16_t result;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbl %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vshrn_n_u64(a, b)                                               \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x2_t a_ = (a);                                             \
-       uint32x2_t result;                                               \
-       __asm__ ("shrn %0.2s,%1.2d,%2"                                   \
-                : "=w"(result)                                          \
-                : "w"(a_), "i"(b)                                       \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vqtbl3_s8 (int8x16x3_t tab, int8x8_t idx)
+{
+  int8x8_t result;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vsli_n_p8(a, b, c)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x8_t b_ = (b);                                              \
-       poly8x8_t a_ = (a);                                              \
-       poly8x8_t result;                                                \
-       __asm__ ("sli %0.8b,%2.8b,%3"                                    \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vqtbl3_u8 (uint8x16x3_t tab, uint8x8_t idx)
+{
+  uint8x8_t result;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vsli_n_p16(a, b, c)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x4_t b_ = (b);                                             \
-       poly16x4_t a_ = (a);                                             \
-       poly16x4_t result;                                               \
-       __asm__ ("sli %0.4h,%2.4h,%3"                                    \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vqtbl3_p8 (poly8x16x3_t tab, uint8x8_t idx)
+{
+  poly8x8_t result;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
+
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vqtbl3q_s8 (int8x16x3_t tab, int8x16_t idx)
+{
+  int8x16_t result;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbl %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vsliq_n_p8(a, b, c)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x16_t b_ = (b);                                             \
-       poly8x16_t a_ = (a);                                             \
-       poly8x16_t result;                                               \
-       __asm__ ("sli %0.16b,%2.16b,%3"                                  \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqtbl3q_u8 (uint8x16x3_t tab, uint8x16_t idx)
+{
+  uint8x16_t result;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbl %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vsliq_n_p16(a, b, c)                                            \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x8_t b_ = (b);                                             \
-       poly16x8_t a_ = (a);                                             \
-       poly16x8_t result;                                               \
-       __asm__ ("sli %0.8h,%2.8h,%3"                                    \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vqtbl3q_p8 (poly8x16x3_t tab, uint8x16_t idx)
+{
+  poly8x16_t result;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbl %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vsri_n_p8(a, b, c)                                              \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x8_t b_ = (b);                                              \
-       poly8x8_t a_ = (a);                                              \
-       poly8x8_t result;                                                \
-       __asm__ ("sri %0.8b,%2.8b,%3"                                    \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vqtbl4_s8 (int8x16x4_t tab, int8x8_t idx)
+{
+  int8x8_t result;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
+  return result;
+}
 
-#define vsri_n_p16(a, b, c)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x4_t b_ = (b);                                             \
-       poly16x4_t a_ = (a);                                             \
-       poly16x4_t result;                                               \
-       __asm__ ("sri %0.4h,%2.4h,%3"                                    \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vqtbl4_u8 (uint8x16x4_t tab, uint8x8_t idx)
+{
+  uint8x8_t result;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
+  return result;
+}
 
-#define vsriq_n_p8(a, b, c)                                             \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x16_t b_ = (b);                                             \
-       poly8x16_t a_ = (a);                                             \
-       poly8x16_t result;                                               \
-       __asm__ ("sri %0.16b,%2.16b,%3"                                  \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vqtbl4_p8 (poly8x16x4_t tab, uint8x8_t idx)
+{
+  poly8x8_t result;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
+  return result;
+}
 
-#define vsriq_n_p16(a, b, c)                                            \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x8_t b_ = (b);                                             \
-       poly16x8_t a_ = (a);                                             \
-       poly16x8_t result;                                               \
-       __asm__ ("sri %0.8h,%2.8h,%3"                                    \
-                : "=w"(result)                                          \
-                : "0"(a_), "w"(b_), "i"(c)                              \
-                : /* No clobbers */);                                   \
-       result;                                                          \
-     })
 
-#define vst1_lane_f32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t b_ = (b);                                            \
-       float32_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vqtbl4q_s8 (int8x16x4_t tab, int8x16_t idx)
+{
+  int8x16_t result;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbl %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
+  return result;
+}
 
-#define vst1_lane_f64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x1_t b_ = (b);                                            \
-       float64_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqtbl4q_u8 (uint8x16x4_t tab, uint8x16_t idx)
+{
+  uint8x16_t result;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbl %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
+  return result;
+}
 
-#define vst1_lane_p8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x8_t b_ = (b);                                              \
-       poly8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vqtbl4q_p8 (poly8x16x4_t tab, uint8x16_t idx)
+{
+  poly8x16_t result;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbl %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
+	   :"=w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
+  return result;
+}
 
-#define vst1_lane_p16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x4_t b_ = (b);                                             \
-       poly16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
 
-#define vst1_lane_s8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x8_t b_ = (b);                                               \
-       int8_t * a_ = (a);                                               \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vqtbx1_s8 (int8x8_t r, int8x16_t tab, int8x8_t idx)
+{
+  int8x8_t result = r;
+  __asm__ ("tbx %0.8b,{%1.16b},%2.8b"
+           : "+w"(result)
+           : "w"(tab), "w"(idx)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vst1_lane_s16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t b_ = (b);                                              \
-       int16_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vqtbx1_u8 (uint8x8_t r, uint8x16_t tab, uint8x8_t idx)
+{
+  uint8x8_t result = r;
+  __asm__ ("tbx %0.8b,{%1.16b},%2.8b"
+           : "+w"(result)
+           : "w"(tab), "w"(idx)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vst1_lane_s32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t b_ = (b);                                              \
-       int32_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vqtbx1_p8 (poly8x8_t r, poly8x16_t tab, uint8x8_t idx)
+{
+  poly8x8_t result = r;
+  __asm__ ("tbx %0.8b,{%1.16b},%2.8b"
+           : "+w"(result)
+           : "w"(tab), "w"(idx)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vst1_lane_s64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x1_t b_ = (b);                                              \
-       int64_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vqtbx1q_s8 (int8x16_t r, int8x16_t tab, int8x16_t idx)
+{
+  int8x16_t result = r;
+  __asm__ ("tbx %0.16b,{%1.16b},%2.16b"
+           : "+w"(result)
+           : "w"(tab), "w"(idx)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vst1_lane_u8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x8_t b_ = (b);                                              \
-       uint8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqtbx1q_u8 (uint8x16_t r, uint8x16_t tab, uint8x16_t idx)
+{
+  uint8x16_t result = r;
+  __asm__ ("tbx %0.16b,{%1.16b},%2.16b"
+           : "+w"(result)
+           : "w"(tab), "w"(idx)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vst1_lane_u16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t b_ = (b);                                             \
-       uint16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vqtbx1q_p8 (poly8x16_t r, poly8x16_t tab, uint8x16_t idx)
+{
+  poly8x16_t result = r;
+  __asm__ ("tbx %0.16b,{%1.16b},%2.16b"
+           : "+w"(result)
+           : "w"(tab), "w"(idx)
+           : /* No clobbers */);
+  return result;
+}
 
-#define vst1_lane_u32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t b_ = (b);                                             \
-       uint32_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vqtbx2_s8 (int8x8_t r, int8x16x2_t tab, int8x8_t idx)
+{
+  int8x8_t result = r;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbx %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vst1_lane_u64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x1_t b_ = (b);                                             \
-       uint64_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vqtbx2_u8 (uint8x8_t r, uint8x16x2_t tab, uint8x8_t idx)
+{
+  uint8x8_t result = r;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbx %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vqtbx2_p8 (poly8x8_t r, poly8x16x2_t tab, uint8x8_t idx)
+{
+  poly8x8_t result = r;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbx %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vst1q_lane_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
 
-#define vst1q_lane_f64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vqtbx2q_s8 (int8x16_t r, int8x16x2_t tab, int8x16_t idx)
+{
+  int8x16_t result = r;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbx %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vst1q_lane_p8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x16_t b_ = (b);                                             \
-       poly8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqtbx2q_u8 (uint8x16_t r, uint8x16x2_t tab, uint8x16_t idx)
+{
+  uint8x16_t result = r;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbx %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vst1q_lane_p16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x8_t b_ = (b);                                             \
-       poly16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vqtbx2q_p8 (poly8x16_t r, poly8x16x2_t tab, uint8x16_t idx)
+{
+  poly8x16_t result = r;
+  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
+	   "tbx %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17");
+  return result;
+}
 
-#define vst1q_lane_s8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x16_t b_ = (b);                                              \
-       int8_t * a_ = (a);                                               \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
 
-#define vst1q_lane_s16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vqtbx3_s8 (int8x8_t r, int8x16x3_t tab, int8x8_t idx)
+{
+  int8x8_t result = r;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbx %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vst1q_lane_s32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vqtbx3_u8 (uint8x8_t r, uint8x16x3_t tab, uint8x8_t idx)
+{
+  uint8x8_t result = r;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbx %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
+
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vqtbx3_p8 (poly8x8_t r, poly8x16x3_t tab, uint8x8_t idx)
+{
+  poly8x8_t result = r;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbx %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vst1q_lane_s64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       int64_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
 
-#define vst1q_lane_u8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x16_t b_ = (b);                                             \
-       uint8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vqtbx3q_s8 (int8x16_t r, int8x16x3_t tab, int8x16_t idx)
+{
+  int8x16_t result = r;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbx %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vst1q_lane_u16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqtbx3q_u8 (uint8x16_t r, uint8x16x3_t tab, uint8x16_t idx)
+{
+  uint8x16_t result = r;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbx %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vst1q_lane_u32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vqtbx3q_p8 (poly8x16_t r, poly8x16x3_t tab, uint8x16_t idx)
+{
+  poly8x16_t result = r;
+  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
+	   "tbx %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18");
+  return result;
+}
 
-#define vst1q_lane_u64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x2_t b_ = (b);                                             \
-       uint64_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vsubhn_high_s16 (int8x8_t a, int16x8_t b, int16x8_t c)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vqtbx4_s8 (int8x8_t r, int8x16x4_t tab, int8x8_t idx)
 {
-  int8x16_t result = vcombine_s8 (a, vcreate_s8 (UINT64_C (0x0)));
-  __asm__ ("subhn2 %0.16b, %1.8h, %2.8h"
-           : "+w"(result)
-           : "w"(b), "w"(c)
-           : /* No clobbers */);
+  int8x8_t result = r;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbx %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vsubhn_high_s32 (int16x4_t a, int32x4_t b, int32x4_t c)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vqtbx4_u8 (uint8x8_t r, uint8x16x4_t tab, uint8x8_t idx)
 {
-  int16x8_t result = vcombine_s16 (a, vcreate_s16 (UINT64_C (0x0)));
-  __asm__ ("subhn2 %0.8h, %1.4s, %2.4s"
-           : "+w"(result)
-           : "w"(b), "w"(c)
-           : /* No clobbers */);
+  uint8x8_t result = r;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbx %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vsubhn_high_s64 (int32x2_t a, int64x2_t b, int64x2_t c)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vqtbx4_p8 (poly8x8_t r, poly8x16x4_t tab, uint8x8_t idx)
 {
-  int32x4_t result = vcombine_s32 (a, vcreate_s32 (UINT64_C (0x0)));
-  __asm__ ("subhn2 %0.4s, %1.2d, %2.2d"
-           : "+w"(result)
-           : "w"(b), "w"(c)
-           : /* No clobbers */);
+  poly8x8_t result = r;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbx %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
   return result;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vsubhn_high_u16 (uint8x8_t a, uint16x8_t b, uint16x8_t c)
+
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vqtbx4q_s8 (int8x16_t r, int8x16x4_t tab, int8x16_t idx)
 {
-  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (UINT64_C (0x0)));
-  __asm__ ("subhn2 %0.16b, %1.8h, %2.8h"
-           : "+w"(result)
-           : "w"(b), "w"(c)
-           : /* No clobbers */);
+  int8x16_t result = r;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbx %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
   return result;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vsubhn_high_u32 (uint16x4_t a, uint32x4_t b, uint32x4_t c)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vqtbx4q_u8 (uint8x16_t r, uint8x16x4_t tab, uint8x16_t idx)
 {
-  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (UINT64_C (0x0)));
-  __asm__ ("subhn2 %0.8h, %1.4s, %2.4s"
-           : "+w"(result)
-           : "w"(b), "w"(c)
-           : /* No clobbers */);
+  uint8x16_t result = r;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbx %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
   return result;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vsubhn_high_u64 (uint32x2_t a, uint64x2_t b, uint64x2_t c)
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vqtbx4q_p8 (poly8x16_t r, poly8x16x4_t tab, uint8x16_t idx)
 {
-  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (UINT64_C (0x0)));
-  __asm__ ("subhn2 %0.4s, %1.2d, %2.2d"
-           : "+w"(result)
-           : "w"(b), "w"(c)
-           : /* No clobbers */);
+  poly8x16_t result = r;
+  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
+	   "tbx %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
+	   :"+w"(result)
+	   :"Q"(tab),"w"(idx)
+	   :"memory", "v16", "v17", "v18", "v19");
   return result;
 }
 
+/* V7 legacy table intrinsics.  */
+
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vsubhn_s16 (int16x8_t a, int16x8_t b)
+vtbl1_s8 (int8x8_t tab, int8x8_t idx)
 {
   int8x8_t result;
-  __asm__ ("subhn %0.8b, %1.8h, %2.8h"
+  int8x16_t temp = vcombine_s8 (tab, vcreate_s8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(temp), "w"(idx)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vsubhn_s32 (int32x4_t a, int32x4_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vtbl1_u8 (uint8x8_t tab, uint8x8_t idx)
 {
-  int16x4_t result;
-  __asm__ ("subhn %0.4h, %1.4s, %2.4s"
+  uint8x8_t result;
+  uint8x16_t temp = vcombine_u8 (tab, vcreate_u8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(temp), "w"(idx)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vsubhn_s64 (int64x2_t a, int64x2_t b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vtbl1_p8 (poly8x8_t tab, uint8x8_t idx)
 {
-  int32x2_t result;
-  __asm__ ("subhn %0.2s, %1.2d, %2.2d"
+  poly8x8_t result;
+  poly8x16_t temp = vcombine_p8 (tab, vcreate_p8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(temp), "w"(idx)
+           : /* No clobbers */);
+  return result;
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vtbl2_s8 (int8x8x2_t tab, int8x8_t idx)
+{
+  int8x8_t result;
+  int8x16_t temp = vcombine_s8 (tab.val[0], tab.val[1]);
+  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
+           : "=w"(result)
+           : "w"(temp), "w"(idx)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vsubhn_u16 (uint16x8_t a, uint16x8_t b)
+vtbl2_u8 (uint8x8x2_t tab, uint8x8_t idx)
 {
   uint8x8_t result;
-  __asm__ ("subhn %0.8b, %1.8h, %2.8h"
+  uint8x16_t temp = vcombine_u8 (tab.val[0], tab.val[1]);
+  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(temp), "w"(idx)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vsubhn_u32 (uint32x4_t a, uint32x4_t b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vtbl2_p8 (poly8x8x2_t tab, uint8x8_t idx)
 {
-  uint16x4_t result;
-  __asm__ ("subhn %0.4h, %1.4s, %2.4s"
+  poly8x8_t result;
+  poly8x16_t temp = vcombine_p8 (tab.val[0], tab.val[1]);
+  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
            : "=w"(result)
-           : "w"(a), "w"(b)
+           : "w"(temp), "w"(idx)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vsubhn_u64 (uint64x2_t a, uint64x2_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vtbl3_s8 (int8x8x3_t tab, int8x8_t idx)
 {
-  uint32x2_t result;
-  __asm__ ("subhn %0.2s, %1.2d, %2.2d"
+  int8x8_t result;
+  int8x16x2_t temp;
+  temp.val[0] = vcombine_s8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_s8 (tab.val[2], vcreate_s8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
            : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+           : "Q"(temp), "w"(idx)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vtrn1_f32 (float32x2_t a, float32x2_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vtbl3_u8 (uint8x8x3_t tab, uint8x8_t idx)
 {
-  float32x2_t result;
-  __asm__ ("trn1 %0.2s,%1.2s,%2.2s"
+  uint8x8_t result;
+  uint8x16x2_t temp;
+  temp.val[0] = vcombine_u8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_u8 (tab.val[2], vcreate_u8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
            : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+           : "Q"(temp), "w"(idx)
+           : "v16", "v17", "memory");
   return result;
 }
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtrn1_p8 (poly8x8_t a, poly8x8_t b)
+vtbl3_p8 (poly8x8x3_t tab, uint8x8_t idx)
 {
   poly8x8_t result;
-  __asm__ ("trn1 %0.8b,%1.8b,%2.8b"
+  poly8x16x2_t temp;
+  temp.val[0] = vcombine_p8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_p8 (tab.val[2], vcreate_p8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
            : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+           : "Q"(temp), "w"(idx)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vtrn1_p16 (poly16x4_t a, poly16x4_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vtbl4_s8 (int8x8x4_t tab, int8x8_t idx)
 {
-  poly16x4_t result;
-  __asm__ ("trn1 %0.4h,%1.4h,%2.4h"
+  int8x8_t result;
+  int8x16x2_t temp;
+  temp.val[0] = vcombine_s8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_s8 (tab.val[2], tab.val[3]);
+  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
            : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+           : "Q"(temp), "w"(idx)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtrn1_s8 (int8x8_t a, int8x8_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vtbl4_u8 (uint8x8x4_t tab, uint8x8_t idx)
 {
-  int8x8_t result;
-  __asm__ ("trn1 %0.8b,%1.8b,%2.8b"
+  uint8x8_t result;
+  uint8x16x2_t temp;
+  temp.val[0] = vcombine_u8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_u8 (tab.val[2], tab.val[3]);
+  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
            : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+           : "Q"(temp), "w"(idx)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vtrn1_s16 (int16x4_t a, int16x4_t b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vtbl4_p8 (poly8x8x4_t tab, uint8x8_t idx)
 {
-  int16x4_t result;
-  __asm__ ("trn1 %0.4h,%1.4h,%2.4h"
+  poly8x8_t result;
+  poly8x16x2_t temp;
+  temp.val[0] = vcombine_p8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_p8 (tab.val[2], tab.val[3]);
+  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
+	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
            : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+           : "Q"(temp), "w"(idx)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vtrn1_s32 (int32x2_t a, int32x2_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vtbx1_s8 (int8x8_t r, int8x8_t tab, int8x8_t idx)
 {
-  int32x2_t result;
-  __asm__ ("trn1 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  int8x8_t result;
+  int8x8_t tmp1;
+  int8x16_t temp = vcombine_s8 (tab, vcreate_s8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("movi %0.8b, 8\n\t"
+	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
+	   "tbl %1.8b, {%2.16b}, %3.8b\n\t"
+	   "bsl %0.8b, %4.8b, %1.8b\n\t"
+           : "+w"(result), "=&w"(tmp1)
+           : "w"(temp), "w"(idx), "w"(r)
            : /* No clobbers */);
   return result;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtrn1_u8 (uint8x8_t a, uint8x8_t b)
+vtbx1_u8 (uint8x8_t r, uint8x8_t tab, uint8x8_t idx)
 {
   uint8x8_t result;
-  __asm__ ("trn1 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  uint8x8_t tmp1;
+  uint8x16_t temp = vcombine_u8 (tab, vcreate_u8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("movi %0.8b, 8\n\t"
+	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
+	   "tbl %1.8b, {%2.16b}, %3.8b\n\t"
+	   "bsl %0.8b, %4.8b, %1.8b\n\t"
+           : "+w"(result), "=&w"(tmp1)
+           : "w"(temp), "w"(idx), "w"(r)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vtrn1_u16 (uint16x4_t a, uint16x4_t b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vtbx1_p8 (poly8x8_t r, poly8x8_t tab, uint8x8_t idx)
 {
-  uint16x4_t result;
-  __asm__ ("trn1 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  poly8x8_t result;
+  poly8x8_t tmp1;
+  poly8x16_t temp = vcombine_p8 (tab, vcreate_p8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("movi %0.8b, 8\n\t"
+	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
+	   "tbl %1.8b, {%2.16b}, %3.8b\n\t"
+	   "bsl %0.8b, %4.8b, %1.8b\n\t"
+           : "+w"(result), "=&w"(tmp1)
+           : "w"(temp), "w"(idx), "w"(r)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vtrn1_u32 (uint32x2_t a, uint32x2_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vtbx2_s8 (int8x8_t r, int8x8x2_t tab, int8x8_t idx)
 {
-  uint32x2_t result;
-  __asm__ ("trn1 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  int8x8_t result = r;
+  int8x16_t temp = vcombine_s8 (tab.val[0], tab.val[1]);
+  __asm__ ("tbx %0.8b, {%1.16b}, %2.8b"
+           : "+w"(result)
+           : "w"(temp), "w"(idx)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vtrn1q_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vtbx2_u8 (uint8x8_t r, uint8x8x2_t tab, uint8x8_t idx)
 {
-  float32x4_t result;
-  __asm__ ("trn1 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  uint8x8_t result = r;
+  uint8x16_t temp = vcombine_u8 (tab.val[0], tab.val[1]);
+  __asm__ ("tbx %0.8b, {%1.16b}, %2.8b"
+           : "+w"(result)
+           : "w"(temp), "w"(idx)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vtrn1q_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vtbx2_p8 (poly8x8_t r, poly8x8x2_t tab, uint8x8_t idx)
 {
-  float64x2_t result;
-  __asm__ ("trn1 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
+  poly8x8_t result = r;
+  poly8x16_t temp = vcombine_p8 (tab.val[0], tab.val[1]);
+  __asm__ ("tbx %0.8b, {%1.16b}, %2.8b"
+           : "+w"(result)
+           : "w"(temp), "w"(idx)
            : /* No clobbers */);
   return result;
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vtrn1q_p8 (poly8x16_t a, poly8x16_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vtbx3_s8 (int8x8_t r, int8x8x3_t tab, int8x8_t idx)
 {
-  poly8x16_t result;
-  __asm__ ("trn1 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+  int8x8_t result;
+  int8x8_t tmp1;
+  int8x16x2_t temp;
+  temp.val[0] = vcombine_s8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_s8 (tab.val[2], vcreate_s8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("ld1 {v16.16b - v17.16b}, %2\n\t"
+	   "movi %0.8b, 24\n\t"
+	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
+	   "tbl %1.8b, {v16.16b - v17.16b}, %3.8b\n\t"
+	   "bsl %0.8b, %4.8b, %1.8b\n\t"
+           : "+w"(result), "=&w"(tmp1)
+           : "Q"(temp), "w"(idx), "w"(r)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vtrn1q_p16 (poly16x8_t a, poly16x8_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vtbx3_u8 (uint8x8_t r, uint8x8x3_t tab, uint8x8_t idx)
 {
-  poly16x8_t result;
-  __asm__ ("trn1 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+  uint8x8_t result;
+  uint8x8_t tmp1;
+  uint8x16x2_t temp;
+  temp.val[0] = vcombine_u8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_u8 (tab.val[2], vcreate_u8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("ld1 {v16.16b - v17.16b}, %2\n\t"
+	   "movi %0.8b, 24\n\t"
+	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
+	   "tbl %1.8b, {v16.16b - v17.16b}, %3.8b\n\t"
+	   "bsl %0.8b, %4.8b, %1.8b\n\t"
+           : "+w"(result), "=&w"(tmp1)
+           : "Q"(temp), "w"(idx), "w"(r)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vtrn1q_s8 (int8x16_t a, int8x16_t b)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vtbx3_p8 (poly8x8_t r, poly8x8x3_t tab, uint8x8_t idx)
 {
-  int8x16_t result;
-  __asm__ ("trn1 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+  poly8x8_t result;
+  poly8x8_t tmp1;
+  poly8x16x2_t temp;
+  temp.val[0] = vcombine_p8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_p8 (tab.val[2], vcreate_p8 (__AARCH64_UINT64_C (0x0)));
+  __asm__ ("ld1 {v16.16b - v17.16b}, %2\n\t"
+	   "movi %0.8b, 24\n\t"
+	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
+	   "tbl %1.8b, {v16.16b - v17.16b}, %3.8b\n\t"
+	   "bsl %0.8b, %4.8b, %1.8b\n\t"
+           : "+w"(result), "=&w"(tmp1)
+           : "Q"(temp), "w"(idx), "w"(r)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vtrn1q_s16 (int16x8_t a, int16x8_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vtbx4_s8 (int8x8_t r, int8x8x4_t tab, int8x8_t idx)
 {
-  int16x8_t result;
-  __asm__ ("trn1 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+  int8x8_t result = r;
+  int8x16x2_t temp;
+  temp.val[0] = vcombine_s8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_s8 (tab.val[2], tab.val[3]);
+  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
+	   "tbx %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
+           : "+w"(result)
+           : "Q"(temp), "w"(idx)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vtrn1q_s32 (int32x4_t a, int32x4_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vtbx4_u8 (uint8x8_t r, uint8x8x4_t tab, uint8x8_t idx)
 {
-  int32x4_t result;
-  __asm__ ("trn1 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
+  uint8x8_t result = r;
+  uint8x16x2_t temp;
+  temp.val[0] = vcombine_u8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_u8 (tab.val[2], tab.val[3]);
+  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
+	   "tbx %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
+           : "+w"(result)
+           : "Q"(temp), "w"(idx)
+           : "v16", "v17", "memory");
+  return result;
+}
+
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vtbx4_p8 (poly8x8_t r, poly8x8x4_t tab, uint8x8_t idx)
+{
+  poly8x8_t result = r;
+  poly8x16x2_t temp;
+  temp.val[0] = vcombine_p8 (tab.val[0], tab.val[1]);
+  temp.val[1] = vcombine_p8 (tab.val[2], tab.val[3]);
+  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
+	   "tbx %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
+           : "+w"(result)
+           : "Q"(temp), "w"(idx)
+           : "v16", "v17", "memory");
   return result;
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vtrn1q_s64 (int64x2_t a, int64x2_t b)
+/* End of temporary inline asm.  */
+
+/* Start of optimal implementations in approved order.  */
+
+/* vabs  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vabs_f32 (float32x2_t __a)
 {
-  int64x2_t result;
-  __asm__ ("trn1 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv2sf (__a);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vtrn1q_u8 (uint8x16_t a, uint8x16_t b)
+__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))
+vabs_f64 (float64x1_t __a)
 {
-  uint8x16_t result;
-  __asm__ ("trn1 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_fabs (__a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vtrn1q_u16 (uint16x8_t a, uint16x8_t b)
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vabs_s8 (int8x8_t __a)
 {
-  uint16x8_t result;
-  __asm__ ("trn1 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv8qi (__a);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vtrn1q_u32 (uint32x4_t a, uint32x4_t b)
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vabs_s16 (int16x4_t __a)
 {
-  uint32x4_t result;
-  __asm__ ("trn1 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv4hi (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vtrn1q_u64 (uint64x2_t a, uint64x2_t b)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vabs_s32 (int32x2_t __a)
 {
-  uint64x2_t result;
-  __asm__ ("trn1 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv2si (__a);
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vtrn2_f32 (float32x2_t a, float32x2_t b)
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vabs_s64 (int64x1_t __a)
 {
-  float32x2_t result;
-  __asm__ ("trn2 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_llabs (__a);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtrn2_p8 (poly8x8_t a, poly8x8_t b)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vabsq_f32 (float32x4_t __a)
 {
-  poly8x8_t result;
-  __asm__ ("trn2 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv4sf (__a);
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vtrn2_p16 (poly16x4_t a, poly16x4_t b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vabsq_f64 (float64x2_t __a)
 {
-  poly16x4_t result;
-  __asm__ ("trn2 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv2df (__a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtrn2_s8 (int8x8_t a, int8x8_t b)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vabsq_s8 (int8x16_t __a)
 {
-  int8x8_t result;
-  __asm__ ("trn2 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv16qi (__a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vtrn2_s16 (int16x4_t a, int16x4_t b)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vabsq_s16 (int16x8_t __a)
 {
-  int16x4_t result;
-  __asm__ ("trn2 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv8hi (__a);
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vtrn2_s32 (int32x2_t a, int32x2_t b)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vabsq_s32 (int32x4_t __a)
 {
-  int32x2_t result;
-  __asm__ ("trn2 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv4si (__a);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtrn2_u8 (uint8x8_t a, uint8x8_t b)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vabsq_s64 (int64x2_t __a)
 {
-  uint8x8_t result;
-  __asm__ ("trn2 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_absv2di (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vtrn2_u16 (uint16x4_t a, uint16x4_t b)
+/* vadd */
+
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vaddd_s64 (int64x1_t __a, int64x1_t __b)
 {
-  uint16x4_t result;
-  __asm__ ("trn2 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a + __b;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vtrn2_u32 (uint32x2_t a, uint32x2_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vaddd_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  uint32x2_t result;
-  __asm__ ("trn2 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a + __b;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vtrn2q_f32 (float32x4_t a, float32x4_t b)
+/* vaddv */
+
+__extension__ static __inline int8_t __attribute__ ((__always_inline__))
+vaddv_s8 (int8x8_t __a)
 {
-  float32x4_t result;
-  __asm__ ("trn2 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vget_lane_s8 (__builtin_aarch64_reduc_splus_v8qi (__a), 0);
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vtrn2q_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline int16_t __attribute__ ((__always_inline__))
+vaddv_s16 (int16x4_t __a)
 {
-  float64x2_t result;
-  __asm__ ("trn2 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vget_lane_s16 (__builtin_aarch64_reduc_splus_v4hi (__a), 0);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vtrn2q_p8 (poly8x16_t a, poly8x16_t b)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vaddv_s32 (int32x2_t __a)
 {
-  poly8x16_t result;
-  __asm__ ("trn2 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vget_lane_s32 (__builtin_aarch64_reduc_splus_v2si (__a), 0);
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vtrn2q_p16 (poly16x8_t a, poly16x8_t b)
+__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
+vaddv_u8 (uint8x8_t __a)
 {
-  poly16x8_t result;
-  __asm__ ("trn2 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vget_lane_u8 ((uint8x8_t)
+		__builtin_aarch64_reduc_uplus_v8qi ((int8x8_t) __a), 0);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vtrn2q_s8 (int8x16_t a, int8x16_t b)
+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
+vaddv_u16 (uint16x4_t __a)
 {
-  int8x16_t result;
-  __asm__ ("trn2 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vget_lane_u16 ((uint16x4_t)
+		__builtin_aarch64_reduc_uplus_v4hi ((int16x4_t) __a), 0);
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vtrn2q_s16 (int16x8_t a, int16x8_t b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vaddv_u32 (uint32x2_t __a)
 {
-  int16x8_t result;
-  __asm__ ("trn2 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vget_lane_u32 ((uint32x2_t)
+		__builtin_aarch64_reduc_uplus_v2si ((int32x2_t) __a), 0);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vtrn2q_s32 (int32x4_t a, int32x4_t b)
+__extension__ static __inline int8_t __attribute__ ((__always_inline__))
+vaddvq_s8 (int8x16_t __a)
 {
-  int32x4_t result;
-  __asm__ ("trn2 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vgetq_lane_s8 (__builtin_aarch64_reduc_splus_v16qi (__a), 0);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vtrn2q_s64 (int64x2_t a, int64x2_t b)
+__extension__ static __inline int16_t __attribute__ ((__always_inline__))
+vaddvq_s16 (int16x8_t __a)
 {
-  int64x2_t result;
-  __asm__ ("trn2 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vgetq_lane_s16 (__builtin_aarch64_reduc_splus_v8hi (__a), 0);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vtrn2q_u8 (uint8x16_t a, uint8x16_t b)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vaddvq_s32 (int32x4_t __a)
 {
-  uint8x16_t result;
-  __asm__ ("trn2 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vgetq_lane_s32 (__builtin_aarch64_reduc_splus_v4si (__a), 0);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vtrn2q_u16 (uint16x8_t a, uint16x8_t b)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vaddvq_s64 (int64x2_t __a)
 {
-  uint16x8_t result;
-  __asm__ ("trn2 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vgetq_lane_s64 (__builtin_aarch64_reduc_splus_v2di (__a), 0);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vtrn2q_u32 (uint32x4_t a, uint32x4_t b)
+__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
+vaddvq_u8 (uint8x16_t __a)
 {
-  uint32x4_t result;
-  __asm__ ("trn2 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vgetq_lane_u8 ((uint8x16_t)
+		__builtin_aarch64_reduc_uplus_v16qi ((int8x16_t) __a), 0);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vtrn2q_u64 (uint64x2_t a, uint64x2_t b)
+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
+vaddvq_u16 (uint16x8_t __a)
 {
-  uint64x2_t result;
-  __asm__ ("trn2 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vgetq_lane_u16 ((uint16x8_t)
+		__builtin_aarch64_reduc_uplus_v8hi ((int16x8_t) __a), 0);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtst_p8 (poly8x8_t a, poly8x8_t b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vaddvq_u32 (uint32x4_t __a)
 {
-  uint8x8_t result;
-  __asm__ ("cmtst %0.8b, %1.8b, %2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vgetq_lane_u32 ((uint32x4_t)
+		__builtin_aarch64_reduc_uplus_v4si ((int32x4_t) __a), 0);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vtst_p16 (poly16x4_t a, poly16x4_t b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vaddvq_u64 (uint64x2_t __a)
 {
-  uint16x4_t result;
-  __asm__ ("cmtst %0.4h, %1.4h, %2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vgetq_lane_u64 ((uint64x2_t)
+		__builtin_aarch64_reduc_uplus_v2di ((int64x2_t) __a), 0);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vtstq_p8 (poly8x16_t a, poly8x16_t b)
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vaddv_f32 (float32x2_t __a)
 {
-  uint8x16_t result;
-  __asm__ ("cmtst %0.16b, %1.16b, %2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  float32x2_t t = __builtin_aarch64_reduc_splus_v2sf (__a);
+  return vget_lane_f32 (t, 0);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vtstq_p16 (poly16x8_t a, poly16x8_t b)
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vaddvq_f32 (float32x4_t __a)
 {
-  uint16x8_t result;
-  __asm__ ("cmtst %0.8h, %1.8h, %2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  float32x4_t t = __builtin_aarch64_reduc_splus_v4sf (__a);
+  return vgetq_lane_f32 (t, 0);
 }
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vuzp1_f32 (float32x2_t a, float32x2_t b)
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vaddvq_f64 (float64x2_t __a)
 {
-  float32x2_t result;
-  __asm__ ("uzp1 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  float64x2_t t = __builtin_aarch64_reduc_splus_v2df (__a);
+  return vgetq_lane_f64 (t, 0);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vuzp1_p8 (poly8x8_t a, poly8x8_t b)
+#ifdef __ARM_FEATURE_CRYPTO
+
+/* vaes  */
+
+static __inline uint8x16_t
+vaeseq_u8 (uint8x16_t data, uint8x16_t key)
 {
-  poly8x8_t result;
-  __asm__ ("uzp1 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_crypto_aesev16qi_uuu (data, key);
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vuzp1_p16 (poly16x4_t a, poly16x4_t b)
+static __inline uint8x16_t
+__attribute__ ((__always_inline__))
+vaesdq_u8 (uint8x16_t data, uint8x16_t key)
 {
-  poly16x4_t result;
-  __asm__ ("uzp1 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_crypto_aesdv16qi_uuu (data, key);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vuzp1_s8 (int8x8_t a, int8x8_t b)
+static __inline uint8x16_t
+vaesmcq_u8 (uint8x16_t data)
 {
-  int8x8_t result;
-  __asm__ ("uzp1 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_crypto_aesmcv16qi_uu (data);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vuzp1_s16 (int16x4_t a, int16x4_t b)
+static __inline uint8x16_t
+vaesimcq_u8 (uint8x16_t data)
 {
-  int16x4_t result;
-  __asm__ ("uzp1 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_aarch64_crypto_aesimcv16qi_uu (data);
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vuzp1_s32 (int32x2_t a, int32x2_t b)
+#endif
+
+/* vcage  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcages_f32 (float32_t __a, float32_t __b)
 {
-  int32x2_t result;
-  __asm__ ("uzp1 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_fabsf (__a) >= __builtin_fabsf (__b) ? -1 : 0;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vuzp1_u8 (uint8x8_t a, uint8x8_t b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcage_f32 (float32x2_t __a, float32x2_t __b)
 {
-  uint8x8_t result;
-  __asm__ ("uzp1 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabs_f32 (__a) >= vabs_f32 (__b);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vuzp1_u16 (uint16x4_t a, uint16x4_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcageq_f32 (float32x4_t __a, float32x4_t __b)
 {
-  uint16x4_t result;
-  __asm__ ("uzp1 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabsq_f32 (__a) >= vabsq_f32 (__b);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vuzp1_u32 (uint32x2_t a, uint32x2_t b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcaged_f64 (float64_t __a, float64_t __b)
 {
-  uint32x2_t result;
-  __asm__ ("uzp1 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_fabs (__a) >= __builtin_fabs (__b) ? -1 : 0;
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vuzp1q_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcageq_f64 (float64x2_t __a, float64x2_t __b)
 {
-  float32x4_t result;
-  __asm__ ("uzp1 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabsq_f64 (__a) >= vabsq_f64 (__b);
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vuzp1q_f64 (float64x2_t a, float64x2_t b)
+/* vcagt  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcagts_f32 (float32_t __a, float32_t __b)
 {
-  float64x2_t result;
-  __asm__ ("uzp1 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_fabsf (__a) > __builtin_fabsf (__b) ? -1 : 0;
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vuzp1q_p8 (poly8x16_t a, poly8x16_t b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcagt_f32 (float32x2_t __a, float32x2_t __b)
 {
-  poly8x16_t result;
-  __asm__ ("uzp1 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabs_f32 (__a) > vabs_f32 (__b);
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vuzp1q_p16 (poly16x8_t a, poly16x8_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcagtq_f32 (float32x4_t __a, float32x4_t __b)
 {
-  poly16x8_t result;
-  __asm__ ("uzp1 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabsq_f32 (__a) > vabsq_f32 (__b);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vuzp1q_s8 (int8x16_t a, int8x16_t b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcagtd_f64 (float64_t __a, float64_t __b)
 {
-  int8x16_t result;
-  __asm__ ("uzp1 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __builtin_fabs (__a) > __builtin_fabs (__b) ? -1 : 0;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vuzp1q_s16 (int16x8_t a, int16x8_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcagtq_f64 (float64x2_t __a, float64x2_t __b)
 {
-  int16x8_t result;
-  __asm__ ("uzp1 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabsq_f64 (__a) > vabsq_f64 (__b);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vuzp1q_s32 (int32x4_t a, int32x4_t b)
+/* vcale  */
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcale_f32 (float32x2_t __a, float32x2_t __b)
 {
-  int32x4_t result;
-  __asm__ ("uzp1 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabs_f32 (__a) <= vabs_f32 (__b);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vuzp1q_s64 (int64x2_t a, int64x2_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcaleq_f32 (float32x4_t __a, float32x4_t __b)
 {
-  int64x2_t result;
-  __asm__ ("uzp1 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabsq_f32 (__a) <= vabsq_f32 (__b);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vuzp1q_u8 (uint8x16_t a, uint8x16_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcaleq_f64 (float64x2_t __a, float64x2_t __b)
 {
-  uint8x16_t result;
-  __asm__ ("uzp1 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabsq_f64 (__a) <= vabsq_f64 (__b);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vuzp1q_u16 (uint16x8_t a, uint16x8_t b)
+/* vcalt  */
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcalt_f32 (float32x2_t __a, float32x2_t __b)
 {
-  uint16x8_t result;
-  __asm__ ("uzp1 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabs_f32 (__a) < vabs_f32 (__b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vuzp1q_u32 (uint32x4_t a, uint32x4_t b)
+vcaltq_f32 (float32x4_t __a, float32x4_t __b)
 {
-  uint32x4_t result;
-  __asm__ ("uzp1 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabsq_f32 (__a) < vabsq_f32 (__b);
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vuzp1q_u64 (uint64x2_t a, uint64x2_t b)
+vcaltq_f64 (float64x2_t __a, float64x2_t __b)
 {
-  uint64x2_t result;
-  __asm__ ("uzp1 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return vabsq_f64 (__a) < vabsq_f64 (__b);
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vuzp2_f32 (float32x2_t a, float32x2_t b)
+/* vceq - vector.  */
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vceq_f32 (float32x2_t __a, float32x2_t __b)
 {
-  float32x2_t result;
-  __asm__ ("uzp2 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x2_t) __builtin_aarch64_cmeqv2sf (__a, __b);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vuzp2_p8 (poly8x8_t a, poly8x8_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceq_f64 (float64x1_t __a, float64x1_t __b)
 {
-  poly8x8_t result;
-  __asm__ ("uzp2 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vuzp2_p16 (poly16x4_t a, poly16x4_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vceq_p8 (poly8x8_t __a, poly8x8_t __b)
 {
-  poly16x4_t result;
-  __asm__ ("uzp2 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x8_t) __builtin_aarch64_cmeqv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vuzp2_s8 (int8x8_t a, int8x8_t b)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vceq_s8 (int8x8_t __a, int8x8_t __b)
+{
+  return (uint8x8_t) __builtin_aarch64_cmeqv8qi (__a, __b);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vceq_s16 (int16x4_t __a, int16x4_t __b)
 {
-  int8x8_t result;
-  __asm__ ("uzp2 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint16x4_t) __builtin_aarch64_cmeqv4hi (__a, __b);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vuzp2_s16 (int16x4_t a, int16x4_t b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vceq_s32 (int32x2_t __a, int32x2_t __b)
 {
-  int16x4_t result;
-  __asm__ ("uzp2 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x2_t) __builtin_aarch64_cmeqv2si (__a, __b);
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vuzp2_s32 (int32x2_t a, int32x2_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceq_s64 (int64x1_t __a, int64x1_t __b)
 {
-  int32x2_t result;
-  __asm__ ("uzp2 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == __b ? -1ll : 0ll;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vuzp2_u8 (uint8x8_t a, uint8x8_t b)
+vceq_u8 (uint8x8_t __a, uint8x8_t __b)
 {
-  uint8x8_t result;
-  __asm__ ("uzp2 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x8_t) __builtin_aarch64_cmeqv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vuzp2_u16 (uint16x4_t a, uint16x4_t b)
+vceq_u16 (uint16x4_t __a, uint16x4_t __b)
 {
-  uint16x4_t result;
-  __asm__ ("uzp2 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint16x4_t) __builtin_aarch64_cmeqv4hi ((int16x4_t) __a,
+						  (int16x4_t) __b);
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vuzp2_u32 (uint32x2_t a, uint32x2_t b)
+vceq_u32 (uint32x2_t __a, uint32x2_t __b)
 {
-  uint32x2_t result;
-  __asm__ ("uzp2 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x2_t) __builtin_aarch64_cmeqv2si ((int32x2_t) __a,
+						  (int32x2_t) __b);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vuzp2q_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceq_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  float32x4_t result;
-  __asm__ ("uzp2 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vuzp2q_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vceqq_f32 (float32x4_t __a, float32x4_t __b)
 {
-  float64x2_t result;
-  __asm__ ("uzp2 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmeqv4sf (__a, __b);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vuzp2q_p8 (poly8x16_t a, poly8x16_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vceqq_f64 (float64x2_t __a, float64x2_t __b)
 {
-  poly8x16_t result;
-  __asm__ ("uzp2 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmeqv2df (__a, __b);
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vuzp2q_p16 (poly16x8_t a, poly16x8_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vceqq_p8 (poly8x16_t __a, poly8x16_t __b)
 {
-  poly16x8_t result;
-  __asm__ ("uzp2 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vuzp2q_s8 (int8x16_t a, int8x16_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vceqq_s8 (int8x16_t __a, int8x16_t __b)
 {
-  int8x16_t result;
-  __asm__ ("uzp2 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmeqv16qi (__a, __b);
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vuzp2q_s16 (int16x8_t a, int16x8_t b)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vceqq_s16 (int16x8_t __a, int16x8_t __b)
 {
-  int16x8_t result;
-  __asm__ ("uzp2 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint16x8_t) __builtin_aarch64_cmeqv8hi (__a, __b);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vuzp2q_s32 (int32x4_t a, int32x4_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vceqq_s32 (int32x4_t __a, int32x4_t __b)
 {
-  int32x4_t result;
-  __asm__ ("uzp2 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmeqv4si (__a, __b);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vuzp2q_s64 (int64x2_t a, int64x2_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vceqq_s64 (int64x2_t __a, int64x2_t __b)
 {
-  int64x2_t result;
-  __asm__ ("uzp2 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmeqv2di (__a, __b);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vuzp2q_u8 (uint8x16_t a, uint8x16_t b)
+vceqq_u8 (uint8x16_t __a, uint8x16_t __b)
 {
-  uint8x16_t result;
-  __asm__ ("uzp2 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vuzp2q_u16 (uint16x8_t a, uint16x8_t b)
+vceqq_u16 (uint16x8_t __a, uint16x8_t __b)
 {
-  uint16x8_t result;
-  __asm__ ("uzp2 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint16x8_t) __builtin_aarch64_cmeqv8hi ((int16x8_t) __a,
+						  (int16x8_t) __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vuzp2q_u32 (uint32x4_t a, uint32x4_t b)
+vceqq_u32 (uint32x4_t __a, uint32x4_t __b)
 {
-  uint32x4_t result;
-  __asm__ ("uzp2 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmeqv4si ((int32x4_t) __a,
+						  (int32x4_t) __b);
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vuzp2q_u64 (uint64x2_t a, uint64x2_t b)
+vceqq_u64 (uint64x2_t __a, uint64x2_t __b)
 {
-  uint64x2_t result;
-  __asm__ ("uzp2 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmeqv2di ((int64x2_t) __a,
+						  (int64x2_t) __b);
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vzip1_f32 (float32x2_t a, float32x2_t b)
+/* vceq - scalar.  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vceqs_f32 (float32_t __a, float32_t __b)
 {
-  float32x2_t result;
-  __asm__ ("zip1 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == __b ? -1 : 0;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vzip1_p8 (poly8x8_t a, poly8x8_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceqd_s64 (int64x1_t __a, int64x1_t __b)
 {
-  poly8x8_t result;
-  __asm__ ("zip1 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vzip1_p16 (poly16x4_t a, poly16x4_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceqd_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  poly16x4_t result;
-  __asm__ ("zip1 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vzip1_s8 (int8x8_t a, int8x8_t b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vceqd_f64 (float64_t __a, float64_t __b)
 {
-  int8x8_t result;
-  __asm__ ("zip1 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vzip1_s16 (int16x4_t a, int16x4_t b)
+/* vceqz - vector.  */
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vceqz_f32 (float32x2_t __a)
 {
-  int16x4_t result;
-  __asm__ ("zip1 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  float32x2_t __b = {0.0f, 0.0f};
+  return (uint32x2_t) __builtin_aarch64_cmeqv2sf (__a, __b);
+}
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceqz_f64 (float64x1_t __a)
+{
+  return __a == 0.0 ? -1ll : 0ll;
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vceqz_p8 (poly8x8_t __a)
+{
+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmeqv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vceqz_s8 (int8x8_t __a)
+{
+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmeqv8qi (__a, __b);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vceqz_s16 (int16x4_t __a)
+{
+  int16x4_t __b = {0, 0, 0, 0};
+  return (uint16x4_t) __builtin_aarch64_cmeqv4hi (__a, __b);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vceqz_s32 (int32x2_t __a)
+{
+  int32x2_t __b = {0, 0};
+  return (uint32x2_t) __builtin_aarch64_cmeqv2si (__a, __b);
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vzip1_s32 (int32x2_t a, int32x2_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceqz_s64 (int64x1_t __a)
 {
-  int32x2_t result;
-  __asm__ ("zip1 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == 0ll ? -1ll : 0ll;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vzip1_u8 (uint8x8_t a, uint8x8_t b)
+vceqz_u8 (uint8x8_t __a)
 {
-  uint8x8_t result;
-  __asm__ ("zip1 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  uint8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmeqv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vzip1_u16 (uint16x4_t a, uint16x4_t b)
+vceqz_u16 (uint16x4_t __a)
 {
-  uint16x4_t result;
-  __asm__ ("zip1 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  uint16x4_t __b = {0, 0, 0, 0};
+  return (uint16x4_t) __builtin_aarch64_cmeqv4hi ((int16x4_t) __a,
+						  (int16x4_t) __b);
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vzip1_u32 (uint32x2_t a, uint32x2_t b)
+vceqz_u32 (uint32x2_t __a)
 {
-  uint32x2_t result;
-  __asm__ ("zip1 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  uint32x2_t __b = {0, 0};
+  return (uint32x2_t) __builtin_aarch64_cmeqv2si ((int32x2_t) __a,
+						  (int32x2_t) __b);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vzip1q_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceqz_u64 (uint64x1_t __a)
 {
-  float32x4_t result;
-  __asm__ ("zip1 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == 0ll ? -1ll : 0ll;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vzip1q_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vceqzq_f32 (float32x4_t __a)
 {
-  float64x2_t result;
-  __asm__ ("zip1 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};
+  return (uint32x4_t) __builtin_aarch64_cmeqv4sf (__a, __b);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vzip1q_p8 (poly8x16_t a, poly8x16_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vceqzq_f64 (float64x2_t __a)
 {
-  poly8x16_t result;
-  __asm__ ("zip1 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  float64x2_t __b = {0.0, 0.0};
+  return (uint64x2_t) __builtin_aarch64_cmeqv2df (__a, __b);
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vzip1q_p16 (poly16x8_t a, poly16x8_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vceqzq_p8 (poly8x16_t __a)
 {
-  poly16x8_t result;
-  __asm__ ("zip1 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		    0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vzip1q_s8 (int8x16_t a, int8x16_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vceqzq_s8 (int8x16_t __a)
 {
-  int8x16_t result;
-  __asm__ ("zip1 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		   0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmeqv16qi (__a, __b);
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vzip1q_s16 (int16x8_t a, int16x8_t b)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vceqzq_s16 (int16x8_t __a)
 {
-  int16x8_t result;
-  __asm__ ("zip1 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint16x8_t) __builtin_aarch64_cmeqv8hi (__a, __b);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vzip1q_s32 (int32x4_t a, int32x4_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vceqzq_s32 (int32x4_t __a)
 {
-  int32x4_t result;
-  __asm__ ("zip1 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  int32x4_t __b = {0, 0, 0, 0};
+  return (uint32x4_t) __builtin_aarch64_cmeqv4si (__a, __b);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vzip1q_s64 (int64x2_t a, int64x2_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vceqzq_s64 (int64x2_t __a)
 {
-  int64x2_t result;
-  __asm__ ("zip1 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  int64x2_t __b = {0, 0};
+  return (uint64x2_t) __builtin_aarch64_cmeqv2di (__a, __b);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vzip1q_u8 (uint8x16_t a, uint8x16_t b)
+vceqzq_u8 (uint8x16_t __a)
 {
-  uint8x16_t result;
-  __asm__ ("zip1 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  uint8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		    0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vzip1q_u16 (uint16x8_t a, uint16x8_t b)
+vceqzq_u16 (uint16x8_t __a)
 {
-  uint16x8_t result;
-  __asm__ ("zip1 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  uint16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint16x8_t) __builtin_aarch64_cmeqv8hi ((int16x8_t) __a,
+						  (int16x8_t) __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vzip1q_u32 (uint32x4_t a, uint32x4_t b)
+vceqzq_u32 (uint32x4_t __a)
 {
-  uint32x4_t result;
-  __asm__ ("zip1 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  uint32x4_t __b = {0, 0, 0, 0};
+  return (uint32x4_t) __builtin_aarch64_cmeqv4si ((int32x4_t) __a,
+						  (int32x4_t) __b);
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vzip1q_u64 (uint64x2_t a, uint64x2_t b)
+vceqzq_u64 (uint64x2_t __a)
 {
-  uint64x2_t result;
-  __asm__ ("zip1 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  uint64x2_t __b = {0, 0};
+  return (uint64x2_t) __builtin_aarch64_cmeqv2di ((int64x2_t) __a,
+						  (int64x2_t) __b);
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vzip2_f32 (float32x2_t a, float32x2_t b)
+/* vceqz - scalar.  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vceqzs_f32 (float32_t __a)
 {
-  float32x2_t result;
-  __asm__ ("zip2 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == 0.0f ? -1 : 0;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vzip2_p8 (poly8x8_t a, poly8x8_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceqzd_s64 (int64x1_t __a)
 {
-  poly8x8_t result;
-  __asm__ ("zip2 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == 0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vzip2_p16 (poly16x4_t a, poly16x4_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceqzd_u64 (int64x1_t __a)
 {
-  poly16x4_t result;
-  __asm__ ("zip2 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == 0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vzip2_s8 (int8x8_t a, int8x8_t b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vceqzd_f64 (float64_t __a)
 {
-  int8x8_t result;
-  __asm__ ("zip2 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a == 0.0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vzip2_s16 (int16x4_t a, int16x4_t b)
+/* vcge - vector.  */
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcge_f32 (float32x2_t __a, float32x2_t __b)
 {
-  int16x4_t result;
-  __asm__ ("zip2 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x2_t) __builtin_aarch64_cmgev2sf (__a, __b);
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vzip2_s32 (int32x2_t a, int32x2_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcge_f64 (float64x1_t __a, float64x1_t __b)
 {
-  int32x2_t result;
-  __asm__ ("zip2 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a >= __b ? -1ll : 0ll;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vzip2_u8 (uint8x8_t a, uint8x8_t b)
+vcge_p8 (poly8x8_t __a, poly8x8_t __b)
 {
-  uint8x8_t result;
-  __asm__ ("zip2 %0.8b,%1.8b,%2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x8_t) __builtin_aarch64_cmgev8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcge_s8 (int8x8_t __a, int8x8_t __b)
+{
+  return (uint8x8_t) __builtin_aarch64_cmgev8qi (__a, __b);
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vzip2_u16 (uint16x4_t a, uint16x4_t b)
+vcge_s16 (int16x4_t __a, int16x4_t __b)
 {
-  uint16x4_t result;
-  __asm__ ("zip2 %0.4h,%1.4h,%2.4h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint16x4_t) __builtin_aarch64_cmgev4hi (__a, __b);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcge_s32 (int32x2_t __a, int32x2_t __b)
+{
+  return (uint32x2_t) __builtin_aarch64_cmgev2si (__a, __b);
+}
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcge_s64 (int64x1_t __a, int64x1_t __b)
+{
+  return __a >= __b ? -1ll : 0ll;
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcge_u8 (uint8x8_t __a, uint8x8_t __b)
+{
+  return (uint8x8_t) __builtin_aarch64_cmgeuv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vcge_u16 (uint16x4_t __a, uint16x4_t __b)
+{
+  return (uint16x4_t) __builtin_aarch64_cmgeuv4hi ((int16x4_t) __a,
+						  (int16x4_t) __b);
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vzip2_u32 (uint32x2_t a, uint32x2_t b)
+vcge_u32 (uint32x2_t __a, uint32x2_t __b)
 {
-  uint32x2_t result;
-  __asm__ ("zip2 %0.2s,%1.2s,%2.2s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x2_t) __builtin_aarch64_cmgeuv2si ((int32x2_t) __a,
+						  (int32x2_t) __b);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vzip2q_f32 (float32x4_t a, float32x4_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcge_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  float32x4_t result;
-  __asm__ ("zip2 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a >= __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vzip2q_f64 (float64x2_t a, float64x2_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgeq_f32 (float32x4_t __a, float32x4_t __b)
 {
-  float64x2_t result;
-  __asm__ ("zip2 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmgev4sf (__a, __b);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vzip2q_p8 (poly8x16_t a, poly8x16_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgeq_f64 (float64x2_t __a, float64x2_t __b)
 {
-  poly8x16_t result;
-  __asm__ ("zip2 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmgev2df (__a, __b);
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vzip2q_p16 (poly16x8_t a, poly16x8_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgeq_p8 (poly8x16_t __a, poly8x16_t __b)
 {
-  poly16x8_t result;
-  __asm__ ("zip2 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmgev16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vzip2q_s8 (int8x16_t a, int8x16_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgeq_s8 (int8x16_t __a, int8x16_t __b)
 {
-  int8x16_t result;
-  __asm__ ("zip2 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmgev16qi (__a, __b);
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vzip2q_s16 (int16x8_t a, int16x8_t b)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vcgeq_s16 (int16x8_t __a, int16x8_t __b)
 {
-  int16x8_t result;
-  __asm__ ("zip2 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint16x8_t) __builtin_aarch64_cmgev8hi (__a, __b);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vzip2q_s32 (int32x4_t a, int32x4_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgeq_s32 (int32x4_t __a, int32x4_t __b)
 {
-  int32x4_t result;
-  __asm__ ("zip2 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmgev4si (__a, __b);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vzip2q_s64 (int64x2_t a, int64x2_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgeq_s64 (int64x2_t __a, int64x2_t __b)
 {
-  int64x2_t result;
-  __asm__ ("zip2 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmgev2di (__a, __b);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vzip2q_u8 (uint8x16_t a, uint8x16_t b)
+vcgeq_u8 (uint8x16_t __a, uint8x16_t __b)
 {
-  uint8x16_t result;
-  __asm__ ("zip2 %0.16b,%1.16b,%2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmgeuv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vzip2q_u16 (uint16x8_t a, uint16x8_t b)
+vcgeq_u16 (uint16x8_t __a, uint16x8_t __b)
 {
-  uint16x8_t result;
-  __asm__ ("zip2 %0.8h,%1.8h,%2.8h"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint16x8_t) __builtin_aarch64_cmgeuv8hi ((int16x8_t) __a,
+						  (int16x8_t) __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vzip2q_u32 (uint32x4_t a, uint32x4_t b)
+vcgeq_u32 (uint32x4_t __a, uint32x4_t __b)
 {
-  uint32x4_t result;
-  __asm__ ("zip2 %0.4s,%1.4s,%2.4s"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmgeuv4si ((int32x4_t) __a,
+						  (int32x4_t) __b);
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vzip2q_u64 (uint64x2_t a, uint64x2_t b)
+vcgeq_u64 (uint64x2_t __a, uint64x2_t __b)
 {
-  uint64x2_t result;
-  __asm__ ("zip2 %0.2d,%1.2d,%2.2d"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmgeuv2di ((int64x2_t) __a,
+						  (int64x2_t) __b);
 }
 
-/* End of temporary inline asm implementations.  */
-
-/* Start of temporary inline asm for vldn, vstn and friends.  */
-
-/* Create struct element types for duplicating loads.
-
-   Create 2 element structures of:
-
-   +------+----+----+----+----+
-   |      | 8  | 16 | 32 | 64 |
-   +------+----+----+----+----+
-   |int   | Y  | Y  | N  | N  |
-   +------+----+----+----+----+
-   |uint  | Y  | Y  | N  | N  |
-   +------+----+----+----+----+
-   |float | -  | -  | N  | N  |
-   +------+----+----+----+----+
-   |poly  | Y  | Y  | -  | -  |
-   +------+----+----+----+----+
-
-   Create 3 element structures of:
-
-   +------+----+----+----+----+
-   |      | 8  | 16 | 32 | 64 |
-   +------+----+----+----+----+
-   |int   | Y  | Y  | Y  | Y  |
-   +------+----+----+----+----+
-   |uint  | Y  | Y  | Y  | Y  |
-   +------+----+----+----+----+
-   |float | -  | -  | Y  | Y  |
-   +------+----+----+----+----+
-   |poly  | Y  | Y  | -  | -  |
-   +------+----+----+----+----+
-
-   Create 4 element structures of:
-
-   +------+----+----+----+----+
-   |      | 8  | 16 | 32 | 64 |
-   +------+----+----+----+----+
-   |int   | Y  | N  | N  | Y  |
-   +------+----+----+----+----+
-   |uint  | Y  | N  | N  | Y  |
-   +------+----+----+----+----+
-   |float | -  | -  | N  | Y  |
-   +------+----+----+----+----+
-   |poly  | Y  | N  | -  | -  |
-   +------+----+----+----+----+
-
-  This is required for casting memory reference.  */
-#define __STRUCTN(t, sz, nelem)			\
-  typedef struct t ## sz ## x ## nelem ## _t {	\
-    t ## sz ## _t val[nelem];			\
-  }  t ## sz ## x ## nelem ## _t;
-
-/* 2-element structs.  */
-__STRUCTN (int, 8, 2)
-__STRUCTN (int, 16, 2)
-__STRUCTN (uint, 8, 2)
-__STRUCTN (uint, 16, 2)
-__STRUCTN (poly, 8, 2)
-__STRUCTN (poly, 16, 2)
-/* 3-element structs.  */
-__STRUCTN (int, 8, 3)
-__STRUCTN (int, 16, 3)
-__STRUCTN (int, 32, 3)
-__STRUCTN (int, 64, 3)
-__STRUCTN (uint, 8, 3)
-__STRUCTN (uint, 16, 3)
-__STRUCTN (uint, 32, 3)
-__STRUCTN (uint, 64, 3)
-__STRUCTN (float, 32, 3)
-__STRUCTN (float, 64, 3)
-__STRUCTN (poly, 8, 3)
-__STRUCTN (poly, 16, 3)
-/* 4-element structs.  */
-__STRUCTN (int, 8, 4)
-__STRUCTN (int, 64, 4)
-__STRUCTN (uint, 8, 4)
-__STRUCTN (uint, 64, 4)
-__STRUCTN (poly, 8, 4)
-__STRUCTN (float, 64, 4)
-#undef __STRUCTN
+/* vcge - scalar.  */
 
-#define __LD2R_FUNC(rettype, structtype, ptrtype,			\
-		    regsuffix, funcsuffix, Q)				\
-  __extension__ static __inline rettype					\
-  __attribute__ ((__always_inline__)) 					\
-  vld2 ## Q ## _dup_ ## funcsuffix (const ptrtype *ptr)			\
-  {									\
-    rettype result;							\
-    __asm__ ("ld2r {v16." #regsuffix ", v17." #regsuffix "}, %1\n\t"	\
-	     "st1 {v16." #regsuffix ", v17." #regsuffix "}, %0\n\t"	\
-	     : "=Q"(result)						\
-	     : "Q"(*(const structtype *)ptr)				\
-	     : "memory", "v16", "v17");					\
-    return result;							\
-  }
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcges_f32 (float32_t __a, float32_t __b)
+{
+  return __a >= __b ? -1 : 0;
+}
 
-__LD2R_FUNC (float32x2x2_t, float32x2_t, float32_t, 2s, f32,)
-__LD2R_FUNC (float64x1x2_t, float64x2_t, float64_t, 1d, f64,)
-__LD2R_FUNC (poly8x8x2_t, poly8x2_t, poly8_t, 8b, p8,)
-__LD2R_FUNC (poly16x4x2_t, poly16x2_t, poly16_t, 4h, p16,)
-__LD2R_FUNC (int8x8x2_t, int8x2_t, int8_t, 8b, s8,)
-__LD2R_FUNC (int16x4x2_t, int16x2_t, int16_t, 4h, s16,)
-__LD2R_FUNC (int32x2x2_t, int32x2_t, int32_t, 2s, s32,)
-__LD2R_FUNC (int64x1x2_t, int64x2_t, int64_t, 1d, s64,)
-__LD2R_FUNC (uint8x8x2_t, uint8x2_t, uint8_t, 8b, u8,)
-__LD2R_FUNC (uint16x4x2_t, uint16x2_t, uint16_t, 4h, u16,)
-__LD2R_FUNC (uint32x2x2_t, uint32x2_t, uint32_t, 2s, u32,)
-__LD2R_FUNC (uint64x1x2_t, uint64x2_t, uint64_t, 1d, u64,)
-__LD2R_FUNC (float32x4x2_t, float32x2_t, float32_t, 4s, f32, q)
-__LD2R_FUNC (float64x2x2_t, float64x2_t, float64_t, 2d, f64, q)
-__LD2R_FUNC (poly8x16x2_t, poly8x2_t, poly8_t, 16b, p8, q)
-__LD2R_FUNC (poly16x8x2_t, poly16x2_t, poly16_t, 8h, p16, q)
-__LD2R_FUNC (int8x16x2_t, int8x2_t, int8_t, 16b, s8, q)
-__LD2R_FUNC (int16x8x2_t, int16x2_t, int16_t, 8h, s16, q)
-__LD2R_FUNC (int32x4x2_t, int32x2_t, int32_t, 4s, s32, q)
-__LD2R_FUNC (int64x2x2_t, int64x2_t, int64_t, 2d, s64, q)
-__LD2R_FUNC (uint8x16x2_t, uint8x2_t, uint8_t, 16b, u8, q)
-__LD2R_FUNC (uint16x8x2_t, uint16x2_t, uint16_t, 8h, u16, q)
-__LD2R_FUNC (uint32x4x2_t, uint32x2_t, uint32_t, 4s, u32, q)
-__LD2R_FUNC (uint64x2x2_t, uint64x2_t, uint64_t, 2d, u64, q)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcged_s64 (int64x1_t __a, int64x1_t __b)
+{
+  return __a >= __b ? -1ll : 0ll;
+}
 
-#define __LD2_LANE_FUNC(rettype, ptrtype, regsuffix,			\
-			lnsuffix, funcsuffix, Q)			\
-  __extension__ static __inline rettype					\
-  __attribute__ ((__always_inline__))					\
-  vld2 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
-				     rettype b, const int c)		\
-  {									\
-    rettype result;							\
-    __asm__ ("ld1 {v16." #regsuffix ", v17." #regsuffix "}, %1\n\t"	\
-	     "ld2 {v16." #lnsuffix ", v17." #lnsuffix "}[%3], %2\n\t"	\
-	     "st1 {v16." #regsuffix ", v17." #regsuffix "}, %0\n\t"	\
-	     : "=Q"(result)						\
-	     : "Q"(b), "Q"(*(const rettype *)ptr), "i"(c)		\
-	     : "memory", "v16", "v17");					\
-    return result;							\
-  }
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcged_u64 (uint64x1_t __a, uint64x1_t __b)
+{
+  return __a >= __b ? -1ll : 0ll;
+}
 
-__LD2_LANE_FUNC (int8x8x2_t, uint8_t, 8b, b, s8,)
-__LD2_LANE_FUNC (float32x2x2_t, float32_t, 2s, s, f32,)
-__LD2_LANE_FUNC (float64x1x2_t, float64_t, 1d, d, f64,)
-__LD2_LANE_FUNC (poly8x8x2_t, poly8_t, 8b, b, p8,)
-__LD2_LANE_FUNC (poly16x4x2_t, poly16_t, 4h, h, p16,)
-__LD2_LANE_FUNC (int16x4x2_t, int16_t, 4h, h, s16,)
-__LD2_LANE_FUNC (int32x2x2_t, int32_t, 2s, s, s32,)
-__LD2_LANE_FUNC (int64x1x2_t, int64_t, 1d, d, s64,)
-__LD2_LANE_FUNC (uint8x8x2_t, uint8_t, 8b, b, u8,)
-__LD2_LANE_FUNC (uint16x4x2_t, uint16_t, 4h, h, u16,)
-__LD2_LANE_FUNC (uint32x2x2_t, uint32_t, 2s, s, u32,)
-__LD2_LANE_FUNC (uint64x1x2_t, uint64_t, 1d, d, u64,)
-__LD2_LANE_FUNC (float32x4x2_t, float32_t, 4s, s, f32, q)
-__LD2_LANE_FUNC (float64x2x2_t, float64_t, 2d, d, f64, q)
-__LD2_LANE_FUNC (poly8x16x2_t, poly8_t, 16b, b, p8, q)
-__LD2_LANE_FUNC (poly16x8x2_t, poly16_t, 8h, h, p16, q)
-__LD2_LANE_FUNC (int8x16x2_t, int8_t, 16b, b, s8, q)
-__LD2_LANE_FUNC (int16x8x2_t, int16_t, 8h, h, s16, q)
-__LD2_LANE_FUNC (int32x4x2_t, int32_t, 4s, s, s32, q)
-__LD2_LANE_FUNC (int64x2x2_t, int64_t, 2d, d, s64, q)
-__LD2_LANE_FUNC (uint8x16x2_t, uint8_t, 16b, b, u8, q)
-__LD2_LANE_FUNC (uint16x8x2_t, uint16_t, 8h, h, u16, q)
-__LD2_LANE_FUNC (uint32x4x2_t, uint32_t, 4s, s, u32, q)
-__LD2_LANE_FUNC (uint64x2x2_t, uint64_t, 2d, d, u64, q)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcged_f64 (float64_t __a, float64_t __b)
+{
+  return __a >= __b ? -1ll : 0ll;
+}
 
-#define __LD3R_FUNC(rettype, structtype, ptrtype,			\
-		    regsuffix, funcsuffix, Q)				\
-  __extension__ static __inline rettype					\
-  __attribute__ ((__always_inline__))					\
-  vld3 ## Q ## _dup_ ## funcsuffix (const ptrtype *ptr)			\
-  {									\
-    rettype result;							\
-    __asm__ ("ld3r {v16." #regsuffix " - v18." #regsuffix "}, %1\n\t"	\
-	     "st1 {v16." #regsuffix " - v18." #regsuffix "}, %0\n\t"	\
-	     : "=Q"(result)						\
-	     : "Q"(*(const structtype *)ptr)				\
-	     : "memory", "v16", "v17", "v18");				\
-    return result;							\
-  }
+/* vcgez - vector.  */
 
-__LD3R_FUNC (float32x2x3_t, float32x3_t, float32_t, 2s, f32,)
-__LD3R_FUNC (float64x1x3_t, float64x3_t, float64_t, 1d, f64,)
-__LD3R_FUNC (poly8x8x3_t, poly8x3_t, poly8_t, 8b, p8,)
-__LD3R_FUNC (poly16x4x3_t, poly16x3_t, poly16_t, 4h, p16,)
-__LD3R_FUNC (int8x8x3_t, int8x3_t, int8_t, 8b, s8,)
-__LD3R_FUNC (int16x4x3_t, int16x3_t, int16_t, 4h, s16,)
-__LD3R_FUNC (int32x2x3_t, int32x3_t, int32_t, 2s, s32,)
-__LD3R_FUNC (int64x1x3_t, int64x3_t, int64_t, 1d, s64,)
-__LD3R_FUNC (uint8x8x3_t, uint8x3_t, uint8_t, 8b, u8,)
-__LD3R_FUNC (uint16x4x3_t, uint16x3_t, uint16_t, 4h, u16,)
-__LD3R_FUNC (uint32x2x3_t, uint32x3_t, uint32_t, 2s, u32,)
-__LD3R_FUNC (uint64x1x3_t, uint64x3_t, uint64_t, 1d, u64,)
-__LD3R_FUNC (float32x4x3_t, float32x3_t, float32_t, 4s, f32, q)
-__LD3R_FUNC (float64x2x3_t, float64x3_t, float64_t, 2d, f64, q)
-__LD3R_FUNC (poly8x16x3_t, poly8x3_t, poly8_t, 16b, p8, q)
-__LD3R_FUNC (poly16x8x3_t, poly16x3_t, poly16_t, 8h, p16, q)
-__LD3R_FUNC (int8x16x3_t, int8x3_t, int8_t, 16b, s8, q)
-__LD3R_FUNC (int16x8x3_t, int16x3_t, int16_t, 8h, s16, q)
-__LD3R_FUNC (int32x4x3_t, int32x3_t, int32_t, 4s, s32, q)
-__LD3R_FUNC (int64x2x3_t, int64x3_t, int64_t, 2d, s64, q)
-__LD3R_FUNC (uint8x16x3_t, uint8x3_t, uint8_t, 16b, u8, q)
-__LD3R_FUNC (uint16x8x3_t, uint16x3_t, uint16_t, 8h, u16, q)
-__LD3R_FUNC (uint32x4x3_t, uint32x3_t, uint32_t, 4s, u32, q)
-__LD3R_FUNC (uint64x2x3_t, uint64x3_t, uint64_t, 2d, u64, q)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcgez_f32 (float32x2_t __a)
+{
+  float32x2_t __b = {0.0f, 0.0f};
+  return (uint32x2_t) __builtin_aarch64_cmgev2sf (__a, __b);
+}
 
-#define __LD3_LANE_FUNC(rettype, ptrtype, regsuffix,			\
-			lnsuffix, funcsuffix, Q)			\
-  __extension__ static __inline rettype					\
-  __attribute__ ((__always_inline__))					\
-  vld3 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
-				     rettype b, const int c)		\
-  {									\
-    rettype result;							\
-    __asm__ ("ld1 {v16." #regsuffix " - v18." #regsuffix "}, %1\n\t"	\
-	     "ld3 {v16." #lnsuffix " - v18." #lnsuffix "}[%3], %2\n\t"	\
-	     "st1 {v16." #regsuffix " - v18." #regsuffix "}, %0\n\t"	\
-	     : "=Q"(result)						\
-	     : "Q"(b), "Q"(*(const rettype *)ptr), "i"(c)		\
-	     : "memory", "v16", "v17", "v18");				\
-    return result;							\
-  }
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgez_f64 (float64x1_t __a)
+{
+  return __a >= 0.0 ? -1ll : 0ll;
+}
 
-__LD3_LANE_FUNC (int8x8x3_t, uint8_t, 8b, b, s8,)
-__LD3_LANE_FUNC (float32x2x3_t, float32_t, 2s, s, f32,)
-__LD3_LANE_FUNC (float64x1x3_t, float64_t, 1d, d, f64,)
-__LD3_LANE_FUNC (poly8x8x3_t, poly8_t, 8b, b, p8,)
-__LD3_LANE_FUNC (poly16x4x3_t, poly16_t, 4h, h, p16,)
-__LD3_LANE_FUNC (int16x4x3_t, int16_t, 4h, h, s16,)
-__LD3_LANE_FUNC (int32x2x3_t, int32_t, 2s, s, s32,)
-__LD3_LANE_FUNC (int64x1x3_t, int64_t, 1d, d, s64,)
-__LD3_LANE_FUNC (uint8x8x3_t, uint8_t, 8b, b, u8,)
-__LD3_LANE_FUNC (uint16x4x3_t, uint16_t, 4h, h, u16,)
-__LD3_LANE_FUNC (uint32x2x3_t, uint32_t, 2s, s, u32,)
-__LD3_LANE_FUNC (uint64x1x3_t, uint64_t, 1d, d, u64,)
-__LD3_LANE_FUNC (float32x4x3_t, float32_t, 4s, s, f32, q)
-__LD3_LANE_FUNC (float64x2x3_t, float64_t, 2d, d, f64, q)
-__LD3_LANE_FUNC (poly8x16x3_t, poly8_t, 16b, b, p8, q)
-__LD3_LANE_FUNC (poly16x8x3_t, poly16_t, 8h, h, p16, q)
-__LD3_LANE_FUNC (int8x16x3_t, int8_t, 16b, b, s8, q)
-__LD3_LANE_FUNC (int16x8x3_t, int16_t, 8h, h, s16, q)
-__LD3_LANE_FUNC (int32x4x3_t, int32_t, 4s, s, s32, q)
-__LD3_LANE_FUNC (int64x2x3_t, int64_t, 2d, d, s64, q)
-__LD3_LANE_FUNC (uint8x16x3_t, uint8_t, 16b, b, u8, q)
-__LD3_LANE_FUNC (uint16x8x3_t, uint16_t, 8h, h, u16, q)
-__LD3_LANE_FUNC (uint32x4x3_t, uint32_t, 4s, s, u32, q)
-__LD3_LANE_FUNC (uint64x2x3_t, uint64_t, 2d, d, u64, q)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcgez_p8 (poly8x8_t __a)
+{
+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmgev8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
+}
 
-#define __LD4R_FUNC(rettype, structtype, ptrtype,			\
-		    regsuffix, funcsuffix, Q)				\
-  __extension__ static __inline rettype					\
-  __attribute__ ((__always_inline__))					\
-  vld4 ## Q ## _dup_ ## funcsuffix (const ptrtype *ptr)			\
-  {									\
-    rettype result;							\
-    __asm__ ("ld4r {v16." #regsuffix " - v19." #regsuffix "}, %1\n\t"	\
-	     "st1 {v16." #regsuffix " - v19." #regsuffix "}, %0\n\t"	\
-	     : "=Q"(result)						\
-	     : "Q"(*(const structtype *)ptr)				\
-	     : "memory", "v16", "v17", "v18", "v19");			\
-    return result;							\
-  }
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcgez_s8 (int8x8_t __a)
+{
+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmgev8qi (__a, __b);
+}
 
-__LD4R_FUNC (float32x2x4_t, float32x4_t, float32_t, 2s, f32,)
-__LD4R_FUNC (float64x1x4_t, float64x4_t, float64_t, 1d, f64,)
-__LD4R_FUNC (poly8x8x4_t, poly8x4_t, poly8_t, 8b, p8,)
-__LD4R_FUNC (poly16x4x4_t, poly16x4_t, poly16_t, 4h, p16,)
-__LD4R_FUNC (int8x8x4_t, int8x4_t, int8_t, 8b, s8,)
-__LD4R_FUNC (int16x4x4_t, int16x4_t, int16_t, 4h, s16,)
-__LD4R_FUNC (int32x2x4_t, int32x4_t, int32_t, 2s, s32,)
-__LD4R_FUNC (int64x1x4_t, int64x4_t, int64_t, 1d, s64,)
-__LD4R_FUNC (uint8x8x4_t, uint8x4_t, uint8_t, 8b, u8,)
-__LD4R_FUNC (uint16x4x4_t, uint16x4_t, uint16_t, 4h, u16,)
-__LD4R_FUNC (uint32x2x4_t, uint32x4_t, uint32_t, 2s, u32,)
-__LD4R_FUNC (uint64x1x4_t, uint64x4_t, uint64_t, 1d, u64,)
-__LD4R_FUNC (float32x4x4_t, float32x4_t, float32_t, 4s, f32, q)
-__LD4R_FUNC (float64x2x4_t, float64x4_t, float64_t, 2d, f64, q)
-__LD4R_FUNC (poly8x16x4_t, poly8x4_t, poly8_t, 16b, p8, q)
-__LD4R_FUNC (poly16x8x4_t, poly16x4_t, poly16_t, 8h, p16, q)
-__LD4R_FUNC (int8x16x4_t, int8x4_t, int8_t, 16b, s8, q)
-__LD4R_FUNC (int16x8x4_t, int16x4_t, int16_t, 8h, s16, q)
-__LD4R_FUNC (int32x4x4_t, int32x4_t, int32_t, 4s, s32, q)
-__LD4R_FUNC (int64x2x4_t, int64x4_t, int64_t, 2d, s64, q)
-__LD4R_FUNC (uint8x16x4_t, uint8x4_t, uint8_t, 16b, u8, q)
-__LD4R_FUNC (uint16x8x4_t, uint16x4_t, uint16_t, 8h, u16, q)
-__LD4R_FUNC (uint32x4x4_t, uint32x4_t, uint32_t, 4s, u32, q)
-__LD4R_FUNC (uint64x2x4_t, uint64x4_t, uint64_t, 2d, u64, q)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vcgez_s16 (int16x4_t __a)
+{
+  int16x4_t __b = {0, 0, 0, 0};
+  return (uint16x4_t) __builtin_aarch64_cmgev4hi (__a, __b);
+}
 
-#define __LD4_LANE_FUNC(rettype, ptrtype, regsuffix,			\
-			lnsuffix, funcsuffix, Q)			\
-  __extension__ static __inline rettype					\
-  __attribute__ ((__always_inline__))					\
-  vld4 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
-				     rettype b, const int c)		\
-  {									\
-    rettype result;							\
-    __asm__ ("ld1 {v16." #regsuffix " - v19." #regsuffix "}, %1\n\t"	\
-	     "ld4 {v16." #lnsuffix " - v19." #lnsuffix "}[%3], %2\n\t"	\
-	     "st1 {v16." #regsuffix " - v19." #regsuffix "}, %0\n\t"	\
-	     : "=Q"(result)						\
-	     : "Q"(b), "Q"(*(const rettype *)ptr), "i"(c)		\
-	     : "memory", "v16", "v17", "v18", "v19");			\
-    return result;							\
-  }
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcgez_s32 (int32x2_t __a)
+{
+  int32x2_t __b = {0, 0};
+  return (uint32x2_t) __builtin_aarch64_cmgev2si (__a, __b);
+}
 
-__LD4_LANE_FUNC (int8x8x4_t, uint8_t, 8b, b, s8,)
-__LD4_LANE_FUNC (float32x2x4_t, float32_t, 2s, s, f32,)
-__LD4_LANE_FUNC (float64x1x4_t, float64_t, 1d, d, f64,)
-__LD4_LANE_FUNC (poly8x8x4_t, poly8_t, 8b, b, p8,)
-__LD4_LANE_FUNC (poly16x4x4_t, poly16_t, 4h, h, p16,)
-__LD4_LANE_FUNC (int16x4x4_t, int16_t, 4h, h, s16,)
-__LD4_LANE_FUNC (int32x2x4_t, int32_t, 2s, s, s32,)
-__LD4_LANE_FUNC (int64x1x4_t, int64_t, 1d, d, s64,)
-__LD4_LANE_FUNC (uint8x8x4_t, uint8_t, 8b, b, u8,)
-__LD4_LANE_FUNC (uint16x4x4_t, uint16_t, 4h, h, u16,)
-__LD4_LANE_FUNC (uint32x2x4_t, uint32_t, 2s, s, u32,)
-__LD4_LANE_FUNC (uint64x1x4_t, uint64_t, 1d, d, u64,)
-__LD4_LANE_FUNC (float32x4x4_t, float32_t, 4s, s, f32, q)
-__LD4_LANE_FUNC (float64x2x4_t, float64_t, 2d, d, f64, q)
-__LD4_LANE_FUNC (poly8x16x4_t, poly8_t, 16b, b, p8, q)
-__LD4_LANE_FUNC (poly16x8x4_t, poly16_t, 8h, h, p16, q)
-__LD4_LANE_FUNC (int8x16x4_t, int8_t, 16b, b, s8, q)
-__LD4_LANE_FUNC (int16x8x4_t, int16_t, 8h, h, s16, q)
-__LD4_LANE_FUNC (int32x4x4_t, int32_t, 4s, s, s32, q)
-__LD4_LANE_FUNC (int64x2x4_t, int64_t, 2d, d, s64, q)
-__LD4_LANE_FUNC (uint8x16x4_t, uint8_t, 16b, b, u8, q)
-__LD4_LANE_FUNC (uint16x8x4_t, uint16_t, 8h, h, u16, q)
-__LD4_LANE_FUNC (uint32x4x4_t, uint32_t, 4s, s, u32, q)
-__LD4_LANE_FUNC (uint64x2x4_t, uint64_t, 2d, d, u64, q)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgez_s64 (int64x1_t __a)
+{
+  return __a >= 0ll ? -1ll : 0ll;
+}
 
-#define __ST2_LANE_FUNC(intype, ptrtype, regsuffix,			\
-			lnsuffix, funcsuffix, Q)			\
-  __extension__ static __inline void					\
-  __attribute__ ((__always_inline__))					\
-  vst2 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
-				     intype b, const int c)		\
-  {									\
-    __asm__ ("ld1 {v16." #regsuffix ", v17." #regsuffix "}, %1\n\t"	\
-	     "st2 {v16." #lnsuffix ", v17." #lnsuffix "}[%2], %0\n\t"	\
-	     : "=Q"(*(intype *) ptr)					\
-	     : "Q"(b), "i"(c)						\
-	     : "memory", "v16", "v17");					\
-  }
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcgez_u8 (uint8x8_t __a)
+{
+  uint8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmgeuv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vcgez_u16 (uint16x4_t __a)
+{
+  uint16x4_t __b = {0, 0, 0, 0};
+  return (uint16x4_t) __builtin_aarch64_cmgeuv4hi ((int16x4_t) __a,
+						  (int16x4_t) __b);
+}
 
-__ST2_LANE_FUNC (int8x8x2_t, int8_t, 8b, b, s8,)
-__ST2_LANE_FUNC (float32x2x2_t, float32_t, 2s, s, f32,)
-__ST2_LANE_FUNC (float64x1x2_t, float64_t, 1d, d, f64,)
-__ST2_LANE_FUNC (poly8x8x2_t, poly8_t, 8b, b, p8,)
-__ST2_LANE_FUNC (poly16x4x2_t, poly16_t, 4h, h, p16,)
-__ST2_LANE_FUNC (int16x4x2_t, int16_t, 4h, h, s16,)
-__ST2_LANE_FUNC (int32x2x2_t, int32_t, 2s, s, s32,)
-__ST2_LANE_FUNC (int64x1x2_t, int64_t, 1d, d, s64,)
-__ST2_LANE_FUNC (uint8x8x2_t, uint8_t, 8b, b, u8,)
-__ST2_LANE_FUNC (uint16x4x2_t, uint16_t, 4h, h, u16,)
-__ST2_LANE_FUNC (uint32x2x2_t, uint32_t, 2s, s, u32,)
-__ST2_LANE_FUNC (uint64x1x2_t, uint64_t, 1d, d, u64,)
-__ST2_LANE_FUNC (float32x4x2_t, float32_t, 4s, s, f32, q)
-__ST2_LANE_FUNC (float64x2x2_t, float64_t, 2d, d, f64, q)
-__ST2_LANE_FUNC (poly8x16x2_t, poly8_t, 16b, b, p8, q)
-__ST2_LANE_FUNC (poly16x8x2_t, poly16_t, 8h, h, p16, q)
-__ST2_LANE_FUNC (int8x16x2_t, int8_t, 16b, b, s8, q)
-__ST2_LANE_FUNC (int16x8x2_t, int16_t, 8h, h, s16, q)
-__ST2_LANE_FUNC (int32x4x2_t, int32_t, 4s, s, s32, q)
-__ST2_LANE_FUNC (int64x2x2_t, int64_t, 2d, d, s64, q)
-__ST2_LANE_FUNC (uint8x16x2_t, uint8_t, 16b, b, u8, q)
-__ST2_LANE_FUNC (uint16x8x2_t, uint16_t, 8h, h, u16, q)
-__ST2_LANE_FUNC (uint32x4x2_t, uint32_t, 4s, s, u32, q)
-__ST2_LANE_FUNC (uint64x2x2_t, uint64_t, 2d, d, u64, q)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcgez_u32 (uint32x2_t __a)
+{
+  uint32x2_t __b = {0, 0};
+  return (uint32x2_t) __builtin_aarch64_cmgeuv2si ((int32x2_t) __a,
+						  (int32x2_t) __b);
+}
 
-#define __ST3_LANE_FUNC(intype, ptrtype, regsuffix,			\
-			lnsuffix, funcsuffix, Q)			\
-  __extension__ static __inline void					\
-  __attribute__ ((__always_inline__))					\
-  vst3 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
-				     intype b, const int c)		\
-  {									\
-    __asm__ ("ld1 {v16." #regsuffix " - v18." #regsuffix "}, %1\n\t"	\
-	     "st3 {v16." #lnsuffix " - v18." #lnsuffix "}[%2], %0\n\t"	\
-	     : "=Q"(*(intype *) ptr)					\
-	     : "Q"(b), "i"(c)						\
-	     : "memory", "v16", "v17", "v18");				\
-  }
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgez_u64 (uint64x1_t __a)
+{
+  return __a >= 0ll ? -1ll : 0ll;
+}
 
-__ST3_LANE_FUNC (int8x8x3_t, int8_t, 8b, b, s8,)
-__ST3_LANE_FUNC (float32x2x3_t, float32_t, 2s, s, f32,)
-__ST3_LANE_FUNC (float64x1x3_t, float64_t, 1d, d, f64,)
-__ST3_LANE_FUNC (poly8x8x3_t, poly8_t, 8b, b, p8,)
-__ST3_LANE_FUNC (poly16x4x3_t, poly16_t, 4h, h, p16,)
-__ST3_LANE_FUNC (int16x4x3_t, int16_t, 4h, h, s16,)
-__ST3_LANE_FUNC (int32x2x3_t, int32_t, 2s, s, s32,)
-__ST3_LANE_FUNC (int64x1x3_t, int64_t, 1d, d, s64,)
-__ST3_LANE_FUNC (uint8x8x3_t, uint8_t, 8b, b, u8,)
-__ST3_LANE_FUNC (uint16x4x3_t, uint16_t, 4h, h, u16,)
-__ST3_LANE_FUNC (uint32x2x3_t, uint32_t, 2s, s, u32,)
-__ST3_LANE_FUNC (uint64x1x3_t, uint64_t, 1d, d, u64,)
-__ST3_LANE_FUNC (float32x4x3_t, float32_t, 4s, s, f32, q)
-__ST3_LANE_FUNC (float64x2x3_t, float64_t, 2d, d, f64, q)
-__ST3_LANE_FUNC (poly8x16x3_t, poly8_t, 16b, b, p8, q)
-__ST3_LANE_FUNC (poly16x8x3_t, poly16_t, 8h, h, p16, q)
-__ST3_LANE_FUNC (int8x16x3_t, int8_t, 16b, b, s8, q)
-__ST3_LANE_FUNC (int16x8x3_t, int16_t, 8h, h, s16, q)
-__ST3_LANE_FUNC (int32x4x3_t, int32_t, 4s, s, s32, q)
-__ST3_LANE_FUNC (int64x2x3_t, int64_t, 2d, d, s64, q)
-__ST3_LANE_FUNC (uint8x16x3_t, uint8_t, 16b, b, u8, q)
-__ST3_LANE_FUNC (uint16x8x3_t, uint16_t, 8h, h, u16, q)
-__ST3_LANE_FUNC (uint32x4x3_t, uint32_t, 4s, s, u32, q)
-__ST3_LANE_FUNC (uint64x2x3_t, uint64_t, 2d, d, u64, q)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgezq_f32 (float32x4_t __a)
+{
+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};
+  return (uint32x4_t) __builtin_aarch64_cmgev4sf (__a, __b);
+}
 
-#define __ST4_LANE_FUNC(intype, ptrtype, regsuffix,			\
-			lnsuffix, funcsuffix, Q)			\
-  __extension__ static __inline void					\
-  __attribute__ ((__always_inline__))					\
-  vst4 ## Q ## _lane_ ## funcsuffix (const ptrtype *ptr,		\
-				     intype b, const int c)		\
-  {									\
-    __asm__ ("ld1 {v16." #regsuffix " - v19." #regsuffix "}, %1\n\t"	\
-	     "st4 {v16." #lnsuffix " - v19." #lnsuffix "}[%2], %0\n\t"	\
-	     : "=Q"(*(intype *) ptr)					\
-	     : "Q"(b), "i"(c)						\
-	     : "memory", "v16", "v17", "v18", "v19");			\
-  }
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgezq_f64 (float64x2_t __a)
+{
+  float64x2_t __b = {0.0, 0.0};
+  return (uint64x2_t) __builtin_aarch64_cmgev2df (__a, __b);
+}
 
-__ST4_LANE_FUNC (int8x8x4_t, int8_t, 8b, b, s8,)
-__ST4_LANE_FUNC (float32x2x4_t, float32_t, 2s, s, f32,)
-__ST4_LANE_FUNC (float64x1x4_t, float64_t, 1d, d, f64,)
-__ST4_LANE_FUNC (poly8x8x4_t, poly8_t, 8b, b, p8,)
-__ST4_LANE_FUNC (poly16x4x4_t, poly16_t, 4h, h, p16,)
-__ST4_LANE_FUNC (int16x4x4_t, int16_t, 4h, h, s16,)
-__ST4_LANE_FUNC (int32x2x4_t, int32_t, 2s, s, s32,)
-__ST4_LANE_FUNC (int64x1x4_t, int64_t, 1d, d, s64,)
-__ST4_LANE_FUNC (uint8x8x4_t, uint8_t, 8b, b, u8,)
-__ST4_LANE_FUNC (uint16x4x4_t, uint16_t, 4h, h, u16,)
-__ST4_LANE_FUNC (uint32x2x4_t, uint32_t, 2s, s, u32,)
-__ST4_LANE_FUNC (uint64x1x4_t, uint64_t, 1d, d, u64,)
-__ST4_LANE_FUNC (float32x4x4_t, float32_t, 4s, s, f32, q)
-__ST4_LANE_FUNC (float64x2x4_t, float64_t, 2d, d, f64, q)
-__ST4_LANE_FUNC (poly8x16x4_t, poly8_t, 16b, b, p8, q)
-__ST4_LANE_FUNC (poly16x8x4_t, poly16_t, 8h, h, p16, q)
-__ST4_LANE_FUNC (int8x16x4_t, int8_t, 16b, b, s8, q)
-__ST4_LANE_FUNC (int16x8x4_t, int16_t, 8h, h, s16, q)
-__ST4_LANE_FUNC (int32x4x4_t, int32_t, 4s, s, s32, q)
-__ST4_LANE_FUNC (int64x2x4_t, int64_t, 2d, d, s64, q)
-__ST4_LANE_FUNC (uint8x16x4_t, uint8_t, 16b, b, u8, q)
-__ST4_LANE_FUNC (uint16x8x4_t, uint16_t, 8h, h, u16, q)
-__ST4_LANE_FUNC (uint32x4x4_t, uint32_t, 4s, s, u32, q)
-__ST4_LANE_FUNC (uint64x2x4_t, uint64_t, 2d, d, u64, q)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgezq_p8 (poly8x16_t __a)
+{
+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		    0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmgev16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
+}
 
-__extension__ static __inline int64_t __attribute__ ((__always_inline__))
-vaddlv_s32 (int32x2_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgezq_s8 (int8x16_t __a)
 {
-  int64_t result;
-  __asm__ ("saddlp %0.1d, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		   0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmgev16qi (__a, __b);
 }
 
-__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
-vaddlv_u32 (uint32x2_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vcgezq_s16 (int16x8_t __a)
 {
-  uint64_t result;
-  __asm__ ("uaddlp %0.1d, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint16x8_t) __builtin_aarch64_cmgev8hi (__a, __b);
 }
 
-__extension__ static __inline int32_t __attribute__ ((__always_inline__))
-vaddv_s32 (int32x2_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgezq_s32 (int32x4_t __a)
 {
-  int32_t result;
-  __asm__ ("addp %0.2s, %1.2s, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  int32x4_t __b = {0, 0, 0, 0};
+  return (uint32x4_t) __builtin_aarch64_cmgev4si (__a, __b);
 }
 
-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
-vaddv_u32 (uint32x2_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgezq_s64 (int64x2_t __a)
 {
-  uint32_t result;
-  __asm__ ("addp %0.2s, %1.2s, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  int64x2_t __b = {0, 0};
+  return (uint64x2_t) __builtin_aarch64_cmgev2di (__a, __b);
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vmaxnmv_f32 (float32x2_t a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgezq_u8 (uint8x16_t __a)
 {
-  float32_t result;
-  __asm__ ("fmaxnmp %0.2s, %1.2s, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  uint8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		    0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmgeuv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
-__extension__ static __inline float32_t __attribute__ ((__always_inline__))
-vminnmv_f32 (float32x2_t a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vcgezq_u16 (uint16x8_t __a)
 {
-  float32_t result;
-  __asm__ ("fminnmp %0.2s, %1.2s, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  uint16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint16x8_t) __builtin_aarch64_cmgeuv8hi ((int16x8_t) __a,
+						  (int16x8_t) __b);
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vmaxnmvq_f64 (float64x2_t a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgezq_u32 (uint32x4_t __a)
 {
-  float64_t result;
-  __asm__ ("fmaxnmp %0.2d, %1.2d, %1.2d" : "=w"(result) : "w"(a) : );
-  return result;
+  uint32x4_t __b = {0, 0, 0, 0};
+  return (uint32x4_t) __builtin_aarch64_cmgeuv4si ((int32x4_t) __a,
+						  (int32x4_t) __b);
 }
 
-__extension__ static __inline int32_t __attribute__ ((__always_inline__))
-vmaxv_s32 (int32x2_t a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgezq_u64 (uint64x2_t __a)
 {
-  int32_t result;
-  __asm__ ("smaxp %0.2s, %1.2s, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  uint64x2_t __b = {0, 0};
+  return (uint64x2_t) __builtin_aarch64_cmgeuv2di ((int64x2_t) __a,
+						  (int64x2_t) __b);
 }
 
+/* vcgez - scalar.  */
+
 __extension__ static __inline uint32_t __attribute__ ((__always_inline__))
-vmaxv_u32 (uint32x2_t a)
+vcgezs_f32 (float32_t __a)
 {
-  uint32_t result;
-  __asm__ ("umaxp %0.2s, %1.2s, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  return __a >= 0.0f ? -1 : 0;
 }
 
-__extension__ static __inline float64_t __attribute__ ((__always_inline__))
-vminnmvq_f64 (float64x2_t a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgezd_s64 (int64x1_t __a)
 {
-  float64_t result;
-  __asm__ ("fminnmp %0.2d, %1.2d, %1.2d" : "=w"(result) : "w"(a) : );
-  return result;
+  return __a >= 0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline int32_t __attribute__ ((__always_inline__))
-vminv_s32 (int32x2_t a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgezd_u64 (int64x1_t __a)
 {
-  int32_t result;
-  __asm__ ("sminp %0.2s, %1.2s, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  return __a >= 0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
-vminv_u32 (uint32x2_t a)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcgezd_f64 (float64_t __a)
 {
-  uint32_t result;
-  __asm__ ("uminp %0.2s, %1.2s, %1.2s" : "=w"(result) : "w"(a) : );
-  return result;
+  return __a >= 0.0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vpaddd_s64 (int64x2_t __a)
+/* vcgt - vector.  */
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcgt_f32 (float32x2_t __a, float32x2_t __b)
 {
-  return __builtin_aarch64_addpdi (__a);
+  return (uint32x2_t) __builtin_aarch64_cmgtv2sf (__a, __b);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vqdmulh_laneq_s16 (int16x4_t __a, int16x8_t __b, const int __c)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgt_f64 (float64x1_t __a, float64x1_t __b)
 {
-  return __builtin_aarch64_sqdmulh_laneqv4hi (__a, __b, __c);
+  return __a > __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vqdmulh_laneq_s32 (int32x2_t __a, int32x4_t __b, const int __c)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcgt_p8 (poly8x8_t __a, poly8x8_t __b)
 {
-  return __builtin_aarch64_sqdmulh_laneqv2si (__a, __b, __c);
+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vqdmulhq_laneq_s16 (int16x8_t __a, int16x8_t __b, const int __c)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcgt_s8 (int8x8_t __a, int8x8_t __b)
 {
-  return __builtin_aarch64_sqdmulh_laneqv8hi (__a, __b, __c);
+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi (__a, __b);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vqdmulhq_laneq_s32 (int32x4_t __a, int32x4_t __b, const int __c)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vcgt_s16 (int16x4_t __a, int16x4_t __b)
 {
-  return __builtin_aarch64_sqdmulh_laneqv4si (__a, __b, __c);
+  return (uint16x4_t) __builtin_aarch64_cmgtv4hi (__a, __b);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vqrdmulh_laneq_s16 (int16x4_t __a, int16x8_t __b, const int __c)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcgt_s32 (int32x2_t __a, int32x2_t __b)
 {
-  return  __builtin_aarch64_sqrdmulh_laneqv4hi (__a, __b, __c);
+  return (uint32x2_t) __builtin_aarch64_cmgtv2si (__a, __b);
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vqrdmulh_laneq_s32 (int32x2_t __a, int32x4_t __b, const int __c)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgt_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return __builtin_aarch64_sqrdmulh_laneqv2si (__a, __b, __c);
+  return __a > __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vqrdmulhq_laneq_s16 (int16x8_t __a, int16x8_t __b, const int __c)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcgt_u8 (uint8x8_t __a, uint8x8_t __b)
 {
-  return __builtin_aarch64_sqrdmulh_laneqv8hi (__a, __b, __c);
+  return (uint8x8_t) __builtin_aarch64_cmgtuv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vqrdmulhq_laneq_s32 (int32x4_t __a, int32x4_t __b, const int __c)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vcgt_u16 (uint16x4_t __a, uint16x4_t __b)
 {
-  return __builtin_aarch64_sqrdmulh_laneqv4si (__a, __b, __c);
+  return (uint16x4_t) __builtin_aarch64_cmgtuv4hi ((int16x4_t) __a,
+						  (int16x4_t) __b);
 }
 
-/* Table intrinsics.  */
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcgt_u32 (uint32x2_t __a, uint32x2_t __b)
+{
+  return (uint32x2_t) __builtin_aarch64_cmgtuv2si ((int32x2_t) __a,
+						  (int32x2_t) __b);
+}
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vqtbl1_p8 (poly8x16_t a, uint8x8_t b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgt_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  poly8x8_t result;
-  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return __a > __b ? -1ll : 0ll;
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgtq_f32 (float32x4_t __a, float32x4_t __b)
+{
+  return (uint32x4_t) __builtin_aarch64_cmgtv4sf (__a, __b);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgtq_f64 (float64x2_t __a, float64x2_t __b)
+{
+  return (uint64x2_t) __builtin_aarch64_cmgtv2df (__a, __b);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vqtbl1_s8 (int8x16_t a, int8x8_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgtq_p8 (poly8x16_t __a, poly8x16_t __b)
 {
-  int8x8_t result;
-  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vqtbl1_u8 (uint8x16_t a, uint8x8_t b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgtq_s8 (int8x16_t __a, int8x16_t __b)
 {
-  uint8x8_t result;
-  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi (__a, __b);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vqtbl1q_p8 (poly8x16_t a, uint8x16_t b)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vcgtq_s16 (int16x8_t __a, int16x8_t __b)
 {
-  poly8x16_t result;
-  __asm__ ("tbl %0.16b, {%1.16b}, %2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint16x8_t) __builtin_aarch64_cmgtv8hi (__a, __b);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vqtbl1q_s8 (int8x16_t a, int8x16_t b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgtq_s32 (int32x4_t __a, int32x4_t __b)
 {
-  int8x16_t result;
-  __asm__ ("tbl %0.16b, {%1.16b}, %2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmgtv4si (__a, __b);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqtbl1q_u8 (uint8x16_t a, uint8x16_t b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgtq_s64 (int64x2_t __a, int64x2_t __b)
 {
-  uint8x16_t result;
-  __asm__ ("tbl %0.16b, {%1.16b}, %2.16b"
-           : "=w"(result)
-           : "w"(a), "w"(b)
-           : /* No clobbers */);
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmgtv2di (__a, __b);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vqtbl2_s8 (int8x16x2_t tab, int8x8_t idx)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgtq_u8 (uint8x16_t __a, uint8x16_t __b)
 {
-  int8x8_t result;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbl %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmgtuv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vqtbl2_u8 (uint8x16x2_t tab, uint8x8_t idx)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vcgtq_u16 (uint16x8_t __a, uint16x8_t __b)
 {
-  uint8x8_t result;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbl %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return (uint16x8_t) __builtin_aarch64_cmgtuv8hi ((int16x8_t) __a,
+						  (int16x8_t) __b);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vqtbl2_p8 (poly8x16x2_t tab, uint8x8_t idx)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgtq_u32 (uint32x4_t __a, uint32x4_t __b)
 {
-  poly8x8_t result;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbl %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmgtuv4si ((int32x4_t) __a,
+						  (int32x4_t) __b);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vqtbl2q_s8 (int8x16x2_t tab, int8x16_t idx)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgtq_u64 (uint64x2_t __a, uint64x2_t __b)
 {
-  int8x16_t result;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbl %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmgtuv2di ((int64x2_t) __a,
+						  (int64x2_t) __b);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqtbl2q_u8 (uint8x16x2_t tab, uint8x16_t idx)
+/* vcgt - scalar.  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcgts_f32 (float32_t __a, float32_t __b)
 {
-  uint8x16_t result;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbl %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return __a > __b ? -1 : 0;
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vqtbl2q_p8 (poly8x16x2_t tab, uint8x16_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgtd_s64 (int64x1_t __a, int64x1_t __b)
 {
-  poly8x16_t result;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbl %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return __a > __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vqtbl3_s8 (int8x16x3_t tab, int8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgtd_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  int8x8_t result;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  return __a > __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vqtbl3_u8 (uint8x16x3_t tab, uint8x8_t idx)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcgtd_f64 (float64_t __a, float64_t __b)
 {
-  uint8x8_t result;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  return __a > __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vqtbl3_p8 (poly8x16x3_t tab, uint8x8_t idx)
+/* vcgtz - vector.  */
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcgtz_f32 (float32x2_t __a)
 {
-  poly8x8_t result;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  float32x2_t __b = {0.0f, 0.0f};
+  return (uint32x2_t) __builtin_aarch64_cmgtv2sf (__a, __b);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vqtbl3q_s8 (int8x16x3_t tab, int8x16_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgtz_f64 (float64x1_t __a)
 {
-  int8x16_t result;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbl %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  return __a > 0.0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqtbl3q_u8 (uint8x16x3_t tab, uint8x16_t idx)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcgtz_p8 (poly8x8_t __a)
 {
-  uint8x16_t result;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbl %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vqtbl3q_p8 (poly8x16x3_t tab, uint8x16_t idx)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcgtz_s8 (int8x8_t __a)
 {
-  poly8x16_t result;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbl %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi (__a, __b);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vqtbl4_s8 (int8x16x4_t tab, int8x8_t idx)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vcgtz_s16 (int16x4_t __a)
 {
-  int8x8_t result;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  int16x4_t __b = {0, 0, 0, 0};
+  return (uint16x4_t) __builtin_aarch64_cmgtv4hi (__a, __b);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vqtbl4_u8 (uint8x16x4_t tab, uint8x8_t idx)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcgtz_s32 (int32x2_t __a)
 {
-  uint8x8_t result;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  int32x2_t __b = {0, 0};
+  return (uint32x2_t) __builtin_aarch64_cmgtv2si (__a, __b);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vqtbl4_p8 (poly8x16x4_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgtz_s64 (int64x1_t __a)
 {
-  poly8x8_t result;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  return __a > 0ll ? -1ll : 0ll;
 }
 
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcgtz_u8 (uint8x8_t __a)
+{
+  uint8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmgtuv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
+}
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vqtbl4q_s8 (int8x16x4_t tab, int8x16_t idx)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vcgtz_u16 (uint16x4_t __a)
 {
-  int8x16_t result;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbl %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  uint16x4_t __b = {0, 0, 0, 0};
+  return (uint16x4_t) __builtin_aarch64_cmgtuv4hi ((int16x4_t) __a,
+						  (int16x4_t) __b);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqtbl4q_u8 (uint8x16x4_t tab, uint8x16_t idx)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcgtz_u32 (uint32x2_t __a)
 {
-  uint8x16_t result;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbl %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  uint32x2_t __b = {0, 0};
+  return (uint32x2_t) __builtin_aarch64_cmgtuv2si ((int32x2_t) __a,
+						  (int32x2_t) __b);
+}
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgtz_u64 (uint64x1_t __a)
+{
+  return __a > 0ll ? -1ll : 0ll;
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vqtbl4q_p8 (poly8x16x4_t tab, uint8x16_t idx)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgtzq_f32 (float32x4_t __a)
 {
-  poly8x16_t result;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbl %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
-	   :"=w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};
+  return (uint32x4_t) __builtin_aarch64_cmgtv4sf (__a, __b);
 }
 
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgtzq_f64 (float64x2_t __a)
+{
+  float64x2_t __b = {0.0, 0.0};
+  return (uint64x2_t) __builtin_aarch64_cmgtv2df (__a, __b);
+}
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vqtbx1_s8 (int8x8_t r, int8x16_t tab, int8x8_t idx)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgtzq_p8 (poly8x16_t __a)
 {
-  int8x8_t result = r;
-  __asm__ ("tbx %0.8b,{%1.16b},%2.8b"
-           : "+w"(result)
-           : "w"(tab), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		    0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vqtbx1_u8 (uint8x8_t r, uint8x16_t tab, uint8x8_t idx)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcgtzq_s8 (int8x16_t __a)
 {
-  uint8x8_t result = r;
-  __asm__ ("tbx %0.8b,{%1.16b},%2.8b"
-           : "+w"(result)
-           : "w"(tab), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		   0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi (__a, __b);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vqtbx1_p8 (poly8x8_t r, poly8x16_t tab, uint8x8_t idx)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vcgtzq_s16 (int16x8_t __a)
 {
-  poly8x8_t result = r;
-  __asm__ ("tbx %0.8b,{%1.16b},%2.8b"
-           : "+w"(result)
-           : "w"(tab), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint16x8_t) __builtin_aarch64_cmgtv8hi (__a, __b);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vqtbx1q_s8 (int8x16_t r, int8x16_t tab, int8x16_t idx)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgtzq_s32 (int32x4_t __a)
 {
-  int8x16_t result = r;
-  __asm__ ("tbx %0.16b,{%1.16b},%2.16b"
-           : "+w"(result)
-           : "w"(tab), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  int32x4_t __b = {0, 0, 0, 0};
+  return (uint32x4_t) __builtin_aarch64_cmgtv4si (__a, __b);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgtzq_s64 (int64x2_t __a)
+{
+  int64x2_t __b = {0, 0};
+  return (uint64x2_t) __builtin_aarch64_cmgtv2di (__a, __b);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqtbx1q_u8 (uint8x16_t r, uint8x16_t tab, uint8x16_t idx)
+vcgtzq_u8 (uint8x16_t __a)
 {
-  uint8x16_t result = r;
-  __asm__ ("tbx %0.16b,{%1.16b},%2.16b"
-           : "+w"(result)
-           : "w"(tab), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  uint8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		    0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmgtuv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vqtbx1q_p8 (poly8x16_t r, poly8x16_t tab, uint8x16_t idx)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vcgtzq_u16 (uint16x8_t __a)
 {
-  poly8x16_t result = r;
-  __asm__ ("tbx %0.16b,{%1.16b},%2.16b"
-           : "+w"(result)
-           : "w"(tab), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  uint16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint16x8_t) __builtin_aarch64_cmgtuv8hi ((int16x8_t) __a,
+						  (int16x8_t) __b);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vqtbx2_s8 (int8x8_t r, int8x16x2_t tab, int8x8_t idx)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcgtzq_u32 (uint32x4_t __a)
 {
-  int8x8_t result = r;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbx %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  uint32x4_t __b = {0, 0, 0, 0};
+  return (uint32x4_t) __builtin_aarch64_cmgtuv4si ((int32x4_t) __a,
+						  (int32x4_t) __b);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vqtbx2_u8 (uint8x8_t r, uint8x16x2_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcgtzq_u64 (uint64x2_t __a)
 {
-  uint8x8_t result = r;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbx %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  uint64x2_t __b = {0, 0};
+  return (uint64x2_t) __builtin_aarch64_cmgtuv2di ((int64x2_t) __a,
+						  (int64x2_t) __b);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vqtbx2_p8 (poly8x8_t r, poly8x16x2_t tab, uint8x8_t idx)
+/* vcgtz - scalar.  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcgtzs_f32 (float32_t __a)
 {
-  poly8x8_t result = r;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbx %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return __a > 0.0f ? -1 : 0;
 }
 
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgtzd_s64 (int64x1_t __a)
+{
+  return __a > 0 ? -1ll : 0ll;
+}
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vqtbx2q_s8 (int8x16_t r, int8x16x2_t tab, int8x16_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcgtzd_u64 (int64x1_t __a)
 {
-  int8x16_t result = r;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbx %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return __a > 0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqtbx2q_u8 (uint8x16_t r, uint8x16x2_t tab, uint8x16_t idx)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcgtzd_f64 (float64_t __a)
 {
-  uint8x16_t result = r;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbx %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return __a > 0.0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vqtbx2q_p8 (poly8x16_t r, poly8x16x2_t tab, uint8x16_t idx)
+/* vcle - vector.  */
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcle_f32 (float32x2_t __a, float32x2_t __b)
 {
-  poly8x16_t result = r;
-  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
-	   "tbx %0.16b, {v16.16b, v17.16b}, %2.16b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17");
-  return result;
+  return (uint32x2_t) __builtin_aarch64_cmgev2sf (__b, __a);
 }
 
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcle_f64 (float64x1_t __a, float64x1_t __b)
+{
+  return __a <= __b ? -1ll : 0ll;
+}
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vqtbx3_s8 (int8x8_t r, int8x16x3_t tab, int8x8_t idx)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcle_p8 (poly8x8_t __a, poly8x8_t __b)
 {
-  int8x8_t result = r;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbx %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  return (uint8x8_t) __builtin_aarch64_cmgev8qi ((int8x8_t) __b,
+						 (int8x8_t) __a);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vqtbx3_u8 (uint8x8_t r, uint8x16x3_t tab, uint8x8_t idx)
+vcle_s8 (int8x8_t __a, int8x8_t __b)
 {
-  uint8x8_t result = r;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbx %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  return (uint8x8_t) __builtin_aarch64_cmgev8qi (__b, __a);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vqtbx3_p8 (poly8x8_t r, poly8x16x3_t tab, uint8x8_t idx)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vcle_s16 (int16x4_t __a, int16x4_t __b)
 {
-  poly8x8_t result = r;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbx %0.8b, {v16.16b - v18.16b}, %2.8b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  return (uint16x4_t) __builtin_aarch64_cmgev4hi (__b, __a);
 }
 
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcle_s32 (int32x2_t __a, int32x2_t __b)
+{
+  return (uint32x2_t) __builtin_aarch64_cmgev2si (__b, __a);
+}
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vqtbx3q_s8 (int8x16_t r, int8x16x3_t tab, int8x16_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcle_s64 (int64x1_t __a, int64x1_t __b)
 {
-  int8x16_t result = r;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbx %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  return __a <= __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqtbx3q_u8 (uint8x16_t r, uint8x16x3_t tab, uint8x16_t idx)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcle_u8 (uint8x8_t __a, uint8x8_t __b)
 {
-  uint8x16_t result = r;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbx %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  return (uint8x8_t) __builtin_aarch64_cmgeuv8qi ((int8x8_t) __b,
+						 (int8x8_t) __a);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vqtbx3q_p8 (poly8x16_t r, poly8x16x3_t tab, uint8x16_t idx)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vcle_u16 (uint16x4_t __a, uint16x4_t __b)
 {
-  poly8x16_t result = r;
-  __asm__ ("ld1 {v16.16b - v18.16b}, %1\n\t"
-	   "tbx %0.16b, {v16.16b - v18.16b}, %2.16b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18");
-  return result;
+  return (uint16x4_t) __builtin_aarch64_cmgeuv4hi ((int16x4_t) __b,
+						  (int16x4_t) __a);
 }
 
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcle_u32 (uint32x2_t __a, uint32x2_t __b)
+{
+  return (uint32x2_t) __builtin_aarch64_cmgeuv2si ((int32x2_t) __b,
+						  (int32x2_t) __a);
+}
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vqtbx4_s8 (int8x8_t r, int8x16x4_t tab, int8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcle_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  int8x8_t result = r;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbx %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  return __a <= __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vqtbx4_u8 (uint8x8_t r, uint8x16x4_t tab, uint8x8_t idx)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcleq_f32 (float32x4_t __a, float32x4_t __b)
 {
-  uint8x8_t result = r;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbx %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmgev4sf (__b, __a);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcleq_f64 (float64x2_t __a, float64x2_t __b)
+{
+  return (uint64x2_t) __builtin_aarch64_cmgev2df (__b, __a);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vqtbx4_p8 (poly8x8_t r, poly8x16x4_t tab, uint8x8_t idx)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcleq_p8 (poly8x16_t __a, poly8x16_t __b)
 {
-  poly8x8_t result = r;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbx %0.8b, {v16.16b - v19.16b}, %2.8b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  return (uint8x16_t) __builtin_aarch64_cmgev16qi ((int8x16_t) __b,
+						   (int8x16_t) __a);
 }
 
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcleq_s8 (int8x16_t __a, int8x16_t __b)
+{
+  return (uint8x16_t) __builtin_aarch64_cmgev16qi (__b, __a);
+}
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vqtbx4q_s8 (int8x16_t r, int8x16x4_t tab, int8x16_t idx)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vcleq_s16 (int16x8_t __a, int16x8_t __b)
 {
-  int8x16_t result = r;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbx %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  return (uint16x8_t) __builtin_aarch64_cmgev8hi (__b, __a);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vqtbx4q_u8 (uint8x16_t r, uint8x16x4_t tab, uint8x16_t idx)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcleq_s32 (int32x4_t __a, int32x4_t __b)
 {
-  uint8x16_t result = r;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbx %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmgev4si (__b, __a);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vqtbx4q_p8 (poly8x16_t r, poly8x16x4_t tab, uint8x16_t idx)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcleq_s64 (int64x2_t __a, int64x2_t __b)
 {
-  poly8x16_t result = r;
-  __asm__ ("ld1 {v16.16b - v19.16b}, %1\n\t"
-	   "tbx %0.16b, {v16.16b - v19.16b}, %2.16b\n\t"
-	   :"+w"(result)
-	   :"Q"(tab),"w"(idx)
-	   :"memory", "v16", "v17", "v18", "v19");
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmgev2di (__b, __a);
 }
 
-/* V7 legacy table intrinsics.  */
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcleq_u8 (uint8x16_t __a, uint8x16_t __b)
+{
+  return (uint8x16_t) __builtin_aarch64_cmgeuv16qi ((int8x16_t) __b,
+						   (int8x16_t) __a);
+}
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtbl1_s8 (int8x8_t tab, int8x8_t idx)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vcleq_u16 (uint16x8_t __a, uint16x8_t __b)
 {
-  int8x8_t result;
-  int8x16_t temp = vcombine_s8 (tab, vcreate_s8 (UINT64_C (0x0)));
-  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
-           : "=w"(result)
-           : "w"(temp), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  return (uint16x8_t) __builtin_aarch64_cmgeuv8hi ((int16x8_t) __b,
+						  (int16x8_t) __a);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtbl1_u8 (uint8x8_t tab, uint8x8_t idx)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcleq_u32 (uint32x4_t __a, uint32x4_t __b)
 {
-  uint8x8_t result;
-  uint8x16_t temp = vcombine_u8 (tab, vcreate_u8 (UINT64_C (0x0)));
-  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
-           : "=w"(result)
-           : "w"(temp), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  return (uint32x4_t) __builtin_aarch64_cmgeuv4si ((int32x4_t) __b,
+						  (int32x4_t) __a);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtbl1_p8 (poly8x8_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcleq_u64 (uint64x2_t __a, uint64x2_t __b)
 {
-  poly8x8_t result;
-  poly8x16_t temp = vcombine_p8 (tab, vcreate_p8 (UINT64_C (0x0)));
-  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
-           : "=w"(result)
-           : "w"(temp), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  return (uint64x2_t) __builtin_aarch64_cmgeuv2di ((int64x2_t) __b,
+						  (int64x2_t) __a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtbl2_s8 (int8x8x2_t tab, int8x8_t idx)
+/* vcle - scalar.  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcles_f32 (float32_t __a, float32_t __b)
 {
-  int8x8_t result;
-  int8x16_t temp = vcombine_s8 (tab.val[0], tab.val[1]);
-  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
-           : "=w"(result)
-           : "w"(temp), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  return __a <= __b ? -1 : 0;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtbl2_u8 (uint8x8x2_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcled_s64 (int64x1_t __a, int64x1_t __b)
 {
-  uint8x8_t result;
-  uint8x16_t temp = vcombine_u8 (tab.val[0], tab.val[1]);
-  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
-           : "=w"(result)
-           : "w"(temp), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  return __a <= __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtbl2_p8 (poly8x8x2_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcled_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  poly8x8_t result;
-  poly8x16_t temp = vcombine_p8 (tab.val[0], tab.val[1]);
-  __asm__ ("tbl %0.8b, {%1.16b}, %2.8b"
-           : "=w"(result)
-           : "w"(temp), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  return __a <= __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtbl3_s8 (int8x8x3_t tab, int8x8_t idx)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcled_f64 (float64_t __a, float64_t __b)
 {
-  int8x8_t result;
-  int8x16x2_t temp;
-  temp.val[0] = vcombine_s8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_s8 (tab.val[2], vcreate_s8 (UINT64_C (0x0)));
-  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
-           : "=w"(result)
-           : "Q"(temp), "w"(idx)
-           : "v16", "v17", "memory");
-  return result;
+  return __a <= __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtbl3_u8 (uint8x8x3_t tab, uint8x8_t idx)
+/* vclez - vector.  */
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vclez_f32 (float32x2_t __a)
 {
-  uint8x8_t result;
-  uint8x16x2_t temp;
-  temp.val[0] = vcombine_u8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_u8 (tab.val[2], vcreate_u8 (UINT64_C (0x0)));
-  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
-           : "=w"(result)
-           : "Q"(temp), "w"(idx)
-           : "v16", "v17", "memory");
-  return result;
+  float32x2_t __b = {0.0f, 0.0f};
+  return (uint32x2_t) __builtin_aarch64_cmlev2sf (__a, __b);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtbl3_p8 (poly8x8x3_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vclez_f64 (float64x1_t __a)
 {
-  poly8x8_t result;
-  poly8x16x2_t temp;
-  temp.val[0] = vcombine_p8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_p8 (tab.val[2], vcreate_p8 (UINT64_C (0x0)));
-  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
-           : "=w"(result)
-           : "Q"(temp), "w"(idx)
-           : "v16", "v17", "memory");
-  return result;
+  return __a <= 0.0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtbl4_s8 (int8x8x4_t tab, int8x8_t idx)
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vclez_p8 (poly8x8_t __a)
 {
-  int8x8_t result;
-  int8x16x2_t temp;
-  temp.val[0] = vcombine_s8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_s8 (tab.val[2], tab.val[3]);
-  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
-           : "=w"(result)
-           : "Q"(temp), "w"(idx)
-           : "v16", "v17", "memory");
-  return result;
+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmlev8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtbl4_u8 (uint8x8x4_t tab, uint8x8_t idx)
+vclez_s8 (int8x8_t __a)
 {
-  uint8x8_t result;
-  uint8x16x2_t temp;
-  temp.val[0] = vcombine_u8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_u8 (tab.val[2], tab.val[3]);
-  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
-           : "=w"(result)
-           : "Q"(temp), "w"(idx)
-           : "v16", "v17", "memory");
-  return result;
+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmlev8qi (__a, __b);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtbl4_p8 (poly8x8x4_t tab, uint8x8_t idx)
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vclez_s16 (int16x4_t __a)
 {
-  poly8x8_t result;
-  poly8x16x2_t temp;
-  temp.val[0] = vcombine_p8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_p8 (tab.val[2], tab.val[3]);
-  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
-	   "tbl %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
-           : "=w"(result)
-           : "Q"(temp), "w"(idx)
-           : "v16", "v17", "memory");
-  return result;
+  int16x4_t __b = {0, 0, 0, 0};
+  return (uint16x4_t) __builtin_aarch64_cmlev4hi (__a, __b);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtbx1_s8 (int8x8_t r, int8x8_t tab, int8x8_t idx)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vclez_s32 (int32x2_t __a)
 {
-  int8x8_t result;
-  int8x8_t tmp1;
-  int8x16_t temp = vcombine_s8 (tab, vcreate_s8 (UINT64_C (0x0)));
-  __asm__ ("movi %0.8b, 8\n\t"
-	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
-	   "tbl %1.8b, {%2.16b}, %3.8b\n\t"
-	   "bsl %0.8b, %4.8b, %1.8b\n\t"
-           : "+w"(result), "=&w"(tmp1)
-           : "w"(temp), "w"(idx), "w"(r)
-           : /* No clobbers */);
-  return result;
+  int32x2_t __b = {0, 0};
+  return (uint32x2_t) __builtin_aarch64_cmlev2si (__a, __b);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtbx1_u8 (uint8x8_t r, uint8x8_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vclez_s64 (int64x1_t __a)
 {
-  uint8x8_t result;
-  uint8x8_t tmp1;
-  uint8x16_t temp = vcombine_u8 (tab, vcreate_u8 (UINT64_C (0x0)));
-  __asm__ ("movi %0.8b, 8\n\t"
-	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
-	   "tbl %1.8b, {%2.16b}, %3.8b\n\t"
-	   "bsl %0.8b, %4.8b, %1.8b\n\t"
-           : "+w"(result), "=&w"(tmp1)
-           : "w"(temp), "w"(idx), "w"(r)
-           : /* No clobbers */);
-  return result;
+  return __a <= 0ll ? -1ll : 0ll;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtbx1_p8 (poly8x8_t r, poly8x8_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vclez_u64 (uint64x1_t __a)
 {
-  poly8x8_t result;
-  poly8x8_t tmp1;
-  poly8x16_t temp = vcombine_p8 (tab, vcreate_p8 (UINT64_C (0x0)));
-  __asm__ ("movi %0.8b, 8\n\t"
-	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
-	   "tbl %1.8b, {%2.16b}, %3.8b\n\t"
-	   "bsl %0.8b, %4.8b, %1.8b\n\t"
-           : "+w"(result), "=&w"(tmp1)
-           : "w"(temp), "w"(idx), "w"(r)
-           : /* No clobbers */);
-  return result;
+  return __a <= 0ll ? -1ll : 0ll;
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtbx2_s8 (int8x8_t r, int8x8x2_t tab, int8x8_t idx)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vclezq_f32 (float32x4_t __a)
 {
-  int8x8_t result = r;
-  int8x16_t temp = vcombine_s8 (tab.val[0], tab.val[1]);
-  __asm__ ("tbx %0.8b, {%1.16b}, %2.8b"
-           : "+w"(result)
-           : "w"(temp), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};
+  return (uint32x4_t) __builtin_aarch64_cmlev4sf (__a, __b);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vclezq_f64 (float64x2_t __a)
+{
+  float64x2_t __b = {0.0, 0.0};
+  return (uint64x2_t) __builtin_aarch64_cmlev2df (__a, __b);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtbx2_u8 (uint8x8_t r, uint8x8x2_t tab, uint8x8_t idx)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vclezq_p8 (poly8x16_t __a)
 {
-  uint8x8_t result = r;
-  uint8x16_t temp = vcombine_u8 (tab.val[0], tab.val[1]);
-  __asm__ ("tbx %0.8b, {%1.16b}, %2.8b"
-           : "+w"(result)
-           : "w"(temp), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		    0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmlev16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtbx2_p8 (poly8x8_t r, poly8x8x2_t tab, uint8x8_t idx)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vclezq_s8 (int8x16_t __a)
 {
-  poly8x8_t result = r;
-  poly8x16_t temp = vcombine_p8 (tab.val[0], tab.val[1]);
-  __asm__ ("tbx %0.8b, {%1.16b}, %2.8b"
-           : "+w"(result)
-           : "w"(temp), "w"(idx)
-           : /* No clobbers */);
-  return result;
+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		   0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmlev16qi (__a, __b);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtbx3_s8 (int8x8_t r, int8x8x3_t tab, int8x8_t idx)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vclezq_s16 (int16x8_t __a)
 {
-  int8x8_t result;
-  int8x8_t tmp1;
-  int8x16x2_t temp;
-  temp.val[0] = vcombine_s8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_s8 (tab.val[2], vcreate_s8 (UINT64_C (0x0)));
-  __asm__ ("ld1 {v16.16b - v17.16b}, %2\n\t"
-	   "movi %0.8b, 24\n\t"
-	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
-	   "tbl %1.8b, {v16.16b - v17.16b}, %3.8b\n\t"
-	   "bsl %0.8b, %4.8b, %1.8b\n\t"
-           : "+w"(result), "=&w"(tmp1)
-           : "Q"(temp), "w"(idx), "w"(r)
-           : "v16", "v17", "memory");
-  return result;
+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint16x8_t) __builtin_aarch64_cmlev8hi (__a, __b);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtbx3_u8 (uint8x8_t r, uint8x8x3_t tab, uint8x8_t idx)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vclezq_s32 (int32x4_t __a)
 {
-  uint8x8_t result;
-  uint8x8_t tmp1;
-  uint8x16x2_t temp;
-  temp.val[0] = vcombine_u8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_u8 (tab.val[2], vcreate_u8 (UINT64_C (0x0)));
-  __asm__ ("ld1 {v16.16b - v17.16b}, %2\n\t"
-	   "movi %0.8b, 24\n\t"
-	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
-	   "tbl %1.8b, {v16.16b - v17.16b}, %3.8b\n\t"
-	   "bsl %0.8b, %4.8b, %1.8b\n\t"
-           : "+w"(result), "=&w"(tmp1)
-           : "Q"(temp), "w"(idx), "w"(r)
-           : "v16", "v17", "memory");
-  return result;
+  int32x4_t __b = {0, 0, 0, 0};
+  return (uint32x4_t) __builtin_aarch64_cmlev4si (__a, __b);
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtbx3_p8 (poly8x8_t r, poly8x8x3_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vclezq_s64 (int64x2_t __a)
 {
-  poly8x8_t result;
-  poly8x8_t tmp1;
-  poly8x16x2_t temp;
-  temp.val[0] = vcombine_p8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_p8 (tab.val[2], vcreate_p8 (UINT64_C (0x0)));
-  __asm__ ("ld1 {v16.16b - v17.16b}, %2\n\t"
-	   "movi %0.8b, 24\n\t"
-	   "cmhs %0.8b, %3.8b, %0.8b\n\t"
-	   "tbl %1.8b, {v16.16b - v17.16b}, %3.8b\n\t"
-	   "bsl %0.8b, %4.8b, %1.8b\n\t"
-           : "+w"(result), "=&w"(tmp1)
-           : "Q"(temp), "w"(idx), "w"(r)
-           : "v16", "v17", "memory");
-  return result;
+  int64x2_t __b = {0, 0};
+  return (uint64x2_t) __builtin_aarch64_cmlev2di (__a, __b);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vtbx4_s8 (int8x8_t r, int8x8x4_t tab, int8x8_t idx)
+/* vclez - scalar.  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vclezs_f32 (float32_t __a)
 {
-  int8x8_t result = r;
-  int8x16x2_t temp;
-  temp.val[0] = vcombine_s8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_s8 (tab.val[2], tab.val[3]);
-  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
-	   "tbx %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
-           : "+w"(result)
-           : "Q"(temp), "w"(idx)
-           : "v16", "v17", "memory");
-  return result;
+  return __a <= 0.0f ? -1 : 0;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vtbx4_u8 (uint8x8_t r, uint8x8x4_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vclezd_s64 (int64x1_t __a)
 {
-  uint8x8_t result = r;
-  uint8x16x2_t temp;
-  temp.val[0] = vcombine_u8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_u8 (tab.val[2], tab.val[3]);
-  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
-	   "tbx %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
-           : "+w"(result)
-           : "Q"(temp), "w"(idx)
-           : "v16", "v17", "memory");
-  return result;
+  return __a <= 0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vtbx4_p8 (poly8x8_t r, poly8x8x4_t tab, uint8x8_t idx)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vclezd_u64 (int64x1_t __a)
 {
-  poly8x8_t result = r;
-  poly8x16x2_t temp;
-  temp.val[0] = vcombine_p8 (tab.val[0], tab.val[1]);
-  temp.val[1] = vcombine_p8 (tab.val[2], tab.val[3]);
-  __asm__ ("ld1 {v16.16b - v17.16b }, %1\n\t"
-	   "tbx %0.8b, {v16.16b - v17.16b}, %2.8b\n\t"
-           : "+w"(result)
-           : "Q"(temp), "w"(idx)
-           : "v16", "v17", "memory");
-  return result;
+  return __a <= 0 ? -1ll : 0ll;
 }
 
-/* End of temporary inline asm.  */
-
-/* Start of optimal implementations in approved order.  */
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vclezd_f64 (float64_t __a)
+{
+  return __a <= 0.0 ? -1ll : 0ll;
+}
 
-/* vadd */
+/* vclt - vector.  */
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vaddd_s64 (int64x1_t __a, int64x1_t __b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vclt_f32 (float32x2_t __a, float32x2_t __b)
 {
-  return __a + __b;
+  return (uint32x2_t) __builtin_aarch64_cmgtv2sf (__b, __a);
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vaddd_u64 (uint64x1_t __a, uint64x1_t __b)
+vclt_f64 (float64x1_t __a, float64x1_t __b)
 {
-  return __a + __b;
+  return __a < __b ? -1ll : 0ll;
 }
 
-/* vceq */
-
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vceq_p8 (poly8x8_t __a, poly8x8_t __b)
+vclt_p8 (poly8x8_t __a, poly8x8_t __b)
 {
-  return (uint8x8_t) __builtin_aarch64_cmeqv8qi ((int8x8_t) __a,
-						 (int8x8_t) __b);
+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi ((int8x8_t) __b,
+						 (int8x8_t) __a);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vceq_s8 (int8x8_t __a, int8x8_t __b)
+vclt_s8 (int8x8_t __a, int8x8_t __b)
 {
-  return (uint8x8_t) __builtin_aarch64_cmeqv8qi (__a, __b);
+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi (__b, __a);
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vceq_s16 (int16x4_t __a, int16x4_t __b)
+vclt_s16 (int16x4_t __a, int16x4_t __b)
 {
-  return (uint16x4_t) __builtin_aarch64_cmeqv4hi (__a, __b);
+  return (uint16x4_t) __builtin_aarch64_cmgtv4hi (__b, __a);
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vceq_s32 (int32x2_t __a, int32x2_t __b)
+vclt_s32 (int32x2_t __a, int32x2_t __b)
 {
-  return (uint32x2_t) __builtin_aarch64_cmeqv2si (__a, __b);
+  return (uint32x2_t) __builtin_aarch64_cmgtv2si (__b, __a);
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vceq_s64 (int64x1_t __a, int64x1_t __b)
+vclt_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmeqdi (__a, __b);
+  return __a < __b ? -1ll : 0ll;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vceq_u8 (uint8x8_t __a, uint8x8_t __b)
+vclt_u8 (uint8x8_t __a, uint8x8_t __b)
 {
-  return (uint8x8_t) __builtin_aarch64_cmeqv8qi ((int8x8_t) __a,
-						 (int8x8_t) __b);
+  return (uint8x8_t) __builtin_aarch64_cmgtuv8qi ((int8x8_t) __b,
+						 (int8x8_t) __a);
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vceq_u16 (uint16x4_t __a, uint16x4_t __b)
+vclt_u16 (uint16x4_t __a, uint16x4_t __b)
 {
-  return (uint16x4_t) __builtin_aarch64_cmeqv4hi ((int16x4_t) __a,
-						  (int16x4_t) __b);
+  return (uint16x4_t) __builtin_aarch64_cmgtuv4hi ((int16x4_t) __b,
+						  (int16x4_t) __a);
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vceq_u32 (uint32x2_t __a, uint32x2_t __b)
+vclt_u32 (uint32x2_t __a, uint32x2_t __b)
 {
-  return (uint32x2_t) __builtin_aarch64_cmeqv2si ((int32x2_t) __a,
-						  (int32x2_t) __b);
+  return (uint32x2_t) __builtin_aarch64_cmgtuv2si ((int32x2_t) __b,
+						  (int32x2_t) __a);
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vceq_u64 (uint64x1_t __a, uint64x1_t __b)
+vclt_u64 (uint64x1_t __a, uint64x1_t __b)
+{
+  return __a < __b ? -1ll : 0ll;
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcltq_f32 (float32x4_t __a, float32x4_t __b)
+{
+  return (uint32x4_t) __builtin_aarch64_cmgtv4sf (__b, __a);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcltq_f64 (float64x2_t __a, float64x2_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmeqdi ((int64x1_t) __a,
-						(int64x1_t) __b);
+  return (uint64x2_t) __builtin_aarch64_cmgtv2df (__b, __a);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vceqq_p8 (poly8x16_t __a, poly8x16_t __b)
+vcltq_p8 (poly8x16_t __a, poly8x16_t __b)
 {
-  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,
-						   (int8x16_t) __b);
+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi ((int8x16_t) __b,
+						   (int8x16_t) __a);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vceqq_s8 (int8x16_t __a, int8x16_t __b)
+vcltq_s8 (int8x16_t __a, int8x16_t __b)
 {
-  return (uint8x16_t) __builtin_aarch64_cmeqv16qi (__a, __b);
+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi (__b, __a);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vceqq_s16 (int16x8_t __a, int16x8_t __b)
+vcltq_s16 (int16x8_t __a, int16x8_t __b)
 {
-  return (uint16x8_t) __builtin_aarch64_cmeqv8hi (__a, __b);
+  return (uint16x8_t) __builtin_aarch64_cmgtv8hi (__b, __a);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vceqq_s32 (int32x4_t __a, int32x4_t __b)
+vcltq_s32 (int32x4_t __a, int32x4_t __b)
 {
-  return (uint32x4_t) __builtin_aarch64_cmeqv4si (__a, __b);
+  return (uint32x4_t) __builtin_aarch64_cmgtv4si (__b, __a);
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vceqq_s64 (int64x2_t __a, int64x2_t __b)
+vcltq_s64 (int64x2_t __a, int64x2_t __b)
 {
-  return (uint64x2_t) __builtin_aarch64_cmeqv2di (__a, __b);
+  return (uint64x2_t) __builtin_aarch64_cmgtv2di (__b, __a);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vceqq_u8 (uint8x16_t __a, uint8x16_t __b)
+vcltq_u8 (uint8x16_t __a, uint8x16_t __b)
 {
-  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,
-						   (int8x16_t) __b);
+  return (uint8x16_t) __builtin_aarch64_cmgtuv16qi ((int8x16_t) __b,
+						   (int8x16_t) __a);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vceqq_u16 (uint16x8_t __a, uint16x8_t __b)
+vcltq_u16 (uint16x8_t __a, uint16x8_t __b)
 {
-  return (uint16x8_t) __builtin_aarch64_cmeqv8hi ((int16x8_t) __a,
-						  (int16x8_t) __b);
+  return (uint16x8_t) __builtin_aarch64_cmgtuv8hi ((int16x8_t) __b,
+						  (int16x8_t) __a);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vceqq_u32 (uint32x4_t __a, uint32x4_t __b)
+vcltq_u32 (uint32x4_t __a, uint32x4_t __b)
 {
-  return (uint32x4_t) __builtin_aarch64_cmeqv4si ((int32x4_t) __a,
-						  (int32x4_t) __b);
+  return (uint32x4_t) __builtin_aarch64_cmgtuv4si ((int32x4_t) __b,
+						  (int32x4_t) __a);
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vceqq_u64 (uint64x2_t __a, uint64x2_t __b)
+vcltq_u64 (uint64x2_t __a, uint64x2_t __b)
 {
-  return (uint64x2_t) __builtin_aarch64_cmeqv2di ((int64x2_t) __a,
-						  (int64x2_t) __b);
+  return (uint64x2_t) __builtin_aarch64_cmgtuv2di ((int64x2_t) __b,
+						  (int64x2_t) __a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vceqd_s64 (int64x1_t __a, int64x1_t __b)
+/* vclt - scalar.  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vclts_f32 (float32_t __a, float32_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmeqdi (__a, __b);
+  return __a < __b ? -1 : 0;
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vceqd_u64 (uint64x1_t __a, uint64x1_t __b)
+vcltd_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmeqdi (__a, __b);
+  return __a < __b ? -1ll : 0ll;
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vceqzd_s64 (int64x1_t __a)
+vcltd_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmeqdi (__a, 0);
+  return __a < __b ? -1ll : 0ll;
 }
 
-/* vcge */
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vcge_s8 (int8x8_t __a, int8x8_t __b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcltd_f64 (float64_t __a, float64_t __b)
 {
-  return (uint8x8_t) __builtin_aarch64_cmgev8qi (__a, __b);
+  return __a < __b ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vcge_s16 (int16x4_t __a, int16x4_t __b)
-{
-  return (uint16x4_t) __builtin_aarch64_cmgev4hi (__a, __b);
-}
+/* vcltz - vector.  */
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcge_s32 (int32x2_t __a, int32x2_t __b)
+vcltz_f32 (float32x2_t __a)
 {
-  return (uint32x2_t) __builtin_aarch64_cmgev2si (__a, __b);
+  float32x2_t __b = {0.0f, 0.0f};
+  return (uint32x2_t) __builtin_aarch64_cmltv2sf (__a, __b);
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcge_s64 (int64x1_t __a, int64x1_t __b)
+vcltz_f64 (float64x1_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgedi (__a, __b);
+  return __a < 0.0 ? -1ll : 0ll;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vcge_u8 (uint8x8_t __a, uint8x8_t __b)
+vcltz_p8 (poly8x8_t __a)
 {
-  return (uint8x8_t) __builtin_aarch64_cmgeuv8qi ((int8x8_t) __a,
+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmltv8qi ((int8x8_t) __a,
 						 (int8x8_t) __b);
 }
 
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vcltz_s8 (int8x8_t __a)
+{
+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x8_t) __builtin_aarch64_cmltv8qi (__a, __b);
+}
+
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vcge_u16 (uint16x4_t __a, uint16x4_t __b)
+vcltz_s16 (int16x4_t __a)
 {
-  return (uint16x4_t) __builtin_aarch64_cmgeuv4hi ((int16x4_t) __a,
-						  (int16x4_t) __b);
+  int16x4_t __b = {0, 0, 0, 0};
+  return (uint16x4_t) __builtin_aarch64_cmltv4hi (__a, __b);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcltz_s32 (int32x2_t __a)
+{
+  int32x2_t __b = {0, 0};
+  return (uint32x2_t) __builtin_aarch64_cmltv2si (__a, __b);
+}
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcltz_s64 (int64x1_t __a)
+{
+  return __a < 0ll ? -1ll : 0ll;
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcltzq_f32 (float32x4_t __a)
+{
+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};
+  return (uint32x4_t) __builtin_aarch64_cmltv4sf (__a, __b);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcge_u32 (uint32x2_t __a, uint32x2_t __b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcltzq_f64 (float64x2_t __a)
 {
-  return (uint32x2_t) __builtin_aarch64_cmgeuv2si ((int32x2_t) __a,
-						  (int32x2_t) __b);
+  float64x2_t __b = {0.0, 0.0};
+  return (uint64x2_t) __builtin_aarch64_cmltv2df (__a, __b);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcge_u64 (uint64x1_t __a, uint64x1_t __b)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vcltzq_p8 (poly8x16_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgeudi ((int64x1_t) __a,
-						(int64x1_t) __b);
+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		    0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmltv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vcgeq_s8 (int8x16_t __a, int8x16_t __b)
+vcltzq_s8 (int8x16_t __a)
 {
-  return (uint8x16_t) __builtin_aarch64_cmgev16qi (__a, __b);
+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
+		   0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint8x16_t) __builtin_aarch64_cmltv16qi (__a, __b);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vcgeq_s16 (int16x8_t __a, int16x8_t __b)
+vcltzq_s16 (int16x8_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_cmgev8hi (__a, __b);
+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};
+  return (uint16x8_t) __builtin_aarch64_cmltv8hi (__a, __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcgeq_s32 (int32x4_t __a, int32x4_t __b)
+vcltzq_s32 (int32x4_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_cmgev4si (__a, __b);
+  int32x4_t __b = {0, 0, 0, 0};
+  return (uint32x4_t) __builtin_aarch64_cmltv4si (__a, __b);
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcgeq_s64 (int64x2_t __a, int64x2_t __b)
+vcltzq_s64 (int64x2_t __a)
 {
-  return (uint64x2_t) __builtin_aarch64_cmgev2di (__a, __b);
+  int64x2_t __b = {0, 0};
+  return (uint64x2_t) __builtin_aarch64_cmltv2di (__a, __b);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vcgeq_u8 (uint8x16_t __a, uint8x16_t __b)
+/* vcltz - scalar.  */
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcltzs_f32 (float32_t __a)
 {
-  return (uint8x16_t) __builtin_aarch64_cmgeuv16qi ((int8x16_t) __a,
-						   (int8x16_t) __b);
+  return __a < 0.0f ? -1 : 0;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vcgeq_u16 (uint16x8_t __a, uint16x8_t __b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcltzd_s64 (int64x1_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_cmgeuv8hi ((int16x8_t) __a,
-						  (int16x8_t) __b);
+  return __a < 0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcgeq_u32 (uint32x4_t __a, uint32x4_t __b)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vcltzd_u64 (int64x1_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_cmgeuv4si ((int32x4_t) __a,
-						  (int32x4_t) __b);
+  return __a < 0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcgeq_u64 (uint64x2_t __a, uint64x2_t __b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcltzd_f64 (float64_t __a)
 {
-  return (uint64x2_t) __builtin_aarch64_cmgeuv2di ((int64x2_t) __a,
-						  (int64x2_t) __b);
+  return __a < 0.0 ? -1ll : 0ll;
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcged_s64 (int64x1_t __a, int64x1_t __b)
+/* vcvt (double -> float).  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vcvt_f32_f64 (float64x2_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgedi (__a, __b);
+  return __builtin_aarch64_float_truncate_lo_v2sf (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcged_u64 (uint64x1_t __a, uint64x1_t __b)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vcvt_high_f32_f64 (float32x2_t __a, float64x2_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgeudi ((int64x1_t) __a,
-						(int64x1_t) __b);
+  return __builtin_aarch64_float_truncate_hi_v4sf (__a, __b);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcgezd_s64 (int64x1_t __a)
+/* vcvt (float -> double).  */
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vcvt_f64_f32 (float32x2_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgedi (__a, 0);
+
+  return __builtin_aarch64_float_extend_lo_v2df (__a);
 }
 
-/* vcgt */
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vcvt_high_f64_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_vec_unpacks_hi_v4sf (__a);
+}
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vcgt_s8 (int8x8_t __a, int8x8_t __b)
+/* vcvt  (<u>int -> float)  */
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vcvtd_f64_s64 (int64_t __a)
 {
-  return (uint8x8_t) __builtin_aarch64_cmgtv8qi (__a, __b);
+  return (float64_t) __a;
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vcgt_s16 (int16x4_t __a, int16x4_t __b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vcvtd_f64_u64 (uint64_t __a)
 {
-  return (uint16x4_t) __builtin_aarch64_cmgtv4hi (__a, __b);
+  return (float64_t) __a;
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcgt_s32 (int32x2_t __a, int32x2_t __b)
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vcvts_f32_s32 (int32_t __a)
 {
-  return (uint32x2_t) __builtin_aarch64_cmgtv2si (__a, __b);
+  return (float32_t) __a;
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcgt_s64 (int64x1_t __a, int64x1_t __b)
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vcvts_f32_u32 (uint32_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__a, __b);
+  return (float32_t) __a;
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vcgt_u8 (uint8x8_t __a, uint8x8_t __b)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vcvt_f32_s32 (int32x2_t __a)
 {
-  return (uint8x8_t) __builtin_aarch64_cmgtuv8qi ((int8x8_t) __a,
-						 (int8x8_t) __b);
+  return __builtin_aarch64_floatv2siv2sf (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vcgt_u16 (uint16x4_t __a, uint16x4_t __b)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vcvt_f32_u32 (uint32x2_t __a)
 {
-  return (uint16x4_t) __builtin_aarch64_cmgtuv4hi ((int16x4_t) __a,
-						  (int16x4_t) __b);
+  return __builtin_aarch64_floatunsv2siv2sf ((int32x2_t) __a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcgt_u32 (uint32x2_t __a, uint32x2_t __b)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vcvtq_f32_s32 (int32x4_t __a)
 {
-  return (uint32x2_t) __builtin_aarch64_cmgtuv2si ((int32x2_t) __a,
-						  (int32x2_t) __b);
+  return __builtin_aarch64_floatv4siv4sf (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcgt_u64 (uint64x1_t __a, uint64x1_t __b)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vcvtq_f32_u32 (uint32x4_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgtudi ((int64x1_t) __a,
-						(int64x1_t) __b);
+  return __builtin_aarch64_floatunsv4siv4sf ((int32x4_t) __a);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vcgtq_s8 (int8x16_t __a, int8x16_t __b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vcvtq_f64_s64 (int64x2_t __a)
 {
-  return (uint8x16_t) __builtin_aarch64_cmgtv16qi (__a, __b);
+  return __builtin_aarch64_floatv2div2df (__a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vcgtq_s16 (int16x8_t __a, int16x8_t __b)
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vcvtq_f64_u64 (uint64x2_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_cmgtv8hi (__a, __b);
+  return __builtin_aarch64_floatunsv2div2df ((int64x2_t) __a);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcgtq_s32 (int32x4_t __a, int32x4_t __b)
+/* vcvt (float -> <u>int)  */
+
+__extension__ static __inline int64_t __attribute__ ((__always_inline__))
+vcvtd_s64_f64 (float64_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_cmgtv4si (__a, __b);
+  return (int64_t) __a;
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcgtq_s64 (int64x2_t __a, int64x2_t __b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcvtd_u64_f64 (float64_t __a)
 {
-  return (uint64x2_t) __builtin_aarch64_cmgtv2di (__a, __b);
+  return (uint64_t) __a;
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vcgtq_u8 (uint8x16_t __a, uint8x16_t __b)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vcvts_s32_f32 (float32_t __a)
 {
-  return (uint8x16_t) __builtin_aarch64_cmgtuv16qi ((int8x16_t) __a,
-						   (int8x16_t) __b);
+  return (int32_t) __a;
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vcgtq_u16 (uint16x8_t __a, uint16x8_t __b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcvts_u32_f32 (float32_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_cmgtuv8hi ((int16x8_t) __a,
-						  (int16x8_t) __b);
+  return (uint32_t) __a;
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vcvt_s32_f32 (float32x2_t __a)
+{
+  return __builtin_aarch64_lbtruncv2sfv2si (__a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcvt_u32_f32 (float32x2_t __a)
+{
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x2_t) __builtin_aarch64_lbtruncuv2sfv2si (__a);
+}
+
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vcvtq_s32_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_lbtruncv4sfv4si (__a);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcgtq_u32 (uint32x4_t __a, uint32x4_t __b)
+vcvtq_u32_f32 (float32x4_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_cmgtuv4si ((int32x4_t) __a,
-						  (int32x4_t) __b);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x4_t) __builtin_aarch64_lbtruncuv4sfv4si (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcgtq_u64 (uint64x2_t __a, uint64x2_t __b)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vcvtq_s64_f64 (float64x2_t __a)
 {
-  return (uint64x2_t) __builtin_aarch64_cmgtuv2di ((int64x2_t) __a,
-						  (int64x2_t) __b);
+  return __builtin_aarch64_lbtruncv2dfv2di (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcgtd_s64 (int64x1_t __a, int64x1_t __b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcvtq_u64_f64 (float64x2_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__a, __b);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint64x2_t) __builtin_aarch64_lbtruncuv2dfv2di (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcgtd_u64 (uint64x1_t __a, uint64x1_t __b)
+/* vcvta  */
+
+__extension__ static __inline int64_t __attribute__ ((__always_inline__))
+vcvtad_s64_f64 (float64_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgtudi ((int64x1_t) __a,
-						(int64x1_t) __b);
+  return __builtin_aarch64_lrounddfdi (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcgtzd_s64 (int64x1_t __a)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcvtad_u64_f64 (float64_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__a, 0);
+  return __builtin_aarch64_lroundudfdi (__a);
 }
 
-/* vcle */
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vcvtas_s32_f32 (float32_t __a)
+{
+  return __builtin_aarch64_lroundsfsi (__a);
+}
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vcle_s8 (int8x8_t __a, int8x8_t __b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcvtas_u32_f32 (float32_t __a)
 {
-  return (uint8x8_t) __builtin_aarch64_cmgev8qi (__b, __a);
+  return __builtin_aarch64_lroundusfsi (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vcle_s16 (int16x4_t __a, int16x4_t __b)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vcvta_s32_f32 (float32x2_t __a)
 {
-  return (uint16x4_t) __builtin_aarch64_cmgev4hi (__b, __a);
+  return __builtin_aarch64_lroundv2sfv2si (__a);
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcle_s32 (int32x2_t __a, int32x2_t __b)
+vcvta_u32_f32 (float32x2_t __a)
 {
-  return (uint32x2_t) __builtin_aarch64_cmgev2si (__b, __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x2_t) __builtin_aarch64_lrounduv2sfv2si (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcle_s64 (int64x1_t __a, int64x1_t __b)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vcvtaq_s32_f32 (float32x4_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgedi (__b, __a);
+  return __builtin_aarch64_lroundv4sfv4si (__a);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vcle_u8 (uint8x8_t __a, uint8x8_t __b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcvtaq_u32_f32 (float32x4_t __a)
 {
-  return (uint8x8_t) __builtin_aarch64_cmgeuv8qi ((int8x8_t) __b,
-						 (int8x8_t) __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x4_t) __builtin_aarch64_lrounduv4sfv4si (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vcle_u16 (uint16x4_t __a, uint16x4_t __b)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vcvtaq_s64_f64 (float64x2_t __a)
 {
-  return (uint16x4_t) __builtin_aarch64_cmgeuv4hi ((int16x4_t) __b,
-						  (int16x4_t) __a);
+  return __builtin_aarch64_lroundv2dfv2di (__a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vcle_u32 (uint32x2_t __a, uint32x2_t __b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcvtaq_u64_f64 (float64x2_t __a)
 {
-  return (uint32x2_t) __builtin_aarch64_cmgeuv2si ((int32x2_t) __b,
-						  (int32x2_t) __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint64x2_t) __builtin_aarch64_lrounduv2dfv2di (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcle_u64 (uint64x1_t __a, uint64x1_t __b)
+/* vcvtm  */
+
+__extension__ static __inline int64_t __attribute__ ((__always_inline__))
+vcvtmd_s64_f64 (float64_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgeudi ((int64x1_t) __b,
-						(int64x1_t) __a);
+  return __builtin_lfloor (__a);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vcleq_s8 (int8x16_t __a, int8x16_t __b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcvtmd_u64_f64 (float64_t __a)
 {
-  return (uint8x16_t) __builtin_aarch64_cmgev16qi (__b, __a);
+  return __builtin_aarch64_lfloorudfdi (__a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vcleq_s16 (int16x8_t __a, int16x8_t __b)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vcvtms_s32_f32 (float32_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_cmgev8hi (__b, __a);
+  return __builtin_ifloorf (__a);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcleq_s32 (int32x4_t __a, int32x4_t __b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcvtms_u32_f32 (float32_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_cmgev4si (__b, __a);
+  return __builtin_aarch64_lfloorusfsi (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcleq_s64 (int64x2_t __a, int64x2_t __b)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vcvtm_s32_f32 (float32x2_t __a)
 {
-  return (uint64x2_t) __builtin_aarch64_cmgev2di (__b, __a);
+  return __builtin_aarch64_lfloorv2sfv2si (__a);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vcleq_u8 (uint8x16_t __a, uint8x16_t __b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcvtm_u32_f32 (float32x2_t __a)
 {
-  return (uint8x16_t) __builtin_aarch64_cmgeuv16qi ((int8x16_t) __b,
-						   (int8x16_t) __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x2_t) __builtin_aarch64_lflooruv2sfv2si (__a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vcleq_u16 (uint16x8_t __a, uint16x8_t __b)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vcvtmq_s32_f32 (float32x4_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_cmgeuv8hi ((int16x8_t) __b,
-						  (int16x8_t) __a);
+  return __builtin_aarch64_lfloorv4sfv4si (__a);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcleq_u32 (uint32x4_t __a, uint32x4_t __b)
+vcvtmq_u32_f32 (float32x4_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_cmgeuv4si ((int32x4_t) __b,
-						  (int32x4_t) __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x4_t) __builtin_aarch64_lflooruv4sfv4si (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcleq_u64 (uint64x2_t __a, uint64x2_t __b)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vcvtmq_s64_f64 (float64x2_t __a)
 {
-  return (uint64x2_t) __builtin_aarch64_cmgeuv2di ((int64x2_t) __b,
-						  (int64x2_t) __a);
+  return __builtin_aarch64_lfloorv2dfv2di (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcled_s64 (int64x1_t __a, int64x1_t __b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcvtmq_u64_f64 (float64x2_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgedi (__b, __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint64x2_t) __builtin_aarch64_lflooruv2dfv2di (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vclezd_s64 (int64x1_t __a)
+/* vcvtn  */
+
+__extension__ static __inline int64_t __attribute__ ((__always_inline__))
+vcvtnd_s64_f64 (float64_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmledi (__a, 0);
+  return __builtin_aarch64_lfrintndfdi (__a);
 }
 
-/* vclt */
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcvtnd_u64_f64 (float64_t __a)
+{
+  return __builtin_aarch64_lfrintnudfdi (__a);
+}
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vclt_s8 (int8x8_t __a, int8x8_t __b)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vcvtns_s32_f32 (float32_t __a)
 {
-  return (uint8x8_t) __builtin_aarch64_cmgtv8qi (__b, __a);
+  return __builtin_aarch64_lfrintnsfsi (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vclt_s16 (int16x4_t __a, int16x4_t __b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcvtns_u32_f32 (float32_t __a)
 {
-  return (uint16x4_t) __builtin_aarch64_cmgtv4hi (__b, __a);
+  return __builtin_aarch64_lfrintnusfsi (__a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vclt_s32 (int32x2_t __a, int32x2_t __b)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vcvtn_s32_f32 (float32x2_t __a)
 {
-  return (uint32x2_t) __builtin_aarch64_cmgtv2si (__b, __a);
+  return __builtin_aarch64_lfrintnv2sfv2si (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vclt_s64 (int64x1_t __a, int64x1_t __b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcvtn_u32_f32 (float32x2_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__b, __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x2_t) __builtin_aarch64_lfrintnuv2sfv2si (__a);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vclt_u8 (uint8x8_t __a, uint8x8_t __b)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vcvtnq_s32_f32 (float32x4_t __a)
 {
-  return (uint8x8_t) __builtin_aarch64_cmgtuv8qi ((int8x8_t) __b,
-						 (int8x8_t) __a);
+  return __builtin_aarch64_lfrintnv4sfv4si (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vclt_u16 (uint16x4_t __a, uint16x4_t __b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcvtnq_u32_f32 (float32x4_t __a)
 {
-  return (uint16x4_t) __builtin_aarch64_cmgtuv4hi ((int16x4_t) __b,
-						  (int16x4_t) __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x4_t) __builtin_aarch64_lfrintnuv4sfv4si (__a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vclt_u32 (uint32x2_t __a, uint32x2_t __b)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vcvtnq_s64_f64 (float64x2_t __a)
 {
-  return (uint32x2_t) __builtin_aarch64_cmgtuv2si ((int32x2_t) __b,
-						  (int32x2_t) __a);
+  return __builtin_aarch64_lfrintnv2dfv2di (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vclt_u64 (uint64x1_t __a, uint64x1_t __b)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcvtnq_u64_f64 (float64x2_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgtudi ((int64x1_t) __b,
-						(int64x1_t) __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint64x2_t) __builtin_aarch64_lfrintnuv2dfv2di (__a);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vcltq_s8 (int8x16_t __a, int8x16_t __b)
+/* vcvtp  */
+
+__extension__ static __inline int64_t __attribute__ ((__always_inline__))
+vcvtpd_s64_f64 (float64_t __a)
 {
-  return (uint8x16_t) __builtin_aarch64_cmgtv16qi (__b, __a);
+  return __builtin_lceil (__a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vcltq_s16 (int16x8_t __a, int16x8_t __b)
+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))
+vcvtpd_u64_f64 (float64_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_cmgtv8hi (__b, __a);
+  return __builtin_aarch64_lceiludfdi (__a);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcltq_s32 (int32x4_t __a, int32x4_t __b)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vcvtps_s32_f32 (float32_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_cmgtv4si (__b, __a);
+  return __builtin_iceilf (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcltq_s64 (int64x2_t __a, int64x2_t __b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vcvtps_u32_f32 (float32_t __a)
 {
-  return (uint64x2_t) __builtin_aarch64_cmgtv2di (__b, __a);
+  return __builtin_aarch64_lceilusfsi (__a);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vcltq_u8 (uint8x16_t __a, uint8x16_t __b)
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vcvtp_s32_f32 (float32x2_t __a)
 {
-  return (uint8x16_t) __builtin_aarch64_cmgtuv16qi ((int8x16_t) __b,
-						   (int8x16_t) __a);
+  return __builtin_aarch64_lceilv2sfv2si (__a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vcltq_u16 (uint16x8_t __a, uint16x8_t __b)
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vcvtp_u32_f32 (float32x2_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_cmgtuv8hi ((int16x8_t) __b,
-						  (int16x8_t) __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x2_t) __builtin_aarch64_lceiluv2sfv2si (__a);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vcltq_u32 (uint32x4_t __a, uint32x4_t __b)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vcvtpq_s32_f32 (float32x4_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_cmgtuv4si ((int32x4_t) __b,
-						  (int32x4_t) __a);
+  return __builtin_aarch64_lceilv4sfv4si (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vcltq_u64 (uint64x2_t __a, uint64x2_t __b)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vcvtpq_u32_f32 (float32x4_t __a)
 {
-  return (uint64x2_t) __builtin_aarch64_cmgtuv2di ((int64x2_t) __b,
-						  (int64x2_t) __a);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint32x4_t) __builtin_aarch64_lceiluv4sfv4si (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcltd_s64 (int64x1_t __a, int64x1_t __b)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vcvtpq_s64_f64 (float64x2_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__b, __a);
+  return __builtin_aarch64_lceilv2dfv2di (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vcltzd_s64 (int64x1_t __a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vcvtpq_u64_f64 (float64x2_t __a)
 {
-  return (uint64x1_t) __builtin_aarch64_cmltdi (__a, 0);
+  /* TODO: This cast should go away when builtins have
+     their correct types.  */
+  return (uint64x2_t) __builtin_aarch64_lceiluv2dfv2di (__a);
 }
 
 /* vdup */
@@ -20013,49 +19795,49 @@ vcltzd_s64 (int64x1_t __a)
 __extension__ static __inline int8x1_t __attribute__ ((__always_inline__))
 vdupb_lane_s8 (int8x16_t a, int const b)
 {
-  return __builtin_aarch64_dup_laneqi (a, b);
+  return __aarch64_vgetq_lane_s8 (a, b);
 }
 
 __extension__ static __inline uint8x1_t __attribute__ ((__always_inline__))
 vdupb_lane_u8 (uint8x16_t a, int const b)
 {
-  return (uint8x1_t) __builtin_aarch64_dup_laneqi ((int8x16_t) a, b);
+  return __aarch64_vgetq_lane_u8 (a, b);
 }
 
 __extension__ static __inline int16x1_t __attribute__ ((__always_inline__))
 vduph_lane_s16 (int16x8_t a, int const b)
 {
-  return __builtin_aarch64_dup_lanehi (a, b);
+  return __aarch64_vgetq_lane_s16 (a, b);
 }
 
 __extension__ static __inline uint16x1_t __attribute__ ((__always_inline__))
 vduph_lane_u16 (uint16x8_t a, int const b)
 {
-  return (uint16x1_t) __builtin_aarch64_dup_lanehi ((int16x8_t) a, b);
+  return __aarch64_vgetq_lane_u16 (a, b);
 }
 
 __extension__ static __inline int32x1_t __attribute__ ((__always_inline__))
 vdups_lane_s32 (int32x4_t a, int const b)
 {
-  return __builtin_aarch64_dup_lanesi (a, b);
+  return __aarch64_vgetq_lane_s32 (a, b);
 }
 
 __extension__ static __inline uint32x1_t __attribute__ ((__always_inline__))
 vdups_lane_u32 (uint32x4_t a, int const b)
 {
-  return (uint32x1_t) __builtin_aarch64_dup_lanesi ((int32x4_t) a, b);
+  return __aarch64_vgetq_lane_u32 (a, b);
 }
 
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vdupd_lane_s64 (int64x2_t a, int const b)
 {
-  return __builtin_aarch64_dup_lanedi (a, b);
+  return __aarch64_vgetq_lane_s64 (a, b);
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vdupd_lane_u64 (uint64x2_t a, int const b)
 {
-  return (uint64x1_t) __builtin_aarch64_dup_lanedi ((int64x2_t) a, b);
+  return __aarch64_vgetq_lane_u64 (a, b);
 }
 
 /* vld1 */
@@ -21088,7 +20870,7 @@ vld4q_f64 (const float64_t * __a)
 __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
 vmax_f32 (float32x2_t __a, float32x2_t __b)
 {
-  return __builtin_aarch64_fmaxv2sf (__a, __b);
+  return __builtin_aarch64_smax_nanv2sf (__a, __b);
 }
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
@@ -21133,13 +20915,13 @@ vmax_u32 (uint32x2_t __a, uint32x2_t __b
 __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
 vmaxq_f32 (float32x4_t __a, float32x4_t __b)
 {
-  return __builtin_aarch64_fmaxv4sf (__a, __b);
+  return __builtin_aarch64_smax_nanv4sf (__a, __b);
 }
 
 __extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
 vmaxq_f64 (float64x2_t __a, float64x2_t __b)
 {
-  return __builtin_aarch64_fmaxv2df (__a, __b);
+  return __builtin_aarch64_smax_nanv2df (__a, __b);
 }
 
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
@@ -21167,116 +20949,392 @@ vmaxq_u8 (uint8x16_t __a, uint8x16_t __b
 						   (int8x16_t) __b);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vmaxq_u16 (uint16x8_t __a, uint16x8_t __b)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vmaxq_u16 (uint16x8_t __a, uint16x8_t __b)
+{
+  return (uint16x8_t) __builtin_aarch64_umaxv8hi ((int16x8_t) __a,
+						  (int16x8_t) __b);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vmaxq_u32 (uint32x4_t __a, uint32x4_t __b)
+{
+  return (uint32x4_t) __builtin_aarch64_umaxv4si ((int32x4_t) __a,
+						  (int32x4_t) __b);
+}
+
+/* vmaxnm  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vmaxnm_f32 (float32x2_t __a, float32x2_t __b)
+{
+  return __builtin_aarch64_smaxv2sf (__a, __b);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vmaxnmq_f32 (float32x4_t __a, float32x4_t __b)
+{
+  return __builtin_aarch64_smaxv4sf (__a, __b);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vmaxnmq_f64 (float64x2_t __a, float64x2_t __b)
+{
+  return __builtin_aarch64_smaxv2df (__a, __b);
+}
+
+/* vmaxv  */
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vmaxv_f32 (float32x2_t __a)
+{
+  return vget_lane_f32 (__builtin_aarch64_reduc_smax_nan_v2sf (__a), 0);
+}
+
+__extension__ static __inline int8_t __attribute__ ((__always_inline__))
+vmaxv_s8 (int8x8_t __a)
+{
+  return vget_lane_s8 (__builtin_aarch64_reduc_smax_v8qi (__a), 0);
+}
+
+__extension__ static __inline int16_t __attribute__ ((__always_inline__))
+vmaxv_s16 (int16x4_t __a)
+{
+  return vget_lane_s16 (__builtin_aarch64_reduc_smax_v4hi (__a), 0);
+}
+
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vmaxv_s32 (int32x2_t __a)
+{
+  return vget_lane_s32 (__builtin_aarch64_reduc_smax_v2si (__a), 0);
+}
+
+__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
+vmaxv_u8 (uint8x8_t __a)
+{
+  return vget_lane_u8 ((uint8x8_t)
+		__builtin_aarch64_reduc_umax_v8qi ((int8x8_t) __a), 0);
+}
+
+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
+vmaxv_u16 (uint16x4_t __a)
+{
+  return vget_lane_u16 ((uint16x4_t)
+		__builtin_aarch64_reduc_umax_v4hi ((int16x4_t) __a), 0);
+}
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vmaxv_u32 (uint32x2_t __a)
+{
+  return vget_lane_u32 ((uint32x2_t)
+		__builtin_aarch64_reduc_umax_v2si ((int32x2_t) __a), 0);
+}
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vmaxvq_f32 (float32x4_t __a)
+{
+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smax_nan_v4sf (__a), 0);
+}
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vmaxvq_f64 (float64x2_t __a)
+{
+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smax_nan_v2df (__a), 0);
+}
+
+__extension__ static __inline int8_t __attribute__ ((__always_inline__))
+vmaxvq_s8 (int8x16_t __a)
+{
+  return vgetq_lane_s8 (__builtin_aarch64_reduc_smax_v16qi (__a), 0);
+}
+
+__extension__ static __inline int16_t __attribute__ ((__always_inline__))
+vmaxvq_s16 (int16x8_t __a)
+{
+  return vgetq_lane_s16 (__builtin_aarch64_reduc_smax_v8hi (__a), 0);
+}
+
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vmaxvq_s32 (int32x4_t __a)
+{
+  return vgetq_lane_s32 (__builtin_aarch64_reduc_smax_v4si (__a), 0);
+}
+
+__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
+vmaxvq_u8 (uint8x16_t __a)
+{
+  return vgetq_lane_u8 ((uint8x16_t)
+		__builtin_aarch64_reduc_umax_v16qi ((int8x16_t) __a), 0);
+}
+
+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
+vmaxvq_u16 (uint16x8_t __a)
+{
+  return vgetq_lane_u16 ((uint16x8_t)
+		__builtin_aarch64_reduc_umax_v8hi ((int16x8_t) __a), 0);
+}
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vmaxvq_u32 (uint32x4_t __a)
+{
+  return vgetq_lane_u32 ((uint32x4_t)
+		__builtin_aarch64_reduc_umax_v4si ((int32x4_t) __a), 0);
+}
+
+/* vmaxnmv  */
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vmaxnmv_f32 (float32x2_t __a)
+{
+  return vget_lane_f32 (__builtin_aarch64_reduc_smax_v2sf (__a), 0);
+}
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vmaxnmvq_f32 (float32x4_t __a)
+{
+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smax_v4sf (__a), 0);
+}
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vmaxnmvq_f64 (float64x2_t __a)
+{
+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smax_v2df (__a), 0);
+}
+
+/* vmin  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vmin_f32 (float32x2_t __a, float32x2_t __b)
+{
+  return __builtin_aarch64_smin_nanv2sf (__a, __b);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vmin_s8 (int8x8_t __a, int8x8_t __b)
+{
+  return __builtin_aarch64_sminv8qi (__a, __b);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vmin_s16 (int16x4_t __a, int16x4_t __b)
+{
+  return __builtin_aarch64_sminv4hi (__a, __b);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vmin_s32 (int32x2_t __a, int32x2_t __b)
+{
+  return __builtin_aarch64_sminv2si (__a, __b);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vmin_u8 (uint8x8_t __a, uint8x8_t __b)
+{
+  return (uint8x8_t) __builtin_aarch64_uminv8qi ((int8x8_t) __a,
+						 (int8x8_t) __b);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vmin_u16 (uint16x4_t __a, uint16x4_t __b)
+{
+  return (uint16x4_t) __builtin_aarch64_uminv4hi ((int16x4_t) __a,
+						  (int16x4_t) __b);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vmin_u32 (uint32x2_t __a, uint32x2_t __b)
+{
+  return (uint32x2_t) __builtin_aarch64_uminv2si ((int32x2_t) __a,
+						  (int32x2_t) __b);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vminq_f32 (float32x4_t __a, float32x4_t __b)
+{
+  return __builtin_aarch64_smin_nanv4sf (__a, __b);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vminq_f64 (float64x2_t __a, float64x2_t __b)
+{
+  return __builtin_aarch64_smin_nanv2df (__a, __b);
+}
+
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vminq_s8 (int8x16_t __a, int8x16_t __b)
+{
+  return __builtin_aarch64_sminv16qi (__a, __b);
+}
+
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vminq_s16 (int16x8_t __a, int16x8_t __b)
+{
+  return __builtin_aarch64_sminv8hi (__a, __b);
+}
+
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vminq_s32 (int32x4_t __a, int32x4_t __b)
+{
+  return __builtin_aarch64_sminv4si (__a, __b);
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vminq_u8 (uint8x16_t __a, uint8x16_t __b)
+{
+  return (uint8x16_t) __builtin_aarch64_uminv16qi ((int8x16_t) __a,
+						   (int8x16_t) __b);
+}
+
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vminq_u16 (uint16x8_t __a, uint16x8_t __b)
+{
+  return (uint16x8_t) __builtin_aarch64_uminv8hi ((int16x8_t) __a,
+						  (int16x8_t) __b);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vminq_u32 (uint32x4_t __a, uint32x4_t __b)
+{
+  return (uint32x4_t) __builtin_aarch64_uminv4si ((int32x4_t) __a,
+						  (int32x4_t) __b);
+}
+
+/* vminnm  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vminnm_f32 (float32x2_t __a, float32x2_t __b)
+{
+  return __builtin_aarch64_sminv2sf (__a, __b);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vminnmq_f32 (float32x4_t __a, float32x4_t __b)
+{
+  return __builtin_aarch64_sminv4sf (__a, __b);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vminnmq_f64 (float64x2_t __a, float64x2_t __b)
+{
+  return __builtin_aarch64_sminv2df (__a, __b);
+}
+
+/* vminv  */
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vminv_f32 (float32x2_t __a)
+{
+  return vget_lane_f32 (__builtin_aarch64_reduc_smin_nan_v2sf (__a), 0);
+}
+
+__extension__ static __inline int8_t __attribute__ ((__always_inline__))
+vminv_s8 (int8x8_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_umaxv8hi ((int16x8_t) __a,
-						  (int16x8_t) __b);
+  return vget_lane_s8 (__builtin_aarch64_reduc_smin_v8qi (__a), 0);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vmaxq_u32 (uint32x4_t __a, uint32x4_t __b)
+__extension__ static __inline int16_t __attribute__ ((__always_inline__))
+vminv_s16 (int16x4_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_umaxv4si ((int32x4_t) __a,
-						  (int32x4_t) __b);
+  return vget_lane_s16 (__builtin_aarch64_reduc_smin_v4hi (__a), 0);
 }
 
-/* vmin */
-
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vmin_f32 (float32x2_t __a, float32x2_t __b)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vminv_s32 (int32x2_t __a)
 {
-  return __builtin_aarch64_fminv2sf (__a, __b);
+  return vget_lane_s32 (__builtin_aarch64_reduc_smin_v2si (__a), 0);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vmin_s8 (int8x8_t __a, int8x8_t __b)
+__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
+vminv_u8 (uint8x8_t __a)
 {
-  return __builtin_aarch64_sminv8qi (__a, __b);
+  return vget_lane_u8 ((uint8x8_t)
+		__builtin_aarch64_reduc_umin_v8qi ((int8x8_t) __a), 0);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vmin_s16 (int16x4_t __a, int16x4_t __b)
+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
+vminv_u16 (uint16x4_t __a)
 {
-  return __builtin_aarch64_sminv4hi (__a, __b);
+  return vget_lane_u16 ((uint16x4_t)
+		__builtin_aarch64_reduc_umin_v4hi ((int16x4_t) __a), 0);
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vmin_s32 (int32x2_t __a, int32x2_t __b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vminv_u32 (uint32x2_t __a)
 {
-  return __builtin_aarch64_sminv2si (__a, __b);
+  return vget_lane_u32 ((uint32x2_t)
+		__builtin_aarch64_reduc_umin_v2si ((int32x2_t) __a), 0);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vmin_u8 (uint8x8_t __a, uint8x8_t __b)
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vminvq_f32 (float32x4_t __a)
 {
-  return (uint8x8_t) __builtin_aarch64_uminv8qi ((int8x8_t) __a,
-						 (int8x8_t) __b);
+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smin_nan_v4sf (__a), 0);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vmin_u16 (uint16x4_t __a, uint16x4_t __b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vminvq_f64 (float64x2_t __a)
 {
-  return (uint16x4_t) __builtin_aarch64_uminv4hi ((int16x4_t) __a,
-						  (int16x4_t) __b);
+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smin_nan_v2df (__a), 0);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vmin_u32 (uint32x2_t __a, uint32x2_t __b)
+__extension__ static __inline int8_t __attribute__ ((__always_inline__))
+vminvq_s8 (int8x16_t __a)
 {
-  return (uint32x2_t) __builtin_aarch64_uminv2si ((int32x2_t) __a,
-						  (int32x2_t) __b);
+  return vgetq_lane_s8 (__builtin_aarch64_reduc_smin_v16qi (__a), 0);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vminq_f32 (float32x4_t __a, float32x4_t __b)
+__extension__ static __inline int16_t __attribute__ ((__always_inline__))
+vminvq_s16 (int16x8_t __a)
 {
-  return __builtin_aarch64_fminv4sf (__a, __b);
+  return vgetq_lane_s16 (__builtin_aarch64_reduc_smin_v8hi (__a), 0);
 }
 
-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
-vminq_f64 (float64x2_t __a, float64x2_t __b)
+__extension__ static __inline int32_t __attribute__ ((__always_inline__))
+vminvq_s32 (int32x4_t __a)
 {
-  return __builtin_aarch64_fminv2df (__a, __b);
+  return vgetq_lane_s32 (__builtin_aarch64_reduc_smin_v4si (__a), 0);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vminq_s8 (int8x16_t __a, int8x16_t __b)
+__extension__ static __inline uint8_t __attribute__ ((__always_inline__))
+vminvq_u8 (uint8x16_t __a)
 {
-  return __builtin_aarch64_sminv16qi (__a, __b);
+  return vgetq_lane_u8 ((uint8x16_t)
+		__builtin_aarch64_reduc_umin_v16qi ((int8x16_t) __a), 0);
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vminq_s16 (int16x8_t __a, int16x8_t __b)
+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))
+vminvq_u16 (uint16x8_t __a)
 {
-  return __builtin_aarch64_sminv8hi (__a, __b);
+  return vgetq_lane_u16 ((uint16x8_t)
+		__builtin_aarch64_reduc_umin_v8hi ((int16x8_t) __a), 0);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vminq_s32 (int32x4_t __a, int32x4_t __b)
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vminvq_u32 (uint32x4_t __a)
 {
-  return __builtin_aarch64_sminv4si (__a, __b);
+  return vgetq_lane_u32 ((uint32x4_t)
+		__builtin_aarch64_reduc_umin_v4si ((int32x4_t) __a), 0);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vminq_u8 (uint8x16_t __a, uint8x16_t __b)
+/* vminnmv  */
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vminnmv_f32 (float32x2_t __a)
 {
-  return (uint8x16_t) __builtin_aarch64_uminv16qi ((int8x16_t) __a,
-						   (int8x16_t) __b);
+  return vget_lane_f32 (__builtin_aarch64_reduc_smin_v2sf (__a), 0);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vminq_u16 (uint16x8_t __a, uint16x8_t __b)
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vminnmvq_f32 (float32x4_t __a)
 {
-  return (uint16x8_t) __builtin_aarch64_uminv8hi ((int16x8_t) __a,
-						  (int16x8_t) __b);
+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smin_v4sf (__a), 0);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vminq_u32 (uint32x4_t __a, uint32x4_t __b)
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vminnmvq_f64 (float64x2_t __a)
 {
-  return (uint32x4_t) __builtin_aarch64_uminv4si ((int32x4_t) __a,
-						  (int32x4_t) __b);
+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smin_v2df (__a), 0);
 }
 
 /* vmla */
@@ -21430,7 +21488,7 @@ vqdmlal_high_n_s16 (int32x4_t __a, int16
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
 vqdmlal_lane_s16 (int32x4_t __a, int16x4_t __b, int16x4_t __c, int const __d)
 {
-  int16x8_t __tmp = vcombine_s16 (__c, vcreate_s16 (INT64_C (0)));
+  int16x8_t __tmp = vcombine_s16 (__c, vcreate_s16 (__AARCH64_INT64_C (0)));
   return __builtin_aarch64_sqdmlal_lanev4hi (__a, __b, __tmp, __d);
 }
 
@@ -21481,7 +21539,7 @@ vqdmlal_high_n_s32 (int64x2_t __a, int32
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
 vqdmlal_lane_s32 (int64x2_t __a, int32x2_t __b, int32x2_t __c, int const __d)
 {
-  int32x4_t __tmp = vcombine_s32 (__c, vcreate_s32 (INT64_C (0)));
+  int32x4_t __tmp = vcombine_s32 (__c, vcreate_s32 (__AARCH64_INT64_C (0)));
   return __builtin_aarch64_sqdmlal_lanev2si (__a, __b, __tmp, __d);
 }
 
@@ -21558,7 +21616,7 @@ vqdmlsl_high_n_s16 (int32x4_t __a, int16
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
 vqdmlsl_lane_s16 (int32x4_t __a, int16x4_t __b, int16x4_t __c, int const __d)
 {
-  int16x8_t __tmp = vcombine_s16 (__c, vcreate_s16 (INT64_C (0)));
+  int16x8_t __tmp = vcombine_s16 (__c, vcreate_s16 (__AARCH64_INT64_C (0)));
   return __builtin_aarch64_sqdmlsl_lanev4hi (__a, __b, __tmp, __d);
 }
 
@@ -21609,7 +21667,7 @@ vqdmlsl_high_n_s32 (int64x2_t __a, int32
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
 vqdmlsl_lane_s32 (int64x2_t __a, int32x2_t __b, int32x2_t __c, int const __d)
 {
-  int32x4_t __tmp = vcombine_s32 (__c, vcreate_s32 (INT64_C (0)));
+  int32x4_t __tmp = vcombine_s32 (__c, vcreate_s32 (__AARCH64_INT64_C (0)));
   return __builtin_aarch64_sqdmlsl_lanev2si (__a, __b, __tmp, __d);
 }
 
@@ -21734,7 +21792,7 @@ vqdmull_high_n_s16 (int16x8_t __a, int16
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
 vqdmull_lane_s16 (int16x4_t __a, int16x4_t __b, int const __c)
 {
-  int16x8_t __tmp = vcombine_s16 (__b, vcreate_s16 (INT64_C (0)));
+  int16x8_t __tmp = vcombine_s16 (__b, vcreate_s16 (__AARCH64_INT64_C (0)));
   return __builtin_aarch64_sqdmull_lanev4hi (__a, __tmp, __c);
 }
 
@@ -21783,7 +21841,7 @@ vqdmull_high_n_s32 (int32x4_t __a, int32
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
 vqdmull_lane_s32 (int32x2_t __a, int32x2_t __b, int const __c)
 {
-  int32x4_t __tmp = vcombine_s32 (__b, vcreate_s32 (INT64_C (0)));
+  int32x4_t __tmp = vcombine_s32 (__b, vcreate_s32 (__AARCH64_INT64_C (0)));
   return __builtin_aarch64_sqdmull_lanev2si (__a, __tmp, __c);
 }
 
@@ -22795,6 +22853,223 @@ vqsubd_u64 (uint64x1_t __a, uint64x1_t _
   return (uint64x1_t) __builtin_aarch64_uqsubdi (__a, __b);
 }
 
+/* vrecpe  */
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vrecpes_f32 (float32_t __a)
+{
+  return __builtin_aarch64_frecpesf (__a);
+}
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vrecped_f64 (float64_t __a)
+{
+  return __builtin_aarch64_frecpedf (__a);
+}
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrecpe_f32 (float32x2_t __a)
+{
+  return __builtin_aarch64_frecpev2sf (__a);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrecpeq_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_frecpev4sf (__a);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrecpeq_f64 (float64x2_t __a)
+{
+  return __builtin_aarch64_frecpev2df (__a);
+}
+
+/* vrecps  */
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vrecpss_f32 (float32_t __a, float32_t __b)
+{
+  return __builtin_aarch64_frecpssf (__a, __b);
+}
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vrecpsd_f64 (float64_t __a, float64_t __b)
+{
+  return __builtin_aarch64_frecpsdf (__a, __b);
+}
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrecps_f32 (float32x2_t __a, float32x2_t __b)
+{
+  return __builtin_aarch64_frecpsv2sf (__a, __b);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrecpsq_f32 (float32x4_t __a, float32x4_t __b)
+{
+  return __builtin_aarch64_frecpsv4sf (__a, __b);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrecpsq_f64 (float64x2_t __a, float64x2_t __b)
+{
+  return __builtin_aarch64_frecpsv2df (__a, __b);
+}
+
+/* vrecpx  */
+
+__extension__ static __inline float32_t __attribute__ ((__always_inline__))
+vrecpxs_f32 (float32_t __a)
+{
+  return __builtin_aarch64_frecpxsf (__a);
+}
+
+__extension__ static __inline float64_t __attribute__ ((__always_inline__))
+vrecpxd_f64 (float64_t __a)
+{
+  return __builtin_aarch64_frecpxdf (__a);
+}
+
+/* vrnd  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrnd_f32 (float32x2_t __a)
+{
+  return __builtin_aarch64_btruncv2sf (__a);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrndq_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_btruncv4sf (__a);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrndq_f64 (float64x2_t __a)
+{
+  return __builtin_aarch64_btruncv2df (__a);
+}
+
+/* vrnda  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrnda_f32 (float32x2_t __a)
+{
+  return __builtin_aarch64_roundv2sf (__a);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrndaq_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_roundv4sf (__a);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrndaq_f64 (float64x2_t __a)
+{
+  return __builtin_aarch64_roundv2df (__a);
+}
+
+/* vrndi  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrndi_f32 (float32x2_t __a)
+{
+  return __builtin_aarch64_nearbyintv2sf (__a);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrndiq_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_nearbyintv4sf (__a);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrndiq_f64 (float64x2_t __a)
+{
+  return __builtin_aarch64_nearbyintv2df (__a);
+}
+
+/* vrndm  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrndm_f32 (float32x2_t __a)
+{
+  return __builtin_aarch64_floorv2sf (__a);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrndmq_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_floorv4sf (__a);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrndmq_f64 (float64x2_t __a)
+{
+  return __builtin_aarch64_floorv2df (__a);
+}
+
+/* vrndn  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrndn_f32 (float32x2_t __a)
+{
+  return __builtin_aarch64_frintnv2sf (__a);
+}
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrndnq_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_frintnv4sf (__a);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrndnq_f64 (float64x2_t __a)
+{
+  return __builtin_aarch64_frintnv2df (__a);
+}
+
+/* vrndp  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrndp_f32 (float32x2_t __a)
+{
+  return __builtin_aarch64_ceilv2sf (__a);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrndpq_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_ceilv4sf (__a);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrndpq_f64 (float64x2_t __a)
+{
+  return __builtin_aarch64_ceilv2df (__a);
+}
+
+/* vrndx  */
+
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vrndx_f32 (float32x2_t __a)
+{
+  return __builtin_aarch64_rintv2sf (__a);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vrndxq_f32 (float32x4_t __a)
+{
+  return __builtin_aarch64_rintv4sf (__a);
+}
+
+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))
+vrndxq_f64 (float64x2_t __a)
+{
+  return __builtin_aarch64_rintv2df (__a);
+}
+
 /* vrshl */
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
@@ -23133,114 +23408,191 @@ vrsrad_n_u64 (uint64x1_t __a, uint64x1_t
   return (uint64x1_t) __builtin_aarch64_ursra_ndi (__a, __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+
+/* vsha1  */
+
+static __inline uint32x4_t
+vsha1cq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
+{
+  return __builtin_aarch64_crypto_sha1cv4si_uuuu (hash_abcd, hash_e, wk);
+}
+static __inline uint32x4_t
+vsha1mq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
+{
+  return __builtin_aarch64_crypto_sha1mv4si_uuuu (hash_abcd, hash_e, wk);
+}
+static __inline uint32x4_t
+vsha1pq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
+{
+  return __builtin_aarch64_crypto_sha1pv4si_uuuu (hash_abcd, hash_e, wk);
+}
+
+static __inline uint32_t
+vsha1h_u32 (uint32_t hash_e)
+{
+  return __builtin_aarch64_crypto_sha1hsi_uu (hash_e);
+}
+
+static __inline uint32x4_t
+vsha1su0q_u32 (uint32x4_t w0_3, uint32x4_t w4_7, uint32x4_t w8_11)
+{
+  return __builtin_aarch64_crypto_sha1su0v4si_uuuu (w0_3, w4_7, w8_11);
+}
+
+static __inline uint32x4_t
+vsha1su1q_u32 (uint32x4_t tw0_3, uint32x4_t w12_15)
+{
+  return __builtin_aarch64_crypto_sha1su1v4si_uuu (tw0_3, w12_15);
+}
+
+static __inline uint32x4_t
+vsha256hq_u32 (uint32x4_t hash_abcd, uint32x4_t hash_efgh, uint32x4_t wk)
+{
+  return __builtin_aarch64_crypto_sha256hv4si_uuuu (hash_abcd, hash_efgh, wk);
+}
+
+static __inline uint32x4_t
+vsha256h2q_u32 (uint32x4_t hash_efgh, uint32x4_t hash_abcd, uint32x4_t wk)
+{
+  return __builtin_aarch64_crypto_sha256h2v4si_uuuu (hash_efgh, hash_abcd, wk);
+}
+
+static __inline uint32x4_t
+vsha256su0q_u32 (uint32x4_t w0_3, uint32x4_t w4_7)
+{
+  return __builtin_aarch64_crypto_sha256su0v4si_uuu (w0_3, w4_7);
+}
+
+static __inline uint32x4_t
+vsha256su1q_u32 (uint32x4_t tw0_3, uint32x4_t w8_11, uint32x4_t w12_15)
+{
+  return __builtin_aarch64_crypto_sha256su1v4si_uuuu (tw0_3, w8_11, w12_15);
+}
+
+static __inline poly128_t
+vmull_p64 (poly64_t a, poly64_t b)
+{
+  return
+    __builtin_aarch64_crypto_pmulldi_ppp (a, b);
+}
+
+static __inline poly128_t
+vmull_high_p64 (poly64x2_t a, poly64x2_t b)
+{
+  return __builtin_aarch64_crypto_pmullv2di_ppp (a, b);
+}
+
+#endif
+
 /* vshl */
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vshl_n_s8 (int8x8_t __a, const int __b)
 {
-  return (int8x8_t) __builtin_aarch64_sshl_nv8qi (__a, __b);
+  return (int8x8_t) __builtin_aarch64_ashlv8qi (__a, __b);
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
 vshl_n_s16 (int16x4_t __a, const int __b)
 {
-  return (int16x4_t) __builtin_aarch64_sshl_nv4hi (__a, __b);
+  return (int16x4_t) __builtin_aarch64_ashlv4hi (__a, __b);
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
 vshl_n_s32 (int32x2_t __a, const int __b)
 {
-  return (int32x2_t) __builtin_aarch64_sshl_nv2si (__a, __b);
+  return (int32x2_t) __builtin_aarch64_ashlv2si (__a, __b);
 }
 
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vshl_n_s64 (int64x1_t __a, const int __b)
 {
-  return (int64x1_t) __builtin_aarch64_sshl_ndi (__a, __b);
+  return (int64x1_t) __builtin_aarch64_ashldi (__a, __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
 vshl_n_u8 (uint8x8_t __a, const int __b)
 {
-  return (uint8x8_t) __builtin_aarch64_ushl_nv8qi ((int8x8_t) __a, __b);
+  return (uint8x8_t) __builtin_aarch64_ashlv8qi ((int8x8_t) __a, __b);
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
 vshl_n_u16 (uint16x4_t __a, const int __b)
 {
-  return (uint16x4_t) __builtin_aarch64_ushl_nv4hi ((int16x4_t) __a, __b);
+  return (uint16x4_t) __builtin_aarch64_ashlv4hi ((int16x4_t) __a, __b);
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
 vshl_n_u32 (uint32x2_t __a, const int __b)
 {
-  return (uint32x2_t) __builtin_aarch64_ushl_nv2si ((int32x2_t) __a, __b);
+  return (uint32x2_t) __builtin_aarch64_ashlv2si ((int32x2_t) __a, __b);
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vshl_n_u64 (uint64x1_t __a, const int __b)
 {
-  return (uint64x1_t) __builtin_aarch64_ushl_ndi ((int64x1_t) __a, __b);
+  return (uint64x1_t) __builtin_aarch64_ashldi ((int64x1_t) __a, __b);
 }
 
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vshlq_n_s8 (int8x16_t __a, const int __b)
 {
-  return (int8x16_t) __builtin_aarch64_sshl_nv16qi (__a, __b);
+  return (int8x16_t) __builtin_aarch64_ashlv16qi (__a, __b);
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
 vshlq_n_s16 (int16x8_t __a, const int __b)
 {
-  return (int16x8_t) __builtin_aarch64_sshl_nv8hi (__a, __b);
+  return (int16x8_t) __builtin_aarch64_ashlv8hi (__a, __b);
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
 vshlq_n_s32 (int32x4_t __a, const int __b)
 {
-  return (int32x4_t) __builtin_aarch64_sshl_nv4si (__a, __b);
+  return (int32x4_t) __builtin_aarch64_ashlv4si (__a, __b);
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
 vshlq_n_s64 (int64x2_t __a, const int __b)
 {
-  return (int64x2_t) __builtin_aarch64_sshl_nv2di (__a, __b);
+  return (int64x2_t) __builtin_aarch64_ashlv2di (__a, __b);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vshlq_n_u8 (uint8x16_t __a, const int __b)
 {
-  return (uint8x16_t) __builtin_aarch64_ushl_nv16qi ((int8x16_t) __a, __b);
+  return (uint8x16_t) __builtin_aarch64_ashlv16qi ((int8x16_t) __a, __b);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
 vshlq_n_u16 (uint16x8_t __a, const int __b)
 {
-  return (uint16x8_t) __builtin_aarch64_ushl_nv8hi ((int16x8_t) __a, __b);
+  return (uint16x8_t) __builtin_aarch64_ashlv8hi ((int16x8_t) __a, __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vshlq_n_u32 (uint32x4_t __a, const int __b)
 {
-  return (uint32x4_t) __builtin_aarch64_ushl_nv4si ((int32x4_t) __a, __b);
+  return (uint32x4_t) __builtin_aarch64_ashlv4si ((int32x4_t) __a, __b);
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
 vshlq_n_u64 (uint64x2_t __a, const int __b)
 {
-  return (uint64x2_t) __builtin_aarch64_ushl_nv2di ((int64x2_t) __a, __b);
+  return (uint64x2_t) __builtin_aarch64_ashlv2di ((int64x2_t) __a, __b);
 }
 
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vshld_n_s64 (int64x1_t __a, const int __b)
 {
-  return (int64x1_t) __builtin_aarch64_sshl_ndi (__a, __b);
+  return (int64x1_t) __builtin_aarch64_ashldi (__a, __b);
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vshld_n_u64 (uint64x1_t __a, const int __b)
 {
-  return (uint64x1_t) __builtin_aarch64_ushl_ndi (__a, __b);
+  return (uint64x1_t) __builtin_aarch64_ashldi (__a, __b);
 }
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
@@ -23428,109 +23780,109 @@ vshll_n_u32 (uint32x2_t __a, const int _
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vshr_n_s8 (int8x8_t __a, const int __b)
 {
-  return (int8x8_t) __builtin_aarch64_sshr_nv8qi (__a, __b);
+  return (int8x8_t) __builtin_aarch64_ashrv8qi (__a, __b);
 }
 
 __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
 vshr_n_s16 (int16x4_t __a, const int __b)
 {
-  return (int16x4_t) __builtin_aarch64_sshr_nv4hi (__a, __b);
+  return (int16x4_t) __builtin_aarch64_ashrv4hi (__a, __b);
 }
 
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
 vshr_n_s32 (int32x2_t __a, const int __b)
 {
-  return (int32x2_t) __builtin_aarch64_sshr_nv2si (__a, __b);
+  return (int32x2_t) __builtin_aarch64_ashrv2si (__a, __b);
 }
 
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vshr_n_s64 (int64x1_t __a, const int __b)
 {
-  return (int64x1_t) __builtin_aarch64_sshr_ndi (__a, __b);
+  return (int64x1_t) __builtin_aarch64_ashrdi (__a, __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
 vshr_n_u8 (uint8x8_t __a, const int __b)
 {
-  return (uint8x8_t) __builtin_aarch64_ushr_nv8qi ((int8x8_t) __a, __b);
+  return (uint8x8_t) __builtin_aarch64_lshrv8qi ((int8x8_t) __a, __b);
 }
 
 __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
 vshr_n_u16 (uint16x4_t __a, const int __b)
 {
-  return (uint16x4_t) __builtin_aarch64_ushr_nv4hi ((int16x4_t) __a, __b);
+  return (uint16x4_t) __builtin_aarch64_lshrv4hi ((int16x4_t) __a, __b);
 }
 
 __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
 vshr_n_u32 (uint32x2_t __a, const int __b)
 {
-  return (uint32x2_t) __builtin_aarch64_ushr_nv2si ((int32x2_t) __a, __b);
+  return (uint32x2_t) __builtin_aarch64_lshrv2si ((int32x2_t) __a, __b);
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vshr_n_u64 (uint64x1_t __a, const int __b)
 {
-  return (uint64x1_t) __builtin_aarch64_ushr_ndi ((int64x1_t) __a, __b);
+  return (uint64x1_t) __builtin_aarch64_lshrdi ((int64x1_t) __a, __b);
 }
 
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vshrq_n_s8 (int8x16_t __a, const int __b)
 {
-  return (int8x16_t) __builtin_aarch64_sshr_nv16qi (__a, __b);
+  return (int8x16_t) __builtin_aarch64_ashrv16qi (__a, __b);
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
 vshrq_n_s16 (int16x8_t __a, const int __b)
 {
-  return (int16x8_t) __builtin_aarch64_sshr_nv8hi (__a, __b);
+  return (int16x8_t) __builtin_aarch64_ashrv8hi (__a, __b);
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
 vshrq_n_s32 (int32x4_t __a, const int __b)
 {
-  return (int32x4_t) __builtin_aarch64_sshr_nv4si (__a, __b);
+  return (int32x4_t) __builtin_aarch64_ashrv4si (__a, __b);
 }
 
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
 vshrq_n_s64 (int64x2_t __a, const int __b)
 {
-  return (int64x2_t) __builtin_aarch64_sshr_nv2di (__a, __b);
+  return (int64x2_t) __builtin_aarch64_ashrv2di (__a, __b);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vshrq_n_u8 (uint8x16_t __a, const int __b)
 {
-  return (uint8x16_t) __builtin_aarch64_ushr_nv16qi ((int8x16_t) __a, __b);
+  return (uint8x16_t) __builtin_aarch64_lshrv16qi ((int8x16_t) __a, __b);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
 vshrq_n_u16 (uint16x8_t __a, const int __b)
 {
-  return (uint16x8_t) __builtin_aarch64_ushr_nv8hi ((int16x8_t) __a, __b);
+  return (uint16x8_t) __builtin_aarch64_lshrv8hi ((int16x8_t) __a, __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vshrq_n_u32 (uint32x4_t __a, const int __b)
 {
-  return (uint32x4_t) __builtin_aarch64_ushr_nv4si ((int32x4_t) __a, __b);
+  return (uint32x4_t) __builtin_aarch64_lshrv4si ((int32x4_t) __a, __b);
 }
 
 __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
 vshrq_n_u64 (uint64x2_t __a, const int __b)
 {
-  return (uint64x2_t) __builtin_aarch64_ushr_nv2di ((int64x2_t) __a, __b);
+  return (uint64x2_t) __builtin_aarch64_lshrv2di ((int64x2_t) __a, __b);
 }
 
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vshrd_n_s64 (int64x1_t __a, const int __b)
 {
-  return (int64x1_t) __builtin_aarch64_sshr_ndi (__a, __b);
+  return (int64x1_t) __builtin_aarch64_ashrdi (__a, __b);
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vshrd_n_u64 (uint64x1_t __a, const int __b)
 {
-  return (uint64x1_t) __builtin_aarch64_ushr_ndi (__a, __b);
+  return (uint64x1_t) __builtin_aarch64_lshrdi (__a, __b);
 }
 
 /* vsli */
@@ -24153,8 +24505,8 @@ vst2_s64 (int64_t * __a, int64x1x2_t val
 {
   __builtin_aarch64_simd_oi __o;
   int64x2x2_t temp;
-  temp.val[0] = vcombine_s64 (val.val[0], vcreate_s64 (INT64_C (0)));
-  temp.val[1] = vcombine_s64 (val.val[1], vcreate_s64 (INT64_C (0)));
+  temp.val[0] = vcombine_s64 (val.val[0], vcreate_s64 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s64 (val.val[1], vcreate_s64 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) temp.val[1], 1);
   __builtin_aarch64_st2di ((__builtin_aarch64_simd_di *) __a, __o);
@@ -24165,8 +24517,8 @@ vst2_u64 (uint64_t * __a, uint64x1x2_t v
 {
   __builtin_aarch64_simd_oi __o;
   uint64x2x2_t temp;
-  temp.val[0] = vcombine_u64 (val.val[0], vcreate_u64 (UINT64_C (0)));
-  temp.val[1] = vcombine_u64 (val.val[1], vcreate_u64 (UINT64_C (0)));
+  temp.val[0] = vcombine_u64 (val.val[0], vcreate_u64 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u64 (val.val[1], vcreate_u64 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) temp.val[1], 1);
   __builtin_aarch64_st2di ((__builtin_aarch64_simd_di *) __a, __o);
@@ -24177,8 +24529,8 @@ vst2_f64 (float64_t * __a, float64x1x2_t
 {
   __builtin_aarch64_simd_oi __o;
   float64x2x2_t temp;
-  temp.val[0] = vcombine_f64 (val.val[0], vcreate_f64 (UINT64_C (0)));
-  temp.val[1] = vcombine_f64 (val.val[1], vcreate_f64 (UINT64_C (0)));
+  temp.val[0] = vcombine_f64 (val.val[0], vcreate_f64 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_f64 (val.val[1], vcreate_f64 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv2df (__o, (float64x2_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv2df (__o, (float64x2_t) temp.val[1], 1);
   __builtin_aarch64_st2df ((__builtin_aarch64_simd_df *) __a, __o);
@@ -24189,8 +24541,8 @@ vst2_s8 (int8_t * __a, int8x8x2_t val)
 {
   __builtin_aarch64_simd_oi __o;
   int8x16x2_t temp;
-  temp.val[0] = vcombine_s8 (val.val[0], vcreate_s8 (INT64_C (0)));
-  temp.val[1] = vcombine_s8 (val.val[1], vcreate_s8 (INT64_C (0)));
+  temp.val[0] = vcombine_s8 (val.val[0], vcreate_s8 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s8 (val.val[1], vcreate_s8 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[1], 1);
   __builtin_aarch64_st2v8qi ((__builtin_aarch64_simd_qi *) __a, __o);
@@ -24201,8 +24553,8 @@ vst2_p8 (poly8_t * __a, poly8x8x2_t val)
 {
   __builtin_aarch64_simd_oi __o;
   poly8x16x2_t temp;
-  temp.val[0] = vcombine_p8 (val.val[0], vcreate_p8 (UINT64_C (0)));
-  temp.val[1] = vcombine_p8 (val.val[1], vcreate_p8 (UINT64_C (0)));
+  temp.val[0] = vcombine_p8 (val.val[0], vcreate_p8 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_p8 (val.val[1], vcreate_p8 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[1], 1);
   __builtin_aarch64_st2v8qi ((__builtin_aarch64_simd_qi *) __a, __o);
@@ -24213,8 +24565,8 @@ vst2_s16 (int16_t * __a, int16x4x2_t val
 {
   __builtin_aarch64_simd_oi __o;
   int16x8x2_t temp;
-  temp.val[0] = vcombine_s16 (val.val[0], vcreate_s16 (INT64_C (0)));
-  temp.val[1] = vcombine_s16 (val.val[1], vcreate_s16 (INT64_C (0)));
+  temp.val[0] = vcombine_s16 (val.val[0], vcreate_s16 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s16 (val.val[1], vcreate_s16 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[1], 1);
   __builtin_aarch64_st2v4hi ((__builtin_aarch64_simd_hi *) __a, __o);
@@ -24225,8 +24577,8 @@ vst2_p16 (poly16_t * __a, poly16x4x2_t v
 {
   __builtin_aarch64_simd_oi __o;
   poly16x8x2_t temp;
-  temp.val[0] = vcombine_p16 (val.val[0], vcreate_p16 (UINT64_C (0)));
-  temp.val[1] = vcombine_p16 (val.val[1], vcreate_p16 (UINT64_C (0)));
+  temp.val[0] = vcombine_p16 (val.val[0], vcreate_p16 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_p16 (val.val[1], vcreate_p16 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[1], 1);
   __builtin_aarch64_st2v4hi ((__builtin_aarch64_simd_hi *) __a, __o);
@@ -24237,8 +24589,8 @@ vst2_s32 (int32_t * __a, int32x2x2_t val
 {
   __builtin_aarch64_simd_oi __o;
   int32x4x2_t temp;
-  temp.val[0] = vcombine_s32 (val.val[0], vcreate_s32 (INT64_C (0)));
-  temp.val[1] = vcombine_s32 (val.val[1], vcreate_s32 (INT64_C (0)));
+  temp.val[0] = vcombine_s32 (val.val[0], vcreate_s32 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s32 (val.val[1], vcreate_s32 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) temp.val[1], 1);
   __builtin_aarch64_st2v2si ((__builtin_aarch64_simd_si *) __a, __o);
@@ -24249,8 +24601,8 @@ vst2_u8 (uint8_t * __a, uint8x8x2_t val)
 {
   __builtin_aarch64_simd_oi __o;
   uint8x16x2_t temp;
-  temp.val[0] = vcombine_u8 (val.val[0], vcreate_u8 (UINT64_C (0)));
-  temp.val[1] = vcombine_u8 (val.val[1], vcreate_u8 (UINT64_C (0)));
+  temp.val[0] = vcombine_u8 (val.val[0], vcreate_u8 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u8 (val.val[1], vcreate_u8 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[1], 1);
   __builtin_aarch64_st2v8qi ((__builtin_aarch64_simd_qi *) __a, __o);
@@ -24261,8 +24613,8 @@ vst2_u16 (uint16_t * __a, uint16x4x2_t v
 {
   __builtin_aarch64_simd_oi __o;
   uint16x8x2_t temp;
-  temp.val[0] = vcombine_u16 (val.val[0], vcreate_u16 (UINT64_C (0)));
-  temp.val[1] = vcombine_u16 (val.val[1], vcreate_u16 (UINT64_C (0)));
+  temp.val[0] = vcombine_u16 (val.val[0], vcreate_u16 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u16 (val.val[1], vcreate_u16 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[1], 1);
   __builtin_aarch64_st2v4hi ((__builtin_aarch64_simd_hi *) __a, __o);
@@ -24273,8 +24625,8 @@ vst2_u32 (uint32_t * __a, uint32x2x2_t v
 {
   __builtin_aarch64_simd_oi __o;
   uint32x4x2_t temp;
-  temp.val[0] = vcombine_u32 (val.val[0], vcreate_u32 (UINT64_C (0)));
-  temp.val[1] = vcombine_u32 (val.val[1], vcreate_u32 (UINT64_C (0)));
+  temp.val[0] = vcombine_u32 (val.val[0], vcreate_u32 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u32 (val.val[1], vcreate_u32 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) temp.val[1], 1);
   __builtin_aarch64_st2v2si ((__builtin_aarch64_simd_si *) __a, __o);
@@ -24285,8 +24637,8 @@ vst2_f32 (float32_t * __a, float32x2x2_t
 {
   __builtin_aarch64_simd_oi __o;
   float32x4x2_t temp;
-  temp.val[0] = vcombine_f32 (val.val[0], vcreate_f32 (UINT64_C (0)));
-  temp.val[1] = vcombine_f32 (val.val[1], vcreate_f32 (UINT64_C (0)));
+  temp.val[0] = vcombine_f32 (val.val[0], vcreate_f32 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_f32 (val.val[1], vcreate_f32 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregoiv4sf (__o, (float32x4_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregoiv4sf (__o, (float32x4_t) temp.val[1], 1);
   __builtin_aarch64_st2v2sf ((__builtin_aarch64_simd_sf *) __a, __o);
@@ -24405,9 +24757,9 @@ vst3_s64 (int64_t * __a, int64x1x3_t val
 {
   __builtin_aarch64_simd_ci __o;
   int64x2x3_t temp;
-  temp.val[0] = vcombine_s64 (val.val[0], vcreate_s64 (INT64_C (0)));
-  temp.val[1] = vcombine_s64 (val.val[1], vcreate_s64 (INT64_C (0)));
-  temp.val[2] = vcombine_s64 (val.val[2], vcreate_s64 (INT64_C (0)));
+  temp.val[0] = vcombine_s64 (val.val[0], vcreate_s64 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s64 (val.val[1], vcreate_s64 (__AARCH64_INT64_C (0)));
+  temp.val[2] = vcombine_s64 (val.val[2], vcreate_s64 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[2], 2);
@@ -24419,9 +24771,9 @@ vst3_u64 (uint64_t * __a, uint64x1x3_t v
 {
   __builtin_aarch64_simd_ci __o;
   uint64x2x3_t temp;
-  temp.val[0] = vcombine_u64 (val.val[0], vcreate_u64 (UINT64_C (0)));
-  temp.val[1] = vcombine_u64 (val.val[1], vcreate_u64 (UINT64_C (0)));
-  temp.val[2] = vcombine_u64 (val.val[2], vcreate_u64 (UINT64_C (0)));
+  temp.val[0] = vcombine_u64 (val.val[0], vcreate_u64 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u64 (val.val[1], vcreate_u64 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_u64 (val.val[2], vcreate_u64 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[2], 2);
@@ -24433,9 +24785,9 @@ vst3_f64 (float64_t * __a, float64x1x3_t
 {
   __builtin_aarch64_simd_ci __o;
   float64x2x3_t temp;
-  temp.val[0] = vcombine_f64 (val.val[0], vcreate_f64 (UINT64_C (0)));
-  temp.val[1] = vcombine_f64 (val.val[1], vcreate_f64 (UINT64_C (0)));
-  temp.val[2] = vcombine_f64 (val.val[2], vcreate_f64 (UINT64_C (0)));
+  temp.val[0] = vcombine_f64 (val.val[0], vcreate_f64 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_f64 (val.val[1], vcreate_f64 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_f64 (val.val[2], vcreate_f64 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregciv2df (__o, (float64x2_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv2df (__o, (float64x2_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv2df (__o, (float64x2_t) temp.val[2], 2);
@@ -24447,9 +24799,9 @@ vst3_s8 (int8_t * __a, int8x8x3_t val)
 {
   __builtin_aarch64_simd_ci __o;
   int8x16x3_t temp;
-  temp.val[0] = vcombine_s8 (val.val[0], vcreate_s8 (INT64_C (0)));
-  temp.val[1] = vcombine_s8 (val.val[1], vcreate_s8 (INT64_C (0)));
-  temp.val[2] = vcombine_s8 (val.val[2], vcreate_s8 (INT64_C (0)));
+  temp.val[0] = vcombine_s8 (val.val[0], vcreate_s8 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s8 (val.val[1], vcreate_s8 (__AARCH64_INT64_C (0)));
+  temp.val[2] = vcombine_s8 (val.val[2], vcreate_s8 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[2], 2);
@@ -24461,9 +24813,9 @@ vst3_p8 (poly8_t * __a, poly8x8x3_t val)
 {
   __builtin_aarch64_simd_ci __o;
   poly8x16x3_t temp;
-  temp.val[0] = vcombine_p8 (val.val[0], vcreate_p8 (UINT64_C (0)));
-  temp.val[1] = vcombine_p8 (val.val[1], vcreate_p8 (UINT64_C (0)));
-  temp.val[2] = vcombine_p8 (val.val[2], vcreate_p8 (UINT64_C (0)));
+  temp.val[0] = vcombine_p8 (val.val[0], vcreate_p8 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_p8 (val.val[1], vcreate_p8 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_p8 (val.val[2], vcreate_p8 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[2], 2);
@@ -24475,9 +24827,9 @@ vst3_s16 (int16_t * __a, int16x4x3_t val
 {
   __builtin_aarch64_simd_ci __o;
   int16x8x3_t temp;
-  temp.val[0] = vcombine_s16 (val.val[0], vcreate_s16 (INT64_C (0)));
-  temp.val[1] = vcombine_s16 (val.val[1], vcreate_s16 (INT64_C (0)));
-  temp.val[2] = vcombine_s16 (val.val[2], vcreate_s16 (INT64_C (0)));
+  temp.val[0] = vcombine_s16 (val.val[0], vcreate_s16 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s16 (val.val[1], vcreate_s16 (__AARCH64_INT64_C (0)));
+  temp.val[2] = vcombine_s16 (val.val[2], vcreate_s16 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[2], 2);
@@ -24489,9 +24841,9 @@ vst3_p16 (poly16_t * __a, poly16x4x3_t v
 {
   __builtin_aarch64_simd_ci __o;
   poly16x8x3_t temp;
-  temp.val[0] = vcombine_p16 (val.val[0], vcreate_p16 (UINT64_C (0)));
-  temp.val[1] = vcombine_p16 (val.val[1], vcreate_p16 (UINT64_C (0)));
-  temp.val[2] = vcombine_p16 (val.val[2], vcreate_p16 (UINT64_C (0)));
+  temp.val[0] = vcombine_p16 (val.val[0], vcreate_p16 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_p16 (val.val[1], vcreate_p16 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_p16 (val.val[2], vcreate_p16 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[2], 2);
@@ -24503,9 +24855,9 @@ vst3_s32 (int32_t * __a, int32x2x3_t val
 {
   __builtin_aarch64_simd_ci __o;
   int32x4x3_t temp;
-  temp.val[0] = vcombine_s32 (val.val[0], vcreate_s32 (INT64_C (0)));
-  temp.val[1] = vcombine_s32 (val.val[1], vcreate_s32 (INT64_C (0)));
-  temp.val[2] = vcombine_s32 (val.val[2], vcreate_s32 (INT64_C (0)));
+  temp.val[0] = vcombine_s32 (val.val[0], vcreate_s32 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s32 (val.val[1], vcreate_s32 (__AARCH64_INT64_C (0)));
+  temp.val[2] = vcombine_s32 (val.val[2], vcreate_s32 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[2], 2);
@@ -24517,9 +24869,9 @@ vst3_u8 (uint8_t * __a, uint8x8x3_t val)
 {
   __builtin_aarch64_simd_ci __o;
   uint8x16x3_t temp;
-  temp.val[0] = vcombine_u8 (val.val[0], vcreate_u8 (UINT64_C (0)));
-  temp.val[1] = vcombine_u8 (val.val[1], vcreate_u8 (UINT64_C (0)));
-  temp.val[2] = vcombine_u8 (val.val[2], vcreate_u8 (UINT64_C (0)));
+  temp.val[0] = vcombine_u8 (val.val[0], vcreate_u8 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u8 (val.val[1], vcreate_u8 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_u8 (val.val[2], vcreate_u8 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[2], 2);
@@ -24531,9 +24883,9 @@ vst3_u16 (uint16_t * __a, uint16x4x3_t v
 {
   __builtin_aarch64_simd_ci __o;
   uint16x8x3_t temp;
-  temp.val[0] = vcombine_u16 (val.val[0], vcreate_u16 (UINT64_C (0)));
-  temp.val[1] = vcombine_u16 (val.val[1], vcreate_u16 (UINT64_C (0)));
-  temp.val[2] = vcombine_u16 (val.val[2], vcreate_u16 (UINT64_C (0)));
+  temp.val[0] = vcombine_u16 (val.val[0], vcreate_u16 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u16 (val.val[1], vcreate_u16 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_u16 (val.val[2], vcreate_u16 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[2], 2);
@@ -24545,9 +24897,9 @@ vst3_u32 (uint32_t * __a, uint32x2x3_t v
 {
   __builtin_aarch64_simd_ci __o;
   uint32x4x3_t temp;
-  temp.val[0] = vcombine_u32 (val.val[0], vcreate_u32 (UINT64_C (0)));
-  temp.val[1] = vcombine_u32 (val.val[1], vcreate_u32 (UINT64_C (0)));
-  temp.val[2] = vcombine_u32 (val.val[2], vcreate_u32 (UINT64_C (0)));
+  temp.val[0] = vcombine_u32 (val.val[0], vcreate_u32 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u32 (val.val[1], vcreate_u32 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_u32 (val.val[2], vcreate_u32 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[2], 2);
@@ -24559,9 +24911,9 @@ vst3_f32 (float32_t * __a, float32x2x3_t
 {
   __builtin_aarch64_simd_ci __o;
   float32x4x3_t temp;
-  temp.val[0] = vcombine_f32 (val.val[0], vcreate_f32 (UINT64_C (0)));
-  temp.val[1] = vcombine_f32 (val.val[1], vcreate_f32 (UINT64_C (0)));
-  temp.val[2] = vcombine_f32 (val.val[2], vcreate_f32 (UINT64_C (0)));
+  temp.val[0] = vcombine_f32 (val.val[0], vcreate_f32 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_f32 (val.val[1], vcreate_f32 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_f32 (val.val[2], vcreate_f32 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregciv4sf (__o, (float32x4_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregciv4sf (__o, (float32x4_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregciv4sf (__o, (float32x4_t) temp.val[2], 2);
@@ -24693,10 +25045,10 @@ vst4_s64 (int64_t * __a, int64x1x4_t val
 {
   __builtin_aarch64_simd_xi __o;
   int64x2x4_t temp;
-  temp.val[0] = vcombine_s64 (val.val[0], vcreate_s64 (INT64_C (0)));
-  temp.val[1] = vcombine_s64 (val.val[1], vcreate_s64 (INT64_C (0)));
-  temp.val[2] = vcombine_s64 (val.val[2], vcreate_s64 (INT64_C (0)));
-  temp.val[3] = vcombine_s64 (val.val[3], vcreate_s64 (INT64_C (0)));
+  temp.val[0] = vcombine_s64 (val.val[0], vcreate_s64 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s64 (val.val[1], vcreate_s64 (__AARCH64_INT64_C (0)));
+  temp.val[2] = vcombine_s64 (val.val[2], vcreate_s64 (__AARCH64_INT64_C (0)));
+  temp.val[3] = vcombine_s64 (val.val[3], vcreate_s64 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) temp.val[2], 2);
@@ -24709,10 +25061,10 @@ vst4_u64 (uint64_t * __a, uint64x1x4_t v
 {
   __builtin_aarch64_simd_xi __o;
   uint64x2x4_t temp;
-  temp.val[0] = vcombine_u64 (val.val[0], vcreate_u64 (UINT64_C (0)));
-  temp.val[1] = vcombine_u64 (val.val[1], vcreate_u64 (UINT64_C (0)));
-  temp.val[2] = vcombine_u64 (val.val[2], vcreate_u64 (UINT64_C (0)));
-  temp.val[3] = vcombine_u64 (val.val[3], vcreate_u64 (UINT64_C (0)));
+  temp.val[0] = vcombine_u64 (val.val[0], vcreate_u64 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u64 (val.val[1], vcreate_u64 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_u64 (val.val[2], vcreate_u64 (__AARCH64_UINT64_C (0)));
+  temp.val[3] = vcombine_u64 (val.val[3], vcreate_u64 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) temp.val[2], 2);
@@ -24725,10 +25077,10 @@ vst4_f64 (float64_t * __a, float64x1x4_t
 {
   __builtin_aarch64_simd_xi __o;
   float64x2x4_t temp;
-  temp.val[0] = vcombine_f64 (val.val[0], vcreate_f64 (UINT64_C (0)));
-  temp.val[1] = vcombine_f64 (val.val[1], vcreate_f64 (UINT64_C (0)));
-  temp.val[2] = vcombine_f64 (val.val[2], vcreate_f64 (UINT64_C (0)));
-  temp.val[3] = vcombine_f64 (val.val[3], vcreate_f64 (UINT64_C (0)));
+  temp.val[0] = vcombine_f64 (val.val[0], vcreate_f64 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_f64 (val.val[1], vcreate_f64 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_f64 (val.val[2], vcreate_f64 (__AARCH64_UINT64_C (0)));
+  temp.val[3] = vcombine_f64 (val.val[3], vcreate_f64 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv2df (__o, (float64x2_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv2df (__o, (float64x2_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv2df (__o, (float64x2_t) temp.val[2], 2);
@@ -24741,10 +25093,10 @@ vst4_s8 (int8_t * __a, int8x8x4_t val)
 {
   __builtin_aarch64_simd_xi __o;
   int8x16x4_t temp;
-  temp.val[0] = vcombine_s8 (val.val[0], vcreate_s8 (INT64_C (0)));
-  temp.val[1] = vcombine_s8 (val.val[1], vcreate_s8 (INT64_C (0)));
-  temp.val[2] = vcombine_s8 (val.val[2], vcreate_s8 (INT64_C (0)));
-  temp.val[3] = vcombine_s8 (val.val[3], vcreate_s8 (INT64_C (0)));
+  temp.val[0] = vcombine_s8 (val.val[0], vcreate_s8 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s8 (val.val[1], vcreate_s8 (__AARCH64_INT64_C (0)));
+  temp.val[2] = vcombine_s8 (val.val[2], vcreate_s8 (__AARCH64_INT64_C (0)));
+  temp.val[3] = vcombine_s8 (val.val[3], vcreate_s8 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) temp.val[2], 2);
@@ -24757,10 +25109,10 @@ vst4_p8 (poly8_t * __a, poly8x8x4_t val)
 {
   __builtin_aarch64_simd_xi __o;
   poly8x16x4_t temp;
-  temp.val[0] = vcombine_p8 (val.val[0], vcreate_p8 (UINT64_C (0)));
-  temp.val[1] = vcombine_p8 (val.val[1], vcreate_p8 (UINT64_C (0)));
-  temp.val[2] = vcombine_p8 (val.val[2], vcreate_p8 (UINT64_C (0)));
-  temp.val[3] = vcombine_p8 (val.val[3], vcreate_p8 (UINT64_C (0)));
+  temp.val[0] = vcombine_p8 (val.val[0], vcreate_p8 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_p8 (val.val[1], vcreate_p8 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_p8 (val.val[2], vcreate_p8 (__AARCH64_UINT64_C (0)));
+  temp.val[3] = vcombine_p8 (val.val[3], vcreate_p8 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) temp.val[2], 2);
@@ -24773,10 +25125,10 @@ vst4_s16 (int16_t * __a, int16x4x4_t val
 {
   __builtin_aarch64_simd_xi __o;
   int16x8x4_t temp;
-  temp.val[0] = vcombine_s16 (val.val[0], vcreate_s16 (INT64_C (0)));
-  temp.val[1] = vcombine_s16 (val.val[1], vcreate_s16 (INT64_C (0)));
-  temp.val[2] = vcombine_s16 (val.val[2], vcreate_s16 (INT64_C (0)));
-  temp.val[3] = vcombine_s16 (val.val[3], vcreate_s16 (INT64_C (0)));
+  temp.val[0] = vcombine_s16 (val.val[0], vcreate_s16 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s16 (val.val[1], vcreate_s16 (__AARCH64_INT64_C (0)));
+  temp.val[2] = vcombine_s16 (val.val[2], vcreate_s16 (__AARCH64_INT64_C (0)));
+  temp.val[3] = vcombine_s16 (val.val[3], vcreate_s16 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) temp.val[2], 2);
@@ -24789,10 +25141,10 @@ vst4_p16 (poly16_t * __a, poly16x4x4_t v
 {
   __builtin_aarch64_simd_xi __o;
   poly16x8x4_t temp;
-  temp.val[0] = vcombine_p16 (val.val[0], vcreate_p16 (UINT64_C (0)));
-  temp.val[1] = vcombine_p16 (val.val[1], vcreate_p16 (UINT64_C (0)));
-  temp.val[2] = vcombine_p16 (val.val[2], vcreate_p16 (UINT64_C (0)));
-  temp.val[3] = vcombine_p16 (val.val[3], vcreate_p16 (UINT64_C (0)));
+  temp.val[0] = vcombine_p16 (val.val[0], vcreate_p16 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_p16 (val.val[1], vcreate_p16 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_p16 (val.val[2], vcreate_p16 (__AARCH64_UINT64_C (0)));
+  temp.val[3] = vcombine_p16 (val.val[3], vcreate_p16 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) temp.val[2], 2);
@@ -24805,10 +25157,10 @@ vst4_s32 (int32_t * __a, int32x2x4_t val
 {
   __builtin_aarch64_simd_xi __o;
   int32x4x4_t temp;
-  temp.val[0] = vcombine_s32 (val.val[0], vcreate_s32 (INT64_C (0)));
-  temp.val[1] = vcombine_s32 (val.val[1], vcreate_s32 (INT64_C (0)));
-  temp.val[2] = vcombine_s32 (val.val[2], vcreate_s32 (INT64_C (0)));
-  temp.val[3] = vcombine_s32 (val.val[3], vcreate_s32 (INT64_C (0)));
+  temp.val[0] = vcombine_s32 (val.val[0], vcreate_s32 (__AARCH64_INT64_C (0)));
+  temp.val[1] = vcombine_s32 (val.val[1], vcreate_s32 (__AARCH64_INT64_C (0)));
+  temp.val[2] = vcombine_s32 (val.val[2], vcreate_s32 (__AARCH64_INT64_C (0)));
+  temp.val[3] = vcombine_s32 (val.val[3], vcreate_s32 (__AARCH64_INT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) temp.val[2], 2);
@@ -24821,10 +25173,10 @@ vst4_u8 (uint8_t * __a, uint8x8x4_t val)
 {
   __builtin_aarch64_simd_xi __o;
   uint8x16x4_t temp;
-  temp.val[0] = vcombine_u8 (val.val[0], vcreate_u8 (UINT64_C (0)));
-  temp.val[1] = vcombine_u8 (val.val[1], vcreate_u8 (UINT64_C (0)));
-  temp.val[2] = vcombine_u8 (val.val[2], vcreate_u8 (UINT64_C (0)));
-  temp.val[3] = vcombine_u8 (val.val[3], vcreate_u8 (UINT64_C (0)));
+  temp.val[0] = vcombine_u8 (val.val[0], vcreate_u8 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u8 (val.val[1], vcreate_u8 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_u8 (val.val[2], vcreate_u8 (__AARCH64_UINT64_C (0)));
+  temp.val[3] = vcombine_u8 (val.val[3], vcreate_u8 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) temp.val[2], 2);
@@ -24837,10 +25189,10 @@ vst4_u16 (uint16_t * __a, uint16x4x4_t v
 {
   __builtin_aarch64_simd_xi __o;
   uint16x8x4_t temp;
-  temp.val[0] = vcombine_u16 (val.val[0], vcreate_u16 (UINT64_C (0)));
-  temp.val[1] = vcombine_u16 (val.val[1], vcreate_u16 (UINT64_C (0)));
-  temp.val[2] = vcombine_u16 (val.val[2], vcreate_u16 (UINT64_C (0)));
-  temp.val[3] = vcombine_u16 (val.val[3], vcreate_u16 (UINT64_C (0)));
+  temp.val[0] = vcombine_u16 (val.val[0], vcreate_u16 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u16 (val.val[1], vcreate_u16 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_u16 (val.val[2], vcreate_u16 (__AARCH64_UINT64_C (0)));
+  temp.val[3] = vcombine_u16 (val.val[3], vcreate_u16 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) temp.val[2], 2);
@@ -24853,10 +25205,10 @@ vst4_u32 (uint32_t * __a, uint32x2x4_t v
 {
   __builtin_aarch64_simd_xi __o;
   uint32x4x4_t temp;
-  temp.val[0] = vcombine_u32 (val.val[0], vcreate_u32 (UINT64_C (0)));
-  temp.val[1] = vcombine_u32 (val.val[1], vcreate_u32 (UINT64_C (0)));
-  temp.val[2] = vcombine_u32 (val.val[2], vcreate_u32 (UINT64_C (0)));
-  temp.val[3] = vcombine_u32 (val.val[3], vcreate_u32 (UINT64_C (0)));
+  temp.val[0] = vcombine_u32 (val.val[0], vcreate_u32 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_u32 (val.val[1], vcreate_u32 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_u32 (val.val[2], vcreate_u32 (__AARCH64_UINT64_C (0)));
+  temp.val[3] = vcombine_u32 (val.val[3], vcreate_u32 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) temp.val[2], 2);
@@ -24869,10 +25221,10 @@ vst4_f32 (float32_t * __a, float32x2x4_t
 {
   __builtin_aarch64_simd_xi __o;
   float32x4x4_t temp;
-  temp.val[0] = vcombine_f32 (val.val[0], vcreate_f32 (UINT64_C (0)));
-  temp.val[1] = vcombine_f32 (val.val[1], vcreate_f32 (UINT64_C (0)));
-  temp.val[2] = vcombine_f32 (val.val[2], vcreate_f32 (UINT64_C (0)));
-  temp.val[3] = vcombine_f32 (val.val[3], vcreate_f32 (UINT64_C (0)));
+  temp.val[0] = vcombine_f32 (val.val[0], vcreate_f32 (__AARCH64_UINT64_C (0)));
+  temp.val[1] = vcombine_f32 (val.val[1], vcreate_f32 (__AARCH64_UINT64_C (0)));
+  temp.val[2] = vcombine_f32 (val.val[2], vcreate_f32 (__AARCH64_UINT64_C (0)));
+  temp.val[3] = vcombine_f32 (val.val[3], vcreate_f32 (__AARCH64_UINT64_C (0)));
   __o = __builtin_aarch64_set_qregxiv4sf (__o, (float32x4_t) temp.val[0], 0);
   __o = __builtin_aarch64_set_qregxiv4sf (__o, (float32x4_t) temp.val[1], 1);
   __o = __builtin_aarch64_set_qregxiv4sf (__o, (float32x4_t) temp.val[2], 2);
@@ -25159,7 +25511,7 @@ vtst_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vtst_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmtstdi (__a, __b);
+  return (__a & __b) ? -1ll : 0ll;
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -25186,8 +25538,7 @@ vtst_u32 (uint32x2_t __a, uint32x2_t __b
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vtst_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmtstdi ((int64x1_t) __a,
-						(int64x1_t) __b);
+  return (__a & __b) ? -1ll : 0ll;
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
@@ -25245,14 +25596,13 @@ vtstq_u64 (uint64x2_t __a, uint64x2_t __
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vtstd_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmtstdi (__a, __b);
+  return (__a & __b) ? -1ll : 0ll;
 }
 
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vtstd_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) __builtin_aarch64_cmtstdi ((int64x1_t) __a,
-						(int64x1_t) __b);
+  return (__a & __b) ? -1ll : 0ll;
 }
 
 /* vuqadd */
@@ -25371,4 +25721,31 @@ __INTERLEAVE_LIST (zip)
 
 /* End of optimal implementations in approved order.  */
 
+#undef __aarch64_vget_lane_any
+#undef __aarch64_vget_lane_f32
+#undef __aarch64_vget_lane_f64
+#undef __aarch64_vget_lane_p8
+#undef __aarch64_vget_lane_p16
+#undef __aarch64_vget_lane_s8
+#undef __aarch64_vget_lane_s16
+#undef __aarch64_vget_lane_s32
+#undef __aarch64_vget_lane_s64
+#undef __aarch64_vget_lane_u8
+#undef __aarch64_vget_lane_u16
+#undef __aarch64_vget_lane_u32
+#undef __aarch64_vget_lane_u64
+
+#undef __aarch64_vgetq_lane_f32
+#undef __aarch64_vgetq_lane_f64
+#undef __aarch64_vgetq_lane_p8
+#undef __aarch64_vgetq_lane_p16
+#undef __aarch64_vgetq_lane_s8
+#undef __aarch64_vgetq_lane_s16
+#undef __aarch64_vgetq_lane_s32
+#undef __aarch64_vgetq_lane_s64
+#undef __aarch64_vgetq_lane_u8
+#undef __aarch64_vgetq_lane_u16
+#undef __aarch64_vgetq_lane_u32
+#undef __aarch64_vgetq_lane_u64
+
 #endif
Index: b/src/gcc/config/aarch64/aarch64.md
===================================================================
--- a/src/gcc/config/aarch64/aarch64.md
+++ b/src/gcc/config/aarch64/aarch64.md
@@ -68,14 +68,19 @@
 (define_c_enum "unspec" [
     UNSPEC_CASESI
     UNSPEC_CLS
+    UNSPEC_FRECPE
+    UNSPEC_FRECPS
+    UNSPEC_FRECPX
     UNSPEC_FRINTA
     UNSPEC_FRINTI
     UNSPEC_FRINTM
+    UNSPEC_FRINTN
     UNSPEC_FRINTP
     UNSPEC_FRINTX
     UNSPEC_FRINTZ
     UNSPEC_GOTSMALLPIC
     UNSPEC_GOTSMALLTLS
+    UNSPEC_GOTTINYPIC
     UNSPEC_LD2
     UNSPEC_LD3
     UNSPEC_LD4
@@ -107,205 +112,9 @@
 ;; Instruction types and attributes
 ;; -------------------------------------------------------------------
 
-;; Main data types used by the insntructions
-
-(define_attr "mode" "unknown,none,QI,HI,SI,DI,TI,SF,DF,TF"
-  (const_string "unknown"))
-
-(define_attr "mode2" "unknown,none,QI,HI,SI,DI,TI,SF,DF,TF"
-  (const_string "unknown"))
-
-; The "v8type" attribute is used to for fine grained classification of
-; AArch64 instructions.  This table briefly explains the meaning of each type.
-
-; adc              add/subtract with carry.
-; adcs             add/subtract with carry (setting condition flags).
-; adr              calculate address.
-; alu              simple alu instruction (no memory or fp regs access).
-; alu_ext          simple alu instruction (sign/zero-extended register).
-; alu_shift        simple alu instruction, with a source operand shifted by a constant.
-; alus             simple alu instruction (setting condition flags).
-; alus_ext         simple alu instruction (sign/zero-extended register, setting condition flags).
-; alus_shift       simple alu instruction, with a source operand shifted by a constant (setting condition flags).
-; bfm              bitfield move operation.
-; branch           branch.
-; call             subroutine call.
-; ccmp             conditional compare.
-; clz              count leading zeros/sign bits.
-; csel             conditional select.
-; dmb              data memory barrier.
-; extend           sign/zero-extend (specialised bitfield move).
-; extr             extract register-sized bitfield encoding.
-; fpsimd_load      load single floating point / simd scalar register from memory.
-; fpsimd_load2     load pair of floating point / simd scalar registers from memory.
-; fpsimd_store     store single floating point / simd scalar register to memory.
-; fpsimd_store2    store pair floating point / simd scalar registers to memory.
-; fadd             floating point add/sub.
-; fccmp            floating point conditional compare.
-; fcmp             floating point comparison.
-; fconst           floating point load immediate.
-; fcsel            floating point conditional select.
-; fcvt             floating point convert (float to float).
-; fcvtf2i          floating point convert (float to integer).
-; fcvti2f          floating point convert (integer to float).
-; fdiv             floating point division operation.
-; ffarith          floating point abs, neg or cpy.
-; fmadd            floating point multiply-add/sub.
-; fminmax          floating point min/max.
-; fmov             floating point move (float to float).
-; fmovf2i          floating point move (float to integer).
-; fmovi2f          floating point move (integer to float).
-; fmul             floating point multiply.
-; frint            floating point round to integral.
-; fsqrt            floating point square root.
-; load_acq         load-acquire.
-; load             load single general register from memory
-; load2            load pair of general registers from memory
-; logic            logical operation (register).
-; logic_imm        and/or/xor operation (immediate).
-; logic_shift      logical operation with shift.
-; logics           logical operation (register, setting condition flags).
-; logics_imm       and/or/xor operation (immediate, setting condition flags).
-; logics_shift     logical operation with shift (setting condition flags).
-; madd             integer multiply-add/sub.
-; maddl            widening integer multiply-add/sub.
-; misc             miscellaneous - any type that doesn't fit into the rest.
-; move             integer move operation.
-; move2            double integer move operation.
-; movk             move 16-bit immediate with keep.
-; movz             move 16-bit immmediate with zero/one.
-; mrs              system/special register move.
-; mulh             64x64 to 128-bit multiply (high part).
-; mull             widening multiply.
-; mult             integer multiply instruction.
-; prefetch         memory prefetch.
-; rbit             reverse bits.
-; rev              reverse bytes.
-; sdiv             integer division operation (signed).
-; shift            variable shift operation.
-; shift_imm        immediate shift operation (specialised bitfield move).
-; store_rel        store-release.
-; store            store single general register to memory.
-; store2           store pair of general registers to memory.
-; udiv             integer division operation (unsigned).
-
-(define_attr "v8type"
-   "adc,\
-   adcs,\
-   adr,\
-   alu,\
-   alu_ext,\
-   alu_shift,\
-   alus,\
-   alus_ext,\
-   alus_shift,\
-   bfm,\
-   branch,\
-   call,\
-   ccmp,\
-   clz,\
-   csel,\
-   dmb,\
-   div,\
-   div64,\
-   extend,\
-   extr,\
-   fpsimd_load,\
-   fpsimd_load2,\
-   fpsimd_store2,\
-   fpsimd_store,\
-   fadd,\
-   fccmp,\
-   fcvt,\
-   fcvtf2i,\
-   fcvti2f,\
-   fcmp,\
-   fconst,\
-   fcsel,\
-   fdiv,\
-   ffarith,\
-   fmadd,\
-   fminmax,\
-   fmov,\
-   fmovf2i,\
-   fmovi2f,\
-   fmul,\
-   frint,\
-   fsqrt,\
-   load_acq,\
-   load1,\
-   load2,\
-   logic,\
-   logic_imm,\
-   logic_shift,\
-   logics,\
-   logics_imm,\
-   logics_shift,\
-   madd,\
-   maddl,\
-   misc,\
-   move,\
-   move2,\
-   movk,\
-   movz,\
-   mrs,\
-   mulh,\
-   mull,\
-   mult,\
-   prefetch,\
-   rbit,\
-   rev,\
-   sdiv,\
-   shift,\
-   shift_imm,\
-   store_rel,\
-   store1,\
-   store2,\
-   udiv"
-  (const_string "alu"))
-
-
-; The "type" attribute is used by the AArch32 backend.  Below is a mapping
-; from "v8type" to "type".
-
-(define_attr "type"
-  "alu,alu_shift,block,branch,call,f_2_r,f_cvt,f_flag,f_loads,
-   f_loadd,f_stored,f_stores,faddd,fadds,fcmpd,fcmps,fconstd,fconsts,
-   fcpys,fdivd,fdivs,ffarithd,ffariths,fmacd,fmacs,fmuld,fmuls,load_byte,
-   load1,load2,mult,r_2_f,store1,store2"
-  (cond [
-	  (eq_attr "v8type" "alu_shift,alus_shift,logic_shift,logics_shift") (const_string "alu_shift")
-	  (eq_attr "v8type" "branch") (const_string "branch")
-	  (eq_attr "v8type" "call") (const_string "call")
-	  (eq_attr "v8type" "fmovf2i") (const_string "f_2_r")
-	  (eq_attr "v8type" "fcvt,fcvtf2i,fcvti2f") (const_string "f_cvt")
-	  (and (eq_attr "v8type" "fpsimd_load") (eq_attr "mode" "SF")) (const_string "f_loads")
-	  (and (eq_attr "v8type" "fpsimd_load") (eq_attr "mode" "DF")) (const_string "f_loadd")
-	  (and (eq_attr "v8type" "fpsimd_store") (eq_attr "mode" "SF")) (const_string "f_stores")
-	  (and (eq_attr "v8type" "fpsimd_store") (eq_attr "mode" "DF")) (const_string "f_stored")
-	  (and (eq_attr "v8type" "fadd,fminmax") (eq_attr "mode" "DF")) (const_string "faddd")
-	  (and (eq_attr "v8type" "fadd,fminmax") (eq_attr "mode" "SF")) (const_string "fadds")
-	  (and (eq_attr "v8type" "fcmp,fccmp") (eq_attr "mode" "DF")) (const_string "fcmpd")
-	  (and (eq_attr "v8type" "fcmp,fccmp") (eq_attr "mode" "SF")) (const_string "fcmps")
-	  (and (eq_attr "v8type" "fconst") (eq_attr "mode" "DF")) (const_string "fconstd")
-	  (and (eq_attr "v8type" "fconst") (eq_attr "mode" "SF")) (const_string "fconsts")
-	  (and (eq_attr "v8type" "fdiv,fsqrt") (eq_attr "mode" "DF")) (const_string "fdivd")
-	  (and (eq_attr "v8type" "fdiv,fsqrt") (eq_attr "mode" "SF")) (const_string "fdivs")
-	  (and (eq_attr "v8type" "ffarith") (eq_attr "mode" "DF")) (const_string "ffarithd")
-	  (and (eq_attr "v8type" "ffarith") (eq_attr "mode" "SF")) (const_string "ffariths")
-	  (and (eq_attr "v8type" "fmadd") (eq_attr "mode" "DF")) (const_string "fmacd")
-	  (and (eq_attr "v8type" "fmadd") (eq_attr "mode" "SF")) (const_string "fmacs")
-	  (and (eq_attr "v8type" "fmul") (eq_attr "mode" "DF")) (const_string "fmuld")
-	  (and (eq_attr "v8type" "fmul") (eq_attr "mode" "SF")) (const_string "fmuls")
-	  (and (eq_attr "v8type" "load1") (eq_attr "mode" "QI,HI")) (const_string "load_byte")
-	  (and (eq_attr "v8type" "load1") (eq_attr "mode" "SI,DI,TI")) (const_string "load1")
-	  (eq_attr "v8type" "load2") (const_string "load2")
-	  (and (eq_attr "v8type" "mulh,mult,mull,madd,sdiv,udiv") (eq_attr "mode" "SI")) (const_string "mult")
-	  (eq_attr "v8type" "fmovi2f") (const_string "r_2_f")
-	  (eq_attr "v8type" "store1") (const_string "store1")
-	  (eq_attr "v8type" "store2") (const_string "store2")
-  ]
-  (const_string "alu")))
+; The "type" attribute is is included here from AArch32 backend to be able
+; to share pipeline descriptions.
+(include "../arm/types.md")
 
 ;; Attribute that specifies whether or not the instruction touches fp
 ;; registers.
@@ -337,10 +146,17 @@
 ;; Processor types.
 (include "aarch64-tune.md")
 
+;; True if the generic scheduling description should be used.
+
+(define_attr "generic_sched" "yes,no"
+  (const (if_then_else
+          (eq_attr "tune" "cortexa53,cortexa15")
+          (const_string "no")
+          (const_string "yes"))))
+
 ;; Scheduling
-(include "aarch64-generic.md")
-(include "large.md")
-(include "small.md")
+(include "../arm/cortex-a53.md")
+(include "../arm/cortex-a15.md")
 
 ;; -------------------------------------------------------------------
 ;; Jumps and other miscellaneous insns
@@ -350,14 +166,14 @@
   [(set (pc) (match_operand:DI 0 "register_operand" "r"))]
   ""
   "br\\t%0"
-  [(set_attr "v8type" "branch")]
+  [(set_attr "type" "branch")]
 )
 
 (define_insn "jump"
   [(set (pc) (label_ref (match_operand 0 "" "")))]
   ""
   "b\\t%l0"
-  [(set_attr "v8type" "branch")]
+  [(set_attr "type" "branch")]
 )
 
 (define_expand "cbranch<mode>4"
@@ -395,7 +211,7 @@
 			   (pc)))]
   ""
   "b%m0\\t%l2"
-  [(set_attr "v8type" "branch")]
+  [(set_attr "type" "branch")]
 )
 
 (define_expand "casesi"
@@ -459,14 +275,14 @@
   return aarch64_output_casesi (operands);
   "
   [(set_attr "length" "16")
-   (set_attr "v8type" "branch")]
+   (set_attr "type" "branch")]
 )
 
 (define_insn "nop"
   [(unspec[(const_int 0)] UNSPEC_NOP)]
   ""
   "nop"
-  [(set_attr "v8type" "misc")]
+  [(set_attr "type" "no_insn")]
 )
 
 (define_expand "prologue"
@@ -500,7 +316,7 @@
   [(return)]
   ""
   "ret"
-  [(set_attr "v8type" "branch")]
+  [(set_attr "type" "branch")]
 )
 
 (define_insn "eh_return"
@@ -508,7 +324,8 @@
     UNSPECV_EH_RETURN)]
   ""
   "#"
-  [(set_attr "v8type" "branch")]
+  [(set_attr "type" "branch")]
+
 )
 
 (define_split
@@ -528,7 +345,8 @@
 			   (pc)))]
   ""
   "<cbz>\\t%<w>0, %l1"
-  [(set_attr "v8type" "branch")]
+  [(set_attr "type" "branch")]
+
 )
 
 (define_insn "*tb<optab><mode>1"
@@ -546,8 +364,7 @@
     return \"ubfx\\t%<w>3, %<w>0, %1, #1\;<cbz>\\t%<w>3, %l2\";
   return \"<tbz>\\t%<w>0, %1, %l2\";
   "
-  [(set_attr "v8type" "branch")
-   (set_attr "mode" "<MODE>")
+  [(set_attr "type" "branch")
    (set (attr "length")
 	(if_then_else (and (ge (minus (match_dup 2) (pc)) (const_int -32768))
 			   (lt (minus (match_dup 2) (pc)) (const_int 32764)))
@@ -567,8 +384,7 @@
     return \"ubfx\\t%<w>2, %<w>0, <sizem1>, #1\;<cbz>\\t%<w>2, %l1\";
   return \"<tbz>\\t%<w>0, <sizem1>, %l1\";
   "
-  [(set_attr "v8type" "branch")
-   (set_attr "mode" "<MODE>")
+  [(set_attr "type" "branch")
    (set (attr "length")
 	(if_then_else (and (ge (minus (match_dup 1) (pc)) (const_int -32768))
 			   (lt (minus (match_dup 1) (pc)) (const_int 32764)))
@@ -612,7 +428,7 @@
    (clobber (reg:DI LR_REGNUM))]
   ""
   "blr\\t%0"
-  [(set_attr "v8type" "call")]
+  [(set_attr "type" "call")]
 )
 
 (define_insn "*call_symbol"
@@ -623,7 +439,7 @@
   "GET_CODE (operands[0]) == SYMBOL_REF
    && !aarch64_is_long_call_p (operands[0])"
   "bl\\t%a0"
-  [(set_attr "v8type" "call")]
+  [(set_attr "type" "call")]
 )
 
 (define_expand "call_value"
@@ -660,7 +476,8 @@
    (clobber (reg:DI LR_REGNUM))]
   ""
   "blr\\t%1"
-  [(set_attr "v8type" "call")]
+  [(set_attr "type" "call")]
+
 )
 
 (define_insn "*call_value_symbol"
@@ -672,7 +489,7 @@
   "GET_CODE (operands[1]) == SYMBOL_REF
    && !aarch64_is_long_call_p (operands[1])"
   "bl\\t%a1"
-  [(set_attr "v8type" "call")]
+  [(set_attr "type" "call")]
 )
 
 (define_expand "sibcall"
@@ -707,7 +524,8 @@
    (use (match_operand 2 "" ""))]
   "GET_CODE (operands[0]) == SYMBOL_REF"
   "b\\t%a0"
-  [(set_attr "v8type" "branch")]
+  [(set_attr "type" "branch")]
+
 )
 
 (define_insn "*sibcall_value_insn"
@@ -718,7 +536,7 @@
    (use (match_operand 3 "" ""))]
   "GET_CODE (operands[1]) == SYMBOL_REF"
   "b\\t%a1"
-  [(set_attr "v8type" "branch")]
+  [(set_attr "type" "branch")]
 )
 
 ;; Call subroutine returning any type.
@@ -763,21 +581,41 @@
 )
 
 (define_insn "*mov<mode>_aarch64"
-  [(set (match_operand:SHORT 0 "nonimmediate_operand" "=r,r,r,m,  r,*w")
-        (match_operand:SHORT 1 "general_operand"      " r,M,m,rZ,*w,r"))]
+  [(set (match_operand:SHORT 0 "nonimmediate_operand" "=r,r,   *w,r,*w, m, m, r,*w,*w")
+        (match_operand:SHORT 1 "general_operand"      " r,M,D<hq>,m, m,rZ,*w,*w, r,*w"))]
   "(register_operand (operands[0], <MODE>mode)
     || aarch64_reg_or_zero (operands[1], <MODE>mode))"
-  "@
-   mov\\t%w0, %w1
-   mov\\t%w0, %1
-   ldr<size>\\t%w0, %1
-   str<size>\\t%w1, %0
-   umov\\t%w0, %1.<v>[0]
-   dup\\t%0.<Vallxd>, %w1"
-  [(set_attr "v8type" "move,alu,load1,store1,*,*")
-   (set_attr "simd_type" "*,*,*,*,simd_movgp,simd_dupgp")
-   (set_attr "mode" "<MODE>")
-   (set_attr "simd_mode" "<MODE>")]
+{
+   switch (which_alternative)
+     {
+     case 0:
+       return "mov\t%w0, %w1";
+     case 1:
+       return "mov\t%w0, %1";
+     case 2:
+       return aarch64_output_scalar_simd_mov_immediate (operands[1],
+							<MODE>mode);
+     case 3:
+       return "ldr<size>\t%w0, %1";
+     case 4:
+       return "ldr\t%<size>0, %1";
+     case 5:
+       return "str<size>\t%w1, %0";
+     case 6:
+       return "str\t%<size>1, %0";
+     case 7:
+       return "umov\t%w0, %1.<v>[0]";
+     case 8:
+       return "dup\t%0.<Vallxd>, %w1";
+     case 9:
+       return "dup\t%0, %1.<v>[0]";
+     default:
+       gcc_unreachable ();
+     }
+}
+  [(set_attr "type" "mov_reg,mov_imm,mov_imm,load1,load1,store1,store1,\
+                     neon_from_gp<q>,neon_from_gp<q>, neon_dup")
+   (set_attr "simd" "*,*,yes,*,*,*,*,yes,yes,yes")]
 )
 
 (define_expand "mov<mode>"
@@ -797,26 +635,27 @@
 )
 
 (define_insn "*movsi_aarch64"
-  [(set (match_operand:SI 0 "nonimmediate_operand" "=r,r,r,m, *w, r,*w")
-	(match_operand:SI 1 "aarch64_mov_operand"     " r,M,m,rZ,rZ,*w,*w"))]
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=r,r,r,*w,m,  m,*w, r,*w")
+	(match_operand:SI 1 "aarch64_mov_operand"  " r,M,m, m,rZ,*w,rZ,*w,*w"))]
   "(register_operand (operands[0], SImode)
     || aarch64_reg_or_zero (operands[1], SImode))"
   "@
    mov\\t%w0, %w1
    mov\\t%w0, %1
    ldr\\t%w0, %1
+   ldr\\t%s0, %1
    str\\t%w1, %0
+   str\\t%s1, %0
    fmov\\t%s0, %w1
    fmov\\t%w0, %s1
    fmov\\t%s0, %s1"
-  [(set_attr "v8type" "move,alu,load1,store1,fmov,fmov,fmov")
-   (set_attr "mode" "SI")
-   (set_attr "fp" "*,*,*,*,yes,yes,yes")]
+  [(set_attr "type" "mov_reg,mov_imm,load1,load1,store1,store1,fmov,fmov,fmov")
+   (set_attr "fp" "*,*,*,yes,*,yes,yes,yes,yes")]
 )
 
 (define_insn "*movdi_aarch64"
-  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,k,r,r,r,m, r,  r,  *w, r,*w,w")
-	(match_operand:DI 1 "aarch64_mov_operand"  " r,r,k,N,m,rZ,Usa,Ush,rZ,*w,*w,Dd"))]
+  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,k,r,r,r,*w,m,  m,r,r,  *w, r,*w,w")
+	(match_operand:DI 1 "aarch64_mov_operand"  " r,r,k,N,m, m,rZ,*w,S,Ush,rZ,*w,*w,Dd"))]
   "(register_operand (operands[0], DImode)
     || aarch64_reg_or_zero (operands[1], DImode))"
   "@
@@ -825,17 +664,19 @@
    mov\\t%x0, %1
    mov\\t%x0, %1
    ldr\\t%x0, %1
+   ldr\\t%d0, %1
    str\\t%x1, %0
+   str\\t%d1, %0
    adr\\t%x0, %a1
    adrp\\t%x0, %A1
    fmov\\t%d0, %x1
    fmov\\t%x0, %d1
    fmov\\t%d0, %d1
    movi\\t%d0, %1"
-  [(set_attr "v8type" "move,move,move,alu,load1,store1,adr,adr,fmov,fmov,fmov,fmov")
-   (set_attr "mode" "DI")
-   (set_attr "fp" "*,*,*,*,*,*,*,*,yes,yes,yes,*")
-   (set_attr "simd" "*,*,*,*,*,*,*,*,*,*,*,yes")]
+  [(set_attr "type" "mov_reg,mov_reg,mov_reg,mov_imm,load1,load1,store1,store1,\
+                     adr,adr,fmov,fmov,fmov,fmov")
+   (set_attr "fp" "*,*,*,*,*,yes,*,yes,*,*,yes,yes,yes,*")
+   (set_attr "simd" "*,*,*,*,*,*,*,*,*,*,*,*,*,yes")]
 )
 
 (define_insn "insv_imm<mode>"
@@ -843,12 +684,10 @@
 			  (const_int 16)
 			  (match_operand:GPI 1 "const_int_operand" "n"))
 	(match_operand:GPI 2 "const_int_operand" "n"))]
-  "INTVAL (operands[1]) < GET_MODE_BITSIZE (<MODE>mode)
-   && INTVAL (operands[1]) % 16 == 0
-   && UINTVAL (operands[2]) <= 0xffff"
+  "UINTVAL (operands[1]) < GET_MODE_BITSIZE (<MODE>mode)
+   && UINTVAL (operands[1]) % 16 == 0"
   "movk\\t%<w>0, %X2, lsl %1"
-  [(set_attr "v8type" "movk")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "mov_imm")]
 )
 
 (define_expand "movti"
@@ -878,13 +717,12 @@
    stp\\txzr, xzr, %0
    ldr\\t%q0, %1
    str\\t%q1, %0"
-  [(set_attr "v8type" "move2,fmovi2f,fmovf2i,*, \
-		       load2,store2,store2,fpsimd_load,fpsimd_store")
-   (set_attr "simd_type" "*,*,*,simd_move,*,*,*,*,*")
-   (set_attr "mode" "DI,DI,DI,TI,DI,DI,DI,TI,TI")
+  [(set_attr "type" "multiple,f_mcr,f_mrc,neon_logic_q, \
+		             load2,store2,store2,f_loadd,f_stored")
    (set_attr "length" "8,8,8,4,4,4,4,4,4")
-   (set_attr "fp" "*,*,*,*,*,*,*,yes,yes")
-   (set_attr "simd" "*,*,*,yes,*,*,*,*,*")])
+   (set_attr "simd" "*,*,*,yes,*,*,*,*,*")
+   (set_attr "fp" "*,*,*,*,*,*,*,yes,yes")]
+)
 
 ;; Split a TImode register-register or register-immediate move into
 ;; its component DImode pieces, taking care to handle overlapping
@@ -930,10 +768,8 @@
    ldr\\t%w0, %1
    str\\t%w1, %0
    mov\\t%w0, %w1"
-  [(set_attr "v8type" "fmovi2f,fmovf2i,\
-		       fmov,fconst,fpsimd_load,\
-		       fpsimd_store,fpsimd_load,fpsimd_store,fmov")
-   (set_attr "mode" "SF")]
+  [(set_attr "type" "f_mcr,f_mrc,fmov,fconsts,\
+                     f_loads,f_stores,f_loads,f_stores,fmov")]
 )
 
 (define_insn "*movdf_aarch64"
@@ -951,10 +787,8 @@
    ldr\\t%x0, %1
    str\\t%x1, %0
    mov\\t%x0, %x1"
-  [(set_attr "v8type" "fmovi2f,fmovf2i,\
-		       fmov,fconst,fpsimd_load,\
-		       fpsimd_store,fpsimd_load,fpsimd_store,move")
-   (set_attr "mode" "DF")]
+  [(set_attr "type" "f_mcr,f_mrc,fmov,fconstd,\
+                     f_loadd,f_stored,f_loadd,f_stored,mov_reg")]
 )
 
 (define_expand "movtf"
@@ -982,22 +816,33 @@
     || register_operand (operands[1], TFmode))"
   "@
    orr\\t%0.16b, %1.16b, %1.16b
-   mov\\t%0, %1\;mov\\t%H0, %H1
-   fmov\\t%d0, %Q1\;fmov\\t%0.d[1], %R1
-   fmov\\t%Q0, %d1\;fmov\\t%R0, %1.d[1]
+   #
+   #
+   #
    movi\\t%0.2d, #0
    fmov\\t%s0, wzr
    ldr\\t%q0, %1
    str\\t%q1, %0
    ldp\\t%0, %H0, %1
    stp\\t%1, %H1, %0"
-  [(set_attr "v8type" "logic,move2,fmovi2f,fmovf2i,fconst,fconst,fpsimd_load,fpsimd_store,fpsimd_load2,fpsimd_store2")
-   (set_attr "mode" "DF,DF,DF,DF,DF,DF,TF,TF,DF,DF")
+  [(set_attr "type" "logic_reg,multiple,f_mcr,f_mrc,fconstd,fconstd,\
+                     f_loadd,f_stored,neon_load1_2reg,neon_store1_2reg")
    (set_attr "length" "4,8,8,8,4,4,4,4,4,4")
    (set_attr "fp" "*,*,yes,yes,*,yes,yes,yes,*,*")
    (set_attr "simd" "yes,*,*,*,yes,*,*,*,*,*")]
 )
 
+(define_split
+   [(set (match_operand:TF 0 "register_operand" "")
+	 (match_operand:TF 1 "aarch64_reg_or_imm" ""))]
+  "reload_completed && aarch64_split_128bit_move_p (operands[0], operands[1])"
+  [(const_int 0)]
+  {
+    aarch64_split_128bit_move (operands[0], operands[1]);
+    DONE;
+  }
+)
+
 ;; Operands 1 and 3 are tied together by the final condition; so we allow
 ;; fairly lax checking on the second memory operation.
 (define_insn "load_pair<mode>"
@@ -1010,8 +855,7 @@
 			       XEXP (operands[1], 0),
 			       GET_MODE_SIZE (<MODE>mode)))"
   "ldp\\t%<w>0, %<w>2, %1"
-  [(set_attr "v8type" "load2")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "load2")]
 )
 
 ;; Operands 0 and 2 are tied together by the final condition; so we allow
@@ -1026,8 +870,7 @@
 			       XEXP (operands[0], 0),
 			       GET_MODE_SIZE (<MODE>mode)))"
   "stp\\t%<w>1, %<w>3, %0"
-  [(set_attr "v8type" "store2")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "store2")]
 )
 
 ;; Operands 1 and 3 are tied together by the final condition; so we allow
@@ -1042,8 +885,7 @@
 			       XEXP (operands[1], 0),
 			       GET_MODE_SIZE (<MODE>mode)))"
   "ldp\\t%<w>0, %<w>2, %1"
-  [(set_attr "v8type" "fpsimd_load2")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "neon_load1_2reg<q>")]
 )
 
 ;; Operands 0 and 2 are tied together by the final condition; so we allow
@@ -1058,8 +900,7 @@
 			       XEXP (operands[0], 0),
 			       GET_MODE_SIZE (<MODE>mode)))"
   "stp\\t%<w>1, %<w>3, %0"
-  [(set_attr "v8type" "fpsimd_load2")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "neon_store1_2reg<q>")]
 )
 
 ;; Load pair with writeback.  This is primarily used in function epilogues
@@ -1077,8 +918,7 @@
                    (match_operand:PTR 5 "const_int_operand" "n"))))])]
   "INTVAL (operands[5]) == INTVAL (operands[4]) + GET_MODE_SIZE (<GPI:MODE>mode)"
   "ldp\\t%<w>2, %<w>3, [%1], %4"
-  [(set_attr "v8type" "load2")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "load2")]
 )
 
 ;; Store pair with writeback.  This is primarily used in function prologues
@@ -1096,8 +936,7 @@
           (match_operand:GPI 3 "register_operand" "r"))])]
   "INTVAL (operands[5]) == INTVAL (operands[4]) + GET_MODE_SIZE (<GPI:MODE>mode)"
   "stp\\t%<w>2, %<w>3, [%0, %4]!"
-  [(set_attr "v8type" "store2")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "store2")]
 )
 
 ;; -------------------------------------------------------------------
@@ -1117,8 +956,7 @@
   "@
    sxtw\t%0, %w1
    ldrsw\t%0, %1"
-  [(set_attr "v8type" "extend,load1")
-   (set_attr "mode" "DI")]
+  [(set_attr "type" "extend,load1")]
 )
 
 (define_insn "*zero_extendsidi2_aarch64"
@@ -1128,8 +966,7 @@
   "@
    uxtw\t%0, %w1
    ldr\t%w0, %1"
-  [(set_attr "v8type" "extend,load1")
-   (set_attr "mode" "DI")]
+  [(set_attr "type" "extend,load1")]
 )
 
 (define_expand "<ANY_EXTEND:optab><SHORT:mode><GPI:mode>2"
@@ -1145,19 +982,18 @@
   "@
    sxt<SHORT:size>\t%<GPI:w>0, %w1
    ldrs<SHORT:size>\t%<GPI:w>0, %1"
-  [(set_attr "v8type" "extend,load1")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "extend,load1")]
 )
 
 (define_insn "*zero_extend<SHORT:mode><GPI:mode>2_aarch64"
-  [(set (match_operand:GPI 0 "register_operand" "=r,r")
-        (zero_extend:GPI (match_operand:SHORT 1 "nonimmediate_operand" "r,m")))]
+  [(set (match_operand:GPI 0 "register_operand" "=r,r,*w")
+        (zero_extend:GPI (match_operand:SHORT 1 "nonimmediate_operand" "r,m,m")))]
   ""
   "@
    uxt<SHORT:size>\t%<GPI:w>0, %w1
-   ldr<SHORT:size>\t%w0, %1"
-  [(set_attr "v8type" "extend,load1")
-   (set_attr "mode" "<GPI:MODE>")]
+   ldr<SHORT:size>\t%w0, %1
+   ldr\t%<SHORT:size>0, %1"
+  [(set_attr "type" "extend,load1,load1")]
 )
 
 (define_expand "<optab>qihi2"
@@ -1173,8 +1009,7 @@
   "@
    <su>xtb\t%w0, %w1
    <ldrxt>b\t%w0, %1"
-  [(set_attr "v8type" "extend,load1")
-   (set_attr "mode" "HI")]
+  [(set_attr "type" "extend,load1")]
 )
 
 ;; -------------------------------------------------------------------
@@ -1217,8 +1052,7 @@
   add\\t%w0, %w1, %2
   add\\t%w0, %w1, %w2
   sub\\t%w0, %w1, #%n2"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_imm,alu_reg,alu_imm")]
 )
 
 ;; zero_extend version of above
@@ -1233,8 +1067,7 @@
   add\\t%w0, %w1, %2
   add\\t%w0, %w1, %w2
   sub\\t%w0, %w1, #%n2"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_imm,alu_reg,alu_imm")]
 )
 
 (define_insn "*adddi3_aarch64"
@@ -1249,67 +1082,165 @@
   add\\t%x0, %x1, %x2
   sub\\t%x0, %x1, #%n2
   add\\t%d0, %d1, %d2"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "DI")
+  [(set_attr "type" "alu_imm,alu_reg,alu_imm,alu_reg")
    (set_attr "simd" "*,*,*,yes")]
 )
 
 (define_insn "*add<mode>3_compare0"
   [(set (reg:CC_NZ CC_REGNUM)
 	(compare:CC_NZ
-	 (plus:GPI (match_operand:GPI 1 "register_operand" "%r,r")
-		   (match_operand:GPI 2 "aarch64_plus_operand" "rI,J"))
+	 (plus:GPI (match_operand:GPI 1 "register_operand" "%r,r,r")
+		   (match_operand:GPI 2 "aarch64_plus_operand" "r,I,J"))
 	 (const_int 0)))
-   (set (match_operand:GPI 0 "register_operand" "=r,r")
+   (set (match_operand:GPI 0 "register_operand" "=r,r,r")
 	(plus:GPI (match_dup 1) (match_dup 2)))]
   ""
   "@
   adds\\t%<w>0, %<w>1, %<w>2
+  adds\\t%<w>0, %<w>1, %<w>2
   subs\\t%<w>0, %<w>1, #%n2"
-  [(set_attr "v8type" "alus")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alus_reg,alus_imm,alus_imm")]
 )
 
 ;; zero_extend version of above
 (define_insn "*addsi3_compare0_uxtw"
   [(set (reg:CC_NZ CC_REGNUM)
 	(compare:CC_NZ
-	 (plus:SI (match_operand:SI 1 "register_operand" "%r,r")
-		  (match_operand:SI 2 "aarch64_plus_operand" "rI,J"))
+	 (plus:SI (match_operand:SI 1 "register_operand" "%r,r,r")
+		  (match_operand:SI 2 "aarch64_plus_operand" "r,I,J"))
 	 (const_int 0)))
-   (set (match_operand:DI 0 "register_operand" "=r,r")
+   (set (match_operand:DI 0 "register_operand" "=r,r,r")
 	(zero_extend:DI (plus:SI (match_dup 1) (match_dup 2))))]
   ""
   "@
   adds\\t%w0, %w1, %w2
+  adds\\t%w0, %w1, %w2
   subs\\t%w0, %w1, #%n2"
-  [(set_attr "v8type" "alus")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alus_reg,alus_imm,alus_imm")]
+)
+
+(define_insn "*adds_mul_imm_<mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (plus:GPI (mult:GPI
+		    (match_operand:GPI 1 "register_operand" "r")
+		    (match_operand:QI 2 "aarch64_pwr_2_<mode>" "n"))
+		   (match_operand:GPI 3 "register_operand" "rk"))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(plus:GPI (mult:GPI (match_dup 1) (match_dup 2))
+		  (match_dup 3)))]
+  ""
+  "adds\\t%<w>0, %<w>3, %<w>1, lsl %p2"
+  [(set_attr "type" "alus_shift_imm")]
+)
+
+(define_insn "*subs_mul_imm_<mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (minus:GPI (match_operand:GPI 1 "register_operand" "rk")
+		    (mult:GPI
+		     (match_operand:GPI 2 "register_operand" "r")
+		     (match_operand:QI 3 "aarch64_pwr_2_<mode>" "n")))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(minus:GPI (match_dup 1)
+		   (mult:GPI (match_dup 2) (match_dup 3))))]
+  ""
+  "subs\\t%<w>0, %<w>1, %<w>2, lsl %p3"
+  [(set_attr "type" "alus_shift_imm")]
+)
+
+(define_insn "*adds_<optab><ALLX:mode>_<GPI:mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (plus:GPI
+	  (ANY_EXTEND:GPI (match_operand:ALLX 1 "register_operand" "r"))
+	  (match_operand:GPI 2 "register_operand" "r"))
+	(const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(plus:GPI (ANY_EXTEND:GPI (match_dup 1)) (match_dup 2)))]
+  ""
+  "adds\\t%<GPI:w>0, %<GPI:w>2, %<GPI:w>1, <su>xt<ALLX:size>"
+  [(set_attr "type" "alus_ext")]
+)
+
+(define_insn "*subs_<optab><ALLX:mode>_<GPI:mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (minus:GPI (match_operand:GPI 1 "register_operand" "r")
+		    (ANY_EXTEND:GPI
+		     (match_operand:ALLX 2 "register_operand" "r")))
+	(const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(minus:GPI (match_dup 1) (ANY_EXTEND:GPI (match_dup 2))))]
+  ""
+  "subs\\t%<GPI:w>0, %<GPI:w>1, %<GPI:w>2, <su>xt<ALLX:size>"
+  [(set_attr "type" "alus_ext")]
+)
+
+(define_insn "*adds_<optab><mode>_multp2"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (plus:GPI (ANY_EXTRACT:GPI
+		    (mult:GPI (match_operand:GPI 1 "register_operand" "r")
+			      (match_operand 2 "aarch64_pwr_imm3" "Up3"))
+		    (match_operand 3 "const_int_operand" "n")
+		    (const_int 0))
+		   (match_operand:GPI 4 "register_operand" "r"))
+	(const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(plus:GPI (ANY_EXTRACT:GPI (mult:GPI (match_dup 1) (match_dup 2))
+				   (match_dup 3)
+				   (const_int 0))
+		  (match_dup 4)))]
+  "aarch64_is_extend_from_extract (<MODE>mode, operands[2], operands[3])"
+  "adds\\t%<w>0, %<w>4, %<w>1, <su>xt%e3 %p2"
+  [(set_attr "type" "alus_ext")]
+)
+
+(define_insn "*subs_<optab><mode>_multp2"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (minus:GPI (match_operand:GPI 4 "register_operand" "r")
+		    (ANY_EXTRACT:GPI
+		     (mult:GPI (match_operand:GPI 1 "register_operand" "r")
+			       (match_operand 2 "aarch64_pwr_imm3" "Up3"))
+		     (match_operand 3 "const_int_operand" "n")
+		     (const_int 0)))
+	(const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(minus:GPI (match_dup 4) (ANY_EXTRACT:GPI
+				  (mult:GPI (match_dup 1) (match_dup 2))
+				  (match_dup 3)
+				  (const_int 0))))]
+  "aarch64_is_extend_from_extract (<MODE>mode, operands[2], operands[3])"
+  "subs\\t%<w>0, %<w>4, %<w>1, <su>xt%e3 %p2"
+  [(set_attr "type" "alus_ext")]
 )
 
 (define_insn "*add<mode>3nr_compare0"
   [(set (reg:CC_NZ CC_REGNUM)
 	(compare:CC_NZ
-	 (plus:GPI (match_operand:GPI 0 "register_operand" "%r,r")
-		   (match_operand:GPI 1 "aarch64_plus_operand" "rI,J"))
+	 (plus:GPI (match_operand:GPI 0 "register_operand" "%r,r,r")
+		   (match_operand:GPI 1 "aarch64_plus_operand" "r,I,J"))
 	 (const_int 0)))]
   ""
   "@
   cmn\\t%<w>0, %<w>1
+  cmn\\t%<w>0, %<w>1
   cmp\\t%<w>0, #%n1"
-  [(set_attr "v8type" "alus")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alus_reg,alus_imm,alus_imm")]
 )
 
 (define_insn "*compare_neg<mode>"
-  [(set (reg:CC CC_REGNUM)
-	(compare:CC
-	 (match_operand:GPI 0 "register_operand" "r")
-	 (neg:GPI (match_operand:GPI 1 "register_operand" "r"))))]
-  ""
-  "cmn\\t%<w>0, %<w>1"
-  [(set_attr "v8type" "alus")
-   (set_attr "mode" "<MODE>")]
+  [(set (reg:CC_Z CC_REGNUM)
+	(compare:CC_Z
+	 (neg:GPI (match_operand:GPI 0 "register_operand" "r"))
+	 (match_operand:GPI 1 "register_operand" "r")))]
+  ""
+  "cmn\\t%<w>1, %<w>0"
+  [(set_attr "type" "alus_reg")]
 )
 
 (define_insn "*add_<shift>_<mode>"
@@ -1319,8 +1250,7 @@
 		  (match_operand:GPI 3 "register_operand" "r")))]
   ""
   "add\\t%<w>0, %<w>3, %<w>1, <shift> %2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 ;; zero_extend version of above
@@ -1332,8 +1262,7 @@
 	          (match_operand:SI 3 "register_operand" "r"))))]
   ""
   "add\\t%w0, %w3, %w1, <shift> %2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 (define_insn "*add_mul_imm_<mode>"
@@ -1343,8 +1272,7 @@
 		  (match_operand:GPI 3 "register_operand" "r")))]
   ""
   "add\\t%<w>0, %<w>3, %<w>1, lsl %p2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 (define_insn "*add_<optab><ALLX:mode>_<GPI:mode>"
@@ -1353,8 +1281,7 @@
 		  (match_operand:GPI 2 "register_operand" "r")))]
   ""
   "add\\t%<GPI:w>0, %<GPI:w>2, %<GPI:w>1, <su>xt<ALLX:size>"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "alu_ext")]
 )
 
 ;; zero_extend version of above
@@ -1365,8 +1292,7 @@
 		  (match_operand:GPI 2 "register_operand" "r"))))]
   ""
   "add\\t%w0, %w2, %w1, <su>xt<SHORT:size>"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_ext")]
 )
 
 (define_insn "*add_<optab><ALLX:mode>_shft_<GPI:mode>"
@@ -1377,8 +1303,7 @@
 		  (match_operand:GPI 3 "register_operand" "r")))]
   ""
   "add\\t%<GPI:w>0, %<GPI:w>3, %<GPI:w>1, <su>xt<ALLX:size> %2"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "alu_ext")]
 )
 
 ;; zero_extend version of above
@@ -1391,8 +1316,7 @@
 		  (match_operand:SI 3 "register_operand" "r"))))]
   ""
   "add\\t%w0, %w3, %w1, <su>xt<SHORT:size> %2"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_ext")]
 )
 
 (define_insn "*add_<optab><ALLX:mode>_mult_<GPI:mode>"
@@ -1403,8 +1327,7 @@
 		  (match_operand:GPI 3 "register_operand" "r")))]
   ""
   "add\\t%<GPI:w>0, %<GPI:w>3, %<GPI:w>1, <su>xt<ALLX:size> %p2"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "alu_ext")]
 )
 
 ;; zero_extend version of above
@@ -1416,8 +1339,7 @@
 		  (match_operand:SI 3 "register_operand" "r"))))]
   ""
   "add\\t%w0, %w3, %w1, <su>xt<SHORT:size> %p2"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_ext")]
 )
 
 (define_insn "*add_<optab><mode>_multp2"
@@ -1430,8 +1352,7 @@
 		  (match_operand:GPI 4 "register_operand" "r")))]
   "aarch64_is_extend_from_extract (<MODE>mode, operands[2], operands[3])"
   "add\\t%<w>0, %<w>4, %<w>1, <su>xt%e3 %p2"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_ext")]
 )
 
 ;; zero_extend version of above
@@ -1446,8 +1367,7 @@
 		  (match_operand:SI 4 "register_operand" "r"))))]
   "aarch64_is_extend_from_extract (SImode, operands[2], operands[3])"
   "add\\t%w0, %w4, %w1, <su>xt%e3 %p2"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_ext")]
 )
 
 (define_insn "*add<mode>3_carryin"
@@ -1459,8 +1379,7 @@
 		(match_operand:GPI 2 "register_operand" "r"))))]
    ""
    "adc\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "adc")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "adc_reg")]
 )
 
 ;; zero_extend version of above
@@ -1474,8 +1393,7 @@
 	       (match_operand:SI 2 "register_operand" "r")))))]
    ""
    "adc\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "adc")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "adc_reg")]
 )
 
 (define_insn "*add<mode>3_carryin_alt1"
@@ -1487,8 +1405,7 @@
               (geu:GPI (reg:CC CC_REGNUM) (const_int 0))))]
    ""
    "adc\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "adc")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "adc_reg")]
 )
 
 ;; zero_extend version of above
@@ -1502,8 +1419,7 @@
               (geu:SI (reg:CC CC_REGNUM) (const_int 0)))))]
    ""
    "adc\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "adc")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "adc_reg")]
 )
 
 (define_insn "*add<mode>3_carryin_alt2"
@@ -1515,8 +1431,7 @@
 	      (match_operand:GPI 2 "register_operand" "r")))]
    ""
    "adc\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "adc")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "adc_reg")]
 )
 
 ;; zero_extend version of above
@@ -1530,8 +1445,7 @@
 	      (match_operand:SI 2 "register_operand" "r"))))]
    ""
    "adc\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "adc")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "adc_reg")]
 )
 
 (define_insn "*add<mode>3_carryin_alt3"
@@ -1543,8 +1457,7 @@
 	      (match_operand:GPI 1 "register_operand" "r")))]
    ""
    "adc\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "adc")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "adc_reg")]
 )
 
 ;; zero_extend version of above
@@ -1558,8 +1471,7 @@
 	      (match_operand:SI 1 "register_operand" "r"))))]
    ""
    "adc\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "adc")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "adc_reg")]
 )
 
 (define_insn "*add_uxt<mode>_multp2"
@@ -1574,8 +1486,7 @@
   operands[3] = GEN_INT (aarch64_uxt_size (exact_log2 (INTVAL (operands[2])),
 					   INTVAL (operands[3])));
   return \"add\t%<w>0, %<w>4, %<w>1, uxt%e3 %p2\";"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_ext")]
 )
 
 ;; zero_extend version of above
@@ -1592,8 +1503,7 @@
   operands[3] = GEN_INT (aarch64_uxt_size (exact_log2 (INTVAL (operands[2])),
 					   INTVAL (operands[3])));
   return \"add\t%w0, %w4, %w1, uxt%e3 %p2\";"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_ext")]
 )
 
 (define_insn "subsi3"
@@ -1602,8 +1512,7 @@
 		   (match_operand:SI 2 "register_operand" "r")))]
   ""
   "sub\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_reg")]
 )
 
 ;; zero_extend version of above
@@ -1614,8 +1523,7 @@
 		   (match_operand:SI 2 "register_operand" "r"))))]
   ""
   "sub\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_reg")]
 )
 
 (define_insn "subdi3"
@@ -1626,8 +1534,7 @@
   "@
    sub\\t%x0, %x1, %x2
    sub\\t%d0, %d1, %d2"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "DI")
+  [(set_attr "type" "alu_reg, neon_sub")
    (set_attr "simd" "*,yes")]
 )
 
@@ -1641,8 +1548,7 @@
 	(minus:GPI (match_dup 1) (match_dup 2)))]
   ""
   "subs\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "alus")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alus_reg")]
 )
 
 ;; zero_extend version of above
@@ -1655,8 +1561,7 @@
 	(zero_extend:DI (minus:SI (match_dup 1) (match_dup 2))))]
   ""
   "subs\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "alus")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alus_reg")]
 )
 
 (define_insn "*sub_<shift>_<mode>"
@@ -1667,8 +1572,7 @@
 		    (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n"))))]
   ""
   "sub\\t%<w>0, %<w>3, %<w>1, <shift> %2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 ;; zero_extend version of above
@@ -1681,8 +1585,7 @@
 		    (match_operand:QI 2 "aarch64_shift_imm_si" "n")))))]
   ""
   "sub\\t%w0, %w3, %w1, <shift> %2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 (define_insn "*sub_mul_imm_<mode>"
@@ -1693,8 +1596,7 @@
 		    (match_operand:QI 2 "aarch64_pwr_2_<mode>" "n"))))]
   ""
   "sub\\t%<w>0, %<w>3, %<w>1, lsl %p2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 ;; zero_extend version of above
@@ -1707,8 +1609,7 @@
 		    (match_operand:QI 2 "aarch64_pwr_2_si" "n")))))]
   ""
   "sub\\t%w0, %w3, %w1, lsl %p2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 (define_insn "*sub_<optab><ALLX:mode>_<GPI:mode>"
@@ -1718,8 +1619,7 @@
 		    (match_operand:ALLX 2 "register_operand" "r"))))]
   ""
   "sub\\t%<GPI:w>0, %<GPI:w>1, %<GPI:w>2, <su>xt<ALLX:size>"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "alu_ext")]
 )
 
 ;; zero_extend version of above
@@ -1731,8 +1631,7 @@
 		    (match_operand:SHORT 2 "register_operand" "r")))))]
   ""
   "sub\\t%w0, %w1, %w2, <su>xt<SHORT:size>"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_ext")]
 )
 
 (define_insn "*sub_<optab><ALLX:mode>_shft_<GPI:mode>"
@@ -1743,8 +1642,7 @@
 			       (match_operand 3 "aarch64_imm3" "Ui3"))))]
   ""
   "sub\\t%<GPI:w>0, %<GPI:w>1, %<GPI:w>2, <su>xt<ALLX:size> %3"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "alu_ext")]
 )
 
 ;; zero_extend version of above
@@ -1757,8 +1655,7 @@
 			      (match_operand 3 "aarch64_imm3" "Ui3")))))]
   ""
   "sub\\t%w0, %w1, %w2, <su>xt<SHORT:size> %3"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_ext")]
 )
 
 (define_insn "*sub_<optab><mode>_multp2"
@@ -1771,8 +1668,7 @@
 		    (const_int 0))))]
   "aarch64_is_extend_from_extract (<MODE>mode, operands[2], operands[3])"
   "sub\\t%<w>0, %<w>4, %<w>1, <su>xt%e3 %p2"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_ext")]
 )
 
 ;; zero_extend version of above
@@ -1787,8 +1683,33 @@
 		    (const_int 0)))))]
   "aarch64_is_extend_from_extract (SImode, operands[2], operands[3])"
   "sub\\t%w0, %w4, %w1, <su>xt%e3 %p2"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_ext")]
+)
+
+(define_insn "*sub<mode>3_carryin"
+  [(set
+    (match_operand:GPI 0 "register_operand" "=r")
+    (minus:GPI (minus:GPI
+		(match_operand:GPI 1 "register_operand" "r")
+		(ltu:GPI (reg:CC CC_REGNUM) (const_int 0)))
+	       (match_operand:GPI 2 "register_operand" "r")))]
+   ""
+   "sbc\\t%<w>0, %<w>1, %<w>2"
+  [(set_attr "type" "adc_reg")]
+)
+
+;; zero_extend version of the above
+(define_insn "*subsi3_carryin_uxtw"
+  [(set
+    (match_operand:DI 0 "register_operand" "=r")
+    (zero_extend:DI
+     (minus:SI (minus:SI
+		(match_operand:SI 1 "register_operand" "r")
+		(ltu:SI (reg:CC CC_REGNUM) (const_int 0)))
+	       (match_operand:SI 2 "register_operand" "r"))))]
+   ""
+   "sbc\\t%w0, %w1, %w2"
+  [(set_attr "type" "adc_reg")]
 )
 
 (define_insn "*sub_uxt<mode>_multp2"
@@ -1803,8 +1724,7 @@
   operands[3] = GEN_INT (aarch64_uxt_size (exact_log2 (INTVAL (operands[2])),
 					   INTVAL (operands[3])));
   return \"sub\t%<w>0, %<w>4, %<w>1, uxt%e3 %p2\";"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_ext")]
 )
 
 ;; zero_extend version of above
@@ -1821,8 +1741,38 @@
   operands[3] = GEN_INT (aarch64_uxt_size (exact_log2 (INTVAL (operands[2])),
 					   INTVAL (operands[3])));
   return \"sub\t%w0, %w4, %w1, uxt%e3 %p2\";"
-  [(set_attr "v8type" "alu_ext")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_ext")]
+)
+
+(define_insn_and_split "absdi2"
+  [(set (match_operand:DI 0 "register_operand" "=r,w")
+	(abs:DI (match_operand:DI 1 "register_operand" "r,w")))
+   (clobber (match_scratch:DI 2 "=&r,X"))]
+  ""
+  "@
+   #
+   abs\\t%d0, %d1"
+  "reload_completed
+   && GP_REGNUM_P (REGNO (operands[0]))
+   && GP_REGNUM_P (REGNO (operands[1]))"
+  [(const_int 0)]
+  {
+    emit_insn (gen_rtx_SET (VOIDmode, operands[2],
+			    gen_rtx_XOR (DImode,
+					 gen_rtx_ASHIFTRT (DImode,
+							   operands[1],
+							   GEN_INT (63)),
+					 operands[1])));
+    emit_insn (gen_rtx_SET (VOIDmode,
+			    operands[0],
+			    gen_rtx_MINUS (DImode,
+					   operands[2],
+					   gen_rtx_ASHIFTRT (DImode,
+							     operands[1],
+							     GEN_INT (63)))));
+    DONE;
+  }
+  [(set_attr "type" "alu_reg")]
 )
 
 (define_insn "neg<mode>2"
@@ -1830,8 +1780,8 @@
 	(neg:GPI (match_operand:GPI 1 "register_operand" "r")))]
   ""
   "neg\\t%<w>0, %<w>1"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_reg")
+   (set_attr "simd" "*")]
 )
 
 ;; zero_extend version of above
@@ -1840,8 +1790,26 @@
 	(zero_extend:DI (neg:SI (match_operand:SI 1 "register_operand" "r"))))]
   ""
   "neg\\t%w0, %w1"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_reg")]
+)
+
+(define_insn "*ngc<mode>"
+  [(set (match_operand:GPI 0 "register_operand" "=r")
+	(minus:GPI (neg:GPI (ltu:GPI (reg:CC CC_REGNUM) (const_int 0)))
+		   (match_operand:GPI 1 "register_operand" "r")))]
+  ""
+  "ngc\\t%<w>0, %<w>1"
+  [(set_attr "type" "adc_reg")]
+)
+
+(define_insn "*ngcsi_uxtw"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI
+	 (minus:SI (neg:SI (ltu:SI (reg:CC CC_REGNUM) (const_int 0)))
+		   (match_operand:SI 1 "register_operand" "r"))))]
+  ""
+  "ngc\\t%w0, %w1"
+  [(set_attr "type" "adc_reg")]
 )
 
 (define_insn "*neg<mode>2_compare0"
@@ -1852,8 +1820,7 @@
 	(neg:GPI (match_dup 1)))]
   ""
   "negs\\t%<w>0, %<w>1"
-  [(set_attr "v8type" "alus")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alus_reg")]
 )
 
 ;; zero_extend version of above
@@ -1865,8 +1832,21 @@
 	(zero_extend:DI (neg:SI (match_dup 1))))]
   ""
   "negs\\t%w0, %w1"
-  [(set_attr "v8type" "alus")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alus_reg")]
+)
+
+(define_insn "*neg_<shift><mode>3_compare0"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (neg:GPI (ASHIFT:GPI
+		   (match_operand:GPI 1 "register_operand" "r")
+		   (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n")))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(neg:GPI (ASHIFT:GPI (match_dup 1) (match_dup 2))))]
+  ""
+  "negs\\t%<w>0, %<w>1, <shift> %2"
+  [(set_attr "type" "alus_shift_imm")]
 )
 
 (define_insn "*neg_<shift>_<mode>2"
@@ -1876,8 +1856,7 @@
 		  (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n"))))]
   ""
   "neg\\t%<w>0, %<w>1, <shift> %2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 ;; zero_extend version of above
@@ -1889,8 +1868,7 @@
 		  (match_operand:QI 2 "aarch64_shift_imm_si" "n")))))]
   ""
   "neg\\t%w0, %w1, <shift> %2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 (define_insn "*neg_mul_imm_<mode>2"
@@ -1900,8 +1878,7 @@
 		  (match_operand:QI 2 "aarch64_pwr_2_<mode>" "n"))))]
   ""
   "neg\\t%<w>0, %<w>1, lsl %p2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 ;; zero_extend version of above
@@ -1913,8 +1890,7 @@
 		  (match_operand:QI 2 "aarch64_pwr_2_si" "n")))))]
   ""
   "neg\\t%w0, %w1, lsl %p2"
-  [(set_attr "v8type" "alu_shift")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "alu_shift_imm")]
 )
 
 (define_insn "mul<mode>3"
@@ -1923,8 +1899,7 @@
 		  (match_operand:GPI 2 "register_operand" "r")))]
   ""
   "mul\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "mult")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "mul")]
 )
 
 ;; zero_extend version of above
@@ -1935,8 +1910,7 @@
 		  (match_operand:SI 2 "register_operand" "r"))))]
   ""
   "mul\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "mult")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "mul")]
 )
 
 (define_insn "*madd<mode>"
@@ -1946,8 +1920,7 @@
 		  (match_operand:GPI 3 "register_operand" "r")))]
   ""
   "madd\\t%<w>0, %<w>1, %<w>2, %<w>3"
-  [(set_attr "v8type" "madd")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "mla")]
 )
 
 ;; zero_extend version of above
@@ -1959,8 +1932,7 @@
 		  (match_operand:SI 3 "register_operand" "r"))))]
   ""
   "madd\\t%w0, %w1, %w2, %w3"
-  [(set_attr "v8type" "madd")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "mla")]
 )
 
 (define_insn "*msub<mode>"
@@ -1971,8 +1943,7 @@
 
   ""
   "msub\\t%<w>0, %<w>1, %<w>2, %<w>3"
-  [(set_attr "v8type" "madd")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "mla")]
 )
 
 ;; zero_extend version of above
@@ -1985,8 +1956,7 @@
 
   ""
   "msub\\t%w0, %w1, %w2, %w3"
-  [(set_attr "v8type" "madd")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "mla")]
 )
 
 (define_insn "*mul<mode>_neg"
@@ -1996,8 +1966,7 @@
 
   ""
   "mneg\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "mult")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "mul")]
 )
 
 ;; zero_extend version of above
@@ -2009,8 +1978,7 @@
 
   ""
   "mneg\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "mult")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "mul")]
 )
 
 (define_insn "<su_optab>mulsidi3"
@@ -2019,8 +1987,7 @@
 		 (ANY_EXTEND:DI (match_operand:SI 2 "register_operand" "r"))))]
   ""
   "<su>mull\\t%0, %w1, %w2"
-  [(set_attr "v8type" "mull")
-   (set_attr "mode" "DI")]
+  [(set_attr "type" "<su>mull")]
 )
 
 (define_insn "<su_optab>maddsidi4"
@@ -2031,8 +1998,7 @@
 		 (match_operand:DI 3 "register_operand" "r")))]
   ""
   "<su>maddl\\t%0, %w1, %w2, %3"
-  [(set_attr "v8type" "maddl")
-   (set_attr "mode" "DI")]
+  [(set_attr "type" "<su>mlal")]
 )
 
 (define_insn "<su_optab>msubsidi4"
@@ -2044,8 +2010,7 @@
 		   (match_operand:SI 2 "register_operand" "r")))))]
   ""
   "<su>msubl\\t%0, %w1, %w2, %3"
-  [(set_attr "v8type" "maddl")
-   (set_attr "mode" "DI")]
+  [(set_attr "type" "<su>mlal")]
 )
 
 (define_insn "*<su_optab>mulsidi_neg"
@@ -2055,8 +2020,7 @@
 		  (ANY_EXTEND:DI (match_operand:SI 2 "register_operand" "r"))))]
   ""
   "<su>mnegl\\t%0, %w1, %w2"
-  [(set_attr "v8type" "mull")
-   (set_attr "mode" "DI")]
+  [(set_attr "type" "<su>mull")]
 )
 
 (define_insn "<su>muldi3_highpart"
@@ -2069,8 +2033,7 @@
 	  (const_int 64))))]
   ""
   "<su>mulh\\t%0, %1, %2"
-  [(set_attr "v8type" "mulh")
-   (set_attr "mode" "DI")]
+  [(set_attr "type" "<su>mull")]
 )
 
 (define_insn "<su_optab>div<mode>3"
@@ -2079,8 +2042,7 @@
 		     (match_operand:GPI 2 "register_operand" "r")))]
   ""
   "<su>div\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "<su>div")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "<su>div")]
 )
 
 ;; zero_extend version of above
@@ -2091,8 +2053,7 @@
 		     (match_operand:SI 2 "register_operand" "r"))))]
   ""
   "<su>div\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "<su>div")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "<su>div")]
 )
 
 ;; -------------------------------------------------------------------
@@ -2101,14 +2062,14 @@
 
 (define_insn "*cmp<mode>"
   [(set (reg:CC CC_REGNUM)
-	(compare:CC (match_operand:GPI 0 "register_operand" "r,r")
-		    (match_operand:GPI 1 "aarch64_plus_operand" "rI,J")))]
+	(compare:CC (match_operand:GPI 0 "register_operand" "r,r,r")
+		    (match_operand:GPI 1 "aarch64_plus_operand" "r,I,J")))]
   ""
   "@
    cmp\\t%<w>0, %<w>1
+   cmp\\t%<w>0, %<w>1
    cmn\\t%<w>0, #%n1"
-  [(set_attr "v8type" "alus")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alus_reg,alus_imm,alus_imm")]
 )
 
 (define_insn "*cmp<mode>"
@@ -2119,8 +2080,7 @@
    "@
     fcmp\\t%<s>0, #0.0
     fcmp\\t%<s>0, %<s>1"
-  [(set_attr "v8type" "fcmp")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fcmp<s>")]
 )
 
 (define_insn "*cmpe<mode>"
@@ -2131,8 +2091,7 @@
    "@
     fcmpe\\t%<s>0, #0.0
     fcmpe\\t%<s>0, %<s>1"
-  [(set_attr "v8type" "fcmp")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fcmp<s>")]
 )
 
 (define_insn "*cmp_swp_<shift>_reg<mode>"
@@ -2143,8 +2102,7 @@
 			(match_operand:GPI 2 "aarch64_reg_or_zero" "rZ")))]
   ""
   "cmp\\t%<w>2, %<w>0, <shift> %1"
-  [(set_attr "v8type" "alus_shift")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "alus_shift_imm")]
 )
 
 (define_insn "*cmp_swp_<optab><ALLX:mode>_reg<GPI:mode>"
@@ -2154,10 +2112,20 @@
 			(match_operand:GPI 1 "register_operand" "r")))]
   ""
   "cmp\\t%<GPI:w>1, %<GPI:w>0, <su>xt<ALLX:size>"
-  [(set_attr "v8type" "alus_ext")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "alus_ext")]
 )
 
+(define_insn "*cmp_swp_<optab><ALLX:mode>_shft_<GPI:mode>"
+  [(set (reg:CC_SWP CC_REGNUM)
+	(compare:CC_SWP (ashift:GPI
+			 (ANY_EXTEND:GPI
+			  (match_operand:ALLX 0 "register_operand" "r"))
+			 (match_operand 1 "aarch64_imm3" "Ui3"))
+	(match_operand:GPI 2 "register_operand" "r")))]
+  ""
+  "cmp\\t%<GPI:w>2, %<GPI:w>0, <su>xt<ALLX:size> %1"
+  [(set_attr "type" "alus_ext")]
+)
 
 ;; -------------------------------------------------------------------
 ;; Store-flag and conditional select insns
@@ -2195,8 +2163,7 @@
 	 [(match_operand 2 "cc_register" "") (const_int 0)]))]
   ""
   "cset\\t%<w>0, %m1"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "csel")]
 )
 
 ;; zero_extend version of the above
@@ -2207,8 +2174,7 @@
 	  [(match_operand 2 "cc_register" "") (const_int 0)])))]
   ""
   "cset\\t%w0, %m1"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "csel")]
 )
 
 (define_insn "cstore<mode>_neg"
@@ -2217,8 +2183,7 @@
 		  [(match_operand 2 "cc_register" "") (const_int 0)])))]
   ""
   "csetm\\t%<w>0, %m1"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "csel")]
 )
 
 ;; zero_extend version of the above
@@ -2229,8 +2194,7 @@
 		  [(match_operand 2 "cc_register" "") (const_int 0)]))))]
   ""
   "csetm\\t%w0, %m1"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "csel")]
 )
 
 (define_expand "cmov<mode>6"
@@ -2283,8 +2247,7 @@
    csinc\\t%<w>0, %<w>4, <w>zr, %M1
    mov\\t%<w>0, -1
    mov\\t%<w>0, 1"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "csel")]
 )
 
 ;; zero_extend version of above
@@ -2307,8 +2270,7 @@
    csinc\\t%w0, %w4, wzr, %M1
    mov\\t%w0, -1
    mov\\t%w0, 1"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "csel")]
 )
 
 (define_insn "*cmov<mode>_insn"
@@ -2320,8 +2282,7 @@
 	 (match_operand:GPF 4 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fcsel\\t%<s>0, %<s>3, %<s>4, %m1"
-  [(set_attr "v8type" "fcsel")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fcsel")]
 )
 
 (define_expand "mov<mode>cc"
@@ -2369,8 +2330,8 @@
 		 (match_operand:GPI 1 "register_operand" "r")))]
   ""
   "csinc\\t%<w>0, %<w>1, %<w>1, %M2"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "csel")]
+)
 
 (define_insn "csinc3<mode>_insn"
   [(set (match_operand:GPI 0 "register_operand" "=r")
@@ -2382,8 +2343,7 @@
 	  (match_operand:GPI 4 "aarch64_reg_or_zero" "rZ")))]
   ""
   "csinc\\t%<w>0, %<w>4, %<w>3, %M1"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "csel")]
 )
 
 (define_insn "*csinv3<mode>_insn"
@@ -2395,8 +2355,8 @@
 	  (match_operand:GPI 4 "aarch64_reg_or_zero" "rZ")))]
   ""
   "csinv\\t%<w>0, %<w>4, %<w>3, %M1"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "csel")]
+)
 
 (define_insn "*csneg3<mode>_insn"
   [(set (match_operand:GPI 0 "register_operand" "=r")
@@ -2407,8 +2367,8 @@
 	  (match_operand:GPI 4 "aarch64_reg_or_zero" "rZ")))]
   ""
   "csneg\\t%<w>0, %<w>4, %<w>3, %M1"
-  [(set_attr "v8type" "csel")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "csel")]
+)
 
 ;; -------------------------------------------------------------------
 ;; Logical operations
@@ -2420,8 +2380,8 @@
 		     (match_operand:GPI 2 "aarch64_logical_operand" "r,<lconst>")))]
   ""
   "<logical>\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "logic,logic_imm")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "logic_reg,logic_imm")]
+)
 
 ;; zero_extend version of above
 (define_insn "*<optab>si3_uxtw"
@@ -2431,8 +2391,67 @@
 		     (match_operand:SI 2 "aarch64_logical_operand" "r,K"))))]
   ""
   "<logical>\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "logic,logic_imm")
-   (set_attr "mode" "SI")])
+  [(set_attr "type" "logic_reg,logic_imm")]
+)
+
+(define_insn "*and<mode>3_compare0"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (and:GPI (match_operand:GPI 1 "register_operand" "%r,r")
+		  (match_operand:GPI 2 "aarch64_logical_operand" "r,<lconst>"))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r,r")
+	(and:GPI (match_dup 1) (match_dup 2)))]
+  ""
+  "ands\\t%<w>0, %<w>1, %<w>2"
+  [(set_attr "type" "logics_reg,logics_imm")]
+)
+
+;; zero_extend version of above
+(define_insn "*andsi3_compare0_uxtw"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (and:SI (match_operand:SI 1 "register_operand" "%r,r")
+		 (match_operand:SI 2 "aarch64_logical_operand" "r,K"))
+	 (const_int 0)))
+   (set (match_operand:DI 0 "register_operand" "=r,r")
+	(zero_extend:DI (and:SI (match_dup 1) (match_dup 2))))]
+  ""
+  "ands\\t%w0, %w1, %w2"
+  [(set_attr "type" "logics_reg,logics_imm")]
+)
+
+(define_insn "*and_<SHIFT:optab><mode>3_compare0"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (and:GPI (SHIFT:GPI
+		   (match_operand:GPI 1 "register_operand" "r")
+		   (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n"))
+		  (match_operand:GPI 3 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(and:GPI (SHIFT:GPI (match_dup 1) (match_dup 2)) (match_dup 3)))]
+  ""
+  "ands\\t%<w>0, %<w>3, %<w>1, <SHIFT:shift> %2"
+  [(set_attr "type" "logics_shift_imm")]
+)
+
+;; zero_extend version of above
+(define_insn "*and_<SHIFT:optab>si3_compare0_uxtw"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (and:SI (SHIFT:SI
+		  (match_operand:SI 1 "register_operand" "r")
+		  (match_operand:QI 2 "aarch64_shift_imm_si" "n"))
+		 (match_operand:SI 3 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI (and:SI (SHIFT:SI (match_dup 1) (match_dup 2))
+				(match_dup 3))))]
+  ""
+  "ands\\t%w0, %w3, %w1, <SHIFT:shift> %2"
+  [(set_attr "type" "logics_shift_imm")]
+)
 
 (define_insn "*<LOGICAL:optab>_<SHIFT:optab><mode>3"
   [(set (match_operand:GPI 0 "register_operand" "=r")
@@ -2442,8 +2461,8 @@
 		     (match_operand:GPI 3 "register_operand" "r")))]
   ""
   "<LOGICAL:logical>\\t%<w>0, %<w>3, %<w>1, <SHIFT:shift> %2"
-  [(set_attr "v8type" "logic_shift")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "logic_shift_imm")]
+)
 
 ;; zero_extend version of above
 (define_insn "*<LOGICAL:optab>_<SHIFT:optab>si3_uxtw"
@@ -2455,16 +2474,16 @@
 		     (match_operand:SI 3 "register_operand" "r"))))]
   ""
   "<LOGICAL:logical>\\t%w0, %w3, %w1, <SHIFT:shift> %2"
-  [(set_attr "v8type" "logic_shift")
-   (set_attr "mode" "SI")])
+  [(set_attr "type" "logic_shift_imm")]
+)
 
 (define_insn "one_cmpl<mode>2"
   [(set (match_operand:GPI 0 "register_operand" "=r")
 	(not:GPI (match_operand:GPI 1 "register_operand" "r")))]
   ""
   "mvn\\t%<w>0, %<w>1"
-  [(set_attr "v8type" "logic")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "logic_reg")]
+)
 
 (define_insn "*one_cmpl_<optab><mode>2"
   [(set (match_operand:GPI 0 "register_operand" "=r")
@@ -2472,8 +2491,8 @@
 			    (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n"))))]
   ""
   "mvn\\t%<w>0, %<w>1, <shift> %2"
-  [(set_attr "v8type" "logic_shift")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "logic_shift_imm")]
+)
 
 (define_insn "*<LOGICAL:optab>_one_cmpl<mode>3"
   [(set (match_operand:GPI 0 "register_operand" "=r")
@@ -2482,8 +2501,37 @@
 		     (match_operand:GPI 2 "register_operand" "r")))]
   ""
   "<LOGICAL:nlogical>\\t%<w>0, %<w>2, %<w>1"
-  [(set_attr "v8type" "logic")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "logic_reg")]
+)
+
+(define_insn "*and_one_cmpl<mode>3_compare0"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (and:GPI (not:GPI
+		   (match_operand:GPI 1 "register_operand" "r"))
+		  (match_operand:GPI 2 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(and:GPI (not:GPI (match_dup 1)) (match_dup 2)))]
+  ""
+  "bics\\t%<w>0, %<w>2, %<w>1"
+  [(set_attr "type" "logics_reg")]
+)
+
+;; zero_extend version of above
+(define_insn "*and_one_cmplsi3_compare0_uxtw"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (and:SI (not:SI
+		  (match_operand:SI 1 "register_operand" "r"))
+		 (match_operand:SI 2 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI (and:SI (not:SI (match_dup 1)) (match_dup 2))))]
+  ""
+  "bics\\t%w0, %w2, %w1"
+  [(set_attr "type" "logics_reg")]
+)
 
 (define_insn "*<LOGICAL:optab>_one_cmpl_<SHIFT:optab><mode>3"
   [(set (match_operand:GPI 0 "register_operand" "=r")
@@ -2494,16 +2542,53 @@
 		     (match_operand:GPI 3 "register_operand" "r")))]
   ""
   "<LOGICAL:nlogical>\\t%<w>0, %<w>3, %<w>1, <SHIFT:shift> %2"
-  [(set_attr "v8type" "logic_shift")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "logics_shift_imm")]
+)
+
+(define_insn "*and_one_cmpl_<SHIFT:optab><mode>3_compare0"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (and:GPI (not:GPI
+		   (SHIFT:GPI
+		    (match_operand:GPI 1 "register_operand" "r")
+		    (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n")))
+		  (match_operand:GPI 3 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(and:GPI (not:GPI
+		  (SHIFT:GPI
+		   (match_dup 1) (match_dup 2))) (match_dup 3)))]
+  ""
+  "bics\\t%<w>0, %<w>3, %<w>1, <SHIFT:shift> %2"
+  [(set_attr "type" "logics_shift_imm")]
+)
+
+;; zero_extend version of above
+(define_insn "*and_one_cmpl_<SHIFT:optab>si3_compare0_uxtw"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (and:SI (not:SI
+		  (SHIFT:SI
+		   (match_operand:SI 1 "register_operand" "r")
+		   (match_operand:QI 2 "aarch64_shift_imm_si" "n")))
+		 (match_operand:SI 3 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI (and:SI
+			 (not:SI
+			  (SHIFT:SI (match_dup 1) (match_dup 2))) (match_dup 3))))]
+  ""
+  "bics\\t%w0, %w3, %w1, <SHIFT:shift> %2"
+  [(set_attr "type" "logics_shift_imm")]
+)
 
 (define_insn "clz<mode>2"
   [(set (match_operand:GPI 0 "register_operand" "=r")
 	(clz:GPI (match_operand:GPI 1 "register_operand" "r")))]
   ""
   "clz\\t%<w>0, %<w>1"
-  [(set_attr "v8type" "clz")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "clz")]
+)
 
 (define_expand "ffs<mode>2"
   [(match_operand:GPI 0 "register_operand")
@@ -2525,16 +2610,16 @@
 	(unspec:GPI [(match_operand:GPI 1 "register_operand" "r")] UNSPEC_CLS))]
   ""
   "cls\\t%<w>0, %<w>1"
-  [(set_attr "v8type" "clz")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "clz")]
+)
 
 (define_insn "rbit<mode>2"
   [(set (match_operand:GPI 0 "register_operand" "=r")
 	(unspec:GPI [(match_operand:GPI 1 "register_operand" "r")] UNSPEC_RBIT))]
   ""
   "rbit\\t%<w>0, %<w>1"
-  [(set_attr "v8type" "rbit")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "rbit")]
+)
 
 (define_expand "ctz<mode>2"
   [(match_operand:GPI 0 "register_operand")
@@ -2555,8 +2640,8 @@
 	 (const_int 0)))]
   ""
   "tst\\t%<w>0, %<w>1"
-  [(set_attr "v8type" "logics")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "logics_reg")]
+)
 
 (define_insn "*and_<SHIFT:optab><mode>3nr_compare0"
   [(set (reg:CC_NZ CC_REGNUM)
@@ -2568,8 +2653,8 @@
 	(const_int 0)))]
   ""
   "tst\\t%<w>2, %<w>0, <SHIFT:shift> %1"
-  [(set_attr "v8type" "logics_shift")
-   (set_attr "mode" "<MODE>")])
+  [(set_attr "type" "logics_shift_imm")]
+)
 
 ;; -------------------------------------------------------------------
 ;; Shifts
@@ -2665,8 +2750,7 @@
 	 (match_operand:QI 2 "aarch64_reg_or_shift_imm_<mode>" "rUs<cmode>")))]
   ""
   "<shift>\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "shift")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "shift_reg")]
 )
 
 ;; zero_extend version of above
@@ -2677,8 +2761,7 @@
 	 (match_operand:QI 2 "aarch64_reg_or_shift_imm_si" "rUss"))))]
   ""
   "<shift>\\t%w0, %w1, %w2"
-  [(set_attr "v8type" "shift")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "shift_reg")]
 )
 
 (define_insn "*ashl<mode>3_insn"
@@ -2687,8 +2770,7 @@
 		      (match_operand:QI 2 "aarch64_reg_or_shift_imm_si" "rUss")))]
   ""
   "lsl\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "v8type" "shift")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "shift_reg")]
 )
 
 (define_insn "*<optab><mode>3_insn"
@@ -2700,8 +2782,59 @@
   operands[3] = GEN_INT (<sizen> - UINTVAL (operands[2]));
   return "<bfshift>\t%w0, %w1, %2, %3";
 }
-  [(set_attr "v8type" "bfm")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "bfm")]
+)
+
+(define_insn "*extr<mode>5_insn"
+  [(set (match_operand:GPI 0 "register_operand" "=r")
+	(ior:GPI (ashift:GPI (match_operand:GPI 1 "register_operand" "r")
+			     (match_operand 3 "const_int_operand" "n"))
+		 (lshiftrt:GPI (match_operand:GPI 2 "register_operand" "r")
+			       (match_operand 4 "const_int_operand" "n"))))]
+  "UINTVAL (operands[3]) < GET_MODE_BITSIZE (<MODE>mode) &&
+   (UINTVAL (operands[3]) + UINTVAL (operands[4]) == GET_MODE_BITSIZE (<MODE>mode))"
+  "extr\\t%<w>0, %<w>1, %<w>2, %4"
+  [(set_attr "type" "shift_imm")]
+)
+
+;; zero_extend version of the above
+(define_insn "*extrsi5_insn_uxtw"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI
+	 (ior:SI (ashift:SI (match_operand:SI 1 "register_operand" "r")
+			    (match_operand 3 "const_int_operand" "n"))
+		 (lshiftrt:SI (match_operand:SI 2 "register_operand" "r")
+			      (match_operand 4 "const_int_operand" "n")))))]
+  "UINTVAL (operands[3]) < 32 &&
+   (UINTVAL (operands[3]) + UINTVAL (operands[4]) == 32)"
+  "extr\\t%w0, %w1, %w2, %4"
+  [(set_attr "type" "shift_imm")]
+)
+
+(define_insn "*ror<mode>3_insn"
+  [(set (match_operand:GPI 0 "register_operand" "=r")
+	(rotate:GPI (match_operand:GPI 1 "register_operand" "r")
+		    (match_operand 2 "const_int_operand" "n")))]
+  "UINTVAL (operands[2]) < GET_MODE_BITSIZE (<MODE>mode)"
+{
+  operands[3] = GEN_INT (<sizen> - UINTVAL (operands[2]));
+  return "ror\\t%<w>0, %<w>1, %3";
+}
+  [(set_attr "type" "shift_imm")]
+)
+
+;; zero_extend version of the above
+(define_insn "*rorsi3_insn_uxtw"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI
+	 (rotate:SI (match_operand:SI 1 "register_operand" "r")
+		    (match_operand 2 "const_int_operand" "n"))))]
+  "UINTVAL (operands[2]) < 32"
+{
+  operands[3] = GEN_INT (32 - UINTVAL (operands[2]));
+  return "ror\\t%w0, %w1, %3";
+}
+  [(set_attr "type" "shift_imm")]
 )
 
 (define_insn "*<ANY_EXTEND:optab><GPI:mode>_ashl<SHORT:mode>"
@@ -2714,8 +2847,7 @@
   operands[3] = GEN_INT (<SHORT:sizen> - UINTVAL (operands[2]));
   return "<su>bfiz\t%<GPI:w>0, %<GPI:w>1, %2, %3";
 }
-  [(set_attr "v8type" "bfm")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "bfm")]
 )
 
 (define_insn "*zero_extend<GPI:mode>_lshr<SHORT:mode>"
@@ -2728,8 +2860,7 @@
   operands[3] = GEN_INT (<SHORT:sizen> - UINTVAL (operands[2]));
   return "ubfx\t%<GPI:w>0, %<GPI:w>1, %2, %3";
 }
-  [(set_attr "v8type" "bfm")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "bfm")]
 )
 
 (define_insn "*extend<GPI:mode>_ashr<SHORT:mode>"
@@ -2742,8 +2873,7 @@
   operands[3] = GEN_INT (<SHORT:sizen> - UINTVAL (operands[2]));
   return "sbfx\\t%<GPI:w>0, %<GPI:w>1, %2, %3";
 }
-  [(set_attr "v8type" "bfm")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "bfm")]
 )
 
 ;; -------------------------------------------------------------------
@@ -2766,8 +2896,64 @@
 			 (match_operand 3 "const_int_operand" "n")))]
   ""
   "<su>bfx\\t%<w>0, %<w>1, %3, %2"
-  [(set_attr "v8type" "bfm")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "bfm")]
+)
+
+;; Bitfield Insert (insv)
+(define_expand "insv<mode>"
+  [(set (zero_extract:GPI (match_operand:GPI 0 "register_operand")
+			  (match_operand 1 "const_int_operand")
+			  (match_operand 2 "const_int_operand"))
+	(match_operand:GPI 3 "general_operand"))]
+  ""
+{
+  unsigned HOST_WIDE_INT width = UINTVAL (operands[1]);
+  unsigned HOST_WIDE_INT pos = UINTVAL (operands[2]);
+  rtx value = operands[3];
+
+  if (width == 0 || (pos + width) > GET_MODE_BITSIZE (<MODE>mode))
+    FAIL;
+
+  if (CONST_INT_P (value))
+    {
+      unsigned HOST_WIDE_INT mask = ((unsigned HOST_WIDE_INT)1 << width) - 1;
+
+      /* Prefer AND/OR for inserting all zeros or all ones.  */
+      if ((UINTVAL (value) & mask) == 0
+	   || (UINTVAL (value) & mask) == mask)
+	FAIL;
+
+      /* 16-bit aligned 16-bit wide insert is handled by insv_imm.  */
+      if (width == 16 && (pos % 16) == 0)
+	DONE;
+    }
+  operands[3] = force_reg (<MODE>mode, value);
+})
+
+(define_insn "*insv_reg<mode>"
+  [(set (zero_extract:GPI (match_operand:GPI 0 "register_operand" "+r")
+			  (match_operand 1 "const_int_operand" "n")
+			  (match_operand 2 "const_int_operand" "n"))
+	(match_operand:GPI 3 "register_operand" "r"))]
+  "!(UINTVAL (operands[1]) == 0
+     || (UINTVAL (operands[2]) + UINTVAL (operands[1])
+	 > GET_MODE_BITSIZE (<MODE>mode)))"
+  "bfi\\t%<w>0, %<w>3, %2, %1"
+  [(set_attr "type" "bfm")]
+)
+
+(define_insn "*extr_insv_lower_reg<mode>"
+  [(set (zero_extract:GPI (match_operand:GPI 0 "register_operand" "+r")
+			  (match_operand 1 "const_int_operand" "n")
+			  (const_int 0))
+	(zero_extract:GPI (match_operand:GPI 2 "register_operand" "+r")
+			  (match_dup 1)
+			  (match_operand 3 "const_int_operand" "n")))]
+  "!(UINTVAL (operands[1]) == 0
+     || (UINTVAL (operands[3]) + UINTVAL (operands[1])
+	 > GET_MODE_BITSIZE (<MODE>mode)))"
+  "bfxil\\t%<w>0, %<w>2, %3, %1"
+  [(set_attr "type" "bfm")]
 )
 
 (define_insn "*<optab><ALLX:mode>_shft_<GPI:mode>"
@@ -2782,8 +2968,7 @@
 	      : GEN_INT (<GPI:sizen> - UINTVAL (operands[2]));
   return "<su>bfiz\t%<GPI:w>0, %<GPI:w>1, %2, %3";
 }
-  [(set_attr "v8type" "bfm")
-   (set_attr "mode" "<GPI:MODE>")]
+  [(set_attr "type" "bfm")]
 )
 
 ;; XXX We should match (any_extend (ashift)) here, like (and (ashift)) below
@@ -2796,8 +2981,7 @@
   "exact_log2 ((INTVAL (operands[3]) >> INTVAL (operands[2])) + 1) >= 0
    && (INTVAL (operands[3]) & ((1 << INTVAL (operands[2])) - 1)) == 0"
   "ubfiz\\t%<w>0, %<w>1, %2, %P3"
-  [(set_attr "v8type" "bfm")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "bfm")]
 )
 
 (define_insn "bswap<mode>2"
@@ -2805,8 +2989,7 @@
         (bswap:GPI (match_operand:GPI 1 "register_operand" "r")))]
   ""
   "rev\\t%<w>0, %<w>1"
-  [(set_attr "v8type" "rev")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "rev")]
 )
 
 (define_insn "bswaphi2"
@@ -2814,8 +2997,7 @@
         (bswap:HI (match_operand:HI 1 "register_operand" "r")))]
   ""
   "rev16\\t%w0, %w1"
-  [(set_attr "v8type" "rev")
-   (set_attr "mode" "HI")]
+  [(set_attr "type" "rev")]
 )
 
 ;; zero_extend version of above
@@ -2824,8 +3006,7 @@
         (zero_extend:DI (bswap:SI (match_operand:SI 1 "register_operand" "r"))))]
   ""
   "rev\\t%w0, %w1"
-  [(set_attr "v8type" "rev")
-   (set_attr "mode" "SI")]
+  [(set_attr "type" "rev")]
 )
 
 ;; -------------------------------------------------------------------
@@ -2841,8 +3022,7 @@
 	 FRINT))]
   "TARGET_FLOAT"
   "frint<frint_suffix>\\t%<s>0, %<s>1"
-  [(set_attr "v8type" "frint")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "f_rint<s>")]
 )
 
 ;; frcvt floating-point round to integer and convert standard patterns.
@@ -2853,9 +3033,7 @@
 		      FCVT)))]
   "TARGET_FLOAT"
   "fcvt<frint_suffix><su>\\t%<GPI:w>0, %<GPF:s>1"
-  [(set_attr "v8type" "fcvtf2i")
-   (set_attr "mode" "<GPF:MODE>")
-   (set_attr "mode2" "<GPI:MODE>")]
+  [(set_attr "type" "f_cvtf2i")]
 )
 
 ;; fma - no throw
@@ -2867,8 +3045,7 @@
 		 (match_operand:GPF 3 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fmadd\\t%<s>0, %<s>1, %<s>2, %<s>3"
-  [(set_attr "v8type" "fmadd")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fmac<s>")]
 )
 
 (define_insn "fnma<mode>4"
@@ -2878,8 +3055,7 @@
 		 (match_operand:GPF 3 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fmsub\\t%<s>0, %<s>1, %<s>2, %<s>3"
-  [(set_attr "v8type" "fmadd")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fmac<s>")]
 )
 
 (define_insn "fms<mode>4"
@@ -2889,8 +3065,7 @@
 		 (neg:GPF (match_operand:GPF 3 "register_operand" "w"))))]
   "TARGET_FLOAT"
   "fnmsub\\t%<s>0, %<s>1, %<s>2, %<s>3"
-  [(set_attr "v8type" "fmadd")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fmac<s>")]
 )
 
 (define_insn "fnms<mode>4"
@@ -2900,8 +3075,7 @@
 		 (neg:GPF (match_operand:GPF 3 "register_operand" "w"))))]
   "TARGET_FLOAT"
   "fnmadd\\t%<s>0, %<s>1, %<s>2, %<s>3"
-  [(set_attr "v8type" "fmadd")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fmac<s>")]
 )
 
 ;; If signed zeros are ignored, -(a * b + c) = -a * b - c.
@@ -2912,8 +3086,7 @@
 			  (match_operand:GPF 3 "register_operand" "w"))))]
   "!HONOR_SIGNED_ZEROS (<MODE>mode) && TARGET_FLOAT"
   "fnmadd\\t%<s>0, %<s>1, %<s>2, %<s>3"
-  [(set_attr "v8type" "fmadd")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fmac<s>")]
 )
 
 ;; -------------------------------------------------------------------
@@ -2925,9 +3098,7 @@
         (float_extend:DF (match_operand:SF 1 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fcvt\\t%d0, %s1"
-  [(set_attr "v8type" "fcvt")
-   (set_attr "mode" "DF")
-   (set_attr "mode2" "SF")]
+  [(set_attr "type" "f_cvt")]
 )
 
 (define_insn "truncdfsf2"
@@ -2935,9 +3106,7 @@
         (float_truncate:SF (match_operand:DF 1 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fcvt\\t%s0, %d1"
-  [(set_attr "v8type" "fcvt")
-   (set_attr "mode" "SF")
-   (set_attr "mode2" "DF")]
+  [(set_attr "type" "f_cvt")]
 )
 
 (define_insn "fix_trunc<GPF:mode><GPI:mode>2"
@@ -2945,9 +3114,7 @@
         (fix:GPI (match_operand:GPF 1 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fcvtzs\\t%<GPI:w>0, %<GPF:s>1"
-  [(set_attr "v8type" "fcvtf2i")
-   (set_attr "mode" "<GPF:MODE>")
-   (set_attr "mode2" "<GPI:MODE>")]
+  [(set_attr "type" "f_cvtf2i")]
 )
 
 (define_insn "fixuns_trunc<GPF:mode><GPI:mode>2"
@@ -2955,9 +3122,7 @@
         (unsigned_fix:GPI (match_operand:GPF 1 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fcvtzu\\t%<GPI:w>0, %<GPF:s>1"
-  [(set_attr "v8type" "fcvtf2i")
-   (set_attr "mode" "<GPF:MODE>")
-   (set_attr "mode2" "<GPI:MODE>")]
+  [(set_attr "type" "f_cvtf2i")]
 )
 
 (define_insn "float<GPI:mode><GPF:mode>2"
@@ -2965,9 +3130,7 @@
         (float:GPF (match_operand:GPI 1 "register_operand" "r")))]
   "TARGET_FLOAT"
   "scvtf\\t%<GPF:s>0, %<GPI:w>1"
-  [(set_attr "v8type" "fcvti2f")
-   (set_attr "mode" "<GPF:MODE>")
-   (set_attr "mode2" "<GPI:MODE>")]
+  [(set_attr "type" "f_cvti2f")]
 )
 
 (define_insn "floatuns<GPI:mode><GPF:mode>2"
@@ -2975,9 +3138,7 @@
         (unsigned_float:GPF (match_operand:GPI 1 "register_operand" "r")))]
   "TARGET_FLOAT"
   "ucvtf\\t%<GPF:s>0, %<GPI:w>1"
-  [(set_attr "v8type" "fcvt")
-   (set_attr "mode" "<GPF:MODE>")
-   (set_attr "mode2" "<GPI:MODE>")]
+  [(set_attr "type" "f_cvt")]
 )
 
 ;; -------------------------------------------------------------------
@@ -2991,8 +3152,7 @@
          (match_operand:GPF 2 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fadd\\t%<s>0, %<s>1, %<s>2"
-  [(set_attr "v8type" "fadd")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fadd<s>")]
 )
 
 (define_insn "sub<mode>3"
@@ -3002,8 +3162,7 @@
          (match_operand:GPF 2 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fsub\\t%<s>0, %<s>1, %<s>2"
-  [(set_attr "v8type" "fadd")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fadd<s>")]
 )
 
 (define_insn "mul<mode>3"
@@ -3013,8 +3172,7 @@
          (match_operand:GPF 2 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fmul\\t%<s>0, %<s>1, %<s>2"
-  [(set_attr "v8type" "fmul")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fmul<s>")]
 )
 
 (define_insn "*fnmul<mode>3"
@@ -3024,8 +3182,7 @@
 		 (match_operand:GPF 2 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fnmul\\t%<s>0, %<s>1, %<s>2"
-  [(set_attr "v8type" "fmul")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fmul<s>")]
 )
 
 (define_insn "div<mode>3"
@@ -3035,8 +3192,7 @@
          (match_operand:GPF 2 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fdiv\\t%<s>0, %<s>1, %<s>2"
-  [(set_attr "v8type" "fdiv")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fdiv<s>")]
 )
 
 (define_insn "neg<mode>2"
@@ -3044,8 +3200,7 @@
         (neg:GPF (match_operand:GPF 1 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fneg\\t%<s>0, %<s>1"
-  [(set_attr "v8type" "ffarith")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "ffarith<s>")]
 )
 
 (define_insn "sqrt<mode>2"
@@ -3053,8 +3208,7 @@
         (sqrt:GPF (match_operand:GPF 1 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fsqrt\\t%<s>0, %<s>1"
-  [(set_attr "v8type" "fsqrt")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "fsqrt<s>")]
 )
 
 (define_insn "abs<mode>2"
@@ -3062,8 +3216,7 @@
         (abs:GPF (match_operand:GPF 1 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fabs\\t%<s>0, %<s>1"
-  [(set_attr "v8type" "ffarith")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "ffarith<s>")]
 )
 
 ;; Given that smax/smin do not specify the result when either input is NaN,
@@ -3076,8 +3229,7 @@
 		  (match_operand:GPF 2 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fmaxnm\\t%<s>0, %<s>1, %<s>2"
-  [(set_attr "v8type" "fminmax")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "f_minmax<s>")]
 )
 
 (define_insn "smin<mode>3"
@@ -3086,8 +3238,7 @@
 		  (match_operand:GPF 2 "register_operand" "w")))]
   "TARGET_FLOAT"
   "fminnm\\t%<s>0, %<s>1, %<s>2"
-  [(set_attr "v8type" "fminmax")
-   (set_attr "mode" "<MODE>")]
+  [(set_attr "type" "f_minmax<s>")]
 )
 
 ;; -------------------------------------------------------------------
@@ -3146,48 +3297,42 @@
 ;; after or during reload as we don't want these patterns to start
 ;; kicking in during the combiner.
  
-(define_insn "aarch64_movdi_tilow"
+(define_insn "aarch64_movdi_<mode>low"
   [(set (match_operand:DI 0 "register_operand" "=r")
-        (truncate:DI (match_operand:TI 1 "register_operand" "w")))]
+        (truncate:DI (match_operand:TX 1 "register_operand" "w")))]
   "reload_completed || reload_in_progress"
   "fmov\\t%x0, %d1"
-  [(set_attr "v8type" "fmovf2i")
-   (set_attr "mode"   "DI")
+  [(set_attr "type" "f_mrc")
    (set_attr "length" "4")
   ])
 
-(define_insn "aarch64_movdi_tihigh"
+(define_insn "aarch64_movdi_<mode>high"
   [(set (match_operand:DI 0 "register_operand" "=r")
         (truncate:DI
-	  (lshiftrt:TI (match_operand:TI 1 "register_operand" "w")
+	  (lshiftrt:TX (match_operand:TX 1 "register_operand" "w")
 		       (const_int 64))))]
   "reload_completed || reload_in_progress"
   "fmov\\t%x0, %1.d[1]"
-  [(set_attr "v8type" "fmovf2i")
-   (set_attr "mode"   "DI")
+  [(set_attr "type" "f_mrc")
    (set_attr "length" "4")
   ])
 
-(define_insn "aarch64_movtihigh_di"
-  [(set (zero_extract:TI (match_operand:TI 0 "register_operand" "+w")
+(define_insn "aarch64_mov<mode>high_di"
+  [(set (zero_extract:TX (match_operand:TX 0 "register_operand" "+w")
                          (const_int 64) (const_int 64))
-        (zero_extend:TI (match_operand:DI 1 "register_operand" "r")))]
+        (zero_extend:TX (match_operand:DI 1 "register_operand" "r")))]
   "reload_completed || reload_in_progress"
   "fmov\\t%0.d[1], %x1"
-
-  [(set_attr "v8type" "fmovi2f")
-   (set_attr "mode"   "DI")
+  [(set_attr "type" "f_mcr")
    (set_attr "length" "4")
   ])
 
-(define_insn "aarch64_movtilow_di"
-  [(set (match_operand:TI 0 "register_operand" "=w")
-        (zero_extend:TI (match_operand:DI 1 "register_operand" "r")))]
+(define_insn "aarch64_mov<mode>low_di"
+  [(set (match_operand:TX 0 "register_operand" "=w")
+        (zero_extend:TX (match_operand:DI 1 "register_operand" "r")))]
   "reload_completed || reload_in_progress"
   "fmov\\t%d0, %x1"
-
-  [(set_attr "v8type" "fmovi2f")
-   (set_attr "mode"   "DI")
+  [(set_attr "type" "f_mcr")
    (set_attr "length" "4")
   ])
 
@@ -3197,9 +3342,7 @@
 	  (truncate:DI (match_operand:TI 1 "register_operand" "w"))))]
   "reload_completed || reload_in_progress"
   "fmov\\t%d0, %d1"
-
-  [(set_attr "v8type" "fmovi2f")
-   (set_attr "mode"   "DI")
+  [(set_attr "type" "f_mcr")
    (set_attr "length" "4")
   ])
 
@@ -3214,9 +3357,7 @@
 		   (match_operand 2 "aarch64_valid_symref" "S")))]
   ""
   "add\\t%0, %1, :lo12:%a2"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "DI")]
-
+  [(set_attr "type" "alu_reg")]
 )
 
 (define_insn "ldr_got_small"
@@ -3227,8 +3368,16 @@
 		   UNSPEC_GOTSMALLPIC))]
   ""
   "ldr\\t%0, [%1, #:got_lo12:%a2]"
-  [(set_attr "v8type" "load1")
-   (set_attr "mode" "DI")]
+  [(set_attr "type" "load1")]
+)
+
+(define_insn "ldr_got_tiny"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(unspec:DI [(match_operand:DI 1 "aarch64_valid_symref" "S")]
+		   UNSPEC_GOTTINYPIC))]
+  ""
+  "ldr\\t%0, %L1"
+  [(set_attr "type" "load1")]
 )
 
 (define_insn "aarch64_load_tp_hard"
@@ -3236,8 +3385,7 @@
 	(unspec:DI [(const_int 0)] UNSPEC_TLS))]
   ""
   "mrs\\t%0, tpidr_el0"
-  [(set_attr "v8type" "mrs")
-   (set_attr "mode" "DI")]
+  [(set_attr "type" "mrs")]
 )
 
 ;; The TLS ABI specifically requires that the compiler does not schedule
@@ -3261,7 +3409,7 @@
   ]
   ""
   "adrp\\tx0, %A1\;add\\tx0, x0, %L1\;bl\\t%2\;nop"
-  [(set_attr "v8type" "call")
+  [(set_attr "type" "call")
    (set_attr "length" "16")])
 
 (define_insn "tlsie_small"
@@ -3270,8 +3418,7 @@
 		   UNSPEC_GOTSMALLTLS))]
   ""
   "adrp\\t%0, %A1\;ldr\\t%0, [%0, #%L1]"
-  [(set_attr "v8type" "load1")
-   (set_attr "mode" "DI")
+  [(set_attr "type" "load1")
    (set_attr "length" "8")]
 )
 
@@ -3282,8 +3429,7 @@
 		   UNSPEC_GOTSMALLTLS))]
   ""
   "add\\t%0, %1, #%G2\;add\\t%0, %0, #%L2"
-  [(set_attr "v8type" "alu")
-   (set_attr "mode" "DI")
+  [(set_attr "type" "alu_reg")
    (set_attr "length" "8")]
 )
 
@@ -3296,7 +3442,7 @@
    (clobber (match_scratch:DI 1 "=r"))]
   "TARGET_TLS_DESC"
   "adrp\\tx0, %A0\;ldr\\t%1, [x0, #%L0]\;add\\tx0, x0, %L0\;.tlsdesccall\\t%0\;blr\\t%1"
-  [(set_attr "v8type" "call")
+  [(set_attr "type" "call")
    (set_attr "length" "16")])
 
 (define_insn "stack_tie"
Index: b/src/gcc/config/aarch64/aarch64-arches.def
===================================================================
--- a/src/gcc/config/aarch64/aarch64-arches.def
+++ b/src/gcc/config/aarch64/aarch64-arches.def
@@ -26,4 +26,4 @@
    this architecture.  ARCH is the architecture revision.  FLAGS are
    the flags implied by the architecture.  */
 
-AARCH64_ARCH("armv8-a",	      generic,	     8,  AARCH64_FL_FOR_ARCH8)
+AARCH64_ARCH("armv8-a",	      cortexa53,	     8,  AARCH64_FL_FOR_ARCH8)
Index: b/src/gcc/config/aarch64/aarch64-option-extensions.def
===================================================================
--- a/src/gcc/config/aarch64/aarch64-option-extensions.def
+++ b/src/gcc/config/aarch64/aarch64-option-extensions.def
@@ -35,3 +35,4 @@
 AARCH64_OPT_EXTENSION("fp",	AARCH64_FL_FP,	AARCH64_FL_FPSIMD | AARCH64_FL_CRYPTO)
 AARCH64_OPT_EXTENSION("simd",	AARCH64_FL_FPSIMD,	AARCH64_FL_SIMD | AARCH64_FL_CRYPTO)
 AARCH64_OPT_EXTENSION("crypto",	AARCH64_FL_CRYPTO | AARCH64_FL_FPSIMD,	AARCH64_FL_CRYPTO)
+AARCH64_OPT_EXTENSION("crc",	AARCH64_FL_CRC,	AARCH64_FL_CRC)
Index: b/src/gcc/config/aarch64/t-aarch64
===================================================================
--- a/src/gcc/config/aarch64/t-aarch64
+++ b/src/gcc/config/aarch64/t-aarch64
@@ -34,3 +34,10 @@ aarch64-builtins.o: $(srcdir)/config/aar
   $(srcdir)/config/aarch64/aarch64-simd-builtins.def
 	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
 		$(srcdir)/config/aarch64/aarch64-builtins.c
+
+aarch-common.o: $(srcdir)/config/arm/aarch-common.c $(CONFIG_H) $(SYSTEM_H) \
+    coretypes.h $(TM_H) $(TM_P_H) $(RTL_H) $(TREE_H) output.h $(C_COMMON_H)
+	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
+		$(srcdir)/config/arm/aarch-common.c
+
+
Index: b/src/gcc/config/aarch64/aarch64-modes.def
===================================================================
--- a/src/gcc/config/aarch64/aarch64-modes.def
+++ b/src/gcc/config/aarch64/aarch64-modes.def
@@ -24,6 +24,7 @@ CC_MODE (CC_SWP);
 CC_MODE (CC_ZESWP); /* zero-extend LHS (but swap to make it RHS).  */
 CC_MODE (CC_SESWP); /* sign-extend LHS (but swap to make it RHS).  */
 CC_MODE (CC_NZ);    /* Only N and Z bits of condition flags are valid.  */
+CC_MODE (CC_Z);     /* Only Z bit of condition flags is valid.  */
 
 /* Vector modes.  */
 VECTOR_MODES (INT, 8);        /*       V8QI V4HI V2SI.  */
Index: b/src/gcc/config/aarch64/aarch64-cores.def
===================================================================
--- a/src/gcc/config/aarch64/aarch64-cores.def
+++ b/src/gcc/config/aarch64/aarch64-cores.def
@@ -35,6 +35,4 @@
    therefore serves as a template for adding more CPUs in the future.  */
 
 AARCH64_CORE("cortex-a53",	  cortexa53,	     8,  AARCH64_FL_FPSIMD,    generic)
-AARCH64_CORE("cortex-a57",	  cortexa57,	     8,  AARCH64_FL_FPSIMD,    generic)
-AARCH64_CORE("example-1",	      large,	     8,  AARCH64_FL_FPSIMD,    generic)
-AARCH64_CORE("example-2",	      small,	     8,  AARCH64_FL_FPSIMD,    generic)
+AARCH64_CORE("cortex-a57",	  cortexa15,	     8,  AARCH64_FL_FPSIMD,    generic)
Index: b/src/gcc/config/aarch64/aarch64-builtins.c
===================================================================
--- a/src/gcc/config/aarch64/aarch64-builtins.c
+++ b/src/gcc/config/aarch64/aarch64-builtins.c
@@ -30,6 +30,7 @@
 #include "langhooks.h"
 #include "diagnostic-core.h"
 #include "optabs.h"
+#include "gimple.h"
 
 enum aarch64_simd_builtin_type_mode
 {
@@ -50,6 +51,7 @@ enum aarch64_simd_builtin_type_mode
   T_OI,
   T_XI,
   T_SI,
+  T_SF,
   T_HI,
   T_QI,
   T_MAX
@@ -72,180 +74,259 @@ enum aarch64_simd_builtin_type_mode
 #define oi_UP	 T_OI
 #define xi_UP	 T_XI
 #define si_UP    T_SI
+#define sf_UP    T_SF
 #define hi_UP    T_HI
 #define qi_UP    T_QI
 
 #define UP(X) X##_UP
 
-typedef enum
+#define SIMD_MAX_BUILTIN_ARGS 5
+
+enum aarch64_type_qualifiers
 {
-  AARCH64_SIMD_BINOP,
-  AARCH64_SIMD_TERNOP,
-  AARCH64_SIMD_QUADOP,
-  AARCH64_SIMD_UNOP,
-  AARCH64_SIMD_GETLANE,
-  AARCH64_SIMD_SETLANE,
-  AARCH64_SIMD_CREATE,
-  AARCH64_SIMD_DUP,
-  AARCH64_SIMD_DUPLANE,
-  AARCH64_SIMD_COMBINE,
-  AARCH64_SIMD_SPLIT,
-  AARCH64_SIMD_LANEMUL,
-  AARCH64_SIMD_LANEMULL,
-  AARCH64_SIMD_LANEMULH,
-  AARCH64_SIMD_LANEMAC,
-  AARCH64_SIMD_SCALARMUL,
-  AARCH64_SIMD_SCALARMULL,
-  AARCH64_SIMD_SCALARMULH,
-  AARCH64_SIMD_SCALARMAC,
-  AARCH64_SIMD_CONVERT,
-  AARCH64_SIMD_FIXCONV,
-  AARCH64_SIMD_SELECT,
-  AARCH64_SIMD_RESULTPAIR,
-  AARCH64_SIMD_REINTERP,
-  AARCH64_SIMD_VTBL,
-  AARCH64_SIMD_VTBX,
-  AARCH64_SIMD_LOAD1,
-  AARCH64_SIMD_LOAD1LANE,
-  AARCH64_SIMD_STORE1,
-  AARCH64_SIMD_STORE1LANE,
-  AARCH64_SIMD_LOADSTRUCT,
-  AARCH64_SIMD_LOADSTRUCTLANE,
-  AARCH64_SIMD_STORESTRUCT,
-  AARCH64_SIMD_STORESTRUCTLANE,
-  AARCH64_SIMD_LOGICBINOP,
-  AARCH64_SIMD_SHIFTINSERT,
-  AARCH64_SIMD_SHIFTIMM,
-  AARCH64_SIMD_SHIFTACC
-} aarch64_simd_itype;
+  /* T foo.  */
+  qualifier_none = 0x0,
+  /* unsigned T foo.  */
+  qualifier_unsigned = 0x1, /* 1 << 0  */
+  /* const T foo.  */
+  qualifier_const = 0x2, /* 1 << 1  */
+  /* T *foo.  */
+  qualifier_pointer = 0x4, /* 1 << 2  */
+  /* const T *foo.  */
+  qualifier_const_pointer = 0x6, /* qualifier_const | qualifier_pointer  */
+  /* Used when expanding arguments if an operand could
+     be an immediate.  */
+  qualifier_immediate = 0x8, /* 1 << 3  */
+  qualifier_maybe_immediate = 0x10, /* 1 << 4  */
+  /* void foo (...).  */
+  qualifier_void = 0x20, /* 1 << 5  */
+  /* Some patterns may have internal operands, this qualifier is an
+     instruction to the initialisation code to skip this operand.  */
+  qualifier_internal = 0x40, /* 1 << 6  */
+  /* Some builtins should use the T_*mode* encoded in a simd_builtin_datum
+     rather than using the type of the operand.  */
+  qualifier_map_mode = 0x80, /* 1 << 7  */
+  /* qualifier_pointer | qualifier_map_mode  */
+  qualifier_pointer_map_mode = 0x84,
+  /* qualifier_const_pointer | qualifier_map_mode  */
+  qualifier_const_pointer_map_mode = 0x86,
+  /* Polynomial types.  */
+  qualifier_poly = 0x100
+};
 
 typedef struct
 {
   const char *name;
-  const aarch64_simd_itype itype;
   enum aarch64_simd_builtin_type_mode mode;
   const enum insn_code code;
   unsigned int fcode;
+  enum aarch64_type_qualifiers *qualifiers;
 } aarch64_simd_builtin_datum;
 
-#define CF(N, X) CODE_FOR_aarch64_##N##X
-
-#define VAR1(T, N, A) \
-  {#N, AARCH64_SIMD_##T, UP (A), CF (N, A), 0},
-#define VAR2(T, N, A, B) \
-  VAR1 (T, N, A) \
-  VAR1 (T, N, B)
-#define VAR3(T, N, A, B, C) \
-  VAR2 (T, N, A, B) \
-  VAR1 (T, N, C)
-#define VAR4(T, N, A, B, C, D) \
-  VAR3 (T, N, A, B, C) \
-  VAR1 (T, N, D)
-#define VAR5(T, N, A, B, C, D, E) \
-  VAR4 (T, N, A, B, C, D) \
-  VAR1 (T, N, E)
-#define VAR6(T, N, A, B, C, D, E, F) \
-  VAR5 (T, N, A, B, C, D, E) \
-  VAR1 (T, N, F)
-#define VAR7(T, N, A, B, C, D, E, F, G) \
-  VAR6 (T, N, A, B, C, D, E, F) \
-  VAR1 (T, N, G)
-#define VAR8(T, N, A, B, C, D, E, F, G, H) \
-  VAR7 (T, N, A, B, C, D, E, F, G) \
-  VAR1 (T, N, H)
-#define VAR9(T, N, A, B, C, D, E, F, G, H, I) \
-  VAR8 (T, N, A, B, C, D, E, F, G, H) \
-  VAR1 (T, N, I)
-#define VAR10(T, N, A, B, C, D, E, F, G, H, I, J) \
-  VAR9 (T, N, A, B, C, D, E, F, G, H, I) \
-  VAR1 (T, N, J)
-#define VAR11(T, N, A, B, C, D, E, F, G, H, I, J, K) \
-  VAR10 (T, N, A, B, C, D, E, F, G, H, I, J) \
-  VAR1 (T, N, K)
-#define VAR12(T, N, A, B, C, D, E, F, G, H, I, J, K, L) \
-  VAR11 (T, N, A, B, C, D, E, F, G, H, I, J, K) \
-  VAR1 (T, N, L)
+static enum aarch64_type_qualifiers
+aarch64_types_unop_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none };
+#define TYPES_UNOP (aarch64_types_unop_qualifiers)
+static enum aarch64_type_qualifiers
+aarch64_types_unopu_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_unsigned, qualifier_unsigned };
+#define TYPES_UNOPU (aarch64_types_unopu_qualifiers)
+#define TYPES_CREATE (aarch64_types_unop_qualifiers)
+#define TYPES_REINTERP (aarch64_types_unop_qualifiers)
+static enum aarch64_type_qualifiers
+aarch64_types_binop_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none, qualifier_maybe_immediate };
+#define TYPES_BINOP (aarch64_types_binop_qualifiers)
+static enum aarch64_type_qualifiers
+aarch64_types_binopu_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_unsigned, qualifier_unsigned, qualifier_unsigned };
+#define TYPES_BINOPU (aarch64_types_binopu_qualifiers)
+static enum aarch64_type_qualifiers
+aarch64_types_binopp_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_poly, qualifier_poly, qualifier_poly };
+#define TYPES_BINOPP (aarch64_types_binopp_qualifiers)
+
+static enum aarch64_type_qualifiers
+aarch64_types_ternop_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none, qualifier_none, qualifier_none };
+#define TYPES_TERNOP (aarch64_types_ternop_qualifiers)
+static enum aarch64_type_qualifiers
+aarch64_types_ternopu_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_unsigned, qualifier_unsigned,
+      qualifier_unsigned, qualifier_unsigned };
+#define TYPES_TERNOPU (aarch64_types_ternopu_qualifiers)
+
+static enum aarch64_type_qualifiers
+aarch64_types_quadop_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none, qualifier_none,
+      qualifier_none, qualifier_none };
+#define TYPES_QUADOP (aarch64_types_quadop_qualifiers)
+
+static enum aarch64_type_qualifiers
+aarch64_types_getlane_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none, qualifier_immediate };
+#define TYPES_GETLANE (aarch64_types_getlane_qualifiers)
+#define TYPES_SHIFTIMM (aarch64_types_getlane_qualifiers)
+static enum aarch64_type_qualifiers
+aarch64_types_setlane_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none, qualifier_none, qualifier_immediate };
+#define TYPES_SETLANE (aarch64_types_setlane_qualifiers)
+#define TYPES_SHIFTINSERT (aarch64_types_setlane_qualifiers)
+#define TYPES_SHIFTACC (aarch64_types_setlane_qualifiers)
+
+static enum aarch64_type_qualifiers
+aarch64_types_combine_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none, qualifier_none };
+#define TYPES_COMBINE (aarch64_types_combine_qualifiers)
+
+static enum aarch64_type_qualifiers
+aarch64_types_load1_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_const_pointer_map_mode };
+#define TYPES_LOAD1 (aarch64_types_load1_qualifiers)
+#define TYPES_LOADSTRUCT (aarch64_types_load1_qualifiers)
+
+/* The first argument (return type) of a store should be void type,
+   which we represent with qualifier_void.  Their first operand will be
+   a DImode pointer to the location to store to, so we must use
+   qualifier_map_mode | qualifier_pointer to build a pointer to the
+   element type of the vector.  */
+static enum aarch64_type_qualifiers
+aarch64_types_store1_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_void, qualifier_pointer_map_mode, qualifier_none };
+#define TYPES_STORE1 (aarch64_types_store1_qualifiers)
+#define TYPES_STORESTRUCT (aarch64_types_store1_qualifiers)
+
+#define CF0(N, X) CODE_FOR_aarch64_##N##X
+#define CF1(N, X) CODE_FOR_##N##X##1
+#define CF2(N, X) CODE_FOR_##N##X##2
+#define CF3(N, X) CODE_FOR_##N##X##3
+#define CF4(N, X) CODE_FOR_##N##X##4
+#define CF10(N, X) CODE_FOR_##N##X
+
+#define VAR1(T, N, MAP, A) \
+  {#N, UP (A), CF##MAP (N, A), 0, TYPES_##T},
+#define VAR2(T, N, MAP, A, B) \
+  VAR1 (T, N, MAP, A) \
+  VAR1 (T, N, MAP, B)
+#define VAR3(T, N, MAP, A, B, C) \
+  VAR2 (T, N, MAP, A, B) \
+  VAR1 (T, N, MAP, C)
+#define VAR4(T, N, MAP, A, B, C, D) \
+  VAR3 (T, N, MAP, A, B, C) \
+  VAR1 (T, N, MAP, D)
+#define VAR5(T, N, MAP, A, B, C, D, E) \
+  VAR4 (T, N, MAP, A, B, C, D) \
+  VAR1 (T, N, MAP, E)
+#define VAR6(T, N, MAP, A, B, C, D, E, F) \
+  VAR5 (T, N, MAP, A, B, C, D, E) \
+  VAR1 (T, N, MAP, F)
+#define VAR7(T, N, MAP, A, B, C, D, E, F, G) \
+  VAR6 (T, N, MAP, A, B, C, D, E, F) \
+  VAR1 (T, N, MAP, G)
+#define VAR8(T, N, MAP, A, B, C, D, E, F, G, H) \
+  VAR7 (T, N, MAP, A, B, C, D, E, F, G) \
+  VAR1 (T, N, MAP, H)
+#define VAR9(T, N, MAP, A, B, C, D, E, F, G, H, I) \
+  VAR8 (T, N, MAP, A, B, C, D, E, F, G, H) \
+  VAR1 (T, N, MAP, I)
+#define VAR10(T, N, MAP, A, B, C, D, E, F, G, H, I, J) \
+  VAR9 (T, N, MAP, A, B, C, D, E, F, G, H, I) \
+  VAR1 (T, N, MAP, J)
+#define VAR11(T, N, MAP, A, B, C, D, E, F, G, H, I, J, K) \
+  VAR10 (T, N, MAP, A, B, C, D, E, F, G, H, I, J) \
+  VAR1 (T, N, MAP, K)
+#define VAR12(T, N, MAP, A, B, C, D, E, F, G, H, I, J, K, L) \
+  VAR11 (T, N, MAP, A, B, C, D, E, F, G, H, I, J, K) \
+  VAR1 (T, N, MAP, L)
 
 /* BUILTIN_<ITERATOR> macros should expand to cover the same range of
    modes as is given for each define_mode_iterator in
    config/aarch64/iterators.md.  */
 
-#define BUILTIN_DX(T, N) \
-  VAR2 (T, N, di, df)
-#define BUILTIN_SDQ_I(T, N) \
-  VAR4 (T, N, qi, hi, si, di)
-#define BUILTIN_SD_HSI(T, N) \
-  VAR2 (T, N, hi, si)
-#define BUILTIN_V2F(T, N) \
-  VAR2 (T, N, v2sf, v2df)
-#define BUILTIN_VALL(T, N) \
-  VAR10 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, v2sf, v4sf, v2df)
-#define BUILTIN_VB(T, N) \
-  VAR2 (T, N, v8qi, v16qi)
-#define BUILTIN_VD(T, N) \
-  VAR4 (T, N, v8qi, v4hi, v2si, v2sf)
-#define BUILTIN_VDC(T, N) \
-  VAR6 (T, N, v8qi, v4hi, v2si, v2sf, di, df)
-#define BUILTIN_VDIC(T, N) \
-  VAR3 (T, N, v8qi, v4hi, v2si)
-#define BUILTIN_VDN(T, N) \
-  VAR3 (T, N, v4hi, v2si, di)
-#define BUILTIN_VDQ(T, N) \
-  VAR7 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di)
-#define BUILTIN_VDQF(T, N) \
-  VAR3 (T, N, v2sf, v4sf, v2df)
-#define BUILTIN_VDQHS(T, N) \
-  VAR4 (T, N, v4hi, v8hi, v2si, v4si)
-#define BUILTIN_VDQIF(T, N) \
-  VAR9 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2sf, v4sf, v2df)
-#define BUILTIN_VDQM(T, N) \
-  VAR6 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si)
-#define BUILTIN_VDQV(T, N) \
-  VAR5 (T, N, v8qi, v16qi, v4hi, v8hi, v4si)
-#define BUILTIN_VDQ_BHSI(T, N) \
-  VAR6 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si)
-#define BUILTIN_VDQ_I(T, N) \
-  VAR7 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di)
-#define BUILTIN_VDW(T, N) \
-  VAR3 (T, N, v8qi, v4hi, v2si)
-#define BUILTIN_VD_BHSI(T, N) \
-  VAR3 (T, N, v8qi, v4hi, v2si)
-#define BUILTIN_VD_HSI(T, N) \
-  VAR2 (T, N, v4hi, v2si)
-#define BUILTIN_VD_RE(T, N) \
-  VAR6 (T, N, v8qi, v4hi, v2si, v2sf, di, df)
-#define BUILTIN_VQ(T, N) \
-  VAR6 (T, N, v16qi, v8hi, v4si, v2di, v4sf, v2df)
-#define BUILTIN_VQN(T, N) \
-  VAR3 (T, N, v8hi, v4si, v2di)
-#define BUILTIN_VQW(T, N) \
-  VAR3 (T, N, v16qi, v8hi, v4si)
-#define BUILTIN_VQ_HSI(T, N) \
-  VAR2 (T, N, v8hi, v4si)
-#define BUILTIN_VQ_S(T, N) \
-  VAR6 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si)
-#define BUILTIN_VSDQ_HSI(T, N) \
-  VAR6 (T, N, v4hi, v8hi, v2si, v4si, hi, si)
-#define BUILTIN_VSDQ_I(T, N) \
-  VAR11 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, qi, hi, si, di)
-#define BUILTIN_VSDQ_I_BHSI(T, N) \
-  VAR10 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, qi, hi, si)
-#define BUILTIN_VSDQ_I_DI(T, N) \
-  VAR8 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, di)
-#define BUILTIN_VSD_HSI(T, N) \
-  VAR4 (T, N, v4hi, v2si, hi, si)
-#define BUILTIN_VSQN_HSDI(T, N) \
-  VAR6 (T, N, v8hi, v4si, v2di, hi, si, di)
-#define BUILTIN_VSTRUCT(T, N) \
-  VAR3 (T, N, oi, ci, xi)
+#define BUILTIN_DX(T, N, MAP) \
+  VAR2 (T, N, MAP, di, df)
+#define BUILTIN_GPF(T, N, MAP) \
+  VAR2 (T, N, MAP, sf, df)
+#define BUILTIN_SDQ_I(T, N, MAP) \
+  VAR4 (T, N, MAP, qi, hi, si, di)
+#define BUILTIN_SD_HSI(T, N, MAP) \
+  VAR2 (T, N, MAP, hi, si)
+#define BUILTIN_V2F(T, N, MAP) \
+  VAR2 (T, N, MAP, v2sf, v2df)
+#define BUILTIN_VALL(T, N, MAP) \
+  VAR10 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, \
+	 v4si, v2di, v2sf, v4sf, v2df)
+#define BUILTIN_VALLDI(T, N, MAP) \
+  VAR11 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, \
+	 v4si, v2di, v2sf, v4sf, v2df, di)
+#define BUILTIN_VB(T, N, MAP) \
+  VAR2 (T, N, MAP, v8qi, v16qi)
+#define BUILTIN_VD(T, N, MAP) \
+  VAR4 (T, N, MAP, v8qi, v4hi, v2si, v2sf)
+#define BUILTIN_VDC(T, N, MAP) \
+  VAR6 (T, N, MAP, v8qi, v4hi, v2si, v2sf, di, df)
+#define BUILTIN_VDIC(T, N, MAP) \
+  VAR3 (T, N, MAP, v8qi, v4hi, v2si)
+#define BUILTIN_VDN(T, N, MAP) \
+  VAR3 (T, N, MAP, v4hi, v2si, di)
+#define BUILTIN_VDQ(T, N, MAP) \
+  VAR7 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di)
+#define BUILTIN_VDQF(T, N, MAP) \
+  VAR3 (T, N, MAP, v2sf, v4sf, v2df)
+#define BUILTIN_VDQH(T, N, MAP) \
+  VAR2 (T, N, MAP, v4hi, v8hi)
+#define BUILTIN_VDQHS(T, N, MAP) \
+  VAR4 (T, N, MAP, v4hi, v8hi, v2si, v4si)
+#define BUILTIN_VDQIF(T, N, MAP) \
+  VAR9 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2sf, v4sf, v2df)
+#define BUILTIN_VDQM(T, N, MAP) \
+  VAR6 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si)
+#define BUILTIN_VDQV(T, N, MAP) \
+  VAR5 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v4si)
+#define BUILTIN_VDQ_BHSI(T, N, MAP) \
+  VAR6 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si)
+#define BUILTIN_VDQ_I(T, N, MAP) \
+  VAR7 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di)
+#define BUILTIN_VDW(T, N, MAP) \
+  VAR3 (T, N, MAP, v8qi, v4hi, v2si)
+#define BUILTIN_VD_BHSI(T, N, MAP) \
+  VAR3 (T, N, MAP, v8qi, v4hi, v2si)
+#define BUILTIN_VD_HSI(T, N, MAP) \
+  VAR2 (T, N, MAP, v4hi, v2si)
+#define BUILTIN_VD_RE(T, N, MAP) \
+  VAR6 (T, N, MAP, v8qi, v4hi, v2si, v2sf, di, df)
+#define BUILTIN_VQ(T, N, MAP) \
+  VAR6 (T, N, MAP, v16qi, v8hi, v4si, v2di, v4sf, v2df)
+#define BUILTIN_VQN(T, N, MAP) \
+  VAR3 (T, N, MAP, v8hi, v4si, v2di)
+#define BUILTIN_VQW(T, N, MAP) \
+  VAR3 (T, N, MAP, v16qi, v8hi, v4si)
+#define BUILTIN_VQ_HSI(T, N, MAP) \
+  VAR2 (T, N, MAP, v8hi, v4si)
+#define BUILTIN_VQ_S(T, N, MAP) \
+  VAR6 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si)
+#define BUILTIN_VSDQ_HSI(T, N, MAP) \
+  VAR6 (T, N, MAP, v4hi, v8hi, v2si, v4si, hi, si)
+#define BUILTIN_VSDQ_I(T, N, MAP) \
+  VAR11 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, qi, hi, si, di)
+#define BUILTIN_VSDQ_I_BHSI(T, N, MAP) \
+  VAR10 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, qi, hi, si)
+#define BUILTIN_VSDQ_I_DI(T, N, MAP) \
+  VAR8 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, di)
+#define BUILTIN_VSD_HSI(T, N, MAP) \
+  VAR4 (T, N, MAP, v4hi, v2si, hi, si)
+#define BUILTIN_VSQN_HSDI(T, N, MAP) \
+  VAR6 (T, N, MAP, v8hi, v4si, v2di, hi, si, di)
+#define BUILTIN_VSTRUCT(T, N, MAP) \
+  VAR3 (T, N, MAP, oi, ci, xi)
 
 static aarch64_simd_builtin_datum aarch64_simd_builtin_data[] = {
 #include "aarch64-simd-builtins.def"
 };
 
 #undef VAR1
-#define VAR1(T, N, A) \
-  AARCH64_SIMD_BUILTIN_##N##A,
+#define VAR1(T, N, MAP, A) \
+  AARCH64_SIMD_BUILTIN_##T##_##N##A,
 
 enum aarch64_builtins
 {
@@ -257,170 +338,217 @@ enum aarch64_builtins
   AARCH64_BUILTIN_MAX
 };
 
-#undef BUILTIN_DX
-#undef BUILTIN_SDQ_I
-#undef BUILTIN_SD_HSI
-#undef BUILTIN_V2F
-#undef BUILTIN_VALL
-#undef BUILTIN_VB
-#undef BUILTIN_VD
-#undef BUILTIN_VDC
-#undef BUILTIN_VDIC
-#undef BUILTIN_VDN
-#undef BUILTIN_VDQ
-#undef BUILTIN_VDQF
-#undef BUILTIN_VDQHS
-#undef BUILTIN_VDQIF
-#undef BUILTIN_VDQM
-#undef BUILTIN_VDQV
-#undef BUILTIN_VDQ_BHSI
-#undef BUILTIN_VDQ_I
-#undef BUILTIN_VDW
-#undef BUILTIN_VD_BHSI
-#undef BUILTIN_VD_HSI
-#undef BUILTIN_VD_RE
-#undef BUILTIN_VQ
-#undef BUILTIN_VQN
-#undef BUILTIN_VQW
-#undef BUILTIN_VQ_HSI
-#undef BUILTIN_VQ_S
-#undef BUILTIN_VSDQ_HSI
-#undef BUILTIN_VSDQ_I
-#undef BUILTIN_VSDQ_I_BHSI
-#undef BUILTIN_VSDQ_I_DI
-#undef BUILTIN_VSD_HSI
-#undef BUILTIN_VSQN_HSDI
-#undef BUILTIN_VSTRUCT
-#undef CF
-#undef VAR1
-#undef VAR2
-#undef VAR3
-#undef VAR4
-#undef VAR5
-#undef VAR6
-#undef VAR7
-#undef VAR8
-#undef VAR9
-#undef VAR10
-#undef VAR11
-
 static GTY(()) tree aarch64_builtin_decls[AARCH64_BUILTIN_MAX];
 
 #define NUM_DREG_TYPES 6
 #define NUM_QREG_TYPES 6
 
+/* Return a tree for a signed or unsigned argument of either
+   the mode specified by MODE, or the inner mode of MODE.  */
+tree
+aarch64_build_scalar_type (enum machine_mode mode,
+			   bool unsigned_p,
+			   bool poly_p)
+{
+#undef INT_TYPES
+#define INT_TYPES \
+  AARCH64_TYPE_BUILDER (QI) \
+  AARCH64_TYPE_BUILDER (HI) \
+  AARCH64_TYPE_BUILDER (SI) \
+  AARCH64_TYPE_BUILDER (DI) \
+  AARCH64_TYPE_BUILDER (EI) \
+  AARCH64_TYPE_BUILDER (OI) \
+  AARCH64_TYPE_BUILDER (CI) \
+  AARCH64_TYPE_BUILDER (XI) \
+  AARCH64_TYPE_BUILDER (TI) \
+
+/* Statically declare all the possible types we might need.  */
+#undef AARCH64_TYPE_BUILDER
+#define AARCH64_TYPE_BUILDER(X) \
+  static tree X##_aarch64_type_node_p = NULL; \
+  static tree X##_aarch64_type_node_s = NULL; \
+  static tree X##_aarch64_type_node_u = NULL;
+
+  INT_TYPES
+
+  static tree float_aarch64_type_node = NULL;
+  static tree double_aarch64_type_node = NULL;
+
+  gcc_assert (!VECTOR_MODE_P (mode));
+
+/* If we've already initialised this type, don't initialise it again,
+   otherwise ask for a new type of the correct size.  */
+#undef AARCH64_TYPE_BUILDER
+#define AARCH64_TYPE_BUILDER(X) \
+  case X##mode: \
+    if (unsigned_p) \
+      return (X##_aarch64_type_node_u \
+	      ? X##_aarch64_type_node_u \
+	      : X##_aarch64_type_node_u \
+		  = make_unsigned_type (GET_MODE_PRECISION (mode))); \
+    else if (poly_p) \
+       return (X##_aarch64_type_node_p \
+	      ? X##_aarch64_type_node_p \
+	      : X##_aarch64_type_node_p \
+		  = make_unsigned_type (GET_MODE_PRECISION (mode))); \
+    else \
+       return (X##_aarch64_type_node_s \
+	      ? X##_aarch64_type_node_s \
+	      : X##_aarch64_type_node_s \
+		  = make_signed_type (GET_MODE_PRECISION (mode))); \
+    break;
+
+  switch (mode)
+    {
+      INT_TYPES
+      case SFmode:
+	if (!float_aarch64_type_node)
+	  {
+	    float_aarch64_type_node = make_node (REAL_TYPE);
+	    TYPE_PRECISION (float_aarch64_type_node) = FLOAT_TYPE_SIZE;
+	    layout_type (float_aarch64_type_node);
+	  }
+	return float_aarch64_type_node;
+	break;
+      case DFmode:
+	if (!double_aarch64_type_node)
+	  {
+	    double_aarch64_type_node = make_node (REAL_TYPE);
+	    TYPE_PRECISION (double_aarch64_type_node) = DOUBLE_TYPE_SIZE;
+	    layout_type (double_aarch64_type_node);
+	  }
+	return double_aarch64_type_node;
+	break;
+      default:
+	gcc_unreachable ();
+    }
+}
+
+tree
+aarch64_build_vector_type (enum machine_mode mode,
+			   bool unsigned_p,
+			   bool poly_p)
+{
+  tree eltype;
+
+#define VECTOR_TYPES \
+  AARCH64_TYPE_BUILDER (V16QI) \
+  AARCH64_TYPE_BUILDER (V8HI) \
+  AARCH64_TYPE_BUILDER (V4SI) \
+  AARCH64_TYPE_BUILDER (V2DI) \
+  AARCH64_TYPE_BUILDER (V8QI) \
+  AARCH64_TYPE_BUILDER (V4HI) \
+  AARCH64_TYPE_BUILDER (V2SI) \
+  \
+  AARCH64_TYPE_BUILDER (V4SF) \
+  AARCH64_TYPE_BUILDER (V2DF) \
+  AARCH64_TYPE_BUILDER (V2SF) \
+/* Declare our "cache" of values.  */
+#undef AARCH64_TYPE_BUILDER
+#define AARCH64_TYPE_BUILDER(X) \
+  static tree X##_aarch64_type_node_s = NULL; \
+  static tree X##_aarch64_type_node_u = NULL; \
+  static tree X##_aarch64_type_node_p = NULL;
+
+  VECTOR_TYPES
+
+  gcc_assert (VECTOR_MODE_P (mode));
+
+#undef AARCH64_TYPE_BUILDER
+#define AARCH64_TYPE_BUILDER(X) \
+  case X##mode: \
+    if (unsigned_p) \
+      return X##_aarch64_type_node_u \
+	     ? X##_aarch64_type_node_u \
+	     : X##_aarch64_type_node_u \
+		= build_vector_type_for_mode (aarch64_build_scalar_type \
+						(GET_MODE_INNER (mode), \
+						 unsigned_p, poly_p), mode); \
+    else if (poly_p) \
+       return X##_aarch64_type_node_p \
+	      ? X##_aarch64_type_node_p \
+	      : X##_aarch64_type_node_p \
+		= build_vector_type_for_mode (aarch64_build_scalar_type \
+						(GET_MODE_INNER (mode), \
+						 unsigned_p, poly_p), mode); \
+    else \
+       return X##_aarch64_type_node_s \
+	      ? X##_aarch64_type_node_s \
+	      : X##_aarch64_type_node_s \
+		= build_vector_type_for_mode (aarch64_build_scalar_type \
+						(GET_MODE_INNER (mode), \
+						 unsigned_p, poly_p), mode); \
+    break;
+
+  switch (mode)
+    {
+      default:
+	eltype = aarch64_build_scalar_type (GET_MODE_INNER (mode),
+					    unsigned_p, poly_p);
+	return build_vector_type_for_mode (eltype, mode);
+	break;
+      VECTOR_TYPES
+   }
+}
+
+tree
+aarch64_build_type (enum machine_mode mode, bool unsigned_p, bool poly_p)
+{
+  if (VECTOR_MODE_P (mode))
+    return aarch64_build_vector_type (mode, unsigned_p, poly_p);
+  else
+    return aarch64_build_scalar_type (mode, unsigned_p, poly_p);
+}
+
+tree
+aarch64_build_signed_type (enum machine_mode mode)
+{
+  return aarch64_build_type (mode, false, false);
+}
+
+tree
+aarch64_build_unsigned_type (enum machine_mode mode)
+{
+  return aarch64_build_type (mode, true, false);
+}
+
+tree
+aarch64_build_poly_type (enum machine_mode mode)
+{
+  return aarch64_build_type (mode, false, true);
+}
+
 static void
 aarch64_init_simd_builtins (void)
 {
   unsigned int i, fcode = AARCH64_SIMD_BUILTIN_BASE + 1;
 
-  /* Scalar type nodes.  */
-  tree aarch64_simd_intQI_type_node;
-  tree aarch64_simd_intHI_type_node;
-  tree aarch64_simd_polyQI_type_node;
-  tree aarch64_simd_polyHI_type_node;
-  tree aarch64_simd_intSI_type_node;
-  tree aarch64_simd_intDI_type_node;
-  tree aarch64_simd_float_type_node;
-  tree aarch64_simd_double_type_node;
-
-  /* Pointer to scalar type nodes.  */
-  tree intQI_pointer_node;
-  tree intHI_pointer_node;
-  tree intSI_pointer_node;
-  tree intDI_pointer_node;
-  tree float_pointer_node;
-  tree double_pointer_node;
-
-  /* Const scalar type nodes.  */
-  tree const_intQI_node;
-  tree const_intHI_node;
-  tree const_intSI_node;
-  tree const_intDI_node;
-  tree const_float_node;
-  tree const_double_node;
-
-  /* Pointer to const scalar type nodes.  */
-  tree const_intQI_pointer_node;
-  tree const_intHI_pointer_node;
-  tree const_intSI_pointer_node;
-  tree const_intDI_pointer_node;
-  tree const_float_pointer_node;
-  tree const_double_pointer_node;
-
-  /* Vector type nodes.  */
-  tree V8QI_type_node;
-  tree V4HI_type_node;
-  tree V2SI_type_node;
-  tree V2SF_type_node;
-  tree V16QI_type_node;
-  tree V8HI_type_node;
-  tree V4SI_type_node;
-  tree V4SF_type_node;
-  tree V2DI_type_node;
-  tree V2DF_type_node;
-
-  /* Scalar unsigned type nodes.  */
-  tree intUQI_type_node;
-  tree intUHI_type_node;
-  tree intUSI_type_node;
-  tree intUDI_type_node;
-
-  /* Opaque integer types for structures of vectors.  */
-  tree intEI_type_node;
-  tree intOI_type_node;
-  tree intCI_type_node;
-  tree intXI_type_node;
-
-  /* Pointer to vector type nodes.  */
-  tree V8QI_pointer_node;
-  tree V4HI_pointer_node;
-  tree V2SI_pointer_node;
-  tree V2SF_pointer_node;
-  tree V16QI_pointer_node;
-  tree V8HI_pointer_node;
-  tree V4SI_pointer_node;
-  tree V4SF_pointer_node;
-  tree V2DI_pointer_node;
-  tree V2DF_pointer_node;
-
-  /* Operations which return results as pairs.  */
-  tree void_ftype_pv8qi_v8qi_v8qi;
-  tree void_ftype_pv4hi_v4hi_v4hi;
-  tree void_ftype_pv2si_v2si_v2si;
-  tree void_ftype_pv2sf_v2sf_v2sf;
-  tree void_ftype_pdi_di_di;
-  tree void_ftype_pv16qi_v16qi_v16qi;
-  tree void_ftype_pv8hi_v8hi_v8hi;
-  tree void_ftype_pv4si_v4si_v4si;
-  tree void_ftype_pv4sf_v4sf_v4sf;
-  tree void_ftype_pv2di_v2di_v2di;
-  tree void_ftype_pv2df_v2df_v2df;
-
-  tree reinterp_ftype_dreg[NUM_DREG_TYPES][NUM_DREG_TYPES];
-  tree reinterp_ftype_qreg[NUM_QREG_TYPES][NUM_QREG_TYPES];
-  tree dreg_types[NUM_DREG_TYPES], qreg_types[NUM_QREG_TYPES];
-
-  /* Create distinguished type nodes for AARCH64_SIMD vector element types,
-     and pointers to values of such types, so we can detect them later.  */
-  aarch64_simd_intQI_type_node =
-    make_signed_type (GET_MODE_PRECISION (QImode));
-  aarch64_simd_intHI_type_node =
-    make_signed_type (GET_MODE_PRECISION (HImode));
-  aarch64_simd_polyQI_type_node =
-    make_signed_type (GET_MODE_PRECISION (QImode));
-  aarch64_simd_polyHI_type_node =
-    make_signed_type (GET_MODE_PRECISION (HImode));
-  aarch64_simd_intSI_type_node =
-    make_signed_type (GET_MODE_PRECISION (SImode));
-  aarch64_simd_intDI_type_node =
-    make_signed_type (GET_MODE_PRECISION (DImode));
-  aarch64_simd_float_type_node = make_node (REAL_TYPE);
-  aarch64_simd_double_type_node = make_node (REAL_TYPE);
-  TYPE_PRECISION (aarch64_simd_float_type_node) = FLOAT_TYPE_SIZE;
-  TYPE_PRECISION (aarch64_simd_double_type_node) = DOUBLE_TYPE_SIZE;
-  layout_type (aarch64_simd_float_type_node);
-  layout_type (aarch64_simd_double_type_node);
+  /* Signed scalar type nodes.  */
+  tree aarch64_simd_intQI_type_node = aarch64_build_signed_type (QImode);
+  tree aarch64_simd_intHI_type_node = aarch64_build_signed_type (HImode);
+  tree aarch64_simd_intSI_type_node = aarch64_build_signed_type (SImode);
+  tree aarch64_simd_intDI_type_node = aarch64_build_signed_type (DImode);
+  tree aarch64_simd_intTI_type_node = aarch64_build_signed_type (TImode);
+  tree aarch64_simd_intEI_type_node = aarch64_build_signed_type (EImode);
+  tree aarch64_simd_intOI_type_node = aarch64_build_signed_type (OImode);
+  tree aarch64_simd_intCI_type_node = aarch64_build_signed_type (CImode);
+  tree aarch64_simd_intXI_type_node = aarch64_build_signed_type (XImode);
+
+  /* Unsigned scalar type nodes.  */
+  tree aarch64_simd_intUQI_type_node = aarch64_build_unsigned_type (QImode);
+  tree aarch64_simd_intUHI_type_node = aarch64_build_unsigned_type (HImode);
+  tree aarch64_simd_intUSI_type_node = aarch64_build_unsigned_type (SImode);
+  tree aarch64_simd_intUDI_type_node = aarch64_build_unsigned_type (DImode);
+
+  /* Poly scalar type nodes.  */
+  tree aarch64_simd_polyQI_type_node = aarch64_build_poly_type (QImode);
+  tree aarch64_simd_polyHI_type_node = aarch64_build_poly_type (HImode);
+  tree aarch64_simd_polyDI_type_node = aarch64_build_poly_type (DImode);
+  tree aarch64_simd_polyTI_type_node = aarch64_build_poly_type (TImode);
+
+  /* Float type nodes.  */
+  tree aarch64_simd_float_type_node = aarch64_build_signed_type (SFmode);
+  tree aarch64_simd_double_type_node = aarch64_build_signed_type (DFmode);
 
   /* Define typedefs which exactly correspond to the modes we are basing vector
      types on.  If you change these names you'll need to change
@@ -441,518 +569,139 @@ aarch64_init_simd_builtins (void)
 					     "__builtin_aarch64_simd_poly8");
   (*lang_hooks.types.register_builtin_type) (aarch64_simd_polyHI_type_node,
 					     "__builtin_aarch64_simd_poly16");
-
-  intQI_pointer_node = build_pointer_type (aarch64_simd_intQI_type_node);
-  intHI_pointer_node = build_pointer_type (aarch64_simd_intHI_type_node);
-  intSI_pointer_node = build_pointer_type (aarch64_simd_intSI_type_node);
-  intDI_pointer_node = build_pointer_type (aarch64_simd_intDI_type_node);
-  float_pointer_node = build_pointer_type (aarch64_simd_float_type_node);
-  double_pointer_node = build_pointer_type (aarch64_simd_double_type_node);
-
-  /* Next create constant-qualified versions of the above types.  */
-  const_intQI_node = build_qualified_type (aarch64_simd_intQI_type_node,
-					   TYPE_QUAL_CONST);
-  const_intHI_node = build_qualified_type (aarch64_simd_intHI_type_node,
-					   TYPE_QUAL_CONST);
-  const_intSI_node = build_qualified_type (aarch64_simd_intSI_type_node,
-					   TYPE_QUAL_CONST);
-  const_intDI_node = build_qualified_type (aarch64_simd_intDI_type_node,
-					   TYPE_QUAL_CONST);
-  const_float_node = build_qualified_type (aarch64_simd_float_type_node,
-					   TYPE_QUAL_CONST);
-  const_double_node = build_qualified_type (aarch64_simd_double_type_node,
-					    TYPE_QUAL_CONST);
-
-  const_intQI_pointer_node = build_pointer_type (const_intQI_node);
-  const_intHI_pointer_node = build_pointer_type (const_intHI_node);
-  const_intSI_pointer_node = build_pointer_type (const_intSI_node);
-  const_intDI_pointer_node = build_pointer_type (const_intDI_node);
-  const_float_pointer_node = build_pointer_type (const_float_node);
-  const_double_pointer_node = build_pointer_type (const_double_node);
-
-  /* Now create vector types based on our AARCH64 SIMD element types.  */
-  /* 64-bit vectors.  */
-  V8QI_type_node =
-    build_vector_type_for_mode (aarch64_simd_intQI_type_node, V8QImode);
-  V4HI_type_node =
-    build_vector_type_for_mode (aarch64_simd_intHI_type_node, V4HImode);
-  V2SI_type_node =
-    build_vector_type_for_mode (aarch64_simd_intSI_type_node, V2SImode);
-  V2SF_type_node =
-    build_vector_type_for_mode (aarch64_simd_float_type_node, V2SFmode);
-  /* 128-bit vectors.  */
-  V16QI_type_node =
-    build_vector_type_for_mode (aarch64_simd_intQI_type_node, V16QImode);
-  V8HI_type_node =
-    build_vector_type_for_mode (aarch64_simd_intHI_type_node, V8HImode);
-  V4SI_type_node =
-    build_vector_type_for_mode (aarch64_simd_intSI_type_node, V4SImode);
-  V4SF_type_node =
-    build_vector_type_for_mode (aarch64_simd_float_type_node, V4SFmode);
-  V2DI_type_node =
-    build_vector_type_for_mode (aarch64_simd_intDI_type_node, V2DImode);
-  V2DF_type_node =
-    build_vector_type_for_mode (aarch64_simd_double_type_node, V2DFmode);
-
-  /* Unsigned integer types for various mode sizes.  */
-  intUQI_type_node = make_unsigned_type (GET_MODE_PRECISION (QImode));
-  intUHI_type_node = make_unsigned_type (GET_MODE_PRECISION (HImode));
-  intUSI_type_node = make_unsigned_type (GET_MODE_PRECISION (SImode));
-  intUDI_type_node = make_unsigned_type (GET_MODE_PRECISION (DImode));
-
-  (*lang_hooks.types.register_builtin_type) (intUQI_type_node,
-					     "__builtin_aarch64_simd_uqi");
-  (*lang_hooks.types.register_builtin_type) (intUHI_type_node,
-					     "__builtin_aarch64_simd_uhi");
-  (*lang_hooks.types.register_builtin_type) (intUSI_type_node,
-					     "__builtin_aarch64_simd_usi");
-  (*lang_hooks.types.register_builtin_type) (intUDI_type_node,
-					     "__builtin_aarch64_simd_udi");
-
-  /* Opaque integer types for structures of vectors.  */
-  intEI_type_node = make_signed_type (GET_MODE_PRECISION (EImode));
-  intOI_type_node = make_signed_type (GET_MODE_PRECISION (OImode));
-  intCI_type_node = make_signed_type (GET_MODE_PRECISION (CImode));
-  intXI_type_node = make_signed_type (GET_MODE_PRECISION (XImode));
-
-  (*lang_hooks.types.register_builtin_type) (intTI_type_node,
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_polyDI_type_node,
+					     "__builtin_aarch64_simd_poly64");
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_polyTI_type_node,
+					     "__builtin_aarch64_simd_poly128");
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intTI_type_node,
 					     "__builtin_aarch64_simd_ti");
-  (*lang_hooks.types.register_builtin_type) (intEI_type_node,
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intEI_type_node,
 					     "__builtin_aarch64_simd_ei");
-  (*lang_hooks.types.register_builtin_type) (intOI_type_node,
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intOI_type_node,
 					     "__builtin_aarch64_simd_oi");
-  (*lang_hooks.types.register_builtin_type) (intCI_type_node,
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intCI_type_node,
 					     "__builtin_aarch64_simd_ci");
-  (*lang_hooks.types.register_builtin_type) (intXI_type_node,
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intXI_type_node,
 					     "__builtin_aarch64_simd_xi");
 
-  /* Pointers to vector types.  */
-  V8QI_pointer_node = build_pointer_type (V8QI_type_node);
-  V4HI_pointer_node = build_pointer_type (V4HI_type_node);
-  V2SI_pointer_node = build_pointer_type (V2SI_type_node);
-  V2SF_pointer_node = build_pointer_type (V2SF_type_node);
-  V16QI_pointer_node = build_pointer_type (V16QI_type_node);
-  V8HI_pointer_node = build_pointer_type (V8HI_type_node);
-  V4SI_pointer_node = build_pointer_type (V4SI_type_node);
-  V4SF_pointer_node = build_pointer_type (V4SF_type_node);
-  V2DI_pointer_node = build_pointer_type (V2DI_type_node);
-  V2DF_pointer_node = build_pointer_type (V2DF_type_node);
-
-  /* Operations which return results as pairs.  */
-  void_ftype_pv8qi_v8qi_v8qi =
-    build_function_type_list (void_type_node, V8QI_pointer_node,
-			      V8QI_type_node, V8QI_type_node, NULL);
-  void_ftype_pv4hi_v4hi_v4hi =
-    build_function_type_list (void_type_node, V4HI_pointer_node,
-			      V4HI_type_node, V4HI_type_node, NULL);
-  void_ftype_pv2si_v2si_v2si =
-    build_function_type_list (void_type_node, V2SI_pointer_node,
-			      V2SI_type_node, V2SI_type_node, NULL);
-  void_ftype_pv2sf_v2sf_v2sf =
-    build_function_type_list (void_type_node, V2SF_pointer_node,
-			      V2SF_type_node, V2SF_type_node, NULL);
-  void_ftype_pdi_di_di =
-    build_function_type_list (void_type_node, intDI_pointer_node,
-			      aarch64_simd_intDI_type_node,
-			      aarch64_simd_intDI_type_node, NULL);
-  void_ftype_pv16qi_v16qi_v16qi =
-    build_function_type_list (void_type_node, V16QI_pointer_node,
-			      V16QI_type_node, V16QI_type_node, NULL);
-  void_ftype_pv8hi_v8hi_v8hi =
-    build_function_type_list (void_type_node, V8HI_pointer_node,
-			      V8HI_type_node, V8HI_type_node, NULL);
-  void_ftype_pv4si_v4si_v4si =
-    build_function_type_list (void_type_node, V4SI_pointer_node,
-			      V4SI_type_node, V4SI_type_node, NULL);
-  void_ftype_pv4sf_v4sf_v4sf =
-    build_function_type_list (void_type_node, V4SF_pointer_node,
-			      V4SF_type_node, V4SF_type_node, NULL);
-  void_ftype_pv2di_v2di_v2di =
-    build_function_type_list (void_type_node, V2DI_pointer_node,
-			      V2DI_type_node, V2DI_type_node, NULL);
-  void_ftype_pv2df_v2df_v2df =
-    build_function_type_list (void_type_node, V2DF_pointer_node,
-			      V2DF_type_node, V2DF_type_node, NULL);
-
-  dreg_types[0] = V8QI_type_node;
-  dreg_types[1] = V4HI_type_node;
-  dreg_types[2] = V2SI_type_node;
-  dreg_types[3] = V2SF_type_node;
-  dreg_types[4] = aarch64_simd_intDI_type_node;
-  dreg_types[5] = aarch64_simd_double_type_node;
-
-  qreg_types[0] = V16QI_type_node;
-  qreg_types[1] = V8HI_type_node;
-  qreg_types[2] = V4SI_type_node;
-  qreg_types[3] = V4SF_type_node;
-  qreg_types[4] = V2DI_type_node;
-  qreg_types[5] = V2DF_type_node;
-
-  /* If NUM_DREG_TYPES != NUM_QREG_TYPES, we will need separate nested loops
-     for qreg and dreg reinterp inits.  */
-  for (i = 0; i < NUM_DREG_TYPES; i++)
-    {
-      int j;
-      for (j = 0; j < NUM_DREG_TYPES; j++)
-	{
-	  reinterp_ftype_dreg[i][j]
-	    = build_function_type_list (dreg_types[i], dreg_types[j], NULL);
-	  reinterp_ftype_qreg[i][j]
-	    = build_function_type_list (qreg_types[i], qreg_types[j], NULL);
-	}
-    }
+  /* Unsigned integer types for various mode sizes.  */
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intUQI_type_node,
+					     "__builtin_aarch64_simd_uqi");
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intUHI_type_node,
+					     "__builtin_aarch64_simd_uhi");
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intUSI_type_node,
+					     "__builtin_aarch64_simd_usi");
+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intUDI_type_node,
+					     "__builtin_aarch64_simd_udi");
 
   for (i = 0; i < ARRAY_SIZE (aarch64_simd_builtin_data); i++, fcode++)
     {
+      bool print_type_signature_p = false;
+      char type_signature[SIMD_MAX_BUILTIN_ARGS] = { 0 };
       aarch64_simd_builtin_datum *d = &aarch64_simd_builtin_data[i];
       const char *const modenames[] =
-      {
-	"v8qi", "v4hi", "v2si", "v2sf", "di", "df",
-	"v16qi", "v8hi", "v4si", "v4sf", "v2di", "v2df",
-	"ti", "ei", "oi", "xi", "si", "hi", "qi"
-      };
+	{
+	  "v8qi", "v4hi", "v2si", "v2sf", "di", "df",
+	  "v16qi", "v8hi", "v4si", "v4sf", "v2di", "v2df",
+	  "ti", "ei", "oi", "xi", "si", "sf", "hi", "qi"
+	};
+      const enum machine_mode modes[] =
+	{
+	  V8QImode, V4HImode, V2SImode, V2SFmode, DImode, DFmode,
+	  V16QImode, V8HImode, V4SImode, V4SFmode, V2DImode,
+	  V2DFmode, TImode, EImode, OImode, XImode, SImode,
+	  SFmode, HImode, QImode
+	};
       char namebuf[60];
       tree ftype = NULL;
       tree fndecl = NULL;
-      int is_load = 0;
-      int is_store = 0;
 
       gcc_assert (ARRAY_SIZE (modenames) == T_MAX);
 
       d->fcode = fcode;
 
-      switch (d->itype)
+      /* We must track two variables here.  op_num is
+	 the operand number as in the RTL pattern.  This is
+	 required to access the mode (e.g. V4SF mode) of the
+	 argument, from which the base type can be derived.
+	 arg_num is an index in to the qualifiers data, which
+	 gives qualifiers to the type (e.g. const unsigned).
+	 The reason these two variables may differ by one is the
+	 void return type.  While all return types take the 0th entry
+	 in the qualifiers array, there is no operand for them in the
+	 RTL pattern.  */
+      int op_num = insn_data[d->code].n_operands - 1;
+      int arg_num = d->qualifiers[0] & qualifier_void
+		      ? op_num + 1
+		      : op_num;
+      tree return_type = void_type_node, args = void_list_node;
+      tree eltype;
+
+      /* Build a function type directly from the insn_data for this
+	 builtin.  The build_function_type () function takes care of
+	 removing duplicates for us.  */
+      for (; op_num >= 0; arg_num--, op_num--)
 	{
-	case AARCH64_SIMD_LOAD1:
-	case AARCH64_SIMD_LOAD1LANE:
-	case AARCH64_SIMD_LOADSTRUCT:
-	case AARCH64_SIMD_LOADSTRUCTLANE:
-	    is_load = 1;
-	  /* Fall through.  */
-	case AARCH64_SIMD_STORE1:
-	case AARCH64_SIMD_STORE1LANE:
-	case AARCH64_SIMD_STORESTRUCT:
-	case AARCH64_SIMD_STORESTRUCTLANE:
-	    if (!is_load)
-	      is_store = 1;
-	  /* Fall through.  */
-	case AARCH64_SIMD_UNOP:
-	case AARCH64_SIMD_BINOP:
-	case AARCH64_SIMD_TERNOP:
-	case AARCH64_SIMD_QUADOP:
-	case AARCH64_SIMD_COMBINE:
-	case AARCH64_SIMD_CONVERT:
-	case AARCH64_SIMD_CREATE:
-	case AARCH64_SIMD_DUP:
-	case AARCH64_SIMD_DUPLANE:
-	case AARCH64_SIMD_FIXCONV:
-	case AARCH64_SIMD_GETLANE:
-	case AARCH64_SIMD_LANEMAC:
-	case AARCH64_SIMD_LANEMUL:
-	case AARCH64_SIMD_LANEMULH:
-	case AARCH64_SIMD_LANEMULL:
-	case AARCH64_SIMD_LOGICBINOP:
-	case AARCH64_SIMD_SCALARMAC:
-	case AARCH64_SIMD_SCALARMUL:
-	case AARCH64_SIMD_SCALARMULH:
-	case AARCH64_SIMD_SCALARMULL:
-	case AARCH64_SIMD_SELECT:
-	case AARCH64_SIMD_SETLANE:
-	case AARCH64_SIMD_SHIFTACC:
-	case AARCH64_SIMD_SHIFTIMM:
-	case AARCH64_SIMD_SHIFTINSERT:
-	case AARCH64_SIMD_SPLIT:
-	case AARCH64_SIMD_VTBL:
-	case AARCH64_SIMD_VTBX:
-	  {
-	    int k;
-	    tree return_type = void_type_node, args = void_list_node;
-	    tree eltype;
-	    /* Build a function type directly from the insn_data for this
-	       builtin.  The build_function_type () function takes care of
-	       removing duplicates for us.  */
-
-	    for (k = insn_data[d->code].n_operands -1; k >= 0; k--)
-	      {
-		/* Skip an internal operand for vget_{low, high}.  */
-		if (k == 2 && d->itype == AARCH64_SIMD_SPLIT)
-		  continue;
-
-		if (is_load && k == 1)
-		  {
-		    /* AdvSIMD load patterns always have the memory operand
-		       (a DImode pointer) in the operand 1 position.  We
-		       want a const pointer to the element type in that
-		       position.  */
-		    gcc_assert (insn_data[d->code].operand[k].mode == DImode);
-
-		    switch (d->mode)
-		      {
-		      case T_V8QI:
-		      case T_V16QI:
-			eltype = const_intQI_pointer_node;
-			break;
-
-		      case T_V4HI:
-		      case T_V8HI:
-			eltype = const_intHI_pointer_node;
-			break;
-
-		      case T_V2SI:
-		      case T_V4SI:
-			eltype = const_intSI_pointer_node;
-			break;
-
-		      case T_V2SF:
-		      case T_V4SF:
-			eltype = const_float_pointer_node;
-			break;
-
-		      case T_DI:
-		      case T_V2DI:
-			eltype = const_intDI_pointer_node;
-			break;
-
-		      case T_DF:
-		      case T_V2DF:
-			eltype = const_double_pointer_node;
-			break;
-
-		      default:
-			gcc_unreachable ();
-		      }
-		  }
-		else if (is_store && k == 0)
-		  {
-		    /* Similarly, AdvSIMD store patterns use operand 0 as
-		       the memory location to store to (a DImode pointer).
-		       Use a pointer to the element type of the store in
-		       that position.  */
-		    gcc_assert (insn_data[d->code].operand[k].mode == DImode);
-
-		    switch (d->mode)
-		      {
-		      case T_V8QI:
-		      case T_V16QI:
-			eltype = intQI_pointer_node;
-			break;
-
-		      case T_V4HI:
-		      case T_V8HI:
-			eltype = intHI_pointer_node;
-			break;
-
-		      case T_V2SI:
-		      case T_V4SI:
-			eltype = intSI_pointer_node;
-			break;
-
-		      case T_V2SF:
-		      case T_V4SF:
-			eltype = float_pointer_node;
-			break;
-
-		      case T_DI:
-		      case T_V2DI:
-			eltype = intDI_pointer_node;
-			break;
-
-		      case T_DF:
-		      case T_V2DF:
-			eltype = double_pointer_node;
-			break;
-
-		      default:
-			gcc_unreachable ();
-		      }
-		  }
-		else
-		  {
-		    switch (insn_data[d->code].operand[k].mode)
-		      {
-		      case VOIDmode:
-			eltype = void_type_node;
-			break;
-			/* Scalars.  */
-		      case QImode:
-			eltype = aarch64_simd_intQI_type_node;
-			break;
-		      case HImode:
-			eltype = aarch64_simd_intHI_type_node;
-			break;
-		      case SImode:
-			eltype = aarch64_simd_intSI_type_node;
-			break;
-		      case SFmode:
-			eltype = aarch64_simd_float_type_node;
-			break;
-		      case DFmode:
-			eltype = aarch64_simd_double_type_node;
-			break;
-		      case DImode:
-			eltype = aarch64_simd_intDI_type_node;
-			break;
-		      case TImode:
-			eltype = intTI_type_node;
-			break;
-		      case EImode:
-			eltype = intEI_type_node;
-			break;
-		      case OImode:
-			eltype = intOI_type_node;
-			break;
-		      case CImode:
-			eltype = intCI_type_node;
-			break;
-		      case XImode:
-			eltype = intXI_type_node;
-			break;
-			/* 64-bit vectors.  */
-		      case V8QImode:
-			eltype = V8QI_type_node;
-			break;
-		      case V4HImode:
-			eltype = V4HI_type_node;
-			break;
-		      case V2SImode:
-			eltype = V2SI_type_node;
-			break;
-		      case V2SFmode:
-			eltype = V2SF_type_node;
-			break;
-			/* 128-bit vectors.  */
-		      case V16QImode:
-			eltype = V16QI_type_node;
-			break;
-		      case V8HImode:
-			eltype = V8HI_type_node;
-			break;
-		      case V4SImode:
-			eltype = V4SI_type_node;
-			break;
-		      case V4SFmode:
-			eltype = V4SF_type_node;
-			break;
-		      case V2DImode:
-			eltype = V2DI_type_node;
-			break;
-		      case V2DFmode:
-			eltype = V2DF_type_node;
-			break;
-		      default:
-			gcc_unreachable ();
-		      }
-		  }
-
-		if (k == 0 && !is_store)
-		  return_type = eltype;
-		else
-		  args = tree_cons (NULL_TREE, eltype, args);
-	      }
-	    ftype = build_function_type (return_type, args);
-	  }
-	  break;
-
-	case AARCH64_SIMD_RESULTPAIR:
-	  {
-	    switch (insn_data[d->code].operand[1].mode)
-	      {
-	      case V8QImode:
-		ftype = void_ftype_pv8qi_v8qi_v8qi;
-		break;
-	      case V4HImode:
-		ftype = void_ftype_pv4hi_v4hi_v4hi;
-		break;
-	      case V2SImode:
-		ftype = void_ftype_pv2si_v2si_v2si;
-		break;
-	      case V2SFmode:
-		ftype = void_ftype_pv2sf_v2sf_v2sf;
-		break;
-	      case DImode:
-		ftype = void_ftype_pdi_di_di;
-		break;
-	      case V16QImode:
-		ftype = void_ftype_pv16qi_v16qi_v16qi;
-		break;
-	      case V8HImode:
-		ftype = void_ftype_pv8hi_v8hi_v8hi;
-		break;
-	      case V4SImode:
-		ftype = void_ftype_pv4si_v4si_v4si;
-		break;
-	      case V4SFmode:
-		ftype = void_ftype_pv4sf_v4sf_v4sf;
-		break;
-	      case V2DImode:
-		ftype = void_ftype_pv2di_v2di_v2di;
-		break;
-	      case V2DFmode:
-		ftype = void_ftype_pv2df_v2df_v2df;
-		break;
-	      default:
-		gcc_unreachable ();
-	      }
-	  }
-	  break;
+	  enum machine_mode op_mode = insn_data[d->code].operand[op_num].mode;
+	  enum aarch64_type_qualifiers qualifiers = d->qualifiers[arg_num];
 
-	case AARCH64_SIMD_REINTERP:
-	  {
-	    /* We iterate over 6 doubleword types, then 6 quadword
-	       types.  */
-	    int rhs_d = d->mode % NUM_DREG_TYPES;
-	    int rhs_q = (d->mode - NUM_DREG_TYPES) % NUM_QREG_TYPES;
-	    switch (insn_data[d->code].operand[0].mode)
-	      {
-	      case V8QImode:
-		ftype = reinterp_ftype_dreg[0][rhs_d];
-		break;
-	      case V4HImode:
-		ftype = reinterp_ftype_dreg[1][rhs_d];
-		break;
-	      case V2SImode:
-		ftype = reinterp_ftype_dreg[2][rhs_d];
-		break;
-	      case V2SFmode:
-		ftype = reinterp_ftype_dreg[3][rhs_d];
-		break;
-	      case DImode:
-		ftype = reinterp_ftype_dreg[4][rhs_d];
-		break;
-	      case DFmode:
-		ftype = reinterp_ftype_dreg[5][rhs_d];
-		break;
-	      case V16QImode:
-		ftype = reinterp_ftype_qreg[0][rhs_q];
-		break;
-	      case V8HImode:
-		ftype = reinterp_ftype_qreg[1][rhs_q];
-		break;
-	      case V4SImode:
-		ftype = reinterp_ftype_qreg[2][rhs_q];
-		break;
-	      case V4SFmode:
-		ftype = reinterp_ftype_qreg[3][rhs_q];
-		break;
-	      case V2DImode:
-		ftype = reinterp_ftype_qreg[4][rhs_q];
-		break;
-	      case V2DFmode:
-		ftype = reinterp_ftype_qreg[5][rhs_q];
-		break;
-	      default:
-		gcc_unreachable ();
-	      }
-	  }
-	  break;
+	  if (qualifiers & qualifier_unsigned)
+	    {
+	      type_signature[arg_num] = 'u';
+	      print_type_signature_p = true;
+	    }
+	  else if (qualifiers & qualifier_poly)
+	    {
+	      type_signature[arg_num] = 'p';
+	      print_type_signature_p = true;
+	    }
+	  else
+	    type_signature[arg_num] = 's';
 
-	default:
-	  gcc_unreachable ();
+	  /* Skip an internal operand for vget_{low, high}.  */
+	  if (qualifiers & qualifier_internal)
+	    continue;
+
+	  /* Some builtins have different user-facing types
+	     for certain arguments, encoded in d->mode.  */
+	  if (qualifiers & qualifier_map_mode)
+	      op_mode = modes[d->mode];
+
+	  /* For pointers, we want a pointer to the basic type
+	     of the vector.  */
+	  if (qualifiers & qualifier_pointer && VECTOR_MODE_P (op_mode))
+	    op_mode = GET_MODE_INNER (op_mode);
+
+	  eltype = aarch64_build_type (op_mode,
+				       qualifiers & qualifier_unsigned,
+				       qualifiers & qualifier_poly);
+
+	  /* Add qualifiers.  */
+	  if (qualifiers & qualifier_const)
+	    eltype = build_qualified_type (eltype, TYPE_QUAL_CONST);
+
+	  if (qualifiers & qualifier_pointer)
+	      eltype = build_pointer_type (eltype);
+
+	  /* If we have reached arg_num == 0, we are at a non-void
+	     return type.  Otherwise, we are still processing
+	     arguments.  */
+	  if (arg_num == 0)
+	    return_type = eltype;
+	  else
+	    args = tree_cons (NULL_TREE, eltype, args);
 	}
+
+      ftype = build_function_type (return_type, args);
+
       gcc_assert (ftype != NULL);
 
-      snprintf (namebuf, sizeof (namebuf), "__builtin_aarch64_%s%s",
-		d->name, modenames[d->mode]);
+      if (print_type_signature_p)
+	snprintf (namebuf, sizeof (namebuf), "__builtin_aarch64_%s%s_%s",
+		  d->name, modenames[d->mode], type_signature);
+      else
+	snprintf (namebuf, sizeof (namebuf), "__builtin_aarch64_%s%s",
+		  d->name, modenames[d->mode]);
 
       fndecl = add_builtin_function (namebuf, ftype, fcode, BUILT_IN_MD,
 				     NULL, NULL_TREE);
@@ -983,8 +732,6 @@ typedef enum
   SIMD_ARG_STOP
 } builtin_simd_arg;
 
-#define SIMD_MAX_BUILTIN_ARGS 5
-
 static rtx
 aarch64_simd_expand_args (rtx target, int icode, int have_retval,
 			  tree exp, ...)
@@ -1110,99 +857,58 @@ aarch64_simd_expand_builtin (int fcode,
 {
   aarch64_simd_builtin_datum *d =
 		&aarch64_simd_builtin_data[fcode - (AARCH64_SIMD_BUILTIN_BASE + 1)];
-  aarch64_simd_itype itype = d->itype;
   enum insn_code icode = d->code;
+  builtin_simd_arg args[SIMD_MAX_BUILTIN_ARGS];
+  int num_args = insn_data[d->code].n_operands;
+  int is_void = 0;
+  int k;
 
-  switch (itype)
-    {
-    case AARCH64_SIMD_UNOP:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_STOP);
+  is_void = !!(d->qualifiers[0] & qualifier_void);
 
-    case AARCH64_SIMD_BINOP:
-      {
-        rtx arg2 = expand_normal (CALL_EXPR_ARG (exp, 1));
-        /* Handle constants only if the predicate allows it.  */
-	bool op1_const_int_p =
-	  (CONST_INT_P (arg2)
-	   && (*insn_data[icode].operand[2].predicate)
-		(arg2, insn_data[icode].operand[2].mode));
-	return aarch64_simd_expand_args
-	  (target, icode, 1, exp,
-	   SIMD_ARG_COPY_TO_REG,
-	   op1_const_int_p ? SIMD_ARG_CONSTANT : SIMD_ARG_COPY_TO_REG,
-	   SIMD_ARG_STOP);
-      }
+  num_args += is_void;
 
-    case AARCH64_SIMD_TERNOP:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_STOP);
-
-    case AARCH64_SIMD_QUADOP:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_STOP);
-    case AARCH64_SIMD_LOAD1:
-    case AARCH64_SIMD_LOADSTRUCT:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);
-
-    case AARCH64_SIMD_STORE1:
-    case AARCH64_SIMD_STORESTRUCT:
-      return aarch64_simd_expand_args (target, icode, 0, exp,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);
-
-    case AARCH64_SIMD_REINTERP:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);
-
-    case AARCH64_SIMD_CREATE:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);
-
-    case AARCH64_SIMD_COMBINE:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);
-
-    case AARCH64_SIMD_GETLANE:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_CONSTANT,
-				       SIMD_ARG_STOP);
-
-    case AARCH64_SIMD_SETLANE:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_CONSTANT,
-				       SIMD_ARG_STOP);
-
-    case AARCH64_SIMD_SHIFTIMM:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_CONSTANT,
-				       SIMD_ARG_STOP);
-
-    case AARCH64_SIMD_SHIFTACC:
-    case AARCH64_SIMD_SHIFTINSERT:
-      return aarch64_simd_expand_args (target, icode, 1, exp,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_COPY_TO_REG,
-				       SIMD_ARG_CONSTANT,
-				       SIMD_ARG_STOP);
+  for (k = 1; k < num_args; k++)
+    {
+      /* We have four arrays of data, each indexed in a different fashion.
+	 qualifiers - element 0 always describes the function return type.
+	 operands - element 0 is either the operand for return value (if
+	   the function has a non-void return type) or the operand for the
+	   first argument.
+	 expr_args - element 0 always holds the first argument.
+	 args - element 0 is always used for the return type.  */
+      int qualifiers_k = k;
+      int operands_k = k - is_void;
+      int expr_args_k = k - 1;
+
+      if (d->qualifiers[qualifiers_k] & qualifier_immediate)
+	args[k] = SIMD_ARG_CONSTANT;
+      else if (d->qualifiers[qualifiers_k] & qualifier_maybe_immediate)
+	{
+	  rtx arg
+	    = expand_normal (CALL_EXPR_ARG (exp,
+					    (expr_args_k)));
+	  /* Handle constants only if the predicate allows it.  */
+	  bool op_const_int_p =
+	    (CONST_INT_P (arg)
+	     && (*insn_data[icode].operand[operands_k].predicate)
+		(arg, insn_data[icode].operand[operands_k].mode));
+	  args[k] = op_const_int_p ? SIMD_ARG_CONSTANT : SIMD_ARG_COPY_TO_REG;
+	}
+      else
+	args[k] = SIMD_ARG_COPY_TO_REG;
 
-    default:
-      gcc_unreachable ();
     }
+  args[k] = SIMD_ARG_STOP;
+
+  /* The interface to aarch64_simd_expand_args expects a 0 if
+     the function is void, and a 1 if it is not.  */
+  return aarch64_simd_expand_args
+	  (target, icode, !is_void, exp,
+	   args[1],
+	   args[2],
+	   args[3],
+	   args[4],
+	   SIMD_ARG_STOP);
 }
 
 /* Expand an expression EXP that calls a built-in function,
@@ -1242,11 +948,11 @@ aarch64_builtin_vectorized_function (tre
 #define AARCH64_CHECK_BUILTIN_MODE(C, N) 1
 #define AARCH64_FIND_FRINT_VARIANT(N) \
   (AARCH64_CHECK_BUILTIN_MODE (2, D) \
-    ? aarch64_builtin_decls[AARCH64_SIMD_BUILTIN_##N##v2df] \
+    ? aarch64_builtin_decls[AARCH64_SIMD_BUILTIN_UNOP_##N##v2df] \
     : (AARCH64_CHECK_BUILTIN_MODE (4, S) \
-	? aarch64_builtin_decls[AARCH64_SIMD_BUILTIN_##N##v4sf] \
+	? aarch64_builtin_decls[AARCH64_SIMD_BUILTIN_UNOP_##N##v4sf] \
 	: (AARCH64_CHECK_BUILTIN_MODE (2, S) \
-	   ? aarch64_builtin_decls[AARCH64_SIMD_BUILTIN_##N##v2sf] \
+	   ? aarch64_builtin_decls[AARCH64_SIMD_BUILTIN_UNOP_##N##v2sf] \
 	   : NULL_TREE)))
   if (DECL_BUILT_IN_CLASS (fndecl) == BUILT_IN_NORMAL)
     {
@@ -1259,30 +965,82 @@ aarch64_builtin_vectorized_function (tre
    && in_mode == N##Fmode && in_n == C)
 	case BUILT_IN_FLOOR:
 	case BUILT_IN_FLOORF:
-	  return AARCH64_FIND_FRINT_VARIANT (frintm);
+	  return AARCH64_FIND_FRINT_VARIANT (floor);
 	case BUILT_IN_CEIL:
 	case BUILT_IN_CEILF:
-	  return AARCH64_FIND_FRINT_VARIANT (frintp);
+	  return AARCH64_FIND_FRINT_VARIANT (ceil);
 	case BUILT_IN_TRUNC:
 	case BUILT_IN_TRUNCF:
-	  return AARCH64_FIND_FRINT_VARIANT (frintz);
+	  return AARCH64_FIND_FRINT_VARIANT (btrunc);
 	case BUILT_IN_ROUND:
 	case BUILT_IN_ROUNDF:
-	  return AARCH64_FIND_FRINT_VARIANT (frinta);
+	  return AARCH64_FIND_FRINT_VARIANT (round);
 	case BUILT_IN_NEARBYINT:
 	case BUILT_IN_NEARBYINTF:
-	  return AARCH64_FIND_FRINT_VARIANT (frinti);
+	  return AARCH64_FIND_FRINT_VARIANT (nearbyint);
 	case BUILT_IN_SQRT:
 	case BUILT_IN_SQRTF:
 	  return AARCH64_FIND_FRINT_VARIANT (sqrt);
 #undef AARCH64_CHECK_BUILTIN_MODE
 #define AARCH64_CHECK_BUILTIN_MODE(C, N) \
+  (out_mode == SImode && out_n == C \
+   && in_mode == N##Imode && in_n == C)
+        case BUILT_IN_CLZ:
+          {
+            if (AARCH64_CHECK_BUILTIN_MODE (4, S))
+              return aarch64_builtin_decls[AARCH64_SIMD_BUILTIN_UNOP_clzv4si];
+            return NULL_TREE;
+          }
+#undef AARCH64_CHECK_BUILTIN_MODE
+#define AARCH64_CHECK_BUILTIN_MODE(C, N) \
   (out_mode == N##Imode && out_n == C \
    && in_mode == N##Fmode && in_n == C)
 	case BUILT_IN_LFLOOR:
-	  return AARCH64_FIND_FRINT_VARIANT (fcvtms);
+	case BUILT_IN_IFLOORF:
+	  {
+	    enum aarch64_builtins builtin;
+	    if (AARCH64_CHECK_BUILTIN_MODE (2, D))
+	      builtin = AARCH64_SIMD_BUILTIN_UNOP_lfloorv2dfv2di;
+	    else if (AARCH64_CHECK_BUILTIN_MODE (4, S))
+	      builtin = AARCH64_SIMD_BUILTIN_UNOP_lfloorv4sfv4si;
+	    else if (AARCH64_CHECK_BUILTIN_MODE (2, S))
+	      builtin = AARCH64_SIMD_BUILTIN_UNOP_lfloorv2sfv2si;
+	    else
+	      return NULL_TREE;
+
+	    return aarch64_builtin_decls[builtin];
+	  }
 	case BUILT_IN_LCEIL:
-	  return AARCH64_FIND_FRINT_VARIANT (fcvtps);
+	case BUILT_IN_ICEILF:
+	  {
+	    enum aarch64_builtins builtin;
+	    if (AARCH64_CHECK_BUILTIN_MODE (2, D))
+	      builtin = AARCH64_SIMD_BUILTIN_UNOP_lceilv2dfv2di;
+	    else if (AARCH64_CHECK_BUILTIN_MODE (4, S))
+	      builtin = AARCH64_SIMD_BUILTIN_UNOP_lceilv4sfv4si;
+	    else if (AARCH64_CHECK_BUILTIN_MODE (2, S))
+	      builtin = AARCH64_SIMD_BUILTIN_UNOP_lceilv2sfv2si;
+	    else
+	      return NULL_TREE;
+
+	    return aarch64_builtin_decls[builtin];
+	  }
+	case BUILT_IN_LROUND:
+	case BUILT_IN_IROUNDF:
+	  {
+	    enum aarch64_builtins builtin;
+	    if (AARCH64_CHECK_BUILTIN_MODE (2, D))
+	      builtin =	AARCH64_SIMD_BUILTIN_UNOP_lroundv2dfv2di;
+	    else if (AARCH64_CHECK_BUILTIN_MODE (4, S))
+	      builtin =	AARCH64_SIMD_BUILTIN_UNOP_lroundv4sfv4si;
+	    else if (AARCH64_CHECK_BUILTIN_MODE (2, S))
+	      builtin =	AARCH64_SIMD_BUILTIN_UNOP_lroundv2sfv2si;
+	    else
+	      return NULL_TREE;
+
+	    return aarch64_builtin_decls[builtin];
+	  }
+
 	default:
 	  return NULL_TREE;
       }
@@ -1290,5 +1048,160 @@ aarch64_builtin_vectorized_function (tre
 
   return NULL_TREE;
 }
+
+#undef VAR1
+#define VAR1(T, N, MAP, A) \
+  case AARCH64_SIMD_BUILTIN_##T##_##N##A:
+
+tree
+aarch64_fold_builtin (tree fndecl, int n_args ATTRIBUTE_UNUSED, tree *args,
+		      bool ignore ATTRIBUTE_UNUSED)
+{
+  int fcode = DECL_FUNCTION_CODE (fndecl);
+  tree type = TREE_TYPE (TREE_TYPE (fndecl));
+
+  switch (fcode)
+    {
+      BUILTIN_VALLDI (UNOP, abs, 2)
+	return fold_build1 (ABS_EXPR, type, args[0]);
+	break;
+      BUILTIN_VALLDI (BINOP, cmge, 0)
+	return fold_build2 (GE_EXPR, type, args[0], args[1]);
+	break;
+      BUILTIN_VALLDI (BINOP, cmgt, 0)
+	return fold_build2 (GT_EXPR, type, args[0], args[1]);
+	break;
+      BUILTIN_VALLDI (BINOP, cmeq, 0)
+	return fold_build2 (EQ_EXPR, type, args[0], args[1]);
+	break;
+      BUILTIN_VSDQ_I_DI (BINOP, cmtst, 0)
+	{
+	  tree and_node = fold_build2 (BIT_AND_EXPR, type, args[0], args[1]);
+	  tree vec_zero_node = build_zero_cst (type);
+	  return fold_build2 (NE_EXPR, type, and_node, vec_zero_node);
+	  break;
+	}
+      VAR1 (UNOP, floatv2si, 2, v2sf)
+      VAR1 (UNOP, floatv4si, 2, v4sf)
+      VAR1 (UNOP, floatv2di, 2, v2df)
+	return fold_build1 (FLOAT_EXPR, type, args[0]);
+      default:
+	break;
+    }
+
+  return NULL_TREE;
+}
+
+bool
+aarch64_gimple_fold_builtin (gimple_stmt_iterator *gsi)
+{
+  bool changed = false;
+  gimple stmt = gsi_stmt (*gsi);
+  tree call = gimple_call_fn (stmt);
+  tree fndecl;
+  gimple new_stmt = NULL;
+  if (call)
+    {
+      fndecl = gimple_call_fndecl (stmt);
+      if (fndecl)
+	{
+	  int fcode = DECL_FUNCTION_CODE (fndecl);
+	  int nargs = gimple_call_num_args (stmt);
+	  tree *args = (nargs > 0
+			? gimple_call_arg_ptr (stmt, 0)
+			: &error_mark_node);
+
+	  switch (fcode)
+	    {
+	      BUILTIN_VALL (UNOP, reduc_splus_, 10)
+		new_stmt = gimple_build_assign_with_ops (
+						REDUC_PLUS_EXPR,
+						gimple_call_lhs (stmt),
+						args[0],
+						NULL_TREE);
+		break;
+	      BUILTIN_VDQIF (UNOP, reduc_smax_, 10)
+		new_stmt = gimple_build_assign_with_ops (
+						REDUC_MAX_EXPR,
+						gimple_call_lhs (stmt),
+						args[0],
+						NULL_TREE);
+		break;
+	      BUILTIN_VDQIF (UNOP, reduc_smin_, 10)
+		new_stmt = gimple_build_assign_with_ops (
+						REDUC_MIN_EXPR,
+						gimple_call_lhs (stmt),
+						args[0],
+						NULL_TREE);
+		break;
+
+	    default:
+	      break;
+	    }
+	}
+    }
+
+  if (new_stmt)
+    {
+      gsi_replace (gsi, new_stmt, true);
+      changed = true;
+    }
+
+  return changed;
+}
+
 #undef AARCH64_CHECK_BUILTIN_MODE
 #undef AARCH64_FIND_FRINT_VARIANT
+#undef BUILTIN_DX
+#undef BUILTIN_SDQ_I
+#undef BUILTIN_SD_HSI
+#undef BUILTIN_V2F
+#undef BUILTIN_VALL
+#undef BUILTIN_VB
+#undef BUILTIN_VD
+#undef BUILTIN_VDC
+#undef BUILTIN_VDIC
+#undef BUILTIN_VDN
+#undef BUILTIN_VDQ
+#undef BUILTIN_VDQF
+#undef BUILTIN_VDQH
+#undef BUILTIN_VDQHS
+#undef BUILTIN_VDQIF
+#undef BUILTIN_VDQM
+#undef BUILTIN_VDQV
+#undef BUILTIN_VDQ_BHSI
+#undef BUILTIN_VDQ_I
+#undef BUILTIN_VDW
+#undef BUILTIN_VD_BHSI
+#undef BUILTIN_VD_HSI
+#undef BUILTIN_VD_RE
+#undef BUILTIN_VQ
+#undef BUILTIN_VQN
+#undef BUILTIN_VQW
+#undef BUILTIN_VQ_HSI
+#undef BUILTIN_VQ_S
+#undef BUILTIN_VSDQ_HSI
+#undef BUILTIN_VSDQ_I
+#undef BUILTIN_VSDQ_I_BHSI
+#undef BUILTIN_VSDQ_I_DI
+#undef BUILTIN_VSD_HSI
+#undef BUILTIN_VSQN_HSDI
+#undef BUILTIN_VSTRUCT
+#undef CF0
+#undef CF1
+#undef CF2
+#undef CF3
+#undef CF4
+#undef CF10
+#undef VAR1
+#undef VAR2
+#undef VAR3
+#undef VAR4
+#undef VAR5
+#undef VAR6
+#undef VAR7
+#undef VAR8
+#undef VAR9
+#undef VAR10
+#undef VAR11
+
Index: b/src/gcc/config/aarch64/aarch64-tune.md
===================================================================
--- a/src/gcc/config/aarch64/aarch64-tune.md
+++ b/src/gcc/config/aarch64/aarch64-tune.md
@@ -1,5 +1,5 @@
 ;; -*- buffer-read-only: t -*-
 ;; Generated automatically by gentune.sh from aarch64-cores.def
 (define_attr "tune"
-	"cortexa53,cortexa57,large,small"
+	"cortexa53,cortexa15"
 	(const (symbol_ref "((enum attr_tune) aarch64_tune)")))
Index: b/src/gcc/config/aarch64/aarch64-protos.h
===================================================================
--- a/src/gcc/config/aarch64/aarch64-protos.h
+++ b/src/gcc/config/aarch64/aarch64-protos.h
@@ -68,6 +68,24 @@ enum aarch64_symbol_context
    Each of of these represents a thread-local symbol, and corresponds to the
    thread local storage relocation operator for the symbol being referred to.
 
+   SYMBOL_TINY_ABSOLUTE
+
+   Generate symbol accesses as a PC relative address using a single
+   instruction.  To compute the address of symbol foo, we generate:
+
+   ADR x0, foo
+
+   SYMBOL_TINY_GOT
+
+   Generate symbol accesses via the GOT using a single PC relative
+   instruction.  To compute the address of symbol foo, we generate:
+
+   ldr t0, :got:foo
+
+   The value of foo can subsequently read using:
+
+   ldrb    t0, [t0]
+
    SYMBOL_FORCE_TO_MEM : Global variables are addressed using
    constant pool.  All variable addresses are spilled into constant
    pools.  The constant pools themselves are addressed using PC
@@ -81,6 +99,8 @@ enum aarch64_symbol_type
   SYMBOL_SMALL_TLSDESC,
   SYMBOL_SMALL_GOTTPREL,
   SYMBOL_SMALL_TPREL,
+  SYMBOL_TINY_ABSOLUTE,
+  SYMBOL_TINY_GOT,
   SYMBOL_FORCE_TO_MEM
 };
 
@@ -126,35 +146,66 @@ struct cpu_regmove_cost
   const int FP2FP;
 };
 
+/* Cost for vector insn classes.  */
+struct cpu_vector_cost
+{
+  const int scalar_stmt_cost;		 /* Cost of any scalar operation,
+					    excluding load and store.  */
+  const int scalar_load_cost;		 /* Cost of scalar load.  */
+  const int scalar_store_cost;		 /* Cost of scalar store.  */
+  const int vec_stmt_cost;		 /* Cost of any vector operation,
+					    excluding load, store,
+					    vector-to-scalar and
+					    scalar-to-vector operation.  */
+  const int vec_to_scalar_cost;		 /* Cost of vec-to-scalar operation.  */
+  const int scalar_to_vec_cost;		 /* Cost of scalar-to-vector
+					    operation.  */
+  const int vec_align_load_cost;	 /* Cost of aligned vector load.  */
+  const int vec_unalign_load_cost;	 /* Cost of unaligned vector load.  */
+  const int vec_unalign_store_cost;	 /* Cost of unaligned vector store.  */
+  const int vec_store_cost;		 /* Cost of vector store.  */
+  const int cond_taken_branch_cost;	 /* Cost of taken branch.  */
+  const int cond_not_taken_branch_cost;  /* Cost of not taken branch.  */
+};
+
 struct tune_params
 {
   const struct cpu_rtx_cost_table *const insn_extra_cost;
   const struct cpu_addrcost_table *const addr_cost;
   const struct cpu_regmove_cost *const regmove_cost;
+  const struct cpu_vector_cost *const vec_costs;
   const int memmov_cost;
 };
 
 HOST_WIDE_INT aarch64_initial_elimination_offset (unsigned, unsigned);
 bool aarch64_bitmask_imm (HOST_WIDE_INT val, enum machine_mode);
+enum aarch64_symbol_type
+aarch64_classify_symbolic_expression (rtx, enum aarch64_symbol_context);
 bool aarch64_constant_address_p (rtx);
 bool aarch64_float_const_zero_rtx_p (rtx);
 bool aarch64_function_arg_regno_p (unsigned);
 bool aarch64_gen_movmemqi (rtx *);
+bool aarch64_gimple_fold_builtin (gimple_stmt_iterator *);
 bool aarch64_is_extend_from_extract (enum machine_mode, rtx, rtx);
 bool aarch64_is_long_call_p (rtx);
 bool aarch64_label_mentioned_p (rtx);
 bool aarch64_legitimate_pic_operand_p (rtx);
 bool aarch64_move_imm (HOST_WIDE_INT, enum machine_mode);
+bool aarch64_mov_operand_p (rtx, enum aarch64_symbol_context,
+			    enum machine_mode);
+char *aarch64_output_scalar_simd_mov_immediate (rtx, enum machine_mode);
+char *aarch64_output_simd_mov_immediate (rtx, enum machine_mode, unsigned);
 bool aarch64_pad_arg_upward (enum machine_mode, const_tree);
 bool aarch64_pad_reg_upward (enum machine_mode, const_tree, bool);
 bool aarch64_regno_ok_for_base_p (int, bool);
 bool aarch64_regno_ok_for_index_p (int, bool);
 bool aarch64_simd_imm_scalar_p (rtx x, enum machine_mode mode);
 bool aarch64_simd_imm_zero_p (rtx, enum machine_mode);
+bool aarch64_simd_scalar_immediate_valid_for_move (rtx, enum machine_mode);
 bool aarch64_simd_shift_imm_p (rtx, enum machine_mode, bool);
+bool aarch64_simd_valid_immediate (rtx, enum machine_mode, bool,
+				   struct simd_immediate_info *);
 bool aarch64_symbolic_address_p (rtx);
-bool aarch64_symbolic_constant_p (rtx, enum aarch64_symbol_context,
-				  enum aarch64_symbol_type *);
 bool aarch64_uimm12_shift (HOST_WIDE_INT);
 const char *aarch64_output_casesi (rtx *);
 enum aarch64_symbol_type aarch64_classify_symbol (rtx,
@@ -165,9 +216,6 @@ int aarch64_asm_preferred_eh_data_format
 int aarch64_hard_regno_mode_ok (unsigned, enum machine_mode);
 int aarch64_hard_regno_nregs (unsigned, enum machine_mode);
 int aarch64_simd_attr_length_move (rtx);
-int aarch64_simd_immediate_valid_for_move (rtx, enum machine_mode, rtx *,
-					   int *, unsigned char *, int *,
-					   int *);
 int aarch64_uxt_size (int, HOST_WIDE_INT);
 rtx aarch64_final_eh_return_addr (void);
 rtx aarch64_legitimize_reload_address (rtx *, enum machine_mode, int, int, int);
@@ -177,6 +225,7 @@ rtx aarch64_simd_gen_const_vector_dup (e
 bool aarch64_simd_mem_operand_p (rtx);
 rtx aarch64_simd_vect_par_cnst_half (enum machine_mode, bool);
 rtx aarch64_tls_get_addr (void);
+tree aarch64_fold_builtin (tree, int, tree *, bool);
 unsigned aarch64_dbx_register_number (unsigned);
 unsigned aarch64_trampoline_size (void);
 void aarch64_asm_output_labelref (FILE *, const char *);
@@ -216,6 +265,10 @@ void aarch64_split_128bit_move (rtx, rtx
 
 bool aarch64_split_128bit_move_p (rtx, rtx);
 
+void aarch64_split_simd_combine (rtx, rtx, rtx);
+
+void aarch64_split_simd_move (rtx, rtx);
+
 /* Check for a legitimate floating point constant for FMOV.  */
 bool aarch64_float_const_representable_p (rtx);
 
@@ -251,6 +304,4 @@ extern bool aarch64_madd_needs_nop (rtx)
 extern void aarch64_final_prescan_insn (rtx);
 extern bool
 aarch64_expand_vec_perm_const (rtx target, rtx op0, rtx op1, rtx sel);
-
-char* aarch64_output_simd_mov_immediate (rtx *, enum machine_mode, unsigned);
 #endif /* GCC_AARCH64_PROTOS_H */
Index: b/src/gcc/config/aarch64/aarch64-simd-builtins.def
===================================================================
--- a/src/gcc/config/aarch64/aarch64-simd-builtins.def
+++ b/src/gcc/config/aarch64/aarch64-simd-builtins.def
@@ -18,248 +18,367 @@
    along with GCC; see the file COPYING3.  If not see
    <http://www.gnu.org/licenses/>.  */
 
-/* In the list below, the BUILTIN_<ITERATOR> macros should
-   correspond to the iterator used to construct the instruction's
-   patterns in aarch64-simd.md.  A helpful idiom to follow when
-   adding new builtins is to add a line for each pattern in the md
-   file.  Thus, ADDP, which has one pattern defined for the VD_BHSI
-   iterator, and one for DImode, has two entries below.  */
-
-  BUILTIN_VD_RE (CREATE, create)
-  BUILTIN_VQ_S (GETLANE, get_lane_signed)
-  BUILTIN_VDQ (GETLANE, get_lane_unsigned)
-  BUILTIN_VDQF (GETLANE, get_lane)
-  VAR1 (GETLANE, get_lane, di)
-  BUILTIN_VDC (COMBINE, combine)
-  BUILTIN_VB (BINOP, pmul)
-  BUILTIN_VDQF (UNOP, sqrt)
-  BUILTIN_VD_BHSI (BINOP, addp)
-  VAR1 (UNOP, addp, di)
-
-  BUILTIN_VD_RE (REINTERP, reinterpretdi)
-  BUILTIN_VDC (REINTERP, reinterpretv8qi)
-  BUILTIN_VDC (REINTERP, reinterpretv4hi)
-  BUILTIN_VDC (REINTERP, reinterpretv2si)
-  BUILTIN_VDC (REINTERP, reinterpretv2sf)
-  BUILTIN_VQ (REINTERP, reinterpretv16qi)
-  BUILTIN_VQ (REINTERP, reinterpretv8hi)
-  BUILTIN_VQ (REINTERP, reinterpretv4si)
-  BUILTIN_VQ (REINTERP, reinterpretv4sf)
-  BUILTIN_VQ (REINTERP, reinterpretv2di)
-  BUILTIN_VQ (REINTERP, reinterpretv2df)
+/* In the list below, the BUILTIN_<ITERATOR> macros expand to create
+   builtins for each of the modes described by <ITERATOR>.  When adding
+   new builtins to this list, a helpful idiom to follow is to add
+   a line for each pattern in the md file.  Thus, ADDP, which has one
+   pattern defined for the VD_BHSI iterator, and one for DImode, has two
+   entries below.
+
+   Parameter 1 is the 'type' of the intrinsic.  This is used to
+   describe the type modifiers (for example; unsigned) applied to
+   each of the parameters to the intrinsic function.
+
+   Parameter 2 is the name of the intrinsic.  This is appended
+   to `__builtin_aarch64_<name><mode>` to give the intrinsic name
+   as exported to the front-ends.
+
+   Parameter 3 describes how to map from the name to the CODE_FOR_
+   macro holding the RTL pattern for the intrinsic.  This mapping is:
+   0 - CODE_FOR_aarch64_<name><mode>
+   1-9 - CODE_FOR_<name><mode><1-9>
+   10 - CODE_FOR_<name><mode>.  */
+
+  BUILTIN_VD_RE (CREATE, create, 0)
+  BUILTIN_VDC (COMBINE, combine, 0)
+  BUILTIN_VB (BINOP, pmul, 0)
+  BUILTIN_VDQF (UNOP, sqrt, 2)
+  BUILTIN_VD_BHSI (BINOP, addp, 0)
+  VAR1 (UNOP, addp, 0, di)
+  VAR1 (UNOP, clz, 2, v4si)
+
+  BUILTIN_VALL (GETLANE, get_lane, 0)
+  VAR1 (GETLANE, get_lane, 0, di)
+
+  BUILTIN_VD_RE (REINTERP, reinterpretdi, 0)
+  BUILTIN_VDC (REINTERP, reinterpretv8qi, 0)
+  BUILTIN_VDC (REINTERP, reinterpretv4hi, 0)
+  BUILTIN_VDC (REINTERP, reinterpretv2si, 0)
+  BUILTIN_VDC (REINTERP, reinterpretv2sf, 0)
+  BUILTIN_VQ (REINTERP, reinterpretv16qi, 0)
+  BUILTIN_VQ (REINTERP, reinterpretv8hi, 0)
+  BUILTIN_VQ (REINTERP, reinterpretv4si, 0)
+  BUILTIN_VQ (REINTERP, reinterpretv4sf, 0)
+  BUILTIN_VQ (REINTERP, reinterpretv2di, 0)
+  BUILTIN_VQ (REINTERP, reinterpretv2df, 0)
 
-  BUILTIN_VDQ_I (BINOP, dup_lane)
-  BUILTIN_SDQ_I (BINOP, dup_lane)
+  BUILTIN_VDQ_I (BINOP, dup_lane, 0)
   /* Implemented by aarch64_<sur>q<r>shl<mode>.  */
-  BUILTIN_VSDQ_I (BINOP, sqshl)
-  BUILTIN_VSDQ_I (BINOP, uqshl)
-  BUILTIN_VSDQ_I (BINOP, sqrshl)
-  BUILTIN_VSDQ_I (BINOP, uqrshl)
+  BUILTIN_VSDQ_I (BINOP, sqshl, 0)
+  BUILTIN_VSDQ_I (BINOP, uqshl, 0)
+  BUILTIN_VSDQ_I (BINOP, sqrshl, 0)
+  BUILTIN_VSDQ_I (BINOP, uqrshl, 0)
   /* Implemented by aarch64_<su_optab><optab><mode>.  */
-  BUILTIN_VSDQ_I (BINOP, sqadd)
-  BUILTIN_VSDQ_I (BINOP, uqadd)
-  BUILTIN_VSDQ_I (BINOP, sqsub)
-  BUILTIN_VSDQ_I (BINOP, uqsub)
+  BUILTIN_VSDQ_I (BINOP, sqadd, 0)
+  BUILTIN_VSDQ_I (BINOP, uqadd, 0)
+  BUILTIN_VSDQ_I (BINOP, sqsub, 0)
+  BUILTIN_VSDQ_I (BINOP, uqsub, 0)
   /* Implemented by aarch64_<sur>qadd<mode>.  */
-  BUILTIN_VSDQ_I (BINOP, suqadd)
-  BUILTIN_VSDQ_I (BINOP, usqadd)
+  BUILTIN_VSDQ_I (BINOP, suqadd, 0)
+  BUILTIN_VSDQ_I (BINOP, usqadd, 0)
 
   /* Implemented by aarch64_get_dreg<VSTRUCT:mode><VDC:mode>.  */
-  BUILTIN_VDC (GETLANE, get_dregoi)
-  BUILTIN_VDC (GETLANE, get_dregci)
-  BUILTIN_VDC (GETLANE, get_dregxi)
+  BUILTIN_VDC (GETLANE, get_dregoi, 0)
+  BUILTIN_VDC (GETLANE, get_dregci, 0)
+  BUILTIN_VDC (GETLANE, get_dregxi, 0)
   /* Implemented by aarch64_get_qreg<VSTRUCT:mode><VQ:mode>.  */
-  BUILTIN_VQ (GETLANE, get_qregoi)
-  BUILTIN_VQ (GETLANE, get_qregci)
-  BUILTIN_VQ (GETLANE, get_qregxi)
+  BUILTIN_VQ (GETLANE, get_qregoi, 0)
+  BUILTIN_VQ (GETLANE, get_qregci, 0)
+  BUILTIN_VQ (GETLANE, get_qregxi, 0)
   /* Implemented by aarch64_set_qreg<VSTRUCT:mode><VQ:mode>.  */
-  BUILTIN_VQ (SETLANE, set_qregoi)
-  BUILTIN_VQ (SETLANE, set_qregci)
-  BUILTIN_VQ (SETLANE, set_qregxi)
+  BUILTIN_VQ (SETLANE, set_qregoi, 0)
+  BUILTIN_VQ (SETLANE, set_qregci, 0)
+  BUILTIN_VQ (SETLANE, set_qregxi, 0)
   /* Implemented by aarch64_ld<VSTRUCT:nregs><VDC:mode>.  */
-  BUILTIN_VDC (LOADSTRUCT, ld2)
-  BUILTIN_VDC (LOADSTRUCT, ld3)
-  BUILTIN_VDC (LOADSTRUCT, ld4)
+  BUILTIN_VDC (LOADSTRUCT, ld2, 0)
+  BUILTIN_VDC (LOADSTRUCT, ld3, 0)
+  BUILTIN_VDC (LOADSTRUCT, ld4, 0)
   /* Implemented by aarch64_ld<VSTRUCT:nregs><VQ:mode>.  */
-  BUILTIN_VQ (LOADSTRUCT, ld2)
-  BUILTIN_VQ (LOADSTRUCT, ld3)
-  BUILTIN_VQ (LOADSTRUCT, ld4)
+  BUILTIN_VQ (LOADSTRUCT, ld2, 0)
+  BUILTIN_VQ (LOADSTRUCT, ld3, 0)
+  BUILTIN_VQ (LOADSTRUCT, ld4, 0)
   /* Implemented by aarch64_st<VSTRUCT:nregs><VDC:mode>.  */
-  BUILTIN_VDC (STORESTRUCT, st2)
-  BUILTIN_VDC (STORESTRUCT, st3)
-  BUILTIN_VDC (STORESTRUCT, st4)
+  BUILTIN_VDC (STORESTRUCT, st2, 0)
+  BUILTIN_VDC (STORESTRUCT, st3, 0)
+  BUILTIN_VDC (STORESTRUCT, st4, 0)
   /* Implemented by aarch64_st<VSTRUCT:nregs><VQ:mode>.  */
-  BUILTIN_VQ (STORESTRUCT, st2)
-  BUILTIN_VQ (STORESTRUCT, st3)
-  BUILTIN_VQ (STORESTRUCT, st4)
-
-  BUILTIN_VQW (BINOP, saddl2)
-  BUILTIN_VQW (BINOP, uaddl2)
-  BUILTIN_VQW (BINOP, ssubl2)
-  BUILTIN_VQW (BINOP, usubl2)
-  BUILTIN_VQW (BINOP, saddw2)
-  BUILTIN_VQW (BINOP, uaddw2)
-  BUILTIN_VQW (BINOP, ssubw2)
-  BUILTIN_VQW (BINOP, usubw2)
+  BUILTIN_VQ (STORESTRUCT, st2, 0)
+  BUILTIN_VQ (STORESTRUCT, st3, 0)
+  BUILTIN_VQ (STORESTRUCT, st4, 0)
+
+  BUILTIN_VQW (BINOP, saddl2, 0)
+  BUILTIN_VQW (BINOP, uaddl2, 0)
+  BUILTIN_VQW (BINOP, ssubl2, 0)
+  BUILTIN_VQW (BINOP, usubl2, 0)
+  BUILTIN_VQW (BINOP, saddw2, 0)
+  BUILTIN_VQW (BINOP, uaddw2, 0)
+  BUILTIN_VQW (BINOP, ssubw2, 0)
+  BUILTIN_VQW (BINOP, usubw2, 0)
   /* Implemented by aarch64_<ANY_EXTEND:su><ADDSUB:optab>l<mode>.  */
-  BUILTIN_VDW (BINOP, saddl)
-  BUILTIN_VDW (BINOP, uaddl)
-  BUILTIN_VDW (BINOP, ssubl)
-  BUILTIN_VDW (BINOP, usubl)
+  BUILTIN_VDW (BINOP, saddl, 0)
+  BUILTIN_VDW (BINOP, uaddl, 0)
+  BUILTIN_VDW (BINOP, ssubl, 0)
+  BUILTIN_VDW (BINOP, usubl, 0)
   /* Implemented by aarch64_<ANY_EXTEND:su><ADDSUB:optab>w<mode>.  */
-  BUILTIN_VDW (BINOP, saddw)
-  BUILTIN_VDW (BINOP, uaddw)
-  BUILTIN_VDW (BINOP, ssubw)
-  BUILTIN_VDW (BINOP, usubw)
+  BUILTIN_VDW (BINOP, saddw, 0)
+  BUILTIN_VDW (BINOP, uaddw, 0)
+  BUILTIN_VDW (BINOP, ssubw, 0)
+  BUILTIN_VDW (BINOP, usubw, 0)
   /* Implemented by aarch64_<sur>h<addsub><mode>.  */
-  BUILTIN_VQ_S (BINOP, shadd)
-  BUILTIN_VQ_S (BINOP, uhadd)
-  BUILTIN_VQ_S (BINOP, srhadd)
-  BUILTIN_VQ_S (BINOP, urhadd)
+  BUILTIN_VQ_S (BINOP, shadd, 0)
+  BUILTIN_VQ_S (BINOP, uhadd, 0)
+  BUILTIN_VQ_S (BINOP, srhadd, 0)
+  BUILTIN_VQ_S (BINOP, urhadd, 0)
   /* Implemented by aarch64_<sur><addsub>hn<mode>.  */
-  BUILTIN_VQN (BINOP, addhn)
-  BUILTIN_VQN (BINOP, raddhn)
+  BUILTIN_VQN (BINOP, addhn, 0)
+  BUILTIN_VQN (BINOP, raddhn, 0)
   /* Implemented by aarch64_<sur><addsub>hn2<mode>.  */
-  BUILTIN_VQN (TERNOP, addhn2)
-  BUILTIN_VQN (TERNOP, raddhn2)
+  BUILTIN_VQN (TERNOP, addhn2, 0)
+  BUILTIN_VQN (TERNOP, raddhn2, 0)
 
-  BUILTIN_VSQN_HSDI (UNOP, sqmovun)
+  BUILTIN_VSQN_HSDI (UNOP, sqmovun, 0)
   /* Implemented by aarch64_<sur>qmovn<mode>.  */
-  BUILTIN_VSQN_HSDI (UNOP, sqmovn)
-  BUILTIN_VSQN_HSDI (UNOP, uqmovn)
+  BUILTIN_VSQN_HSDI (UNOP, sqmovn, 0)
+  BUILTIN_VSQN_HSDI (UNOP, uqmovn, 0)
   /* Implemented by aarch64_s<optab><mode>.  */
-  BUILTIN_VSDQ_I_BHSI (UNOP, sqabs)
-  BUILTIN_VSDQ_I_BHSI (UNOP, sqneg)
+  BUILTIN_VSDQ_I_BHSI (UNOP, sqabs, 0)
+  BUILTIN_VSDQ_I_BHSI (UNOP, sqneg, 0)
 
-  BUILTIN_VSD_HSI (QUADOP, sqdmlal_lane)
-  BUILTIN_VSD_HSI (QUADOP, sqdmlsl_lane)
-  BUILTIN_VSD_HSI (QUADOP, sqdmlal_laneq)
-  BUILTIN_VSD_HSI (QUADOP, sqdmlsl_laneq)
-  BUILTIN_VQ_HSI (TERNOP, sqdmlal2)
-  BUILTIN_VQ_HSI (TERNOP, sqdmlsl2)
-  BUILTIN_VQ_HSI (QUADOP, sqdmlal2_lane)
-  BUILTIN_VQ_HSI (QUADOP, sqdmlsl2_lane)
-  BUILTIN_VQ_HSI (QUADOP, sqdmlal2_laneq)
-  BUILTIN_VQ_HSI (QUADOP, sqdmlsl2_laneq)
-  BUILTIN_VQ_HSI (TERNOP, sqdmlal2_n)
-  BUILTIN_VQ_HSI (TERNOP, sqdmlsl2_n)
+  BUILTIN_VSD_HSI (QUADOP, sqdmlal_lane, 0)
+  BUILTIN_VSD_HSI (QUADOP, sqdmlsl_lane, 0)
+  BUILTIN_VSD_HSI (QUADOP, sqdmlal_laneq, 0)
+  BUILTIN_VSD_HSI (QUADOP, sqdmlsl_laneq, 0)
+  BUILTIN_VQ_HSI (TERNOP, sqdmlal2, 0)
+  BUILTIN_VQ_HSI (TERNOP, sqdmlsl2, 0)
+  BUILTIN_VQ_HSI (QUADOP, sqdmlal2_lane, 0)
+  BUILTIN_VQ_HSI (QUADOP, sqdmlsl2_lane, 0)
+  BUILTIN_VQ_HSI (QUADOP, sqdmlal2_laneq, 0)
+  BUILTIN_VQ_HSI (QUADOP, sqdmlsl2_laneq, 0)
+  BUILTIN_VQ_HSI (TERNOP, sqdmlal2_n, 0)
+  BUILTIN_VQ_HSI (TERNOP, sqdmlsl2_n, 0)
   /* Implemented by aarch64_sqdml<SBINQOPS:as>l<mode>.  */
-  BUILTIN_VSD_HSI (TERNOP, sqdmlal)
-  BUILTIN_VSD_HSI (TERNOP, sqdmlsl)
+  BUILTIN_VSD_HSI (TERNOP, sqdmlal, 0)
+  BUILTIN_VSD_HSI (TERNOP, sqdmlsl, 0)
   /* Implemented by aarch64_sqdml<SBINQOPS:as>l_n<mode>.  */
-  BUILTIN_VD_HSI (TERNOP, sqdmlal_n)
-  BUILTIN_VD_HSI (TERNOP, sqdmlsl_n)
+  BUILTIN_VD_HSI (TERNOP, sqdmlal_n, 0)
+  BUILTIN_VD_HSI (TERNOP, sqdmlsl_n, 0)
 
-  BUILTIN_VSD_HSI (BINOP, sqdmull)
-  BUILTIN_VSD_HSI (TERNOP, sqdmull_lane)
-  BUILTIN_VD_HSI (TERNOP, sqdmull_laneq)
-  BUILTIN_VD_HSI (BINOP, sqdmull_n)
-  BUILTIN_VQ_HSI (BINOP, sqdmull2)
-  BUILTIN_VQ_HSI (TERNOP, sqdmull2_lane)
-  BUILTIN_VQ_HSI (TERNOP, sqdmull2_laneq)
-  BUILTIN_VQ_HSI (BINOP, sqdmull2_n)
+  BUILTIN_VSD_HSI (BINOP, sqdmull, 0)
+  BUILTIN_VSD_HSI (TERNOP, sqdmull_lane, 0)
+  BUILTIN_VD_HSI (TERNOP, sqdmull_laneq, 0)
+  BUILTIN_VD_HSI (BINOP, sqdmull_n, 0)
+  BUILTIN_VQ_HSI (BINOP, sqdmull2, 0)
+  BUILTIN_VQ_HSI (TERNOP, sqdmull2_lane, 0)
+  BUILTIN_VQ_HSI (TERNOP, sqdmull2_laneq, 0)
+  BUILTIN_VQ_HSI (BINOP, sqdmull2_n, 0)
   /* Implemented by aarch64_sq<r>dmulh<mode>.  */
-  BUILTIN_VSDQ_HSI (BINOP, sqdmulh)
-  BUILTIN_VSDQ_HSI (BINOP, sqrdmulh)
+  BUILTIN_VSDQ_HSI (BINOP, sqdmulh, 0)
+  BUILTIN_VSDQ_HSI (BINOP, sqrdmulh, 0)
   /* Implemented by aarch64_sq<r>dmulh_lane<q><mode>.  */
-  BUILTIN_VDQHS (TERNOP, sqdmulh_lane)
-  BUILTIN_VDQHS (TERNOP, sqdmulh_laneq)
-  BUILTIN_VDQHS (TERNOP, sqrdmulh_lane)
-  BUILTIN_VDQHS (TERNOP, sqrdmulh_laneq)
-  BUILTIN_SD_HSI (TERNOP, sqdmulh_lane)
-  BUILTIN_SD_HSI (TERNOP, sqrdmulh_lane)
+  BUILTIN_VDQHS (TERNOP, sqdmulh_lane, 0)
+  BUILTIN_VDQHS (TERNOP, sqdmulh_laneq, 0)
+  BUILTIN_VDQHS (TERNOP, sqrdmulh_lane, 0)
+  BUILTIN_VDQHS (TERNOP, sqrdmulh_laneq, 0)
+  BUILTIN_SD_HSI (TERNOP, sqdmulh_lane, 0)
+  BUILTIN_SD_HSI (TERNOP, sqrdmulh_lane, 0)
 
-  BUILTIN_VSDQ_I_DI (BINOP, sshl_n)
-  BUILTIN_VSDQ_I_DI (BINOP, ushl_n)
+  BUILTIN_VSDQ_I_DI (BINOP, ashl, 3)
   /* Implemented by aarch64_<sur>shl<mode>.  */
-  BUILTIN_VSDQ_I_DI (BINOP, sshl)
-  BUILTIN_VSDQ_I_DI (BINOP, ushl)
-  BUILTIN_VSDQ_I_DI (BINOP, srshl)
-  BUILTIN_VSDQ_I_DI (BINOP, urshl)
+  BUILTIN_VSDQ_I_DI (BINOP, sshl, 0)
+  BUILTIN_VSDQ_I_DI (BINOP, ushl, 0)
+  BUILTIN_VSDQ_I_DI (BINOP, srshl, 0)
+  BUILTIN_VSDQ_I_DI (BINOP, urshl, 0)
 
-  BUILTIN_VSDQ_I_DI (SHIFTIMM, sshr_n)
-  BUILTIN_VSDQ_I_DI (SHIFTIMM, ushr_n)
+  BUILTIN_VSDQ_I_DI (SHIFTIMM, ashr, 3)
+  BUILTIN_VSDQ_I_DI (SHIFTIMM, lshr, 3)
   /* Implemented by aarch64_<sur>shr_n<mode>.  */
-  BUILTIN_VSDQ_I_DI (SHIFTIMM, srshr_n)
-  BUILTIN_VSDQ_I_DI (SHIFTIMM, urshr_n)
+  BUILTIN_VSDQ_I_DI (SHIFTIMM, srshr_n, 0)
+  BUILTIN_VSDQ_I_DI (SHIFTIMM, urshr_n, 0)
   /* Implemented by aarch64_<sur>sra_n<mode>.  */
-  BUILTIN_VSDQ_I_DI (SHIFTACC, ssra_n)
-  BUILTIN_VSDQ_I_DI (SHIFTACC, usra_n)
-  BUILTIN_VSDQ_I_DI (SHIFTACC, srsra_n)
-  BUILTIN_VSDQ_I_DI (SHIFTACC, ursra_n)
+  BUILTIN_VSDQ_I_DI (SHIFTACC, ssra_n, 0)
+  BUILTIN_VSDQ_I_DI (SHIFTACC, usra_n, 0)
+  BUILTIN_VSDQ_I_DI (SHIFTACC, srsra_n, 0)
+  BUILTIN_VSDQ_I_DI (SHIFTACC, ursra_n, 0)
   /* Implemented by aarch64_<sur>shll_n<mode>.  */
-  BUILTIN_VDW (SHIFTIMM, sshll_n)
-  BUILTIN_VDW (SHIFTIMM, ushll_n)
+  BUILTIN_VDW (SHIFTIMM, sshll_n, 0)
+  BUILTIN_VDW (SHIFTIMM, ushll_n, 0)
   /* Implemented by aarch64_<sur>shll2_n<mode>.  */
-  BUILTIN_VQW (SHIFTIMM, sshll2_n)
-  BUILTIN_VQW (SHIFTIMM, ushll2_n)
+  BUILTIN_VQW (SHIFTIMM, sshll2_n, 0)
+  BUILTIN_VQW (SHIFTIMM, ushll2_n, 0)
   /* Implemented by aarch64_<sur>q<r>shr<u>n_n<mode>.  */
-  BUILTIN_VSQN_HSDI (SHIFTIMM, sqshrun_n)
-  BUILTIN_VSQN_HSDI (SHIFTIMM, sqrshrun_n)
-  BUILTIN_VSQN_HSDI (SHIFTIMM, sqshrn_n)
-  BUILTIN_VSQN_HSDI (SHIFTIMM, uqshrn_n)
-  BUILTIN_VSQN_HSDI (SHIFTIMM, sqrshrn_n)
-  BUILTIN_VSQN_HSDI (SHIFTIMM, uqrshrn_n)
+  BUILTIN_VSQN_HSDI (SHIFTIMM, sqshrun_n, 0)
+  BUILTIN_VSQN_HSDI (SHIFTIMM, sqrshrun_n, 0)
+  BUILTIN_VSQN_HSDI (SHIFTIMM, sqshrn_n, 0)
+  BUILTIN_VSQN_HSDI (SHIFTIMM, uqshrn_n, 0)
+  BUILTIN_VSQN_HSDI (SHIFTIMM, sqrshrn_n, 0)
+  BUILTIN_VSQN_HSDI (SHIFTIMM, uqrshrn_n, 0)
   /* Implemented by aarch64_<sur>s<lr>i_n<mode>.  */
-  BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssri_n)
-  BUILTIN_VSDQ_I_DI (SHIFTINSERT, usri_n)
-  BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssli_n)
-  BUILTIN_VSDQ_I_DI (SHIFTINSERT, usli_n)
+  BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssri_n, 0)
+  BUILTIN_VSDQ_I_DI (SHIFTINSERT, usri_n, 0)
+  BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssli_n, 0)
+  BUILTIN_VSDQ_I_DI (SHIFTINSERT, usli_n, 0)
   /* Implemented by aarch64_<sur>qshl<u>_n<mode>.  */
-  BUILTIN_VSDQ_I (SHIFTIMM, sqshlu_n)
-  BUILTIN_VSDQ_I (SHIFTIMM, sqshl_n)
-  BUILTIN_VSDQ_I (SHIFTIMM, uqshl_n)
+  BUILTIN_VSDQ_I (SHIFTIMM, sqshlu_n, 0)
+  BUILTIN_VSDQ_I (SHIFTIMM, sqshl_n, 0)
+  BUILTIN_VSDQ_I (SHIFTIMM, uqshl_n, 0)
 
   /* Implemented by aarch64_cm<cmp><mode>.  */
-  BUILTIN_VSDQ_I_DI (BINOP, cmeq)
-  BUILTIN_VSDQ_I_DI (BINOP, cmge)
-  BUILTIN_VSDQ_I_DI (BINOP, cmgt)
-  BUILTIN_VSDQ_I_DI (BINOP, cmle)
-  BUILTIN_VSDQ_I_DI (BINOP, cmlt)
+  BUILTIN_VALLDI (BINOP, cmeq, 0)
+  BUILTIN_VALLDI (BINOP, cmge, 0)
+  BUILTIN_VALLDI (BINOP, cmgt, 0)
+  BUILTIN_VALLDI (BINOP, cmle, 0)
+  BUILTIN_VALLDI (BINOP, cmlt, 0)
   /* Implemented by aarch64_cm<cmp><mode>.  */
-  BUILTIN_VSDQ_I_DI (BINOP, cmgeu)
-  BUILTIN_VSDQ_I_DI (BINOP, cmgtu)
-  BUILTIN_VSDQ_I_DI (BINOP, cmtst)
-
-  /* Implemented by aarch64_<fmaxmin><mode>.  */
-  BUILTIN_VDQF (BINOP, fmax)
-  BUILTIN_VDQF (BINOP, fmin)
-  /* Implemented by aarch64_<maxmin><mode>.  */
-  BUILTIN_VDQ_BHSI (BINOP, smax)
-  BUILTIN_VDQ_BHSI (BINOP, smin)
-  BUILTIN_VDQ_BHSI (BINOP, umax)
-  BUILTIN_VDQ_BHSI (BINOP, umin)
-
-  /* Implemented by aarch64_frint<frint_suffix><mode>.  */
-  BUILTIN_VDQF (UNOP, frintz)
-  BUILTIN_VDQF (UNOP, frintp)
-  BUILTIN_VDQF (UNOP, frintm)
-  BUILTIN_VDQF (UNOP, frinti)
-  BUILTIN_VDQF (UNOP, frintx)
-  BUILTIN_VDQF (UNOP, frinta)
-
-  /* Implemented by aarch64_fcvt<frint_suffix><su><mode>.  */
-  BUILTIN_VDQF (UNOP, fcvtzs)
-  BUILTIN_VDQF (UNOP, fcvtzu)
-  BUILTIN_VDQF (UNOP, fcvtas)
-  BUILTIN_VDQF (UNOP, fcvtau)
-  BUILTIN_VDQF (UNOP, fcvtps)
-  BUILTIN_VDQF (UNOP, fcvtpu)
-  BUILTIN_VDQF (UNOP, fcvtms)
-  BUILTIN_VDQF (UNOP, fcvtmu)
+  BUILTIN_VSDQ_I_DI (BINOP, cmgeu, 0)
+  BUILTIN_VSDQ_I_DI (BINOP, cmgtu, 0)
+  BUILTIN_VSDQ_I_DI (BINOP, cmtst, 0)
+
+  /* Implemented by reduc_<sur>plus_<mode>.  */
+  BUILTIN_VALL (UNOP, reduc_splus_, 10)
+  BUILTIN_VDQ (UNOP, reduc_uplus_, 10)
+
+  /* Implemented by reduc_<maxmin_uns>_<mode>.  */
+  BUILTIN_VDQIF (UNOP, reduc_smax_, 10)
+  BUILTIN_VDQIF (UNOP, reduc_smin_, 10)
+  BUILTIN_VDQ_BHSI (UNOP, reduc_umax_, 10)
+  BUILTIN_VDQ_BHSI (UNOP, reduc_umin_, 10)
+  BUILTIN_VDQF (UNOP, reduc_smax_nan_, 10)
+  BUILTIN_VDQF (UNOP, reduc_smin_nan_, 10)
+
+  /* Implemented by <maxmin><mode>3.
+     smax variants map to fmaxnm,
+     smax_nan variants map to fmax.  */
+  BUILTIN_VDQIF (BINOP, smax, 3)
+  BUILTIN_VDQIF (BINOP, smin, 3)
+  BUILTIN_VDQ_BHSI (BINOP, umax, 3)
+  BUILTIN_VDQ_BHSI (BINOP, umin, 3)
+  BUILTIN_VDQF (BINOP, smax_nan, 3)
+  BUILTIN_VDQF (BINOP, smin_nan, 3)
+
+  /* Implemented by <frint_pattern><mode>2.  */
+  BUILTIN_VDQF (UNOP, btrunc, 2)
+  BUILTIN_VDQF (UNOP, ceil, 2)
+  BUILTIN_VDQF (UNOP, floor, 2)
+  BUILTIN_VDQF (UNOP, nearbyint, 2)
+  BUILTIN_VDQF (UNOP, rint, 2)
+  BUILTIN_VDQF (UNOP, round, 2)
+  BUILTIN_VDQF (UNOP, frintn, 2)
+
+  /* Implemented by l<fcvt_pattern><su_optab><VQDF:mode><vcvt_target>2.  */
+  VAR1 (UNOP, lbtruncv2sf, 2, v2si)
+  VAR1 (UNOP, lbtruncv4sf, 2, v4si)
+  VAR1 (UNOP, lbtruncv2df, 2, v2di)
+
+  VAR1 (UNOP, lbtruncuv2sf, 2, v2si)
+  VAR1 (UNOP, lbtruncuv4sf, 2, v4si)
+  VAR1 (UNOP, lbtruncuv2df, 2, v2di)
+
+  VAR1 (UNOP, lroundv2sf, 2, v2si)
+  VAR1 (UNOP, lroundv4sf, 2, v4si)
+  VAR1 (UNOP, lroundv2df, 2, v2di)
+  /* Implemented by l<fcvt_pattern><su_optab><GPF:mode><GPI:mode>2.  */
+  VAR1 (UNOP, lroundsf, 2, si)
+  VAR1 (UNOP, lrounddf, 2, di)
+
+  VAR1 (UNOP, lrounduv2sf, 2, v2si)
+  VAR1 (UNOP, lrounduv4sf, 2, v4si)
+  VAR1 (UNOP, lrounduv2df, 2, v2di)
+  VAR1 (UNOP, lroundusf, 2, si)
+  VAR1 (UNOP, lroundudf, 2, di)
+
+  VAR1 (UNOP, lceilv2sf, 2, v2si)
+  VAR1 (UNOP, lceilv4sf, 2, v4si)
+  VAR1 (UNOP, lceilv2df, 2, v2di)
+
+  VAR1 (UNOP, lceiluv2sf, 2, v2si)
+  VAR1 (UNOP, lceiluv4sf, 2, v4si)
+  VAR1 (UNOP, lceiluv2df, 2, v2di)
+  VAR1 (UNOP, lceilusf, 2, si)
+  VAR1 (UNOP, lceiludf, 2, di)
+
+  VAR1 (UNOP, lfloorv2sf, 2, v2si)
+  VAR1 (UNOP, lfloorv4sf, 2, v4si)
+  VAR1 (UNOP, lfloorv2df, 2, v2di)
+
+  VAR1 (UNOP, lflooruv2sf, 2, v2si)
+  VAR1 (UNOP, lflooruv4sf, 2, v4si)
+  VAR1 (UNOP, lflooruv2df, 2, v2di)
+  VAR1 (UNOP, lfloorusf, 2, si)
+  VAR1 (UNOP, lfloorudf, 2, di)
+
+  VAR1 (UNOP, lfrintnv2sf, 2, v2si)
+  VAR1 (UNOP, lfrintnv4sf, 2, v4si)
+  VAR1 (UNOP, lfrintnv2df, 2, v2di)
+  VAR1 (UNOP, lfrintnsf, 2, si)
+  VAR1 (UNOP, lfrintndf, 2, di)
+
+  VAR1 (UNOP, lfrintnuv2sf, 2, v2si)
+  VAR1 (UNOP, lfrintnuv4sf, 2, v4si)
+  VAR1 (UNOP, lfrintnuv2df, 2, v2di)
+  VAR1 (UNOP, lfrintnusf, 2, si)
+  VAR1 (UNOP, lfrintnudf, 2, di)
+
+  /* Implemented by <optab><fcvt_target><VDQF:mode>2.  */
+  VAR1 (UNOP, floatv2si, 2, v2sf)
+  VAR1 (UNOP, floatv4si, 2, v4sf)
+  VAR1 (UNOP, floatv2di, 2, v2df)
+
+  VAR1 (UNOP, floatunsv2si, 2, v2sf)
+  VAR1 (UNOP, floatunsv4si, 2, v4sf)
+  VAR1 (UNOP, floatunsv2di, 2, v2df)
 
   /* Implemented by
      aarch64_<PERMUTE:perm_insn><PERMUTE:perm_hilo><mode>.  */
-  BUILTIN_VALL (BINOP, zip1)
-  BUILTIN_VALL (BINOP, zip2)
-  BUILTIN_VALL (BINOP, uzp1)
-  BUILTIN_VALL (BINOP, uzp2)
-  BUILTIN_VALL (BINOP, trn1)
-  BUILTIN_VALL (BINOP, trn2)
+  BUILTIN_VALL (BINOP, zip1, 0)
+  BUILTIN_VALL (BINOP, zip2, 0)
+  BUILTIN_VALL (BINOP, uzp1, 0)
+  BUILTIN_VALL (BINOP, uzp2, 0)
+  BUILTIN_VALL (BINOP, trn1, 0)
+  BUILTIN_VALL (BINOP, trn2, 0)
+
+  /* Implemented by
+     aarch64_frecp<FRECP:frecp_suffix><mode>.  */
+  BUILTIN_GPF (UNOP, frecpe, 0)
+  BUILTIN_GPF (BINOP, frecps, 0)
+  BUILTIN_GPF (UNOP, frecpx, 0)
+
+  BUILTIN_VDQF (UNOP, frecpe, 0)
+  BUILTIN_VDQF (BINOP, frecps, 0)
+
+  BUILTIN_VALLDI (UNOP, abs, 2)
+
+  VAR1 (UNOP, vec_unpacks_hi_, 10, v4sf)
+  VAR1 (BINOP, float_truncate_hi_, 0, v4sf)
+
+  VAR1 (UNOP, float_extend_lo_, 0, v2df)
+  VAR1 (UNOP, float_truncate_lo_, 0, v2sf)
 
   /* Implemented by aarch64_ld1<VALL:mode>.  */
-  BUILTIN_VALL (LOAD1, ld1)
+  BUILTIN_VALL (LOAD1, ld1, 0)
 
   /* Implemented by aarch64_st1<VALL:mode>.  */
-  BUILTIN_VALL (STORE1, st1)
+  BUILTIN_VALL (STORE1, st1, 0)
 
+  /* Implemented by aarch64_crypto_aes<op><mode>.  */
+  VAR1 (BINOPU, crypto_aese, 0, v16qi)
+  VAR1 (BINOPU, crypto_aesd, 0, v16qi)
+  VAR1 (UNOPU, crypto_aesmc, 0, v16qi)
+  VAR1 (UNOPU, crypto_aesimc, 0, v16qi)
+
+  /* Implemented by aarch64_crypto_sha1<op><mode>.  */
+  VAR1 (UNOPU, crypto_sha1h, 0, si)
+  VAR1 (BINOPU, crypto_sha1su1, 0, v4si)
+  VAR1 (TERNOPU, crypto_sha1c, 0, v4si)
+  VAR1 (TERNOPU, crypto_sha1m, 0, v4si)
+  VAR1 (TERNOPU, crypto_sha1p, 0, v4si)
+  VAR1 (TERNOPU, crypto_sha1su0, 0, v4si)
+
+  /* Implemented by aarch64_crypto_sha256<op><mode>.  */
+  VAR1 (TERNOPU, crypto_sha256h, 0, v4si)
+  VAR1 (TERNOPU, crypto_sha256h2, 0, v4si)
+  VAR1 (BINOPU, crypto_sha256su0, 0, v4si)
+  VAR1 (TERNOPU, crypto_sha256su1, 0, v4si)
+
+  /* Implemented by aarch64_crypto_pmull<mode>.  */
+  VAR1 (BINOPP, crypto_pmull, 0, di)
+  VAR1 (BINOPP, crypto_pmull, 0, v2di)
Index: b/src/gcc/config/aarch64/constraints.md
===================================================================
--- a/src/gcc/config/aarch64/constraints.md
+++ b/src/gcc/config/aarch64/constraints.md
@@ -75,11 +75,6 @@
   "Integer constant zero."
   (match_test "op == const0_rtx"))
 
-(define_constraint "Usa"
-  "A constraint that matches an absolute symbolic address."
-  (and (match_code "const,symbol_ref")
-       (match_test "aarch64_symbolic_address_p (op)")))
-
 (define_constraint "Ush"
   "A constraint that matches an absolute symbolic address high part."
   (and (match_code "high")
@@ -148,9 +143,24 @@
   "@internal
  A constraint that matches vector of immediates."
  (and (match_code "const_vector")
-      (match_test "aarch64_simd_immediate_valid_for_move (op, GET_MODE (op),
-							  NULL, NULL, NULL,
-							  NULL, NULL) != 0")))
+      (match_test "aarch64_simd_valid_immediate (op, GET_MODE (op),
+						 false, NULL)")))
+
+(define_constraint "Dh"
+  "@internal
+ A constraint that matches an immediate operand valid for\
+ AdvSIMD scalar move in HImode."
+ (and (match_code "const_int")
+      (match_test "aarch64_simd_scalar_immediate_valid_for_move (op,
+						 HImode)")))
+
+(define_constraint "Dq"
+  "@internal
+ A constraint that matches an immediate operand valid for\
+ AdvSIMD scalar move in QImode."
+ (and (match_code "const_int")
+      (match_test "aarch64_simd_scalar_immediate_valid_for_move (op,
+						 QImode)")))
 
 (define_constraint "Dl"
   "@internal
Index: b/src/gcc/config/aarch64/aarch64.c
===================================================================
--- a/src/gcc/config/aarch64/aarch64.c
+++ b/src/gcc/config/aarch64/aarch64.c
@@ -45,6 +45,8 @@
 #include "gimple.h"
 #include "optabs.h"
 #include "dwarf2.h"
+#include "cfgloop.h"
+#include "tree-vectorizer.h"
 
 /* Classifies an address.
 
@@ -87,6 +89,15 @@ struct aarch64_address_info {
   enum aarch64_symbol_type symbol_type;
 };
 
+struct simd_immediate_info
+{
+  rtx value;
+  int shift;
+  int element_width;
+  bool mvn;
+  bool msl;
+};
+
 /* The current code model.  */
 enum aarch64_code_model aarch64_cmodel;
 
@@ -103,8 +114,6 @@ static bool aarch64_vfp_is_call_or_retur
 static void aarch64_elf_asm_constructor (rtx, int) ATTRIBUTE_UNUSED;
 static void aarch64_elf_asm_destructor (rtx, int) ATTRIBUTE_UNUSED;
 static void aarch64_override_options_after_change (void);
-static int aarch64_simd_valid_immediate (rtx, enum machine_mode, int, rtx *,
-					 int *, unsigned char *, int *, int *);
 static bool aarch64_vector_mode_supported_p (enum machine_mode);
 static unsigned bit_count (unsigned HOST_WIDE_INT);
 static bool aarch64_const_vec_all_same_int_p (rtx,
@@ -114,7 +123,7 @@ static bool aarch64_vectorize_vec_perm_c
 						 const unsigned char *sel);
 
 /* The processor for which instructions should be scheduled.  */
-enum aarch64_processor aarch64_tune = generic;
+enum aarch64_processor aarch64_tune = cortexa53;
 
 /* The current tuning set.  */
 const struct tune_params *aarch64_tune_params;
@@ -178,6 +187,26 @@ static const struct cpu_regmove_cost gen
   NAMED_PARAM (FP2FP, 4)
 };
 
+/* Generic costs for vector insn classes.  */
+#if HAVE_DESIGNATED_INITIALIZERS && GCC_VERSION >= 2007
+__extension__
+#endif
+static const struct cpu_vector_cost generic_vector_cost =
+{
+  NAMED_PARAM (scalar_stmt_cost, 1),
+  NAMED_PARAM (scalar_load_cost, 1),
+  NAMED_PARAM (scalar_store_cost, 1),
+  NAMED_PARAM (vec_stmt_cost, 1),
+  NAMED_PARAM (vec_to_scalar_cost, 1),
+  NAMED_PARAM (scalar_to_vec_cost, 1),
+  NAMED_PARAM (vec_align_load_cost, 1),
+  NAMED_PARAM (vec_unalign_load_cost, 1),
+  NAMED_PARAM (vec_unalign_store_cost, 1),
+  NAMED_PARAM (vec_store_cost, 1),
+  NAMED_PARAM (cond_taken_branch_cost, 3),
+  NAMED_PARAM (cond_not_taken_branch_cost, 1)
+};
+
 #if HAVE_DESIGNATED_INITIALIZERS && GCC_VERSION >= 2007
 __extension__
 #endif
@@ -186,6 +215,7 @@ static const struct tune_params generic_
   &generic_rtx_cost_table,
   &generic_addrcost_table,
   &generic_regmove_cost,
+  &generic_vector_cost,
   NAMED_PARAM (memmov_cost, 4)
 };
 
@@ -206,7 +236,7 @@ static const struct processor all_cores[
   {NAME, IDENT, #ARCH, FLAGS | AARCH64_FL_FOR_ARCH##ARCH, &COSTS##_tunings},
 #include "aarch64-cores.def"
 #undef AARCH64_CORE
-  {"generic", generic, "8", AARCH64_FL_FPSIMD | AARCH64_FL_FOR_ARCH8, &generic_tunings},
+  {"generic", cortexa53, "8", AARCH64_FL_FPSIMD | AARCH64_FL_FOR_ARCH8, &generic_tunings},
   {NULL, aarch64_none, NULL, 0, NULL}
 };
 
@@ -524,13 +554,15 @@ aarch64_load_symref_appropriately (rtx d
 	return;
       }
 
+    case SYMBOL_TINY_ABSOLUTE:
+      emit_insn (gen_rtx_SET (Pmode, dest, imm));
+      return;
+
     case SYMBOL_SMALL_GOT:
       {
 	rtx tmp_reg = dest;
 	if (can_create_pseudo_p ())
-	  {
-	    tmp_reg =  gen_reg_rtx (Pmode);
-	  }
+	  tmp_reg =  gen_reg_rtx (Pmode);
 	emit_move_insn (tmp_reg, gen_rtx_HIGH (Pmode, imm));
 	emit_insn (gen_ldr_got_small (dest, tmp_reg, imm));
 	return;
@@ -581,6 +613,10 @@ aarch64_load_symref_appropriately (rtx d
 	return;
       }
 
+    case SYMBOL_TINY_GOT:
+      emit_insn (gen_ldr_got_tiny (dest, imm));
+      return;
+
     default:
       gcc_unreachable ();
     }
@@ -604,49 +640,85 @@ aarch64_split_128bit_move (rtx dst, rtx
 {
   rtx low_dst;
 
-  gcc_assert (GET_MODE (dst) == TImode);
+  enum machine_mode src_mode = GET_MODE (src);
+  enum machine_mode dst_mode = GET_MODE (dst);
+  int src_regno = REGNO (src);
+  int dst_regno = REGNO (dst);
+
+  gcc_assert (dst_mode == TImode || dst_mode == TFmode);
 
   if (REG_P (dst) && REG_P (src))
     {
-      int src_regno = REGNO (src);
-      int dst_regno = REGNO (dst);
-
-      gcc_assert (GET_MODE (src) == TImode);
+      gcc_assert (src_mode == TImode || src_mode == TFmode);
 
       /* Handle r -> w, w -> r.  */
       if (FP_REGNUM_P (dst_regno) && GP_REGNUM_P (src_regno))
 	{
-	  emit_insn (gen_aarch64_movtilow_di (dst,
-					      gen_lowpart (word_mode, src)));
-	  emit_insn (gen_aarch64_movtihigh_di (dst,
-					       gen_highpart (word_mode, src)));
-	  return;
+	  switch (src_mode) {
+	  case TImode:
+	    emit_insn
+	      (gen_aarch64_movtilow_di (dst, gen_lowpart (word_mode, src)));
+	    emit_insn
+	      (gen_aarch64_movtihigh_di (dst, gen_highpart (word_mode, src)));
+	    return;
+	  case TFmode:
+	    emit_insn
+	      (gen_aarch64_movtflow_di (dst, gen_lowpart (word_mode, src)));
+	    emit_insn
+	      (gen_aarch64_movtfhigh_di (dst, gen_highpart (word_mode, src)));
+	    return;
+	  default:
+	    gcc_unreachable ();
+	  }
 	}
       else if (GP_REGNUM_P (dst_regno) && FP_REGNUM_P (src_regno))
 	{
-	  emit_insn (gen_aarch64_movdi_tilow (gen_lowpart (word_mode, dst),
-					      src));
-	  emit_insn (gen_aarch64_movdi_tihigh (gen_highpart (word_mode, dst),
-					       src));
-	  return;
+	  switch (src_mode) {
+	  case TImode:
+	    emit_insn
+	      (gen_aarch64_movdi_tilow (gen_lowpart (word_mode, dst), src));
+	    emit_insn
+	      (gen_aarch64_movdi_tihigh (gen_highpart (word_mode, dst), src));
+	    return;
+	  case TFmode:
+	    emit_insn
+	      (gen_aarch64_movdi_tflow (gen_lowpart (word_mode, dst), src));
+	    emit_insn
+	      (gen_aarch64_movdi_tfhigh (gen_highpart (word_mode, dst), src));
+	    return;
+	  default:
+	    gcc_unreachable ();
+	  }
 	}
       /* Fall through to r -> r cases.  */
     }
 
-  low_dst = gen_lowpart (word_mode, dst);
-  if (REG_P (low_dst)
-      && reg_overlap_mentioned_p (low_dst, src))
-    {
-      aarch64_emit_move (gen_highpart (word_mode, dst),
-			 gen_highpart_mode (word_mode, TImode, src));
-      aarch64_emit_move (low_dst, gen_lowpart (word_mode, src));
-    }
-  else
-    {
-      aarch64_emit_move (low_dst, gen_lowpart (word_mode, src));
-      aarch64_emit_move (gen_highpart (word_mode, dst),
-			 gen_highpart_mode (word_mode, TImode, src));
-    }
+  switch (dst_mode) {
+  case TImode:
+    low_dst = gen_lowpart (word_mode, dst);
+    if (REG_P (low_dst)
+	&& reg_overlap_mentioned_p (low_dst, src))
+      {
+	aarch64_emit_move (gen_highpart (word_mode, dst),
+			   gen_highpart_mode (word_mode, TImode, src));
+	aarch64_emit_move (low_dst, gen_lowpart (word_mode, src));
+      }
+    else
+      {
+	aarch64_emit_move (low_dst, gen_lowpart (word_mode, src));
+	aarch64_emit_move (gen_highpart (word_mode, dst),
+			   gen_highpart_mode (word_mode, TImode, src));
+      }
+    return;
+  case TFmode:
+    emit_move_insn (gen_rtx_REG (DFmode, dst_regno),
+		    gen_rtx_REG (DFmode, src_regno));
+    emit_move_insn (gen_rtx_REG (DFmode, dst_regno + 1),
+		    gen_rtx_REG (DFmode, src_regno + 1));
+    return;
+  default:
+    gcc_unreachable ();
+  }
 }
 
 bool
@@ -656,11 +728,99 @@ aarch64_split_128bit_move_p (rtx dst, rt
 	  || ! (FP_REGNUM_P (REGNO (dst)) && FP_REGNUM_P (REGNO (src))));
 }
 
+/* Split a complex SIMD combine.  */
+
+void
+aarch64_split_simd_combine (rtx dst, rtx src1, rtx src2)
+{
+  enum machine_mode src_mode = GET_MODE (src1);
+  enum machine_mode dst_mode = GET_MODE (dst);
+
+  gcc_assert (VECTOR_MODE_P (dst_mode));
+
+  if (REG_P (dst) && REG_P (src1) && REG_P (src2))
+    {
+      rtx (*gen) (rtx, rtx, rtx);
+
+      switch (src_mode)
+	{
+	case V8QImode:
+	  gen = gen_aarch64_simd_combinev8qi;
+	  break;
+	case V4HImode:
+	  gen = gen_aarch64_simd_combinev4hi;
+	  break;
+	case V2SImode:
+	  gen = gen_aarch64_simd_combinev2si;
+	  break;
+	case V2SFmode:
+	  gen = gen_aarch64_simd_combinev2sf;
+	  break;
+	case DImode:
+	  gen = gen_aarch64_simd_combinedi;
+	  break;
+	case DFmode:
+	  gen = gen_aarch64_simd_combinedf;
+	  break;
+	default:
+	  gcc_unreachable ();
+	}
+
+      emit_insn (gen (dst, src1, src2));
+      return;
+    }
+}
+
+/* Split a complex SIMD move.  */
+
+void
+aarch64_split_simd_move (rtx dst, rtx src)
+{
+  enum machine_mode src_mode = GET_MODE (src);
+  enum machine_mode dst_mode = GET_MODE (dst);
+
+  gcc_assert (VECTOR_MODE_P (dst_mode));
+
+  if (REG_P (dst) && REG_P (src))
+    {
+      rtx (*gen) (rtx, rtx);
+
+      gcc_assert (VECTOR_MODE_P (src_mode));
+
+      switch (src_mode)
+	{
+	case V16QImode:
+	  gen = gen_aarch64_split_simd_movv16qi;
+	  break;
+	case V8HImode:
+	  gen = gen_aarch64_split_simd_movv8hi;
+	  break;
+	case V4SImode:
+	  gen = gen_aarch64_split_simd_movv4si;
+	  break;
+	case V2DImode:
+	  gen = gen_aarch64_split_simd_movv2di;
+	  break;
+	case V4SFmode:
+	  gen = gen_aarch64_split_simd_movv4sf;
+	  break;
+	case V2DFmode:
+	  gen = gen_aarch64_split_simd_movv2df;
+	  break;
+	default:
+	  gcc_unreachable ();
+	}
+
+      emit_insn (gen (dst, src));
+      return;
+    }
+}
+
 static rtx
-aarch64_force_temporary (rtx x, rtx value)
+aarch64_force_temporary (enum machine_mode mode, rtx x, rtx value)
 {
   if (can_create_pseudo_p ())
-    return force_reg (Pmode, value);
+    return force_reg (mode, value);
   else
     {
       x = aarch64_emit_move (x, value);
@@ -672,15 +832,16 @@ aarch64_force_temporary (rtx x, rtx valu
 static rtx
 aarch64_add_offset (enum machine_mode mode, rtx temp, rtx reg, HOST_WIDE_INT offset)
 {
-  if (!aarch64_plus_immediate (GEN_INT (offset), DImode))
+  if (!aarch64_plus_immediate (GEN_INT (offset), mode))
     {
       rtx high;
       /* Load the full offset into a register.  This
          might be improvable in the future.  */
       high = GEN_INT (offset);
       offset = 0;
-      high = aarch64_force_temporary (temp, high);
-      reg = aarch64_force_temporary (temp, gen_rtx_PLUS (Pmode, high, reg));
+      high = aarch64_force_temporary (mode, temp, high);
+      reg = aarch64_force_temporary (mode, temp,
+				     gen_rtx_PLUS (mode, high, reg));
     }
   return plus_constant (mode, reg, offset);
 }
@@ -719,7 +880,7 @@ aarch64_expand_mov_immediate (rtx dest,
 	      && targetm.cannot_force_const_mem (mode, imm))
 	    {
 	      gcc_assert(can_create_pseudo_p ());
-	      base = aarch64_force_temporary (dest, base);
+	      base = aarch64_force_temporary (mode, dest, base);
 	      base = aarch64_add_offset (mode, NULL, base, INTVAL (offset));
 	      aarch64_emit_move (dest, base);
 	      return;
@@ -733,10 +894,11 @@ aarch64_expand_mov_immediate (rtx dest,
         case SYMBOL_SMALL_TLSDESC:
         case SYMBOL_SMALL_GOTTPREL:
 	case SYMBOL_SMALL_GOT:
+	case SYMBOL_TINY_GOT:
 	  if (offset != const0_rtx)
 	    {
 	      gcc_assert(can_create_pseudo_p ());
-	      base = aarch64_force_temporary (dest, base);
+	      base = aarch64_force_temporary (mode, dest, base);
 	      base = aarch64_add_offset (mode, NULL, base, INTVAL (offset));
 	      aarch64_emit_move (dest, base);
 	      return;
@@ -745,6 +907,7 @@ aarch64_expand_mov_immediate (rtx dest,
 
         case SYMBOL_SMALL_TPREL:
 	case SYMBOL_SMALL_ABSOLUTE:
+	case SYMBOL_TINY_ABSOLUTE:
 	  aarch64_load_symref_appropriately (dest, imm, sty);
 	  return;
 
@@ -1813,7 +1976,7 @@ aarch64_save_or_restore_callee_save_regi
    Establish the stack frame by decreasing the stack pointer with a
    properly calculated size and, if necessary, create a frame record
    filled with the values of LR and previous frame pointer.  The
-   current FP is also set up is it is in use.  */
+   current FP is also set up if it is in use.  */
 
 void
 aarch64_expand_prologue (void)
@@ -2556,12 +2719,14 @@ static bool
 aarch64_cannot_force_const_mem (enum machine_mode mode ATTRIBUTE_UNUSED, rtx x)
 {
   rtx base, offset;
+
   if (GET_CODE (x) == HIGH)
     return true;
 
   split_const (x, &base, &offset);
   if (GET_CODE (base) == SYMBOL_REF || GET_CODE (base) == LABEL_REF)
-    return (aarch64_classify_symbol (base, SYMBOL_CONTEXT_ADR) != SYMBOL_FORCE_TO_MEM);
+    return (aarch64_classify_symbol (base, SYMBOL_CONTEXT_ADR)
+	    != SYMBOL_FORCE_TO_MEM);
 
   return aarch64_tls_referenced_p (x);
 }
@@ -2999,10 +3164,13 @@ aarch64_symbolic_address_p (rtx x)
 
 /* Classify the base of symbolic expression X, given that X appears in
    context CONTEXT.  */
-static enum aarch64_symbol_type
-aarch64_classify_symbolic_expression (rtx x, enum aarch64_symbol_context context)
+
+enum aarch64_symbol_type
+aarch64_classify_symbolic_expression (rtx x,
+				      enum aarch64_symbol_context context)
 {
   rtx offset;
+
   split_const (x, &x, &offset);
   return aarch64_classify_symbol (x, context);
 }
@@ -3090,7 +3258,8 @@ aarch64_select_cc_mode (RTX_CODE code, r
   if ((GET_MODE (x) == SImode || GET_MODE (x) == DImode)
       && y == const0_rtx
       && (code == EQ || code == NE || code == LT || code == GE)
-      && (GET_CODE (x) == PLUS || GET_CODE (x) == MINUS || GET_CODE (x) == AND))
+      && (GET_CODE (x) == PLUS || GET_CODE (x) == MINUS || GET_CODE (x) == AND
+	  || GET_CODE (x) == NEG))
     return CC_NZmode;
 
   /* A compare with a shifted operand.  Because of canonicalization,
@@ -3103,6 +3272,14 @@ aarch64_select_cc_mode (RTX_CODE code, r
 	  || GET_CODE (x) == ZERO_EXTEND || GET_CODE (x) == SIGN_EXTEND))
     return CC_SWPmode;
 
+  /* Similarly for a negated operand, but we can only do this for
+     equalities.  */
+  if ((GET_MODE (x) == SImode || GET_MODE (x) == DImode)
+      && (GET_CODE (y) == REG || GET_CODE (y) == SUBREG)
+      && (code == EQ || code == NE)
+      && GET_CODE (x) == NEG)
+    return CC_Zmode;
+
   /* A compare of a mode narrower than SI mode against zero can be done
      by extending the value in the comparison.  */
   if ((GET_MODE (x) == QImode || GET_MODE (x) == HImode)
@@ -3193,6 +3370,15 @@ aarch64_get_condition_code (rtx x)
 	}
       break;
 
+    case CC_Zmode:
+      switch (comp_code)
+	{
+	case NE: return AARCH64_NE;
+	case EQ: return AARCH64_EQ;
+	default: gcc_unreachable ();
+	}
+      break;
+
     default:
       gcc_unreachable ();
       break;
@@ -3285,26 +3471,6 @@ aarch64_print_operand (FILE *f, rtx x, c
       asm_fprintf (f, "%s", reg_names [REGNO (x) + 1]);
       break;
 
-    case 'Q':
-      /* Print the least significant register of a pair (TImode) of regs.  */
-      if (GET_CODE (x) != REG || !GP_REGNUM_P (REGNO (x) + 1))
-	{
-	  output_operand_lossage ("invalid operand for '%%%c'", code);
-	  return;
-	}
-      asm_fprintf (f, "%s", reg_names [REGNO (x) + (WORDS_BIG_ENDIAN ? 1 : 0)]);
-      break;
-
-    case 'R':
-      /* Print the most significant register of a pair (TImode) of regs.  */
-      if (GET_CODE (x) != REG || !GP_REGNUM_P (REGNO (x) + 1))
-	{
-	  output_operand_lossage ("invalid operand for '%%%c'", code);
-	  return;
-	}
-      asm_fprintf (f, "%s", reg_names [REGNO (x) + (WORDS_BIG_ENDIAN ? 0 : 1)]);
-      break;
-
     case 'm':
       /* Print a condition (eq, ne, etc).  */
 
@@ -3352,7 +3518,7 @@ aarch64_print_operand (FILE *f, rtx x, c
 	  output_operand_lossage ("incompatible floating point / vector register operand for '%%%c'", code);
 	  return;
 	}
-      asm_fprintf (f, "%s%c%d", REGISTER_PREFIX, code, REGNO (x) - V0_REGNUM);
+      asm_fprintf (f, "%c%d", code, REGNO (x) - V0_REGNUM);
       break;
 
     case 'S':
@@ -3365,18 +3531,17 @@ aarch64_print_operand (FILE *f, rtx x, c
 	  output_operand_lossage ("incompatible floating point / vector register operand for '%%%c'", code);
 	  return;
 	}
-      asm_fprintf (f, "%sv%d", REGISTER_PREFIX,
-			       REGNO (x) - V0_REGNUM + (code - 'S'));
+      asm_fprintf (f, "v%d", REGNO (x) - V0_REGNUM + (code - 'S'));
       break;
 
     case 'X':
-      /* Print integer constant in hex.  */
+      /* Print bottom 16 bits of integer constant in hex.  */
       if (GET_CODE (x) != CONST_INT)
 	{
 	  output_operand_lossage ("invalid operand for '%%%c'", code);
 	  return;
 	}
-      asm_fprintf (f, "0x%wx", UINTVAL (x));
+      asm_fprintf (f, "0x%wx", UINTVAL (x) & 0xffff);
       break;
 
     case 'w':
@@ -3386,20 +3551,19 @@ aarch64_print_operand (FILE *f, rtx x, c
       if (x == const0_rtx
 	  || (CONST_DOUBLE_P (x) && aarch64_float_const_zero_rtx_p (x)))
 	{
-	  asm_fprintf (f, "%s%czr", REGISTER_PREFIX, code);
+	  asm_fprintf (f, "%czr", code);
 	  break;
 	}
 
       if (REG_P (x) && GP_REGNUM_P (REGNO (x)))
 	{
-	  asm_fprintf (f, "%s%c%d", REGISTER_PREFIX, code,
-		       REGNO (x) - R0_REGNUM);
+	  asm_fprintf (f, "%c%d", code, REGNO (x) - R0_REGNUM);
 	  break;
 	}
 
       if (REG_P (x) && REGNO (x) == SP_REGNUM)
 	{
-	  asm_fprintf (f, "%s%ssp", REGISTER_PREFIX, code == 'w' ? "w" : "");
+	  asm_fprintf (f, "%ssp", code == 'w' ? "w" : "");
 	  break;
 	}
 
@@ -3507,6 +3671,10 @@ aarch64_print_operand (FILE *f, rtx x, c
 	  asm_fprintf (asm_out_file, ":tprel:");
 	  break;
 
+	case SYMBOL_TINY_GOT:
+	  gcc_unreachable ();
+	  break;
+
 	default:
 	  break;
 	}
@@ -3536,6 +3704,10 @@ aarch64_print_operand (FILE *f, rtx x, c
 	  asm_fprintf (asm_out_file, ":tprel_lo12_nc:");
 	  break;
 
+	case SYMBOL_TINY_GOT:
+	  asm_fprintf (asm_out_file, ":got:");
+	  break;
+
 	default:
 	  break;
 	}
@@ -3650,13 +3822,6 @@ aarch64_print_operand_address (FILE *f,
   output_addr_const (f, x);
 }
 
-void
-aarch64_function_profiler (FILE *f ATTRIBUTE_UNUSED,
-			   int labelno ATTRIBUTE_UNUSED)
-{
-  sorry ("function profiling");
-}
-
 bool
 aarch64_label_mentioned_p (rtx x)
 {
@@ -3922,7 +4087,7 @@ aarch64_initial_elimination_offset (unsi
 	 return offset - crtl->outgoing_args_size;
 
        if (from == FRAME_POINTER_REGNUM)
-	 return cfun->machine->frame.saved_regs_size;
+	 return cfun->machine->frame.saved_regs_size + get_frame_size ();
      }
 
    if (to == STACK_POINTER_REGNUM)
@@ -3931,6 +4096,7 @@ aarch64_initial_elimination_offset (unsi
          {
            HOST_WIDE_INT elim = crtl->outgoing_args_size
                               + cfun->machine->frame.saved_regs_size
+                              + get_frame_size ()
                               - cfun->machine->frame.fp_lr_offset;
            elim = AARCH64_ROUND_UP (elim, STACK_BOUNDARY / BITS_PER_UNIT);
            return elim;
@@ -4605,6 +4771,101 @@ aarch64_memory_move_cost (enum machine_m
   return aarch64_tune_params->memmov_cost;
 }
 
+/* Vectorizer cost model target hooks.  */
+
+/* Implement targetm.vectorize.builtin_vectorization_cost.  */
+static int
+aarch64_builtin_vectorization_cost (enum vect_cost_for_stmt type_of_cost,
+				    tree vectype,
+				    int misalign ATTRIBUTE_UNUSED)
+{
+  unsigned elements;
+
+  switch (type_of_cost)
+    {
+      case scalar_stmt:
+	return aarch64_tune_params->vec_costs->scalar_stmt_cost;
+
+      case scalar_load:
+	return aarch64_tune_params->vec_costs->scalar_load_cost;
+
+      case scalar_store:
+	return aarch64_tune_params->vec_costs->scalar_store_cost;
+
+      case vector_stmt:
+	return aarch64_tune_params->vec_costs->vec_stmt_cost;
+
+      case vector_load:
+	return aarch64_tune_params->vec_costs->vec_align_load_cost;
+
+      case vector_store:
+	return aarch64_tune_params->vec_costs->vec_store_cost;
+
+      case vec_to_scalar:
+	return aarch64_tune_params->vec_costs->vec_to_scalar_cost;
+
+      case scalar_to_vec:
+	return aarch64_tune_params->vec_costs->scalar_to_vec_cost;
+
+      case unaligned_load:
+	return aarch64_tune_params->vec_costs->vec_unalign_load_cost;
+
+      case unaligned_store:
+	return aarch64_tune_params->vec_costs->vec_unalign_store_cost;
+
+      case cond_branch_taken:
+	return aarch64_tune_params->vec_costs->cond_taken_branch_cost;
+
+      case cond_branch_not_taken:
+	return aarch64_tune_params->vec_costs->cond_not_taken_branch_cost;
+
+      case vec_perm:
+      case vec_promote_demote:
+	return aarch64_tune_params->vec_costs->vec_stmt_cost;
+
+      case vec_construct:
+        elements = TYPE_VECTOR_SUBPARTS (vectype);
+	return elements / 2 + 1;
+
+      default:
+	gcc_unreachable ();
+    }
+}
+
+/* Implement targetm.vectorize.add_stmt_cost.  */
+static unsigned
+aarch64_add_stmt_cost (void *data, int count, enum vect_cost_for_stmt kind,
+		       struct _stmt_vec_info *stmt_info, int misalign,
+		       enum vect_cost_model_location where)
+{
+  unsigned *cost = (unsigned *) data;
+  unsigned retval = 0;
+
+  if (flag_vect_cost_model)
+    {
+      tree vectype = stmt_info ? stmt_vectype (stmt_info) : NULL_TREE;
+      int stmt_cost =
+	    aarch64_builtin_vectorization_cost (kind, vectype, misalign);
+
+      /* Statements in an inner loop relative to the loop being
+	 vectorized are weighted more heavily.  The value here is
+	 a function (linear for now) of the loop nest level.  */
+      if (where == vect_body && stmt_info && stmt_in_inner_loop_p (stmt_info))
+	{
+	  loop_vec_info loop_info = STMT_VINFO_LOOP_VINFO (stmt_info);
+	  struct loop *loop =  LOOP_VINFO_LOOP (loop_info);
+	  unsigned nest_level = loop_depth (loop);
+
+	  count *= nest_level;
+	}
+
+      retval = (unsigned) (count * stmt_cost);
+      cost[where] += retval;
+    }
+
+  return retval;
+}
+
 static void initialize_aarch64_code_model (void);
 
 /* Parse the architecture extension string.  */
@@ -4831,7 +5092,7 @@ aarch64_override_options (void)
 
   /* If the user did not specify a processor, choose the default
      one for them.  This will be the CPU set during configuration using
-     --with-cpu, otherwise it is "generic".  */
+     --with-cpu, otherwise it is "coretex-a53".  */
   if (!selected_cpu)
     {
       selected_cpu = &all_cores[TARGET_CPU_DEFAULT & 0x3f];
@@ -4969,6 +5230,7 @@ aarch64_classify_tls_symbol (rtx x)
 
 /* Return the method that should be used to access SYMBOL_REF or
    LABEL_REF X in context CONTEXT.  */
+
 enum aarch64_symbol_type
 aarch64_classify_symbol (rtx x,
 			 enum aarch64_symbol_context context ATTRIBUTE_UNUSED)
@@ -4982,6 +5244,8 @@ aarch64_classify_symbol (rtx x,
 
 	case AARCH64_CMODEL_TINY_PIC:
 	case AARCH64_CMODEL_TINY:
+	  return SYMBOL_TINY_ABSOLUTE;
+
 	case AARCH64_CMODEL_SMALL_PIC:
 	case AARCH64_CMODEL_SMALL:
 	  return SYMBOL_SMALL_ABSOLUTE;
@@ -4991,70 +5255,46 @@ aarch64_classify_symbol (rtx x,
 	}
     }
 
-  gcc_assert (GET_CODE (x) == SYMBOL_REF);
-
-  switch (aarch64_cmodel)
+  if (GET_CODE (x) == SYMBOL_REF)
     {
-    case AARCH64_CMODEL_LARGE:
-      return SYMBOL_FORCE_TO_MEM;
-
-    case AARCH64_CMODEL_TINY:
-    case AARCH64_CMODEL_SMALL:
-
-      /* This is needed to get DFmode, TImode constants to be loaded off
-         the constant pool.  Is it necessary to dump TImode values into
-         the constant pool.  We don't handle TImode constant loads properly
-         yet and hence need to use the constant pool.  */
-      if (CONSTANT_POOL_ADDRESS_P (x))
+      if (aarch64_cmodel == AARCH64_CMODEL_LARGE
+	  || CONSTANT_POOL_ADDRESS_P (x))
 	return SYMBOL_FORCE_TO_MEM;
 
       if (aarch64_tls_symbol_p (x))
 	return aarch64_classify_tls_symbol (x);
 
-      if (SYMBOL_REF_WEAK (x))
-	return SYMBOL_FORCE_TO_MEM;
-
-      return SYMBOL_SMALL_ABSOLUTE;
-
-    case AARCH64_CMODEL_TINY_PIC:
-    case AARCH64_CMODEL_SMALL_PIC:
-
-      if (CONSTANT_POOL_ADDRESS_P (x))
-	return SYMBOL_FORCE_TO_MEM;
+      switch (aarch64_cmodel)
+	{
+	case AARCH64_CMODEL_TINY:
+	  if (SYMBOL_REF_WEAK (x))
+	    return SYMBOL_FORCE_TO_MEM;
+	  return SYMBOL_TINY_ABSOLUTE;
 
-      if (aarch64_tls_symbol_p (x))
-	return aarch64_classify_tls_symbol (x);
+	case AARCH64_CMODEL_SMALL:
+	  if (SYMBOL_REF_WEAK (x))
+	    return SYMBOL_FORCE_TO_MEM;
+	  return SYMBOL_SMALL_ABSOLUTE;
 
-      if (!aarch64_symbol_binds_local_p (x))
-	return SYMBOL_SMALL_GOT;
+	case AARCH64_CMODEL_TINY_PIC:
+	  if (!aarch64_symbol_binds_local_p (x))
+	    return SYMBOL_TINY_GOT;
+	  return SYMBOL_TINY_ABSOLUTE;
 
-      return SYMBOL_SMALL_ABSOLUTE;
+	case AARCH64_CMODEL_SMALL_PIC:
+	  if (!aarch64_symbol_binds_local_p (x))
+	    return SYMBOL_SMALL_GOT;
+	  return SYMBOL_SMALL_ABSOLUTE;
 
-    default:
-      gcc_unreachable ();
+	default:
+	  gcc_unreachable ();
+	}
     }
+
   /* By default push everything into the constant pool.  */
   return SYMBOL_FORCE_TO_MEM;
 }
 
-/* Return true if X is a symbolic constant that can be used in context
-   CONTEXT.  If it is, store the type of the symbol in *SYMBOL_TYPE.  */
-
-bool
-aarch64_symbolic_constant_p (rtx x, enum aarch64_symbol_context context,
-			     enum aarch64_symbol_type *symbol_type)
-{
-  rtx offset;
-  split_const (x, &x, &offset);
-  if (GET_CODE (x) == SYMBOL_REF || GET_CODE (x) == LABEL_REF)
-    *symbol_type = aarch64_classify_symbol (x, context);
-  else
-    return false;
-
-  /* No checking of offset at this point.  */
-  return true;
-}
-
 bool
 aarch64_constant_address_p (rtx x)
 {
@@ -5105,8 +5345,7 @@ aarch64_legitimate_constant_p (enum mach
   /* This could probably go away because
      we now decompose CONST_INTs according to expand_mov_immediate.  */
   if ((GET_CODE (x) == CONST_VECTOR
-       && aarch64_simd_valid_immediate (x, mode, false,
-					NULL, NULL, NULL, NULL, NULL) != -1)
+       && aarch64_simd_valid_immediate (x, mode, false, NULL))
       || CONST_INT_P (x) || aarch64_valid_floating_const (mode, x))
 	return !targetm.cannot_force_const_mem (mode, x);
 
@@ -5937,32 +6176,57 @@ aarch64_vector_mode_supported_p (enum ma
   return false;
 }
 
-/* Return quad mode as the preferred SIMD mode.  */
+/* Return appropriate SIMD container
+   for MODE within a vector of WIDTH bits.  */
 static enum machine_mode
-aarch64_preferred_simd_mode (enum machine_mode mode)
+aarch64_simd_container_mode (enum machine_mode mode, unsigned width)
 {
+  gcc_assert (width == 64 || width == 128);
   if (TARGET_SIMD)
-    switch (mode)
-      {
-      case DFmode:
-        return V2DFmode;
-      case SFmode:
-        return V4SFmode;
-      case SImode:
-        return V4SImode;
-      case HImode:
-        return V8HImode;
-      case QImode:
-        return V16QImode;
-      case DImode:
-          return V2DImode;
-        break;
-
-      default:;
-      }
+    {
+      if (width == 128)
+	switch (mode)
+	  {
+	  case DFmode:
+	    return V2DFmode;
+	  case SFmode:
+	    return V4SFmode;
+	  case SImode:
+	    return V4SImode;
+	  case HImode:
+	    return V8HImode;
+	  case QImode:
+	    return V16QImode;
+	  case DImode:
+	    return V2DImode;
+	  default:
+	    break;
+	  }
+      else
+	switch (mode)
+	  {
+	  case SFmode:
+	    return V2SFmode;
+	  case SImode:
+	    return V2SImode;
+	  case HImode:
+	    return V4HImode;
+	  case QImode:
+	    return V8QImode;
+	  default:
+	    break;
+	  }
+    }
   return word_mode;
 }
 
+/* Return 128-bit container as the preferred SIMD mode for MODE.  */
+static enum machine_mode
+aarch64_preferred_simd_mode (enum machine_mode mode)
+{
+  return aarch64_simd_container_mode (mode, 128);
+}
+
 /* Return the bitmask of possible vector sizes for the vectorizer
    to iterate over.  */
 static unsigned int
@@ -6012,6 +6276,7 @@ static aarch64_simd_mangle_map_entry aar
   { V2DFmode,  "__builtin_aarch64_simd_df",     "13__Float64x2_t" },
   { V16QImode, "__builtin_aarch64_simd_poly8",  "12__Poly8x16_t" },
   { V8HImode,  "__builtin_aarch64_simd_poly16", "12__Poly16x8_t" },
+  { V2DImode,  "__builtin_aarch64_simd_poly64", "12__Poly64x2_t" },
   { VOIDmode, NULL, NULL }
 };
 
@@ -6087,13 +6352,26 @@ aarch64_prev_real_insn (rtx insn)
   return insn;
 }
 
-/*  Return true iff t1 is the v8type of a multiply-accumulate instruction.  */
+/*  Return true iff t1 is the type of a multiply-accumulate instruction.  */
 
 static bool
-is_madd_op (enum attr_v8type t1)
+is_madd_op (enum attr_type t1)
 {
-  return t1 == V8TYPE_MADD
-         || t1 == V8TYPE_MADDL;
+  unsigned int i;
+  /* A number of these may be AArch32 only.  */
+  enum attr_type mlatypes[] = {
+    TYPE_MLA, TYPE_MLAS, TYPE_SMLAD, TYPE_SMLADX, TYPE_SMLAL, TYPE_SMLALD,
+    TYPE_SMLALS, TYPE_SMLALXY, TYPE_SMLAWX, TYPE_SMLAWY, TYPE_SMLAXY,
+    TYPE_SMMLA, TYPE_UMLAL, TYPE_UMLALS,TYPE_SMLSD, TYPE_SMLSDX, TYPE_SMLSLD
+  };
+
+  for (i = 0; i < sizeof (mlatypes) / sizeof (enum attr_type); i++)
+    {
+      if (t1 == mlatypes[i])
+        return true;
+    }
+
+  return false;
 }
 
 
@@ -6133,7 +6411,7 @@ dep_between_memop_and_curr (rtx memop)
 bool
 aarch64_madd_needs_nop (rtx insn)
 {
-  enum attr_v8type attr_type;
+  enum attr_type attr_type;
   rtx prev;
   rtx body;
 
@@ -6143,7 +6421,7 @@ aarch64_madd_needs_nop (rtx insn)
   if (recog_memoized (insn) < 0)
     return false;
 
-  attr_type = get_attr_v8type (insn);
+  attr_type = get_attr_type (insn);
   if (!is_madd_op (attr_type))
     return false;
 
@@ -6179,7 +6457,7 @@ aarch64_final_prescan_insn (rtx insn)
 
 
 /* Return the equivalent letter for size.  */
-static unsigned char
+static char
 sizetochar (int size)
 {
   switch (size)
@@ -6226,15 +6504,10 @@ aarch64_vect_float_const_representable_p
   return aarch64_float_const_representable_p (x0);
 }
 
-/* TODO: This function returns values similar to those
-   returned by neon_valid_immediate in gcc/config/arm/arm.c
-   but the API here is different enough that these magic numbers
-   are not used.  It should be sufficient to return true or false.  */
-static int
-aarch64_simd_valid_immediate (rtx op, enum machine_mode mode, int inverse,
-			      rtx *modconst, int *elementwidth,
-			      unsigned char *elementchar,
-			      int *mvn, int *shift)
+/* Return true for valid and false for invalid.  */
+bool
+aarch64_simd_valid_immediate (rtx op, enum machine_mode mode, bool inverse,
+			      struct simd_immediate_info *info)
 {
 #define CHECK(STRIDE, ELSIZE, CLASS, TEST, SHIFT, NEG)	\
   matches = 1;						\
@@ -6245,7 +6518,6 @@ aarch64_simd_valid_immediate (rtx op, en
     {							\
       immtype = (CLASS);				\
       elsize = (ELSIZE);				\
-      elchar = sizetochar (elsize);			\
       eshift = (SHIFT);					\
       emvn = (NEG);					\
       break;						\
@@ -6254,36 +6526,25 @@ aarch64_simd_valid_immediate (rtx op, en
   unsigned int i, elsize = 0, idx = 0, n_elts = CONST_VECTOR_NUNITS (op);
   unsigned int innersize = GET_MODE_SIZE (GET_MODE_INNER (mode));
   unsigned char bytes[16];
-  unsigned char elchar = 0;
   int immtype = -1, matches;
   unsigned int invmask = inverse ? 0xff : 0;
   int eshift, emvn;
 
   if (GET_MODE_CLASS (mode) == MODE_VECTOR_FLOAT)
     {
-      bool simd_imm_zero = aarch64_simd_imm_zero_p (op, mode);
-      int elem_width = GET_MODE_BITSIZE (GET_MODE (CONST_VECTOR_ELT (op, 0)));
-
-      if (!(simd_imm_zero
-	    || aarch64_vect_float_const_representable_p (op)))
-	return -1;
-
-	if (modconst)
-	  *modconst = CONST_VECTOR_ELT (op, 0);
-
-	if (elementwidth)
-	  *elementwidth = elem_width;
-
-	if (elementchar)
-	  *elementchar = sizetochar (elem_width);
+      if (! (aarch64_simd_imm_zero_p (op, mode)
+	     || aarch64_vect_float_const_representable_p (op)))
+	return false;
 
-	if (shift)
-	  *shift = 0;
+      if (info)
+	{
+	  info->value = CONST_VECTOR_ELT (op, 0);
+	  info->element_width = GET_MODE_BITSIZE (GET_MODE (info->value));
+	  info->mvn = false;
+	  info->shift = 0;
+	}
 
-	if (simd_imm_zero)
-	  return 19;
-	else
-	  return 18;
+      return true;
     }
 
   /* Splat vector constant out into a byte vector.  */
@@ -6357,16 +6618,16 @@ aarch64_simd_valid_immediate (rtx op, en
       CHECK (2, 16, 11, bytes[i] == 0xff && bytes[i + 1] == bytes[1], 8, 1);
 
       CHECK (4, 32, 12, bytes[i] == 0xff && bytes[i + 1] == bytes[1]
-	     && bytes[i + 2] == 0 && bytes[i + 3] == 0, 0, 0);
+	     && bytes[i + 2] == 0 && bytes[i + 3] == 0, 8, 0);
 
       CHECK (4, 32, 13, bytes[i] == 0 && bytes[i + 1] == bytes[1]
-	     && bytes[i + 2] == 0xff && bytes[i + 3] == 0xff, 0, 1);
+	     && bytes[i + 2] == 0xff && bytes[i + 3] == 0xff, 8, 1);
 
       CHECK (4, 32, 14, bytes[i] == 0xff && bytes[i + 1] == 0xff
-	     && bytes[i + 2] == bytes[2] && bytes[i + 3] == 0, 0, 0);
+	     && bytes[i + 2] == bytes[2] && bytes[i + 3] == 0, 16, 0);
 
       CHECK (4, 32, 15, bytes[i] == 0 && bytes[i + 1] == 0
-	     && bytes[i + 2] == bytes[2] && bytes[i + 3] == 0xff, 0, 1);
+	     && bytes[i + 2] == bytes[2] && bytes[i + 3] == 0xff, 16, 1);
 
       CHECK (1, 8, 16, bytes[i] == bytes[0], 0, 0);
 
@@ -6375,31 +6636,20 @@ aarch64_simd_valid_immediate (rtx op, en
     }
   while (0);
 
-  /* TODO: Currently the assembler cannot handle types 12 to 15.
-     And there is no way to specify cmode through the compiler.
-     Disable them till there is support in the assembler.  */
-  if (immtype == -1
-      || (immtype >= 12 && immtype <= 15)
-      || immtype == 18)
-    return -1;
-
-
-  if (elementwidth)
-    *elementwidth = elsize;
-
-  if (elementchar)
-    *elementchar = elchar;
-
-  if (mvn)
-    *mvn = emvn;
-
-  if (shift)
-    *shift = eshift;
+  if (immtype == -1)
+    return false;
 
-  if (modconst)
+  if (info)
     {
+      info->element_width = elsize;
+      info->mvn = emvn != 0;
+      info->shift = eshift;
+
       unsigned HOST_WIDE_INT imm = 0;
 
+      if (immtype >= 12 && immtype <= 15)
+	info->msl = true;
+
       /* Un-invert bytes of recognized vector, if necessary.  */
       if (invmask != 0)
         for (i = 0; i < idx; i++)
@@ -6414,68 +6664,27 @@ aarch64_simd_valid_immediate (rtx op, en
             imm |= (unsigned HOST_WIDE_INT) (bytes[i] ? 0xff : 0)
 	      << (i * BITS_PER_UNIT);
 
-          *modconst = GEN_INT (imm);
-        }
-      else
-        {
-          unsigned HOST_WIDE_INT imm = 0;
 
-          for (i = 0; i < elsize / BITS_PER_UNIT; i++)
-            imm |= (unsigned HOST_WIDE_INT) bytes[i] << (i * BITS_PER_UNIT);
+	  info->value = GEN_INT (imm);
+	}
+      else
+	{
+	  for (i = 0; i < elsize / BITS_PER_UNIT; i++)
+	    imm |= (unsigned HOST_WIDE_INT) bytes[i] << (i * BITS_PER_UNIT);
 
 	  /* Construct 'abcdefgh' because the assembler cannot handle
-	     generic constants.  */
-	  gcc_assert (shift != NULL && mvn != NULL);
-	  if (*mvn)
+	     generic constants.	 */
+	  if (info->mvn)
 	    imm = ~imm;
-	  imm = (imm >> *shift) & 0xff;
-          *modconst = GEN_INT (imm);
-        }
+	  imm = (imm >> info->shift) & 0xff;
+	  info->value = GEN_INT (imm);
+	}
     }
 
-  return immtype;
+  return true;
 #undef CHECK
 }
 
-/* Return TRUE if rtx X is legal for use as either a AdvSIMD MOVI instruction
-   (or, implicitly, MVNI) immediate.  Write back width per element
-   to *ELEMENTWIDTH, and a modified constant (whatever should be output
-   for a MOVI instruction) in *MODCONST.  */
-int
-aarch64_simd_immediate_valid_for_move (rtx op, enum machine_mode mode,
-				       rtx *modconst, int *elementwidth,
-				       unsigned char *elementchar,
-				       int *mvn, int *shift)
-{
-  rtx tmpconst;
-  int tmpwidth;
-  unsigned char tmpwidthc;
-  int tmpmvn = 0, tmpshift = 0;
-  int retval = aarch64_simd_valid_immediate (op, mode, 0, &tmpconst,
-					     &tmpwidth, &tmpwidthc,
-					     &tmpmvn, &tmpshift);
-
-  if (retval == -1)
-    return 0;
-
-  if (modconst)
-    *modconst = tmpconst;
-
-  if (elementwidth)
-    *elementwidth = tmpwidth;
-
-  if (elementchar)
-    *elementchar = tmpwidthc;
-
-  if (mvn)
-    *mvn = tmpmvn;
-
-  if (shift)
-    *shift = tmpshift;
-
-  return 1;
-}
-
 static bool
 aarch64_const_vec_all_same_int_p (rtx x,
 				  HOST_WIDE_INT minval,
@@ -6537,6 +6746,25 @@ aarch64_simd_imm_scalar_p (rtx x, enum m
   return true;
 }
 
+bool
+aarch64_mov_operand_p (rtx x,
+		       enum aarch64_symbol_context context,
+		       enum machine_mode mode)
+{
+  if (GET_CODE (x) == HIGH
+      && aarch64_valid_symref (XEXP (x, 0), GET_MODE (XEXP (x, 0))))
+    return true;
+
+  if (CONST_INT_P (x) && aarch64_move_imm (INTVAL (x), mode))
+    return true;
+
+  if (GET_CODE (x) == SYMBOL_REF && mode == DImode && CONSTANT_ADDRESS_P (x))
+    return true;
+
+  return aarch64_classify_symbolic_expression (x, context)
+    == SYMBOL_TINY_ABSOLUTE;
+}
+
 /* Return a const_int vector of VAL.  */
 rtx
 aarch64_simd_gen_const_vector_dup (enum machine_mode mode, int val)
@@ -6551,6 +6779,19 @@ aarch64_simd_gen_const_vector_dup (enum
   return gen_rtx_CONST_VECTOR (mode, v);
 }
 
+/* Check OP is a legal scalar immediate for the MOVI instruction.  */
+
+bool
+aarch64_simd_scalar_immediate_valid_for_move (rtx op, enum machine_mode mode)
+{
+  enum machine_mode vmode;
+
+  gcc_assert (!VECTOR_MODE_P (mode));
+  vmode = aarch64_preferred_simd_mode (mode);
+  rtx op_v = aarch64_simd_gen_const_vector_dup (vmode, INTVAL (op));
+  return aarch64_simd_valid_immediate (op_v, vmode, false, NULL);
+}
+
 /* Construct and return a PARALLEL RTX vector.  */
 rtx
 aarch64_simd_vect_par_cnst_half (enum machine_mode mode, bool high)
@@ -6776,8 +7017,7 @@ aarch64_simd_make_constant (rtx vals)
     gcc_unreachable ();
 
   if (const_vec != NULL_RTX
-      && aarch64_simd_immediate_valid_for_move (const_vec, mode, NULL, NULL,
-						NULL, NULL, NULL))
+      && aarch64_simd_valid_immediate (const_vec, mode, false, NULL))
     /* Load using MOVI/MVNI.  */
     return const_vec;
   else if ((const_dup = aarch64_simd_dup_constant (vals)) != NULL_RTX)
@@ -7335,49 +7575,80 @@ aarch64_float_const_representable_p (rtx
 }
 
 char*
-aarch64_output_simd_mov_immediate (rtx *const_vector,
+aarch64_output_simd_mov_immediate (rtx const_vector,
 				   enum machine_mode mode,
 				   unsigned width)
 {
-  int is_valid;
-  unsigned char widthc;
-  int lane_width_bits;
+  bool is_valid;
   static char templ[40];
-  int shift = 0, mvn = 0;
   const char *mnemonic;
+  const char *shift_op;
   unsigned int lane_count = 0;
+  char element_char;
+
+  struct simd_immediate_info info = { NULL_RTX, 0, 0, false, false };
 
-  is_valid =
-    aarch64_simd_immediate_valid_for_move (*const_vector, mode,
-					   const_vector, &lane_width_bits,
-					   &widthc, &mvn, &shift);
+  /* This will return true to show const_vector is legal for use as either
+     a AdvSIMD MOVI instruction (or, implicitly, MVNI) immediate.  It will
+     also update INFO to show how the immediate should be generated.  */
+  is_valid = aarch64_simd_valid_immediate (const_vector, mode, false, &info);
   gcc_assert (is_valid);
 
+  element_char = sizetochar (info.element_width);
+  lane_count = width / info.element_width;
+
   mode = GET_MODE_INNER (mode);
   if (mode == SFmode || mode == DFmode)
     {
-      bool zero_p =
-	aarch64_float_const_zero_rtx_p (*const_vector);
-      gcc_assert (shift == 0);
-      mnemonic = zero_p ? "movi" : "fmov";
+      gcc_assert (info.shift == 0 && ! info.mvn);
+      if (aarch64_float_const_zero_rtx_p (info.value))
+        info.value = GEN_INT (0);
+      else
+	{
+#define buf_size 20
+	  REAL_VALUE_TYPE r;
+	  REAL_VALUE_FROM_CONST_DOUBLE (r, info.value);
+	  char float_buf[buf_size] = {'\0'};
+	  real_to_decimal_for_mode (float_buf, &r, buf_size, buf_size, 1, mode);
+#undef buf_size
+
+	  if (lane_count == 1)
+	    snprintf (templ, sizeof (templ), "fmov\t%%d0, %s", float_buf);
+	  else
+	    snprintf (templ, sizeof (templ), "fmov\t%%0.%d%c, %s",
+		      lane_count, element_char, float_buf);
+	  return templ;
+	}
     }
-  else
-    mnemonic = mvn ? "mvni" : "movi";
 
-  gcc_assert (lane_width_bits != 0);
-  lane_count = width / lane_width_bits;
+  mnemonic = info.mvn ? "mvni" : "movi";
+  shift_op = info.msl ? "msl" : "lsl";
 
   if (lane_count == 1)
-    snprintf (templ, sizeof (templ), "%s\t%%d0, %%1", mnemonic);
-  else if (shift)
-    snprintf (templ, sizeof (templ), "%s\t%%0.%d%c, %%1, lsl %d",
-	      mnemonic, lane_count, widthc, shift);
+    snprintf (templ, sizeof (templ), "%s\t%%d0, " HOST_WIDE_INT_PRINT_HEX,
+	      mnemonic, UINTVAL (info.value));
+  else if (info.shift)
+    snprintf (templ, sizeof (templ), "%s\t%%0.%d%c, " HOST_WIDE_INT_PRINT_HEX
+	      ", %s %d", mnemonic, lane_count, element_char,
+	      UINTVAL (info.value), shift_op, info.shift);
   else
-    snprintf (templ, sizeof (templ), "%s\t%%0.%d%c, %%1",
-	      mnemonic, lane_count, widthc);
+    snprintf (templ, sizeof (templ), "%s\t%%0.%d%c, " HOST_WIDE_INT_PRINT_HEX,
+	      mnemonic, lane_count, element_char, UINTVAL (info.value));
   return templ;
 }
 
+char*
+aarch64_output_scalar_simd_mov_immediate (rtx immediate,
+					  enum machine_mode mode)
+{
+  enum machine_mode vmode;
+
+  gcc_assert (!VECTOR_MODE_P (mode));
+  vmode = aarch64_simd_container_mode (mode, 64);
+  rtx v_op = aarch64_simd_gen_const_vector_dup (vmode, INTVAL (immediate));
+  return aarch64_output_simd_mov_immediate (v_op, vmode, 64);
+}
+
 /* Split operands into moves from op[1] + op[2] into op[0].  */
 
 void
@@ -8002,6 +8273,9 @@ aarch64_vectorize_vec_perm_const_ok (enu
 #undef TARGET_EXPAND_BUILTIN_VA_START
 #define TARGET_EXPAND_BUILTIN_VA_START aarch64_expand_builtin_va_start
 
+#undef TARGET_FOLD_BUILTIN
+#define TARGET_FOLD_BUILTIN aarch64_fold_builtin
+
 #undef TARGET_FUNCTION_ARG
 #define TARGET_FUNCTION_ARG aarch64_function_arg
 
@@ -8023,6 +8297,9 @@ aarch64_vectorize_vec_perm_const_ok (enu
 #undef TARGET_FRAME_POINTER_REQUIRED
 #define TARGET_FRAME_POINTER_REQUIRED aarch64_frame_pointer_required
 
+#undef TARGET_GIMPLE_FOLD_BUILTIN
+#define TARGET_GIMPLE_FOLD_BUILTIN aarch64_gimple_fold_builtin
+
 #undef TARGET_GIMPLIFY_VA_ARG_EXPR
 #define TARGET_GIMPLIFY_VA_ARG_EXPR aarch64_gimplify_va_arg_expr
 
@@ -8102,6 +8379,13 @@ aarch64_vectorize_vec_perm_const_ok (enu
 #undef TARGET_ARRAY_MODE_SUPPORTED_P
 #define TARGET_ARRAY_MODE_SUPPORTED_P aarch64_array_mode_supported_p
 
+#undef TARGET_VECTORIZE_ADD_STMT_COST
+#define TARGET_VECTORIZE_ADD_STMT_COST aarch64_add_stmt_cost
+
+#undef TARGET_VECTORIZE_BUILTIN_VECTORIZATION_COST
+#define TARGET_VECTORIZE_BUILTIN_VECTORIZATION_COST \
+  aarch64_builtin_vectorization_cost
+
 #undef TARGET_VECTORIZE_PREFERRED_SIMD_MODE
 #define TARGET_VECTORIZE_PREFERRED_SIMD_MODE aarch64_preferred_simd_mode
 
Index: b/src/gcc/config/aarch64/iterators.md
===================================================================
--- a/src/gcc/config/aarch64/iterators.md
+++ b/src/gcc/config/aarch64/iterators.md
@@ -83,6 +83,9 @@
 ;; Vector Float modes.
 (define_mode_iterator VDQF [V2SF V4SF V2DF])
 
+;; Modes suitable to use as the return type of a vcond expression.
+(define_mode_iterator VDQF_COND [V2SF V2SI V4SF V4SI V2DF V2DI])
+
 ;; All Float modes.
 (define_mode_iterator VALLF [V2SF V4SF V2DF SF DF])
 
@@ -125,9 +128,15 @@
 ;; Vector modes except double int.
 (define_mode_iterator VDQIF [V8QI V16QI V4HI V8HI V2SI V4SI V2SF V4SF V2DF])
 
+;; Vector modes for Q and H types.
+(define_mode_iterator VDQQH [V8QI V16QI V4HI V8HI])
+
 ;; Vector modes for H and S types.
 (define_mode_iterator VDQHS [V4HI V8HI V2SI V4SI])
 
+;; Vector modes for Q, H and S types.
+(define_mode_iterator VDQQHS [V8QI V16QI V4HI V8HI V2SI V4SI])
+
 ;; Vector and scalar integer modes for H and S
 (define_mode_iterator VSDQ_HSI [V4HI V8HI V2SI V4SI HI SI])
 
@@ -163,10 +172,15 @@
  [
     UNSPEC_ASHIFT_SIGNED	; Used in aarch-simd.md.
     UNSPEC_ASHIFT_UNSIGNED	; Used in aarch64-simd.md.
+    UNSPEC_FMAX		; Used in aarch64-simd.md.
+    UNSPEC_FMAXNMV	; Used in aarch64-simd.md.
     UNSPEC_FMAXV	; Used in aarch64-simd.md.
+    UNSPEC_FMIN		; Used in aarch64-simd.md.
+    UNSPEC_FMINNMV	; Used in aarch64-simd.md.
     UNSPEC_FMINV	; Used in aarch64-simd.md.
     UNSPEC_FADDV	; Used in aarch64-simd.md.
-    UNSPEC_ADDV		; Used in aarch64-simd.md.
+    UNSPEC_SADDV	; Used in aarch64-simd.md.
+    UNSPEC_UADDV	; Used in aarch64-simd.md.
     UNSPEC_SMAXV	; Used in aarch64-simd.md.
     UNSPEC_SMINV	; Used in aarch64-simd.md.
     UNSPEC_UMAXV	; Used in aarch64-simd.md.
@@ -223,9 +237,6 @@
     UNSPEC_SSHLL	; Used in aarch64-simd.md.
     UNSPEC_USHLL	; Used in aarch64-simd.md.
     UNSPEC_ADDP		; Used in aarch64-simd.md.
-    UNSPEC_FMAX		; Used in aarch64-simd.md.
-    UNSPEC_FMIN		; Used in aarch64-simd.md.
-    UNSPEC_BSL		; Used in aarch64-simd.md.
     UNSPEC_TBL		; Used in vector permute patterns.
     UNSPEC_CONCAT	; Used in vector permute patterns.
     UNSPEC_ZIP1		; Used in vector permute patterns.
@@ -234,6 +245,22 @@
     UNSPEC_UZP2		; Used in vector permute patterns.
     UNSPEC_TRN1		; Used in vector permute patterns.
     UNSPEC_TRN2		; Used in vector permute patterns.
+    UNSPEC_AESE		; Used in aarch64-simd.md.
+    UNSPEC_AESD         ; Used in aarch64-simd.md.
+    UNSPEC_AESMC        ; Used in aarch64-simd.md.
+    UNSPEC_AESIMC       ; Used in aarch64-simd.md.
+    UNSPEC_SHA1C	; Used in aarch64-simd.md.
+    UNSPEC_SHA1M        ; Used in aarch64-simd.md.
+    UNSPEC_SHA1P        ; Used in aarch64-simd.md.
+    UNSPEC_SHA1H        ; Used in aarch64-simd.md.
+    UNSPEC_SHA1SU0      ; Used in aarch64-simd.md.
+    UNSPEC_SHA1SU1      ; Used in aarch64-simd.md.
+    UNSPEC_SHA256H      ; Used in aarch64-simd.md.
+    UNSPEC_SHA256H2     ; Used in aarch64-simd.md.
+    UNSPEC_SHA256SU0    ; Used in aarch64-simd.md.
+    UNSPEC_SHA256SU1    ; Used in aarch64-simd.md.
+    UNSPEC_PMULL        ; Used in aarch64-simd.md.
+    UNSPEC_PMULL2       ; Used in aarch64-simd.md.
 ])
 
 ;; -------------------------------------------------------------------
@@ -244,6 +271,9 @@
 ;; 32-bit version and "%x0" in the 64-bit version.
 (define_mode_attr w [(QI "w") (HI "w") (SI "w") (DI "x") (SF "s") (DF "d")])
 
+;; For constraints used in scalar immediate vector moves
+(define_mode_attr hq [(HI "h") (QI "q")])
+
 ;; For scalar usage of vector/FP registers
 (define_mode_attr v [(QI "b") (HI "h") (SI "s") (DI "d")
 		    (SF "s") (DF "d")
@@ -316,6 +346,7 @@
                           (V2SI "s") (V4SI  "s")
 			  (V2DI "d") (V2SF  "s")
 			  (V4SF "s") (V2DF  "d")
+			  (SF   "s") (DF  "d")
 			  (QI "b")   (HI "h")
 			  (SI "s")   (DI "d")])
 
@@ -377,7 +408,8 @@
 ;; Double modes of vector modes (lower case).
 (define_mode_attr Vdbl [(V8QI "v16qi") (V4HI "v8hi")
 			(V2SI "v4si")  (V2SF "v4sf")
-			(SI   "v2si")  (DI   "v2di")])
+			(SI   "v2si")  (DI   "v2di")
+			(DF   "v2df")])
 
 ;; Narrowed modes for VDN.
 (define_mode_attr VNARROWD [(V4HI "V8QI") (V2SI "V4HI")
@@ -432,6 +464,15 @@
                         (V2SF "s") (V4SF "s")
                         (V2DF "d")])
 
+;; Corresponding core element mode for each vector mode.  This is a
+;; variation on <vw> mapping FP modes to GP regs.
+(define_mode_attr vwcore  [(V8QI "w") (V16QI "w")
+			   (V4HI "w") (V8HI "w")
+			   (V2SI "w") (V4SI "w")
+			   (DI   "x") (V2DI "x")
+			   (V2SF "w") (V4SF "w")
+			   (V2DF "x")])
+
 ;; Double vector types for ALLX.
 (define_mode_attr Vallxd [(QI "8b") (HI "4h") (SI "2s")])
 
@@ -491,6 +532,24 @@
 (define_mode_attr fcvt_target [(V2DF "v2di") (V4SF "v4si") (V2SF "v2si")])
 (define_mode_attr FCVT_TARGET [(V2DF "V2DI") (V4SF "V4SI") (V2SF "V2SI")])
 
+;; Defined to '_fp' for types whose element type is a float type.
+(define_mode_attr fp [(V8QI "")  (V16QI "")
+		      (V4HI "")  (V8HI  "")
+		      (V2SI "")  (V4SI  "")
+		      (DI   "")  (V2DI  "")
+		      (V2SF "_fp") (V4SF  "_fp")
+		      (V2DF "_fp") (DF    "_fp")
+		      (SF "_fp")])
+
+;; Defined to '_q' for 128-bit types.
+(define_mode_attr q [(V8QI "") (V16QI "_q")
+		     (V4HI "") (V8HI  "_q")
+		     (V2SI "") (V4SI  "_q")
+		     (DI   "") (V2DI  "_q")
+		     (V2SF "") (V4SF  "_q")
+			       (V2DF  "_q")
+		     (QI "") (HI "") (SI "") (DI "") (SF "") (DF "")])
+
 ;; -------------------------------------------------------------------
 ;; Code Iterators
 ;; -------------------------------------------------------------------
@@ -527,9 +586,14 @@
 ;; Iterator for integer conversions
 (define_code_iterator FIXUORS [fix unsigned_fix])
 
+;; Iterator for float conversions
+(define_code_iterator FLOATUORS [float unsigned_float])
+
 ;; Code iterator for variants of vector max and min.
 (define_code_iterator MAXMIN [smax smin umax umin])
 
+(define_code_iterator FMAXMIN [smax smin])
+
 ;; Code iterator for variants of vector max and min.
 (define_code_iterator ADDSUB [plus minus])
 
@@ -548,6 +612,9 @@
 ;; Unsigned comparison operators.
 (define_code_iterator UCOMPARISONS [ltu leu geu gtu])
 
+;; Unsigned comparison operators.
+(define_code_iterator FAC_COMPARISONS [lt le ge gt])
+
 ;; -------------------------------------------------------------------
 ;; Code Attributes
 ;; -------------------------------------------------------------------
@@ -560,6 +627,10 @@
 			 (zero_extend "zero_extend")
 			 (sign_extract "extv")
 			 (zero_extract "extzv")
+			 (fix "fix")
+			 (unsigned_fix "fixuns")
+			 (float "float")
+			 (unsigned_float "floatuns")
 			 (and "and")
 			 (ior "ior")
 			 (xor "xor")
@@ -599,10 +670,14 @@
 (define_code_attr CMP [(lt "LT") (le "LE") (eq "EQ") (ge "GE") (gt "GT")
 			   (ltu "LTU") (leu "LEU") (geu "GEU") (gtu "GTU")])
 
+(define_code_attr fix_trunc_optab [(fix "fix_trunc")
+				   (unsigned_fix "fixuns_trunc")])
+
 ;; Optab prefix for sign/zero-extending operations
 (define_code_attr su_optab [(sign_extend "") (zero_extend "u")
 			    (div "") (udiv "u")
 			    (fix "") (unsigned_fix "u")
+			    (float "s") (unsigned_float "u")
 			    (ss_plus "s") (us_plus "u")
 			    (ss_minus "s") (us_minus "u")])
 
@@ -627,7 +702,9 @@
 (define_code_attr su [(sign_extend "s") (zero_extend "u")
 		      (sign_extract "s") (zero_extract "u")
 		      (fix "s") (unsigned_fix "u")
-		      (div "s") (udiv "u")])
+		      (div "s") (udiv "u")
+		      (smax "s") (umax "u")
+		      (smin "s") (umin "u")])
 
 ;; Emit cbz/cbnz depending on comparison type.
 (define_code_attr cbz [(eq "cbz") (ne "cbnz") (lt "cbnz") (ge "cbz")])
@@ -636,10 +713,10 @@
 (define_code_attr tbz [(eq "tbz") (ne "tbnz") (lt "tbnz") (ge "tbz")])
 
 ;; Max/min attributes.
-(define_code_attr maxmin [(smax "smax")
-			  (smin "smin")
-			  (umax "umax")
-			  (umin "umin")])
+(define_code_attr maxmin [(smax "max")
+			  (smin "min")
+			  (umax "max")
+			  (umin "min")])
 
 ;; MLA/MLS attributes.
 (define_code_attr as [(ss_plus "a") (ss_minus "s")])
@@ -661,7 +738,10 @@
 (define_int_iterator MAXMINV [UNSPEC_UMAXV UNSPEC_UMINV
 			      UNSPEC_SMAXV UNSPEC_SMINV])
 
-(define_int_iterator FMAXMINV [UNSPEC_FMAXV UNSPEC_FMINV])
+(define_int_iterator FMAXMINV [UNSPEC_FMAXV UNSPEC_FMINV
+			       UNSPEC_FMAXNMV UNSPEC_FMINNMV])
+
+(define_int_iterator SUADDV [UNSPEC_SADDV UNSPEC_UADDV])
 
 (define_int_iterator HADDSUB [UNSPEC_SHADD UNSPEC_UHADD
 			      UNSPEC_SRHADD UNSPEC_URHADD
@@ -675,7 +755,7 @@
 (define_int_iterator ADDSUBHN2 [UNSPEC_ADDHN2 UNSPEC_RADDHN2
 			        UNSPEC_SUBHN2 UNSPEC_RSUBHN2])
 
-(define_int_iterator FMAXMIN [UNSPEC_FMAX UNSPEC_FMIN])
+(define_int_iterator FMAXMIN_UNS [UNSPEC_FMAX UNSPEC_FMIN])
 
 (define_int_iterator VQDMULH [UNSPEC_SQDMULH UNSPEC_SQRDMULH])
 
@@ -711,24 +791,45 @@
 			      UNSPEC_UZP1 UNSPEC_UZP2])
 
 (define_int_iterator FRINT [UNSPEC_FRINTZ UNSPEC_FRINTP UNSPEC_FRINTM
-			     UNSPEC_FRINTI UNSPEC_FRINTX UNSPEC_FRINTA])
+			     UNSPEC_FRINTN UNSPEC_FRINTI UNSPEC_FRINTX
+			     UNSPEC_FRINTA])
 
 (define_int_iterator FCVT [UNSPEC_FRINTZ UNSPEC_FRINTP UNSPEC_FRINTM
-			    UNSPEC_FRINTA])
+			    UNSPEC_FRINTA UNSPEC_FRINTN])
+
+(define_int_iterator FRECP [UNSPEC_FRECPE UNSPEC_FRECPX])
+
+(define_int_iterator CRYPTO_AES [UNSPEC_AESE UNSPEC_AESD])
+(define_int_iterator CRYPTO_AESMC [UNSPEC_AESMC UNSPEC_AESIMC])
+
+(define_int_iterator CRYPTO_SHA1 [UNSPEC_SHA1C UNSPEC_SHA1M UNSPEC_SHA1P])
+
+(define_int_iterator CRYPTO_SHA256 [UNSPEC_SHA256H UNSPEC_SHA256H2])
 
 ;; -------------------------------------------------------------------
 ;; Int Iterators Attributes.
 ;; -------------------------------------------------------------------
-(define_int_attr  maxminv [(UNSPEC_UMAXV "umax")
-			   (UNSPEC_UMINV "umin")
-			   (UNSPEC_SMAXV "smax")
-			   (UNSPEC_SMINV "smin")])
-
-(define_int_attr  fmaxminv [(UNSPEC_FMAXV "max")
-			    (UNSPEC_FMINV "min")])
-
-(define_int_attr  fmaxmin [(UNSPEC_FMAX "fmax")
-			   (UNSPEC_FMIN "fmin")])
+(define_int_attr  maxmin_uns [(UNSPEC_UMAXV "umax")
+			      (UNSPEC_UMINV "umin")
+			      (UNSPEC_SMAXV "smax")
+			      (UNSPEC_SMINV "smin")
+			      (UNSPEC_FMAX  "smax_nan")
+			      (UNSPEC_FMAXNMV "smax")
+			      (UNSPEC_FMAXV "smax_nan")
+			      (UNSPEC_FMIN "smin_nan")
+			      (UNSPEC_FMINNMV "smin")
+			      (UNSPEC_FMINV "smin_nan")])
+
+(define_int_attr  maxmin_uns_op [(UNSPEC_UMAXV "umax")
+				 (UNSPEC_UMINV "umin")
+				 (UNSPEC_SMAXV "smax")
+				 (UNSPEC_SMINV "smin")
+				 (UNSPEC_FMAX "fmax")
+				 (UNSPEC_FMAXNMV "fmaxnm")
+				 (UNSPEC_FMAXV "fmax")
+				 (UNSPEC_FMIN "fmin")
+				 (UNSPEC_FMINNMV "fminnm")
+				 (UNSPEC_FMINV "fmin")])
 
 (define_int_attr sur [(UNSPEC_SHADD "s") (UNSPEC_UHADD "u")
 		      (UNSPEC_SRHADD "sr") (UNSPEC_URHADD "ur")
@@ -740,6 +841,7 @@
 		      (UNSPEC_SUBHN2 "") (UNSPEC_RSUBHN2 "r")
 		      (UNSPEC_SQXTN "s") (UNSPEC_UQXTN "u")
 		      (UNSPEC_USQADD "us") (UNSPEC_SUQADD "su")
+		      (UNSPEC_SADDV "s") (UNSPEC_UADDV "u")
 		      (UNSPEC_SSLI  "s") (UNSPEC_USLI  "u")
 		      (UNSPEC_SSRI  "s") (UNSPEC_USRI  "u")
 		      (UNSPEC_USRA  "u") (UNSPEC_SSRA  "s")
@@ -798,15 +900,18 @@
 				(UNSPEC_FRINTM "floor")
 				(UNSPEC_FRINTI "nearbyint")
 				(UNSPEC_FRINTX "rint")
-				(UNSPEC_FRINTA "round")])
+				(UNSPEC_FRINTA "round")
+				(UNSPEC_FRINTN "frintn")])
 
 ;; frint suffix for floating-point rounding instructions.
 (define_int_attr frint_suffix [(UNSPEC_FRINTZ "z") (UNSPEC_FRINTP "p")
 			       (UNSPEC_FRINTM "m") (UNSPEC_FRINTI "i")
-			       (UNSPEC_FRINTX "x") (UNSPEC_FRINTA "a")])
+			       (UNSPEC_FRINTX "x") (UNSPEC_FRINTA "a")
+			       (UNSPEC_FRINTN "n")])
 
 (define_int_attr fcvt_pattern [(UNSPEC_FRINTZ "btrunc") (UNSPEC_FRINTA "round")
-			       (UNSPEC_FRINTP "ceil") (UNSPEC_FRINTM "floor")])
+			       (UNSPEC_FRINTP "ceil") (UNSPEC_FRINTM "floor")
+			       (UNSPEC_FRINTN "frintn")])
 
 (define_int_attr perm_insn [(UNSPEC_ZIP1 "zip") (UNSPEC_ZIP2 "zip")
 			    (UNSPEC_TRN1 "trn") (UNSPEC_TRN2 "trn")
@@ -815,3 +920,13 @@
 (define_int_attr perm_hilo [(UNSPEC_ZIP1 "1") (UNSPEC_ZIP2 "2")
 			    (UNSPEC_TRN1 "1") (UNSPEC_TRN2 "2")
 			    (UNSPEC_UZP1 "1") (UNSPEC_UZP2 "2")])
+
+(define_int_attr frecp_suffix  [(UNSPEC_FRECPE "e") (UNSPEC_FRECPX "x")])
+
+(define_int_attr aes_op [(UNSPEC_AESE "e") (UNSPEC_AESD "d")])
+(define_int_attr aesmc_op [(UNSPEC_AESMC "mc") (UNSPEC_AESIMC "imc")])
+
+(define_int_attr sha1_op [(UNSPEC_SHA1C "c") (UNSPEC_SHA1P "p")
+			  (UNSPEC_SHA1M "m")])
+
+(define_int_attr sha256_op [(UNSPEC_SHA256H "") (UNSPEC_SHA256H2 "2")])
Index: b/src/gcc/config/aarch64/aarch64.h
===================================================================
--- a/src/gcc/config/aarch64/aarch64.h
+++ b/src/gcc/config/aarch64/aarch64.h
@@ -49,6 +49,8 @@
 	    break;					\
 	}						\
 							\
+      if (TARGET_CRYPTO)				\
+	builtin_define ("__ARM_FEATURE_CRYPTO");	\
     } while (0)
 
 
@@ -151,6 +153,7 @@
 #define AARCH64_FL_FP         (1 << 1)	/* Has FP.  */
 #define AARCH64_FL_CRYPTO     (1 << 2)	/* Has crypto.  */
 #define AARCH64_FL_SLOWMUL    (1 << 3)	/* A slow multiply core.  */
+#define AARCH64_FL_CRC        (1 << 4)	/* Has CRC.  */
 
 /* Has FP and SIMD.  */
 #define AARCH64_FL_FPSIMD     (AARCH64_FL_FP | AARCH64_FL_SIMD)
@@ -163,6 +166,7 @@
 
 /* Macros to test ISA flags.  */
 extern unsigned long aarch64_isa_flags;
+#define AARCH64_ISA_CRC            (aarch64_isa_flags & AARCH64_FL_CRC)
 #define AARCH64_ISA_CRYPTO         (aarch64_isa_flags & AARCH64_FL_CRYPTO)
 #define AARCH64_ISA_FP             (aarch64_isa_flags & AARCH64_FL_FP)
 #define AARCH64_ISA_SIMD           (aarch64_isa_flags & AARCH64_FL_SIMD)
@@ -171,6 +175,8 @@ extern unsigned long aarch64_isa_flags;
 extern unsigned long aarch64_tune_flags;
 #define AARCH64_TUNE_SLOWMUL       (aarch64_tune_flags & AARCH64_FL_SLOWMUL)
 
+/* Crypto is an optional feature.  */
+#define TARGET_CRYPTO AARCH64_ISA_CRYPTO
 
 /* Standard register usage.  */
 
@@ -434,7 +440,7 @@ enum reg_class
 #define INDEX_REG_CLASS	CORE_REGS
 #define BASE_REG_CLASS  POINTER_REGS
 
-/* Register pairs used to eliminate unneeded registers that point intoi
+/* Register pairs used to eliminate unneeded registers that point into
    the stack frame.  */
 #define ELIMINABLE_REGS							\
 {									\
@@ -459,10 +465,10 @@ enum target_cpus
   TARGET_CPU_generic
 };
 
-/* If there is no CPU defined at configure, use "generic" as default.  */
+/* If there is no CPU defined at configure, use "cortex-a53" as default.  */
 #ifndef TARGET_CPU_DEFAULT
 #define TARGET_CPU_DEFAULT \
-  (TARGET_CPU_generic | (AARCH64_CPU_DEFAULT_FLAGS << 6))
+  (TARGET_CPU_cortexa53 | (AARCH64_CPU_DEFAULT_FLAGS << 6))
 #endif
 
 /* If inserting NOP before a mult-accumulate insn remember to adjust the
@@ -487,7 +493,7 @@ extern enum aarch64_processor aarch64_tu
 /* Stack layout; function entry, exit and calling.  */
 #define STACK_GROWS_DOWNWARD	1
 
-#define FRAME_GROWS_DOWNWARD	0
+#define FRAME_GROWS_DOWNWARD	1
 
 #define STARTING_FRAME_OFFSET	0
 
@@ -533,12 +539,6 @@ typedef struct GTY (()) machine_function
 #endif
 
 
-/* Which ABI to use.  */
-enum arm_abi_type
-{
-  ARM_ABI_AAPCS64
-};
-
 enum arm_pcs
 {
   ARM_PCS_AAPCS64,		/* Base standard AAPCS for 64 bit.  */
@@ -546,11 +546,7 @@ enum arm_pcs
 };
 
 
-extern enum arm_abi_type arm_abi;
 extern enum arm_pcs arm_pcs_variant;
-#ifndef ARM_DEFAULT_ABI
-#define ARM_DEFAULT_ABI ARM_ABI_AAPCS64
-#endif
 
 #ifndef ARM_DEFAULT_PCS
 #define ARM_DEFAULT_PCS ARM_PCS_AAPCS64
@@ -721,6 +717,8 @@ do {									     \
 
 #define SELECT_CC_MODE(OP, X, Y)	aarch64_select_cc_mode (OP, X, Y)
 
+#define REVERSIBLE_CC_MODE(MODE) 1
+
 #define REVERSE_CONDITION(CODE, MODE)		\
   (((MODE) == CCFPmode || (MODE) == CCFPEmode)	\
    ? reverse_condition_maybe_unordered (CODE)	\
@@ -770,8 +768,22 @@ do {									     \
 #define PRINT_OPERAND_ADDRESS(STREAM, X) \
   aarch64_print_operand_address (STREAM, X)
 
-#define FUNCTION_PROFILER(STREAM, LABELNO) \
-  aarch64_function_profiler (STREAM, LABELNO)
+#define MCOUNT_NAME "_mcount"
+
+#define NO_PROFILE_COUNTERS 1
+
+/* Emit rtl for profiling.  Output assembler code to FILE
+   to call "_mcount" for profiling a function entry.  */
+#define PROFILE_HOOK(LABEL)                                    \
+{                                                              \
+  rtx fun,lr;                                                  \
+  lr = get_hard_reg_initial_val (Pmode, LR_REGNUM);            \
+  fun = gen_rtx_SYMBOL_REF (Pmode, MCOUNT_NAME);               \
+  emit_library_call (fun, LCT_NORMAL, VOIDmode, 1, lr, Pmode); \
+}
+
+/* All the work done in PROFILE_HOOK, but still required.  */
+#define FUNCTION_PROFILER(STREAM, LABELNO) do { } while (0)
 
 /* For some reason, the Linux headers think they know how to define
    these macros.  They don't!!!  */
Index: b/src/gcc/config/arm/neon-schedgen.ml
===================================================================
--- a/src/gcc/config/arm/neon-schedgen.ml
+++ b/src/gcc/config/arm/neon-schedgen.ml
@@ -1,543 +0,0 @@
-(* Emission of the core of the Cortex-A8 NEON scheduling description.
-   Copyright (C) 2007-2013 Free Software Foundation, Inc.
-   Contributed by CodeSourcery.
-   This file is part of GCC.
-
-   GCC is free software; you can redistribute it and/or modify it under
-   the terms of the GNU General Public License as published by the Free
-   Software Foundation; either version 3, or (at your option) any later
-   version.
-
-   GCC is distributed in the hope that it will be useful, but WITHOUT ANY
-   WARRANTY; without even the implied warranty of MERCHANTABILITY or
-   FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
-   for more details.
-
-   You should have received a copy of the GNU General Public License
-   along with GCC; see the file COPYING3.  If not see
-   <http://www.gnu.org/licenses/>.
-*)
-
-(* This scheduling description generator works as follows.
-   - Each group of instructions has source and destination requirements
-     specified and a list of cores supported. This is then filtered
-     and per core scheduler descriptions are generated out.
-     The reservations generated are prefixed by the name of the
-     core and the check is performed on the basis of what the tuning
-     string is. Running this will generate Neon scheduler descriptions
-     for all cores supported.
-
-     The source requirements may be specified using
-     Source (the stage at which all source operands not otherwise
-     described are read), Source_m (the stage at which Rm operands are
-     read), Source_n (likewise for Rn) and Source_d (likewise for Rd).
-   - For each group of instructions the earliest stage where a source
-     operand may be required is calculated.
-   - Each group of instructions is selected in turn as a producer.
-     The latencies between this group and every other group are then
-     calculated, yielding up to four values for each combination:
-	1. Producer -> consumer Rn latency
-	2. Producer -> consumer Rm latency
-	3. Producer -> consumer Rd (as a source) latency
-	4. Producer -> consumer worst-case latency.
-     Value 4 is calculated from the destination availability requirements
-     of the consumer and the earliest source availability requirements
-     of the producer.
-   - The largest Value 4 calculated for the current producer is the
-     worse-case latency, L, for that instruction group.  This value is written
-     out in a define_insn_reservation for the producer group.
-   - For each producer and consumer pair, the latencies calculated above
-     are collated.  The average (of up to four values) is calculated and
-     if this average is different from the worst-case latency, an
-     unguarded define_bypass construction is issued for that pair.
-     (For each pair only one define_bypass construction will be emitted,
-     and at present we do not emit specific guards.)
-*)
-
-let find_with_result fn lst =
-  let rec scan = function
-      [] -> raise Not_found
-    | l::ls -> 
-      match fn l with
-          Some result -> result
-       | _ -> scan ls in
-    scan lst
-
-let n1 = 1 and n2 = 2 and n3 = 3 and n4 = 4 and n5 = 5 and n6 = 6
-    and n7 = 7 and n8 = 8 and n9 = 9
-
-type availability = Source of int
-                  | Source_n of int
-                  | Source_m of int
-                  | Source_d of int
-                  | Dest of int
-		  | Dest_n_after of int * int
-
-type guard = Guard_none | Guard_only_m | Guard_only_n | Guard_only_d
-
-(* Reservation behaviors.  All but the last row here correspond to one
-   pipeline each.  Each constructor will correspond to one
-   define_reservation.  *)
-type reservation =
-  Mul | Mul_2cycle | Mul_4cycle
-| Shift | Shift_2cycle
-| ALU | ALU_2cycle
-| Fmul | Fmul_2cycle
-| Fadd | Fadd_2cycle
-(* | VFP *)
-| Permute of int
-| Ls of int
-| Fmul_then_fadd | Fmul_then_fadd_2
-
-type core = CortexA8 | CortexA9
-let allCores = [CortexA8; CortexA9]
-let coreStr = function
-    CortexA8 -> "cortex_a8"
-  | CortexA9 -> "cortex_a9"
-
-let tuneStr = function
-    CortexA8 -> "cortexa8"
-   | CortexA9 -> "cortexa9"
-
-
-(* This table must be kept as short as possible by conflating
-   entries with the same availability behavior.
-
-   First components: instruction group names
-   Second components: availability requirements, in the order in which
-   they should appear in the comments in the .md file.
-   Third components: reservation info
-   Fourth components: List of supported cores.
-*)
-let availability_table = [
-  (* NEON integer ALU instructions.  *)
-  (* vbit vbif vbsl vorr vbic vnot vcls vclz vcnt vadd vand vorr
-     veor vbic vorn ddd qqq *)
-  "neon_int_1", [Source n2; Dest n3], ALU, allCores;
-  (* vadd vsub qqd vsub ddd qqq *)
-  "neon_int_2", [Source_m n1; Source_n n2; Dest n3], ALU, allCores;
-  (* vsum vneg dd qq vadd vsub qdd *)
-  "neon_int_3", [Source n1; Dest n3], ALU, allCores;
-  (* vabs vceqz vcgez vcbtz vclez vcltz vadh vradh vsbh vrsbh dqq *)
-  (* vhadd vrhadd vqadd vtst ddd qqq *)
-  "neon_int_4", [Source n2; Dest n4], ALU, allCores;
-  (* vabd qdd vhsub vqsub vabd vceq vcge vcgt vmax vmin vfmx vfmn ddd ddd *)
-  "neon_int_5", [Source_m n1; Source_n n2; Dest n4], ALU, allCores;
-  (* vqneg vqabs dd qq *)
-  "neon_vqneg_vqabs", [Source n1; Dest n4], ALU, allCores;
-  (* vmov vmvn *)
-  "neon_vmov", [Dest n3], ALU, allCores;
-  (* vaba *)
-  "neon_vaba", [Source_n n2; Source_m n1; Source_d n3; Dest n6], ALU, allCores;
-  "neon_vaba_qqq",
-    [Source_n n2; Source_m n1; Source_d n3; Dest_n_after (1, n6)], 
-   ALU_2cycle, allCores;
-  (* vsma *)
-  "neon_vsma", [Source_m n1; Source_d n3; Dest n6], ALU, allCores;
-
-  (* NEON integer multiply instructions.  *)
-  (* vmul, vqdmlh, vqrdmlh *)
-  (* vmul, vqdmul, qdd 16/8 long 32/16 long *)
-  "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long", [Source n2; Dest n6], 
-   Mul, allCores;
-  "neon_mul_qqq_8_16_32_ddd_32", [Source n2; Dest_n_after (1, n6)], 
-   Mul_2cycle, allCores;
-  (* vmul, vqdmul again *)
-  "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar",
-    [Source_n n2; Source_m n1; Dest_n_after (1, n6)], Mul_2cycle, allCores;
-  (* vmla, vmls *)
-  "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long",
-    [Source_n n2; Source_m n2; Source_d n3; Dest n6], Mul, allCores;
-  "neon_mla_qqq_8_16",
-    [Source_n n2; Source_m n2; Source_d n3; Dest_n_after (1, n6)], 
-   Mul_2cycle, allCores;
-  "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long",
-    [Source_n n2; Source_m n1; Source_d n3; Dest_n_after (1, n6)], 
-   Mul_2cycle, allCores;
-  "neon_mla_qqq_32_qqd_32_scalar",
-    [Source_n n2; Source_m n1; Source_d n3; Dest_n_after (3, n6)], 
-   Mul_4cycle, allCores;
-  (* vmul, vqdmulh, vqrdmulh *)
-  (* vmul, vqdmul *)
-  "neon_mul_ddd_16_scalar_32_16_long_scalar",
-    [Source_n n2; Source_m n1; Dest n6], Mul, allCores;
-  "neon_mul_qqd_32_scalar",
-    [Source_n n2; Source_m n1; Dest_n_after (3, n6)], Mul_4cycle, allCores;
-  (* vmla, vmls *)
-  (* vmla, vmla, vqdmla, vqdmls *)
-  "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar",
-    [Source_n n2; Source_m n1; Source_d n3; Dest n6], Mul, allCores;
-
-  (* NEON integer shift instructions.  *)
-  (* vshr/vshl immediate, vshr_narrow, vshl_vmvh, vsli_vsri_ddd *)
-  "neon_shift_1", [Source n1; Dest n3], Shift, allCores;
-  (* vqshl, vrshr immediate; vqshr, vqmov, vrshr, vqrshr narrow, allCores;
-     vqshl_vrshl_vqrshl_ddd *)
-  "neon_shift_2", [Source n1; Dest n4], Shift, allCores;
-  (* vsli, vsri and vshl for qqq *)
-  "neon_shift_3", [Source n1; Dest_n_after (1, n3)], Shift_2cycle, allCores;
-  "neon_vshl_ddd", [Source n1; Dest n1], Shift, allCores;
-  "neon_vqshl_vrshl_vqrshl_qqq", [Source n1; Dest_n_after (1, n4)],
-    Shift_2cycle, allCores;
-  "neon_vsra_vrsra", [Source_m n1; Source_d n3; Dest n6], Shift, allCores;
-
-  (* NEON floating-point instructions.  *)
-  (* vadd, vsub, vabd, vmul, vceq, vcge, vcgt, vcage, vcagt, vmax, vmin *)
-  (* vabs, vneg, vceqz, vcgez, vcgtz, vclez, vcltz, vrecpe, vrsqrte, vcvt *)
-  "neon_fp_vadd_ddd_vabs_dd", [Source n2; Dest n5], Fadd, allCores;
-  "neon_fp_vadd_qqq_vabs_qq", [Source n2; Dest_n_after (1, n5)],
-    Fadd_2cycle, allCores;
-  (* vsum, fvmx, vfmn *)
-  "neon_fp_vsum", [Source n1; Dest n5], Fadd, allCores;
-  "neon_fp_vmul_ddd", [Source_n n2; Source_m n1; Dest n5], Fmul, allCores;
-  "neon_fp_vmul_qqd", [Source_n n2; Source_m n1; Dest_n_after (1, n5)],
-    Fmul_2cycle, allCores;
-  (* vmla, vmls *)
-  "neon_fp_vmla_ddd",
-    [Source_n n2; Source_m n2; Source_d n3; Dest n9], Fmul_then_fadd, allCores;
-  "neon_fp_vmla_qqq",
-    [Source_n n2; Source_m n2; Source_d n3; Dest_n_after (1, n9)],
-    Fmul_then_fadd_2, allCores;
-  "neon_fp_vmla_ddd_scalar",
-    [Source_n n2; Source_m n1; Source_d n3; Dest n9], Fmul_then_fadd, allCores;
-  "neon_fp_vmla_qqq_scalar",
-    [Source_n n2; Source_m n1; Source_d n3; Dest_n_after (1, n9)],
-    Fmul_then_fadd_2, allCores;
-  "neon_fp_vrecps_vrsqrts_ddd", [Source n2; Dest n9], Fmul_then_fadd, allCores;
-  "neon_fp_vrecps_vrsqrts_qqq", [Source n2; Dest_n_after (1, n9)],
-    Fmul_then_fadd_2, allCores;
-
-  (* NEON byte permute instructions.  *)
-  (* vmov; vtrn and vswp for dd; vzip for dd; vuzp for dd; vrev; vext for dd *)
-  "neon_bp_simple", [Source n1; Dest n2], Permute 1, allCores;
-  (* vswp for qq; vext for qqq; vtbl with {Dn} or {Dn, Dn1}, allCores;
-     similarly for vtbx *)
-  "neon_bp_2cycle", [Source n1; Dest_n_after (1, n2)], Permute 2, allCores;
-  (* all the rest *)
-  "neon_bp_3cycle", [Source n1; Dest_n_after (2, n2)], Permute 3, allCores;
-
-  (* NEON load/store instructions.  *)
-  "neon_ldr", [Dest n1], Ls 1, allCores;
-  "neon_str", [Source n1], Ls 1, allCores;
-  "neon_vld1_1_2_regs", [Dest_n_after (1, n1)], Ls 2, allCores;
-  "neon_vld1_3_4_regs", [Dest_n_after (2, n1)], Ls 3, allCores;
-  "neon_vld2_2_regs_vld1_vld2_all_lanes", [Dest_n_after (1, n2)], Ls 2, allCores;
-  "neon_vld2_4_regs", [Dest_n_after (2, n2)], Ls 3, allCores;
-  "neon_vld3_vld4", [Dest_n_after (3, n2)], Ls 4, allCores;
-  "neon_vst1_1_2_regs_vst2_2_regs", [Source n1], Ls 2, allCores;
-  "neon_vst1_3_4_regs", [Source n1], Ls 3, allCores;
-  "neon_vst2_4_regs_vst3_vst4", [Source n1], Ls 4, allCores;
-  "neon_vst3_vst4", [Source n1], Ls 4, allCores;
-  "neon_vld1_vld2_lane", [Source n1; Dest_n_after (2, n2)], Ls 3, allCores;
-  "neon_vld3_vld4_lane", [Source n1; Dest_n_after (4, n2)], Ls 5, allCores;
-  "neon_vst1_vst2_lane", [Source n1], Ls 2, allCores;
-  "neon_vst3_vst4_lane", [Source n1], Ls 3, allCores;
-  "neon_vld3_vld4_all_lanes", [Dest_n_after (1, n2)], Ls 3, allCores;
-
-  (* NEON register transfer instructions.  *)
-  "neon_mcr", [Dest n2], Permute 1, allCores;
-  "neon_mcr_2_mcrr", [Dest n2], Permute 2, allCores;
-  (* MRC instructions are in the .tpl file.  *)
-]
-
-(* Augment the tuples in the availability table with an extra component
-   that describes the earliest stage where a source operand may be
-   required.  (It is also possible that an entry in the table has no
-   source requirements.)  *)
-let calculate_sources =
-  List.map (fun (name, avail, res, cores) ->
-              let earliest_stage =
-                List.fold_left
-                  (fun cur -> fun info ->
-                     match info with
-                       Source stage
-                     | Source_n stage
-                     | Source_m stage
-                     | Source_d stage ->
-                         (match cur with
-                           None -> Some stage
-                         | Some stage' when stage < stage' -> Some stage
-                         | _ -> cur)
-                     | _ -> cur) None avail
-              in
-                (name, avail, res, earliest_stage))
-
-(* Find the stage, if any, at the end of which a group produces a result.  *)
-let find_dest (attr, avail, _, _) =
-  try
-    find_with_result
-      (fun av -> match av with
-                   Dest st -> Some (Some st)
-                 | Dest_n_after (after, st) -> Some (Some (after + st))
-                 | _ -> None) avail
-  with Not_found -> None
-
-(* Find the worst-case latency between a producer and a consumer.  *)
-let worst_case_latency producer (_, _, _, earliest_required) =
-  let dest = find_dest producer in
-    match earliest_required, dest with
-      None, _ ->
-        (* The consumer doesn't have any source requirements.  *)
-        None
-    | _, None ->
-        (* The producer doesn't produce any results (e.g. a store insn).  *)
-        None
-    | Some consumed, Some produced -> Some (produced - consumed + 1)
-
-(* Helper function for below.  *)
-let latency_calc f producer (_, avail, _, _) =
-  try
-    let source_avail = find_with_result f avail in
-      match find_dest producer with
-        None ->
-          (* The producer does not produce a result.  *)
-          Some 0
-      | Some produced ->
-          let latency = produced - source_avail + 1 in
-            (* Latencies below zero are raised to zero since we don't have
-               delay slots.  *)
-            if latency < 0 then Some 0 else Some latency
-  with Not_found -> None
-
-(* Find any Rm latency between a producer and a consumer.  If no
-   Rm source requirement is explicitly specified for the consumer,
-   return "positive infinity".  Also return "positive infinity" if
-   the latency matches the supplied worst-case latency for this
-   producer.  *)
-let get_m_latency producer consumer =
-  match latency_calc (fun av -> match av with Source_m stage -> Some stage
-                                            | _ -> None) producer consumer
-  with None -> [] | Some latency -> [(Guard_only_m, latency)]
-
-(* Likewise for Rn.  *)
-let get_n_latency producer consumer =
-  match latency_calc (fun av -> match av with Source_n stage -> Some stage
-                                            | _ -> None) producer consumer
-  with None -> [] | Some latency -> [(Guard_only_n, latency)]
-
-(* Likewise for Rd.  *)
-let get_d_latency producer consumer =
-  match
-    latency_calc (fun av -> match av with Source_d stage -> Some stage
-                                        | _ -> None) producer consumer
-  with None -> [] | Some latency -> [(Guard_only_d, latency)]
-
-(* Given a producer and a consumer, work out the latency of the producer
-   to the consumer in each of the four cases (availability information
-   permitting) identified at the top of this file.  Return the
-   consumer, the worst-case unguarded latency and any guarded latencies.  *)
-let calculate_latencies producer consumer =
-  let worst = worst_case_latency producer consumer in
-  let m_latency = get_m_latency producer consumer in
-  let n_latency = get_n_latency producer consumer in
-  let d_latency = get_d_latency producer consumer in
-    (consumer, worst, m_latency @ n_latency @ d_latency)
-
-(* Helper function for below.  *)
-let pick_latency largest worst guards =
-  let guards =
-    match worst with
-      None -> guards
-    | Some worst -> (Guard_none, worst) :: guards
-  in
-  if List.length guards = 0 then None else
-    let total_latency =
-      List.fold_left (fun acc -> fun (_, latency) -> acc + latency) 0 guards
-    in
-    let average_latency = (float_of_int total_latency) /.
-                          (float_of_int (List.length guards)) in
-    let rounded_latency = int_of_float (ceil average_latency) in
-      if rounded_latency = largest then None
-      else Some (Guard_none, rounded_latency)
-
-(* Collate all bypasses for a particular producer as required in
-   worst_case_latencies_and_bypasses.  (By this stage there is a maximum
-   of one bypass from this producer to any particular consumer listed
-   in LATENCIES.)  Use a hash table to collate bypasses with the
-   same latency and guard.  *)
-let collate_bypasses (producer_name, _, _, _) largest latencies core =
-  let ht = Hashtbl.create 42 in
-  let keys = ref [] in
-    List.iter (
-      fun ((consumer, _, _, _), worst, guards) ->
-        (* Find out which latency to use.  Ignoring latencies that match
-           the *overall* worst-case latency for this producer (which will
-           be in define_insn_reservation), we have to examine:
-	   1. the latency with no guard between this producer and this
-              consumer; and
-	   2. any guarded latency.  *)
-        let guard_latency_opt = pick_latency largest worst guards in
-          match guard_latency_opt with
-            None -> ()
-          | Some (guard, latency) ->
-            begin
-              (if (try ignore (Hashtbl.find ht (guard, latency)); false
-                   with Not_found -> true) then
-                 keys := (guard, latency) :: !keys);
-              Hashtbl.add ht (guard, latency) ((coreStr core) ^ "_" ^ consumer)
-            end
-    ) latencies;
-    (* The hash table now has bypasses collated so that ones with the
-       same latency and guard have the same keys.  Walk through all the
-       keys, extract the associated bypasses, and concatenate the names
-       of the consumers for each bypass.  *)
-    List.map (
-      fun ((guard, latency) as key) ->
-        let consumers = Hashtbl.find_all ht key in
-          (producer_name,
-           String.concat ",\\\n               " consumers,
-           latency,
-           guard)
-      ) !keys
-
-(* For every producer, find the worst-case latency between it and
-   *any* consumer.  Also determine (if such a thing exists) the
-   lowest-latency bypass from each producer to each consumer.  Group
-   the output in such a way that all bypasses with the same producer
-   and latency are together, and so that bypasses with the worst-case
-   latency are ignored.  *)
-let worst_case_latencies_and_bypasses core =
-  let rec f (worst_acc, bypasses_acc) prev xs =
-    match xs with
-      [] -> (worst_acc, bypasses_acc)
-    | ((producer_name, producer_avail, res_string, _) as producer)::next ->
-      (* For this particular producer, work out the latencies between
-         it and every consumer.  *)
-      let latencies =
-        List.fold_left (fun acc -> fun consumer ->
-                          (calculate_latencies producer consumer) :: acc)
-                       [] (prev @ xs)
-      in
-        (* Now work out what the overall worst case latency was for this
-           particular producer.  *)
-        match latencies with
-          [] -> assert false
-        | _ ->
-          let comp_fn (_, l1, _) (_, l2, _) =
-            if l1 > l2 then -1 else if l1 = l2 then 0 else 1
-          in
-          let largest =
-            match List.hd (List.sort comp_fn latencies) with
-              (_, None, _) -> 0 (* Producer has no consumers. *)
-            | (_, Some worst, _) -> worst
-          in
-          (* Having got the largest latency, collect all bypasses for
-             this producer and filter out those with that larger
-             latency.  Record the others for later emission.  *)
-          let bypasses = collate_bypasses producer largest latencies core in
-            (* Go on to process remaining producers, having noted
-               the result for this one.  *)
-            f ((producer_name, producer_avail, largest,
-                res_string) :: worst_acc,
-               bypasses @ bypasses_acc)
-              (prev @ [producer]) next
-  in
-    f ([], []) []
-
-(* Emit a helpful comment for a define_insn_reservation.  *)
-let write_comment producer avail =
-  let seen_source = ref false in
-  let describe info =
-    let read = if !seen_source then "" else "read " in
-    match info with
-      Source stage ->
-        seen_source := true;
-	Printf.printf "%stheir source operands at N%d" read stage
-    | Source_n stage ->
-        seen_source := true;
-	Printf.printf "%stheir (D|Q)n operands at N%d" read stage
-    | Source_m stage ->
-        seen_source := true;
-	Printf.printf "%stheir (D|Q)m operands at N%d" read stage
-    | Source_d stage ->
-	Printf.printf "%stheir (D|Q)d operands at N%d" read stage
-    | Dest stage ->
-	Printf.printf "produce a result at N%d" stage
-    | Dest_n_after (after, stage) ->
-	Printf.printf "produce a result at N%d on cycle %d" stage (after + 1)
-  in
-    Printf.printf ";; Instructions using this reservation ";
-    let rec f infos x =
-      let sep = if x mod 2 = 1 then "" else "\n;;" in
-      match infos with
-        [] -> assert false
-      | [info] -> describe info; Printf.printf ".\n"
-      | info::(_::[] as infos) ->
-          describe info; Printf.printf ", and%s " sep; f infos (x+1)
-      | info::infos -> describe info; Printf.printf ",%s " sep; f infos (x+1)
-    in
-      f avail 0
-
-
-(* Emit a define_insn_reservation for each producer.  The latency
-   written in will be its worst-case latency.  *)
-let emit_insn_reservations core =
-  let corestring = coreStr core in
-  let tunestring = tuneStr core
-  in  List.iter (
-     fun (producer, avail, latency, reservation) ->
-        write_comment producer avail;
-        Printf.printf "(define_insn_reservation \"%s_%s\" %d\n" 
-            corestring producer latency;
-            Printf.printf "  (and (eq_attr \"tune\" \"%s\")\n" tunestring;
-        Printf.printf "       (eq_attr \"neon_type\" \"%s\"))\n" producer;
-        let str =
-          match reservation with
-	    Mul -> "dp" | Mul_2cycle -> "dp_2" | Mul_4cycle -> "dp_4"
-	  | Shift -> "dp" | Shift_2cycle -> "dp_2"
-	  | ALU -> "dp" | ALU_2cycle -> "dp_2"
-	  | Fmul -> "dp" | Fmul_2cycle -> "dp_2"
-	  | Fadd -> "fadd" | Fadd_2cycle -> "fadd_2"
-	  | Ls 1 -> "ls"
-          | Ls n -> "ls_" ^ (string_of_int n)
-	  | Permute 1 -> "perm"
-          | Permute n -> "perm_" ^ (string_of_int n)
-	  | Fmul_then_fadd -> "fmul_then_fadd"
-	  | Fmul_then_fadd_2 -> "fmul_then_fadd_2"
-        in
-          Printf.printf "  \"%s_neon_%s\")\n\n" corestring str
-    )
-
-(* Given a guard description, return the name of the C function to
-   be used as the guard for define_bypass.  *)
-let guard_fn g =
-  match g with
-    Guard_only_m -> "arm_neon_only_m_dependency"
-  | Guard_only_n -> "arm_neon_only_n_dependency"
-  | Guard_only_d -> "arm_neon_only_d_dependency"
-  | Guard_none -> assert false
-
-(* Emit a define_bypass for each bypass.  *)
-let emit_bypasses core =
-  List.iter (
-      fun (producer, consumers, latency, guard) ->
-        Printf.printf "(define_bypass %d \"%s_%s\"\n" 
-	latency (coreStr core) producer;
-
-        if guard = Guard_none then
-          Printf.printf "               \"%s\")\n\n" consumers
-        else
-          begin
-            Printf.printf "               \"%s\"\n" consumers;
-            Printf.printf "               \"%s\")\n\n" (guard_fn guard)
-          end
-    )
-
-
-let calculate_per_core_availability_table core availability_table =
-  let table = calculate_sources availability_table in
-  let worst_cases, bypasses = worst_case_latencies_and_bypasses core table in
-    emit_insn_reservations core (List.rev worst_cases);
-    Printf.printf ";; Exceptions to the default latencies.\n\n";
-    emit_bypasses core bypasses
-
-let calculate_core_availability_table core availability_table =
-let filter_core = List.filter (fun (_, _, _, cores) 
-				   -> List.exists ((=) core) cores)
-in calculate_per_core_availability_table core (filter_core availability_table)
-
-
-(* Program entry point.  *)
-let main =
-  List.map (fun core -> calculate_core_availability_table 
-		core availability_table) allCores
Index: b/src/gcc/config/arm/arm1020e.md
===================================================================
--- a/src/gcc/config/arm/arm1020e.md
+++ b/src/gcc/config/arm/arm1020e.md
@@ -66,13 +66,21 @@
 ;; ALU operations with no shifted operand
 (define_insn_reservation "1020alu_op" 1 
  (and (eq_attr "tune" "arm1020e,arm1022e")
-      (eq_attr "type" "alu_reg,simple_alu_imm"))
+      (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                       adr,bfm,rev,\
+                       shift_imm,shift_reg,\
+                       mov_imm,mov_reg,mvn_imm,mvn_reg,\
+                       multiple,no_insn"))
  "1020a_e,1020a_m,1020a_w")
 
 ;; ALU operations with a shift-by-constant operand
 (define_insn_reservation "1020alu_shift_op" 1 
  (and (eq_attr "tune" "arm1020e,arm1022e")
-      (eq_attr "type" "simple_alu_shift,alu_shift"))
+      (eq_attr "type" "alu_shift_imm,alus_shift_imm,\
+                       logic_shift_imm,logics_shift_imm,\
+                       extend,mov_shift,mvn_shift"))
  "1020a_e,1020a_m,1020a_w")
 
 ;; ALU operations with a shift-by-register operand
@@ -81,7 +89,9 @@
 ;; the execute stage.
 (define_insn_reservation "1020alu_shift_reg_op" 2 
  (and (eq_attr "tune" "arm1020e,arm1022e")
-      (eq_attr "type" "alu_shift_reg"))
+      (eq_attr "type" "alu_shift_reg,alus_shift_reg,\
+                       logic_shift_reg,logics_shift_reg,\
+                       mov_shift_reg,mvn_shift_reg"))
  "1020a_e*2,1020a_m,1020a_w")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -96,7 +106,7 @@
 ;; until after the memory stage.
 (define_insn_reservation "1020mult1" 2
  (and (eq_attr "tune" "arm1020e,arm1022e")
-      (eq_attr "insn" "smulxy,smulwy"))
+      (eq_attr "type" "smulxy,smulwy"))
  "1020a_e,1020a_m,1020a_w")
 
 ;; The "smlaxy" and "smlawx" instructions require two iterations through
@@ -104,7 +114,7 @@
 ;; the execute stage.
 (define_insn_reservation "1020mult2" 2
  (and (eq_attr "tune" "arm1020e,arm1022e")
-      (eq_attr "insn" "smlaxy,smlalxy,smlawx"))
+      (eq_attr "type" "smlaxy,smlalxy,smlawx"))
  "1020a_e*2,1020a_m,1020a_w")
 
 ;; The "smlalxy", "mul", and "mla" instructions require two iterations
@@ -112,7 +122,7 @@
 ;; the memory stage.
 (define_insn_reservation "1020mult3" 3
  (and (eq_attr "tune" "arm1020e,arm1022e")
-      (eq_attr "insn" "smlalxy,mul,mla"))
+      (eq_attr "type" "smlalxy,mul,mla"))
  "1020a_e*2,1020a_m,1020a_w")
 
 ;; The "muls" and "mlas" instructions loop in the execute stage for
@@ -120,7 +130,7 @@
 ;; available after three iterations.
 (define_insn_reservation "1020mult4" 3
  (and (eq_attr "tune" "arm1020e,arm1022e")
-      (eq_attr "insn" "muls,mlas"))
+      (eq_attr "type" "muls,mlas"))
  "1020a_e*4,1020a_m,1020a_w")
 
 ;; Long multiply instructions that produce two registers of
@@ -135,7 +145,7 @@
 ;; available after the memory cycle.
 (define_insn_reservation "1020mult5" 4
  (and (eq_attr "tune" "arm1020e,arm1022e")
-      (eq_attr "insn" "umull,umlal,smull,smlal"))
+      (eq_attr "type" "umull,umlal,smull,smlal"))
  "1020a_e*3,1020a_m,1020a_w")
 
 ;; The "umulls", "umlals", "smulls", and "smlals" instructions loop in
@@ -143,7 +153,7 @@
 ;; The value result is available after four iterations.
 (define_insn_reservation "1020mult6" 4
  (and (eq_attr "tune" "arm1020e,arm1022e")
-      (eq_attr "insn" "umulls,umlals,smulls,smlals"))
+      (eq_attr "type" "umulls,umlals,smulls,smlals"))
  "1020a_e*5,1020a_m,1020a_w")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -269,7 +279,7 @@
 ;; first execute state.  We model this by using 1020a_e in the first cycle.
 (define_insn_reservation "v10_ffarith" 5
  (and (eq_attr "vfp10" "yes")
-      (eq_attr "type" "fcpys,ffariths,ffarithd,fcmps,fcmpd"))
+      (eq_attr "type" "fmov,ffariths,ffarithd,fcmps,fcmpd"))
  "1020a_e+v10_fmac")
 
 (define_insn_reservation "v10_farith" 5
@@ -279,7 +289,7 @@
 
 (define_insn_reservation "v10_cvt" 5
  (and (eq_attr "vfp10" "yes")
-      (eq_attr "type" "f_cvt"))
+      (eq_attr "type" "f_cvt,f_cvti2f,f_cvtf2i"))
  "1020a_e+v10_fmac")
 
 (define_insn_reservation "v10_fmul" 6
@@ -289,12 +299,12 @@
 
 (define_insn_reservation "v10_fdivs" 18
  (and (eq_attr "vfp10" "yes")
-      (eq_attr "type" "fdivs"))
+      (eq_attr "type" "fdivs, fsqrts"))
  "1020a_e+v10_ds*14")
 
 (define_insn_reservation "v10_fdivd" 32
  (and (eq_attr "vfp10" "yes")
-      (eq_attr "type" "fdivd"))
+      (eq_attr "type" "fdivd, fsqrtd"))
  "1020a_e+v10_fmac+v10_ds*28")
 
 (define_insn_reservation "v10_floads" 4
@@ -315,7 +325,7 @@
 
 (define_insn_reservation "v10_c2v" 4
  (and (eq_attr "vfp10" "yes")
-      (eq_attr "type" "r_2_f"))
+      (eq_attr "type" "f_mcr,f_mcrr"))
  "1020a_e+1020l_e+v10_ls1,v10_ls2")
 
 (define_insn_reservation "v10_fstores" 1
@@ -330,7 +340,7 @@
 
 (define_insn_reservation "v10_v2c" 1
  (and (eq_attr "vfp10" "yes")
-      (eq_attr "type" "f_2_r"))
+      (eq_attr "type" "f_mrc,f_mrrc"))
  "1020a_e+1020l_e,1020l_m,1020l_w")
 
 (define_insn_reservation "v10_to_cpsr" 2
Index: b/src/gcc/config/arm/cortex-a15.md
===================================================================
--- a/src/gcc/config/arm/cortex-a15.md
+++ b/src/gcc/config/arm/cortex-a15.md
@@ -61,23 +61,32 @@
 ;; Simple ALU without shift
 (define_insn_reservation "cortex_a15_alu" 2
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "alu_reg,simple_alu_imm")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        adr,bfm,rev,\
+                        shift_imm,shift_reg,\
+                        mov_imm,mov_reg,\
+                        mvn_imm,mvn_reg,\
+                        mrs,multiple,no_insn"))
   "ca15_issue1,(ca15_sx1,ca15_sx1_alu)|(ca15_sx2,ca15_sx2_alu)")
 
 ;; ALU ops with immediate shift
 (define_insn_reservation "cortex_a15_alu_shift" 3
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "simple_alu_shift,alu_shift")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "extend,\
+                        alu_shift_imm,alus_shift_imm,\
+                        logic_shift_imm,logics_shift_imm,\
+                        mov_shift,mvn_shift"))
   "ca15_issue1,(ca15_sx1,ca15_sx1+ca15_sx1_shf,ca15_sx1_alu)\
 	       |(ca15_sx2,ca15_sx2+ca15_sx2_shf,ca15_sx2_alu)")
 
 ;; ALU ops with register controlled shift
 (define_insn_reservation "cortex_a15_alu_shift_reg" 3
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "alu_shift_reg")
-	    (eq_attr "neon_type" "none")))
+       (eq_attr "type" "alu_shift_reg,alus_shift_reg,\
+                        logic_shift_reg,logics_shift_reg,\
+                        mov_shift_reg,mvn_shift_reg"))
   "(ca15_issue2,ca15_sx1+ca15_sx2,ca15_sx1_shf,ca15_sx2_alu)\
    |(ca15_issue1,(ca15_issue1+ca15_sx2,ca15_sx1+ca15_sx2_shf)\
    |(ca15_issue1+ca15_sx1,ca15_sx1+ca15_sx1_shf),ca15_sx1_alu)")
@@ -87,35 +96,30 @@
 ;; 32-bit multiplies
 (define_insn_reservation "cortex_a15_mult32" 3
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "mult")
-	    (and (eq_attr "neon_type" "none")
-		 (eq_attr "mul64" "no"))))
+       (eq_attr "mul32" "yes"))
   "ca15_issue1,ca15_mx")
 
 ;; 64-bit multiplies
 (define_insn_reservation "cortex_a15_mult64" 4
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "mult")
-	    (and (eq_attr "neon_type" "none")
-		 (eq_attr "mul64" "yes"))))
+       (eq_attr "mul64" "yes"))
   "ca15_issue1,ca15_mx*2")
 
 ;; Integer divide
 (define_insn_reservation "cortex_a15_udiv" 9
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "insn" "udiv"))
+       (eq_attr "type" "udiv"))
   "ca15_issue1,ca15_mx")
 
 (define_insn_reservation "cortex_a15_sdiv" 10
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "insn" "sdiv"))
+       (eq_attr "type" "sdiv"))
   "ca15_issue1,ca15_mx")
 
 ;; Block all issue pipes for a cycle
 (define_insn_reservation "cortex_a15_block" 1
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "block")
-	    (eq_attr "neon_type" "none")))
+       (eq_attr "type" "block"))
   "ca15_issue3")
 
 ;; Branch execution Unit
@@ -124,8 +128,7 @@
 ;; No latency as there is no result
 (define_insn_reservation "cortex_a15_branch" 0
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "branch")
-	    (eq_attr "neon_type" "none")))
+       (eq_attr "type" "branch"))
   "ca15_issue1,ca15_bx")
 
 ;; Load-store execution Unit
@@ -133,40 +136,35 @@
 ;; Loads of up to two words.
 (define_insn_reservation "cortex_a15_load1" 4
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "load_byte,load1,load2")
-	    (eq_attr "neon_type" "none")))
+       (eq_attr "type" "load_byte,load1,load2"))
   "ca15_issue1,ca15_ls,ca15_ldr,nothing")
 
 ;; Loads of three or four words.
 (define_insn_reservation "cortex_a15_load3" 5
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "load3,load4")
-	    (eq_attr "neon_type" "none")))
+       (eq_attr "type" "load3,load4"))
   "ca15_issue2,ca15_ls1+ca15_ls2,ca15_ldr,ca15_ldr,nothing")
 
 ;; Stores of up to two words.
 (define_insn_reservation "cortex_a15_store1" 0
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "store1,store2")
-	    (eq_attr "neon_type" "none")))
+       (eq_attr "type" "store1,store2"))
   "ca15_issue1,ca15_ls,ca15_str")
 
 ;; Stores of three or four words.
 (define_insn_reservation "cortex_a15_store3" 0
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "store3,store4")
-	    (eq_attr "neon_type" "none")))
+       (eq_attr "type" "store3,store4"))
   "ca15_issue2,ca15_ls1+ca15_ls2,ca15_str,ca15_str")
 
 ;; We include Neon.md here to ensure that the branch can block the Neon units.
-(include "cortex-a15-neon.md")
+(include "../arm/cortex-a15-neon.md")
 
 ;; We lie with calls.  They take up all issue slots, and form a block in the
 ;; pipeline.  The result however is available the next cycle.
 (define_insn_reservation "cortex_a15_call" 1
   (and (eq_attr "tune" "cortexa15")
-       (and (eq_attr "type" "call")
-	    (eq_attr "neon_type" "none")))
+       (eq_attr "type" "call"))
   "ca15_issue3,\
    ca15_sx1+ca15_sx2+ca15_bx+ca15_mx+ca15_cx_ij+ca15_cx_ik+ca15_ls1+ca15_ls2+\
    ca15_cx_imac1+ca15_cx_ialu1+ca15_cx_ialu2+ca15_cx_ishf+\
Index: b/src/gcc/config/arm/arm-tables.opt
===================================================================
--- a/src/gcc/config/arm/arm-tables.opt
+++ b/src/gcc/config/arm/arm-tables.opt
@@ -250,6 +250,9 @@ EnumValue
 Enum(processor_type) String(cortex-a15) Value(cortexa15)
 
 EnumValue
+Enum(processor_type) String(cortex-a53) Value(cortexa53)
+
+EnumValue
 Enum(processor_type) String(cortex-r4) Value(cortexr4)
 
 EnumValue
@@ -259,6 +262,9 @@ EnumValue
 Enum(processor_type) String(cortex-r5) Value(cortexr5)
 
 EnumValue
+Enum(processor_type) String(cortex-r7) Value(cortexr7)
+
+EnumValue
 Enum(processor_type) String(cortex-m4) Value(cortexm4)
 
 EnumValue
@@ -353,10 +359,13 @@ EnumValue
 Enum(arm_arch) String(armv8-a) Value(23)
 
 EnumValue
-Enum(arm_arch) String(iwmmxt) Value(24)
+Enum(arm_arch) String(armv8-a+crc) Value(24)
+
+EnumValue
+Enum(arm_arch) String(iwmmxt) Value(25)
 
 EnumValue
-Enum(arm_arch) String(iwmmxt2) Value(25)
+Enum(arm_arch) String(iwmmxt2) Value(26)
 
 Enum
 Name(arm_fpu) Type(int)
Index: b/src/gcc/config/arm/arm1026ejs.md
===================================================================
--- a/src/gcc/config/arm/arm1026ejs.md
+++ b/src/gcc/config/arm/arm1026ejs.md
@@ -66,13 +66,21 @@
 ;; ALU operations with no shifted operand
 (define_insn_reservation "alu_op" 1 
  (and (eq_attr "tune" "arm1026ejs")
-      (eq_attr "type" "alu_reg,simple_alu_imm"))
+      (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                       adr,bfm,rev,\
+                       shift_imm,shift_reg,\
+                       mov_imm,mov_reg,mvn_imm,mvn_reg,\
+                       multiple,no_insn"))
  "a_e,a_m,a_w")
 
 ;; ALU operations with a shift-by-constant operand
 (define_insn_reservation "alu_shift_op" 1 
  (and (eq_attr "tune" "arm1026ejs")
-      (eq_attr "type" "simple_alu_shift,alu_shift"))
+      (eq_attr "type" "alu_shift_imm,alus_shift_imm,\
+                       logic_shift_imm,logics_shift_imm,\
+                       extend,mov_shift,mvn_shift"))
  "a_e,a_m,a_w")
 
 ;; ALU operations with a shift-by-register operand
@@ -81,7 +89,9 @@
 ;; the execute stage.
 (define_insn_reservation "alu_shift_reg_op" 2 
  (and (eq_attr "tune" "arm1026ejs")
-      (eq_attr "type" "alu_shift_reg"))
+      (eq_attr "type" "alu_shift_reg,alus_shift_reg,\
+                       logic_shift_reg,logics_shift_reg,\
+                       mov_shift_reg,mvn_shift_reg"))
  "a_e*2,a_m,a_w")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -96,7 +106,7 @@
 ;; until after the memory stage.
 (define_insn_reservation "mult1" 2
  (and (eq_attr "tune" "arm1026ejs")
-      (eq_attr "insn" "smulxy,smulwy"))
+      (eq_attr "type" "smulxy,smulwy"))
  "a_e,a_m,a_w")
 
 ;; The "smlaxy" and "smlawx" instructions require two iterations through
@@ -104,7 +114,7 @@
 ;; the execute stage.
 (define_insn_reservation "mult2" 2
  (and (eq_attr "tune" "arm1026ejs")
-      (eq_attr "insn" "smlaxy,smlalxy,smlawx"))
+      (eq_attr "type" "smlaxy,smlalxy,smlawx"))
  "a_e*2,a_m,a_w")
 
 ;; The "smlalxy", "mul", and "mla" instructions require two iterations
@@ -112,7 +122,7 @@
 ;; the memory stage.
 (define_insn_reservation "mult3" 3
  (and (eq_attr "tune" "arm1026ejs")
-      (eq_attr "insn" "smlalxy,mul,mla"))
+      (eq_attr "type" "smlalxy,mul,mla"))
  "a_e*2,a_m,a_w")
 
 ;; The "muls" and "mlas" instructions loop in the execute stage for
@@ -120,7 +130,7 @@
 ;; available after three iterations.
 (define_insn_reservation "mult4" 3
  (and (eq_attr "tune" "arm1026ejs")
-      (eq_attr "insn" "muls,mlas"))
+      (eq_attr "type" "muls,mlas"))
  "a_e*4,a_m,a_w")
 
 ;; Long multiply instructions that produce two registers of
@@ -135,7 +145,7 @@
 ;; available after the memory cycle.
 (define_insn_reservation "mult5" 4
  (and (eq_attr "tune" "arm1026ejs")
-      (eq_attr "insn" "umull,umlal,smull,smlal"))
+      (eq_attr "type" "umull,umlal,smull,smlal"))
  "a_e*3,a_m,a_w")
 
 ;; The "umulls", "umlals", "smulls", and "smlals" instructions loop in
@@ -143,7 +153,7 @@
 ;; The value result is available after four iterations.
 (define_insn_reservation "mult6" 4
  (and (eq_attr "tune" "arm1026ejs")
-      (eq_attr "insn" "umulls,umlals,smulls,smlals"))
+      (eq_attr "type" "umulls,umlals,smulls,smlals"))
  "a_e*5,a_m,a_w")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Index: b/src/gcc/config/arm/linux-elf.h
===================================================================
--- a/src/gcc/config/arm/linux-elf.h
+++ b/src/gcc/config/arm/linux-elf.h
@@ -44,9 +44,9 @@
 
 #define SUBTARGET_EXTRA_LINK_SPEC " -m " TARGET_LINKER_EMULATION " -p"
 
+/* We do not have any MULTILIB_OPTIONS specified, so there are no
+   MULTILIB_DEFAULTS.  */
 #undef  MULTILIB_DEFAULTS
-#define MULTILIB_DEFAULTS \
-	{ "marm", "mlittle-endian", "mfloat-abi=hard", "mno-thumb-interwork" }
 
 /* Now we define the strings used to build the spec file.  */
 #undef  LIB_SPEC
Index: b/src/gcc/config/arm/arm1136jfs.md
===================================================================
--- a/src/gcc/config/arm/arm1136jfs.md
+++ b/src/gcc/config/arm/arm1136jfs.md
@@ -75,13 +75,21 @@
 ;; ALU operations with no shifted operand
 (define_insn_reservation "11_alu_op" 2
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "type" "alu_reg,simple_alu_imm"))
+      (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                       adr,bfm,rev,\
+                       shift_imm,shift_reg,\
+                       mov_imm,mov_reg,mvn_imm,mvn_reg,\
+                       multiple,no_insn"))
  "e_1,e_2,e_3,e_wb")
 
 ;; ALU operations with a shift-by-constant operand
 (define_insn_reservation "11_alu_shift_op" 2
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "type" "simple_alu_shift,alu_shift"))
+      (eq_attr "type" "alu_shift_imm,alus_shift_imm,\
+                       logic_shift_imm,logics_shift_imm,\
+                       extend,mov_shift,mvn_shift"))
  "e_1,e_2,e_3,e_wb")
 
 ;; ALU operations with a shift-by-register operand
@@ -90,7 +98,9 @@
 ;; the shift stage.
 (define_insn_reservation "11_alu_shift_reg_op" 3
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "type" "alu_shift_reg"))
+      (eq_attr "type" "alu_shift_reg,alus_shift_reg,\
+                       logic_shift_reg,logics_shift_reg,\
+                       mov_shift_reg,mvn_shift_reg"))
  "e_1*2,e_2,e_3,e_wb")
 
 ;; alu_ops can start sooner, if there is no shifter dependency
@@ -129,13 +139,13 @@
 ;; Multiply and multiply-accumulate results are available after four stages.
 (define_insn_reservation "11_mult1" 4
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "insn" "mul,mla"))
+      (eq_attr "type" "mul,mla"))
  "e_1*2,e_2,e_3,e_wb")
 
 ;; The *S variants set the condition flags, which requires three more cycles.
 (define_insn_reservation "11_mult2" 4
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "insn" "muls,mlas"))
+      (eq_attr "type" "muls,mlas"))
  "e_1*2,e_2,e_3,e_wb")
 
 (define_bypass 3 "11_mult1,11_mult2"
@@ -160,13 +170,13 @@
 ;; the two multiply-accumulate instructions.
 (define_insn_reservation "11_mult3" 5
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "insn" "smull,umull,smlal,umlal"))
+      (eq_attr "type" "smull,umull,smlal,umlal"))
  "e_1*3,e_2,e_3,e_wb*2")
 
 ;; The *S variants set the condition flags, which requires three more cycles.
 (define_insn_reservation "11_mult4" 5
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "insn" "smulls,umulls,smlals,umlals"))
+      (eq_attr "type" "smulls,umulls,smlals,umlals"))
  "e_1*3,e_2,e_3,e_wb*2")
 
 (define_bypass 4 "11_mult3,11_mult4"
@@ -190,7 +200,8 @@
 ;; cycles.
 (define_insn_reservation "11_mult5" 3
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "insn" "smulxy,smlaxy,smulwy,smlawy,smuad,smuadx,smlad,smladx,smusd,smusdx,smlsd,smlsdx"))
+      (eq_attr "type" "smulxy,smlaxy,smulwy,smlawy,smuad,smuadx,smlad,smladx,\
+                       smusd,smusdx,smlsd,smlsdx"))
  "e_1,e_2,e_3,e_wb")
 
 (define_bypass 2 "11_mult5"
@@ -211,14 +222,14 @@
 ;; The same idea, then the 32-bit result is added to a 64-bit quantity.
 (define_insn_reservation "11_mult6" 4
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "insn" "smlalxy"))
+      (eq_attr "type" "smlalxy"))
  "e_1*2,e_2,e_3,e_wb*2")
 
 ;; Signed 32x32 multiply, then the most significant 32 bits are extracted
 ;; and are available after the memory stage.
 (define_insn_reservation "11_mult7" 4
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
-      (eq_attr "insn" "smmul,smmulr"))
+      (eq_attr "type" "smmul,smmulr"))
  "e_1*2,e_2,e_3,e_wb")
 
 (define_bypass 3 "11_mult6,11_mult7"
Index: b/src/gcc/config/arm/marvell-pj4.md
===================================================================
--- a/src/gcc/config/arm/marvell-pj4.md
+++ b/src/gcc/config/arm/marvell-pj4.md
@@ -41,64 +41,90 @@
 
 (define_insn_reservation "pj4_alu_e1" 1
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "simple_alu_imm,alu_reg")
-       (not (eq_attr "conds" "set"))
-       (eq_attr "insn" "mov,mvn"))
+       (eq_attr "type" "mov_imm,mov_reg,mvn_imm,mvn_reg")
+       (not (eq_attr "conds" "set")))
                                "pj4_is,(pj4_alu1,pj4_w1+pj4_cp)|(pj4_alu2,pj4_w2+pj4_cp)")
 
 (define_insn_reservation "pj4_alu_e1_conds" 4
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "simple_alu_imm,alu_reg")
-       (eq_attr "conds" "set")
-       (eq_attr "insn" "mov,mvn"))
+       (eq_attr "type" "mov_imm,mov_reg,mvn_imm,mvn_reg")
+       (eq_attr "conds" "set"))
                                "pj4_is,(pj4_alu1,pj4_w1+pj4_cp)|(pj4_alu2,pj4_w2+pj4_cp)")
 
 (define_insn_reservation "pj4_alu" 1
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "simple_alu_imm,alu_reg")
-       (not (eq_attr "conds" "set"))
-       (not (eq_attr "insn" "mov,mvn")))
+       (eq_attr "type" "alu_imm,alus_imm,alu_reg,alus_reg,\
+                        logic_imm,logics_imm,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        adr,bfm,rev,\
+                        shift_imm,shift_reg")
+       (not (eq_attr "conds" "set")))
                                "pj4_is,(pj4_alu1,pj4_w1+pj4_cp)|(pj4_alu2,pj4_w2+pj4_cp)")
 
 (define_insn_reservation "pj4_alu_conds" 4
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "simple_alu_imm,alu_reg")
-       (eq_attr "conds" "set")
-       (not (eq_attr "insn" "mov,mvn")))
+       (eq_attr "type" "alu_imm,alus_imm,alu_reg,alus_reg,\
+                        logic_imm,logics_imm,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        adr,bfm,rev,\
+                        shift_imm,shift_reg")
+       (eq_attr "conds" "set"))
                                "pj4_is,(pj4_alu1,pj4_w1+pj4_cp)|(pj4_alu2,pj4_w2+pj4_cp)")
 
 (define_insn_reservation "pj4_shift" 1
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "alu_shift,alu_shift_reg,simple_alu_shift")
+       (eq_attr "type" "alu_shift_imm,logic_shift_imm,\
+                        alus_shift_imm,logics_shift_imm,\
+                        alu_shift_reg,logic_shift_reg,\
+                        alus_shift_reg,logics_shift_reg,\
+                        extend,\
+                        mov_shift,mvn_shift,mov_shift_reg,mvn_shift_reg")
        (not (eq_attr "conds" "set"))
        (eq_attr "shift" "1"))  "pj4_is,(pj4_alu1,pj4_w1+pj4_cp)|(pj4_alu2,pj4_w2+pj4_cp)")
 
 (define_insn_reservation "pj4_shift_conds" 4
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "alu_shift,alu_shift_reg,simple_alu_shift")
+       (eq_attr "type" "alu_shift_imm,logic_shift_imm,\
+                        alus_shift_imm,logics_shift_imm,\
+                        alu_shift_reg,logic_shift_reg,\
+                        alus_shift_reg,logics_shift_reg,\
+                        extend,\
+                        mov_shift,mvn_shift,mov_shift_reg,mvn_shift_reg")
        (eq_attr "conds" "set")
        (eq_attr "shift" "1"))  "pj4_is,(pj4_alu1,pj4_w1+pj4_cp)|(pj4_alu2,pj4_w2+pj4_cp)")
 
 (define_insn_reservation "pj4_alu_shift" 1
   (and (eq_attr "tune" "marvell_pj4")
        (not (eq_attr "conds" "set"))
-       (eq_attr "type" "alu_shift,alu_shift_reg,simple_alu_shift"))
+       (eq_attr "type" "alu_shift_imm,logic_shift_imm,\
+                        alus_shift_imm,logics_shift_imm,\
+                        alu_shift_reg,logic_shift_reg,\
+                        alus_shift_reg,logics_shift_reg,\
+                        extend,\
+                        mov_shift,mvn_shift,mov_shift_reg,mvn_shift_reg"))
                                "pj4_is,(pj4_alu1,nothing,pj4_w1+pj4_cp)|(pj4_alu2,nothing,pj4_w2+pj4_cp)")
 
 (define_insn_reservation "pj4_alu_shift_conds" 4
   (and (eq_attr "tune" "marvell_pj4")
        (eq_attr "conds" "set")
-       (eq_attr "type" "alu_shift,alu_shift_reg,simple_alu_shift"))
+       (eq_attr "type" "alu_shift_imm,logic_shift_imm,alus_shift_imm,logics_shift_imm,\
+                        alu_shift_reg,logic_shift_reg,alus_shift_reg,logics_shift_reg,\
+                        extend,\
+                        mov_shift,mvn_shift,mov_shift_reg,mvn_shift_reg"))
                                "pj4_is,(pj4_alu1,nothing,pj4_w1+pj4_cp)|(pj4_alu2,nothing,pj4_w2+pj4_cp)")
 
 (define_bypass 2 "pj4_alu_shift,pj4_shift"
                  "pj4_ir_mul,pj4_ir_div,pj4_core_to_vfp")
 
 (define_insn_reservation "pj4_ir_mul" 3
-  (and (eq_attr "tune" "marvell_pj4") (eq_attr "type" "mult")) "pj4_is,pj4_mul,nothing*2,pj4_cp")
+  (and (eq_attr "tune" "marvell_pj4")
+       (ior (eq_attr "mul32" "yes")
+            (eq_attr "mul64" "yes")))
+                     "pj4_is,pj4_mul,nothing*2,pj4_cp")
 
 (define_insn_reservation "pj4_ir_div" 20
-  (and (eq_attr "tune" "marvell_pj4") (eq_attr "insn" "udiv,sdiv")) "pj4_is,pj4_div*19,pj4_cp")
+  (and (eq_attr "tune" "marvell_pj4") 
+       (eq_attr "type" "udiv,sdiv")) "pj4_is,pj4_div*19,pj4_cp")
 
 ;; Branches and calls.
 
@@ -167,11 +193,11 @@
 
 (define_insn_reservation "pj4_vfp_divs" 20
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "fdivs"))       "pj4_is,nothing*2,vissue,vdiv*18,nothing")
+       (eq_attr "type" "fdivs, fsqrts"))       "pj4_is,nothing*2,vissue,vdiv*18,nothing")
 
 (define_insn_reservation "pj4_vfp_divd" 34
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "fdivd"))       "pj4_is,nothing*2,vissue,vdiv*32,nothing")
+       (eq_attr "type" "fdivd, fsqrtd"))       "pj4_is,nothing*2,vissue,vdiv*32,nothing")
 
 (define_insn_reservation "pj4_vfp_mac"  9
   (and (eq_attr "tune" "marvell_pj4")
@@ -182,8 +208,9 @@
 
 (define_insn_reservation "pj4_vfp_cpy"  4
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "fcpys,ffariths,ffarithd,fconsts,fconstd,\
-                        fcmps,fcmpd,f_cvt"))  "pj4_is,nothing*2,vissue,vfast,nothing*2")
+       (eq_attr "type" "fmov,ffariths,ffarithd,fconsts,fconstd,\
+                        fcmps,fcmpd,f_cvt,f_cvtf2i,f_cvti2f"))
+"pj4_is,nothing*2,vissue,vfast,nothing*2")
 
 ;; Enlarge latency, and wish that more nondependent insns are
 ;; scheduled immediately after VFP load.
@@ -197,9 +224,9 @@
 
 (define_insn_reservation "pj4_vfp_to_core" 7
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "f_2_r,f_flag"))       "pj4_isb,nothing,nothing,vissue,vfast,nothing*2")
+       (eq_attr "type" "f_mrc,f_mrrc,f_flag")) "pj4_isb,nothing,nothing,vissue,vfast,nothing*2")
 
 (define_insn_reservation "pj4_core_to_vfp" 2
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "r_2_f"))              "pj4_isb,pj4_alu1,pj4_w1,vissue,pj4_cp")
+       (eq_attr "type" "f_mcr,f_mcrr")) "pj4_isb,pj4_alu1,pj4_w1,vissue,pj4_cp")
 
Index: b/src/gcc/config/arm/thumb2.md
===================================================================
--- a/src/gcc/config/arm/thumb2.md
+++ b/src/gcc/config/arm/thumb2.md
@@ -60,105 +60,237 @@
   "TARGET_THUMB2"
   "bic%?\\t%0, %1, %2%S4"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "shift" "2")
-   (set_attr "type" "alu_shift")]
+   (set_attr "type" "alu_shift_imm")]
 )
 
-(define_insn "*thumb2_smaxsi3"
-  [(set (match_operand:SI          0 "s_register_operand" "=r,r,r")
-	(smax:SI (match_operand:SI 1 "s_register_operand"  "0,r,?r")
-		 (match_operand:SI 2 "arm_rhs_operand"    "rI,0,rI")))
-   (clobber (reg:CC CC_REGNUM))]
-  "TARGET_THUMB2"
-  "@
-   cmp\\t%1, %2\;it\\tlt\;movlt\\t%0, %2
-   cmp\\t%1, %2\;it\\tge\;movge\\t%0, %1
-   cmp\\t%1, %2\;ite\\tge\;movge\\t%0, %1\;movlt\\t%0, %2"
+;; We use the '0' constraint for operand 1 because reload should
+;; be smart enough to generate an appropriate move for the r/r/r case.
+(define_insn_and_split "*thumb2_smaxsi3"
+  [(set (match_operand:SI          0 "s_register_operand" "=r,l,r")
+	(smax:SI (match_operand:SI 1 "s_register_operand" "%0,0,0")
+		 (match_operand:SI 2 "arm_rhs_operand"    "r,Py,I")))
+   (clobber (reg:CC CC_REGNUM))]
+   "TARGET_THUMB2"
+   "#"
+   ; cmp\\t%1, %2\;it\\tlt\;movlt\\t%0, %2
+  "TARGET_THUMB2 && reload_completed"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 1) (match_dup 2)))
+   (cond_exec (lt:SI (reg:CC CC_REGNUM) (const_int 0))
+              (set (match_dup 0)
+                   (match_dup 2)))]
+  ""
   [(set_attr "conds" "clob")
-   (set_attr "length" "10,10,14")]
+   (set_attr "enabled_for_depr_it" "yes,yes,no")
+   (set_attr "length" "6,6,10")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb2_sminsi3"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r,r")
-	(smin:SI (match_operand:SI 1 "s_register_operand" "0,r,?r")
-		 (match_operand:SI 2 "arm_rhs_operand" "rI,0,rI")))
+(define_insn_and_split "*thumb2_sminsi3"
+  [(set (match_operand:SI 0 "s_register_operand" "=r,l,r")
+	(smin:SI (match_operand:SI 1 "s_register_operand" "%0,0,0")
+		 (match_operand:SI 2 "arm_rhs_operand" "r,Py,I")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB2"
-  "@
-   cmp\\t%1, %2\;it\\tge\;movge\\t%0, %2
-   cmp\\t%1, %2\;it\\tlt\;movlt\\t%0, %1
-   cmp\\t%1, %2\;ite\\tlt\;movlt\\t%0, %1\;movge\\t%0, %2"
+  "#"
+  ; cmp\\t%1, %2\;it\\tge\;movge\\t%0, %2
+  "TARGET_THUMB2 && reload_completed"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 1) (match_dup 2)))
+   (cond_exec (ge:SI (reg:CC CC_REGNUM) (const_int 0))
+              (set (match_dup 0)
+                   (match_dup 2)))]
+  ""
   [(set_attr "conds" "clob")
-   (set_attr "length" "10,10,14")]
+   (set_attr "enabled_for_depr_it" "yes,yes,no")
+   (set_attr "length" "6,6,10")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb32_umaxsi3"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r,r")
-	(umax:SI (match_operand:SI 1 "s_register_operand" "0,r,?r")
-		 (match_operand:SI 2 "arm_rhs_operand" "rI,0,rI")))
-   (clobber (reg:CC CC_REGNUM))]
+(define_insn_and_split "*thumb32_umaxsi3"
+  [(set (match_operand:SI 0 "s_register_operand" "=r,l,r")
+	(umax:SI (match_operand:SI 1 "s_register_operand" "%0,0,0")
+		 (match_operand:SI 2 "arm_rhs_operand" "r,Py,I")))
+  (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB2"
-  "@
-   cmp\\t%1, %2\;it\\tcc\;movcc\\t%0, %2
-   cmp\\t%1, %2\;it\\tcs\;movcs\\t%0, %1
-   cmp\\t%1, %2\;ite\\tcs\;movcs\\t%0, %1\;movcc\\t%0, %2"
+  "#"
+  ; cmp\\t%1, %2\;it\\tcc\;movcc\\t%0, %2
+  "TARGET_THUMB2 && reload_completed"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 1) (match_dup 2)))
+   (cond_exec (ltu:SI (reg:CC CC_REGNUM) (const_int 0))
+              (set (match_dup 0)
+                   (match_dup 2)))]
+  ""
   [(set_attr "conds" "clob")
-   (set_attr "length" "10,10,14")]
+   (set_attr "length" "6,6,10")
+   (set_attr "enabled_for_depr_it" "yes,yes,no")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb2_uminsi3"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r,r")
-	(umin:SI (match_operand:SI 1 "s_register_operand" "0,r,?r")
-		 (match_operand:SI 2 "arm_rhs_operand" "rI,0,rI")))
+(define_insn_and_split "*thumb2_uminsi3"
+  [(set (match_operand:SI 0 "s_register_operand" "=r,l,r")
+	(umin:SI (match_operand:SI 1 "s_register_operand" "%0,0,0")
+		 (match_operand:SI 2 "arm_rhs_operand" "r,Py,I")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB2"
-  "@
-   cmp\\t%1, %2\;it\\tcs\;movcs\\t%0, %2
-   cmp\\t%1, %2\;it\\tcc\;movcc\\t%0, %1
-   cmp\\t%1, %2\;ite\\tcc\;movcc\\t%0, %1\;movcs\\t%0, %2"
+  "#"
+  ; cmp\\t%1, %2\;it\\tcs\;movcs\\t%0, %2
+  "TARGET_THUMB2 && reload_completed"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 1) (match_dup 2)))
+   (cond_exec (geu:SI (reg:CC CC_REGNUM) (const_int 0))
+              (set (match_dup 0)
+                   (match_dup 2)))]
+  ""
   [(set_attr "conds" "clob")
-   (set_attr "length" "10,10,14")]
+   (set_attr "length" "6,6,10")
+   (set_attr "enabled_for_depr_it" "yes,yes,no")
+   (set_attr "type" "multiple")]
 )
 
 ;; Thumb-2 does not have rsc, so use a clever trick with shifter operands.
-(define_insn "*thumb2_negdi2"
+(define_insn_and_split "*thumb2_negdi2"
   [(set (match_operand:DI         0 "s_register_operand" "=&r,r")
 	(neg:DI (match_operand:DI 1 "s_register_operand"  "?r,0")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB2"
-  "negs\\t%Q0, %Q1\;sbc\\t%R0, %R1, %R1, lsl #1"
+  "#" ; negs\\t%Q0, %Q1\;sbc\\t%R0, %R1, %R1, lsl #1
+  "&& reload_completed"
+  [(parallel [(set (reg:CC CC_REGNUM)
+		   (compare:CC (const_int 0) (match_dup 1)))
+	      (set (match_dup 0) (minus:SI (const_int 0) (match_dup 1)))])
+   (set (match_dup 2) (minus:SI (minus:SI (match_dup 3)
+                                          (ashift:SI (match_dup 3)
+                                                     (const_int 1)))
+                                (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  {
+    operands[2] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    operands[3] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+  }
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb2_abssi2"
-  [(set (match_operand:SI         0 "s_register_operand" "=r,&r")
-	(abs:SI (match_operand:SI 1 "s_register_operand" "0,r")))
+(define_insn_and_split "*thumb2_abssi2"
+  [(set (match_operand:SI         0 "s_register_operand" "=&r,l,r")
+	(abs:SI (match_operand:SI 1 "s_register_operand" "r,0,0")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB2"
-  "@
-   cmp\\t%0, #0\;it\tlt\;rsblt\\t%0, %0, #0
-   eor%?\\t%0, %1, %1, asr #31\;sub%?\\t%0, %0, %1, asr #31"
-  [(set_attr "conds" "clob,*")
+  "#"
+   ; eor%?\\t%0, %1, %1, asr #31\;sub%?\\t%0, %0, %1, asr #31
+   ; cmp\\t%0, #0\;it\tlt\;rsblt\\t%0, %0, #0
+   ; cmp\\t%0, #0\;it\tlt\;rsblt\\t%0, %0, #0
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    if (REGNO(operands[0]) == REGNO(operands[1]))
+      {
+       rtx cc_reg = gen_rtx_REG (CCmode, CC_REGNUM);
+
+       emit_insn (gen_rtx_SET (VOIDmode,
+                               cc_reg,
+                               gen_rtx_COMPARE (CCmode, operands[0], const0_rtx)));
+       emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                    (gen_rtx_LT (SImode,
+                                                 cc_reg,
+                                                 const0_rtx)),
+                                    (gen_rtx_SET (VOIDmode,
+                                                  operands[0],
+                                                  (gen_rtx_MINUS (SImode,
+                                                                  const0_rtx,
+                                                                  operands[1]))))));
+      }
+    else
+      {
+        emit_insn (gen_rtx_SET (VOIDmode,
+                                operands[0],
+                                gen_rtx_XOR (SImode,
+                                             gen_rtx_ASHIFTRT (SImode,
+                                                               operands[1],
+                                                               GEN_INT (31)),
+                                             operands[1])));
+        emit_insn (gen_rtx_SET (VOIDmode,
+                                operands[0],
+                                gen_rtx_MINUS (SImode,
+                                               operands[0],
+                                               gen_rtx_ASHIFTRT (SImode,
+                                                                 operands[1],
+                                                                 GEN_INT (31)))));
+      }
+    DONE;
+  }
+  [(set_attr "conds" "*,clob,clob")
    (set_attr "shift" "1")
-   (set_attr "predicable" "no, yes")
+   (set_attr "predicable" "yes,no,no")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "enabled_for_depr_it" "yes,yes,no")
    (set_attr "ce_count" "2")
-   (set_attr "length" "10,8")]
+   (set_attr "length" "8,6,10")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb2_neg_abssi2"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,&r")
-	(neg:SI (abs:SI (match_operand:SI 1 "s_register_operand" "0,r"))))
+(define_insn_and_split "*thumb2_neg_abssi2"
+  [(set (match_operand:SI 0 "s_register_operand" "=&r,l,r")
+	(neg:SI (abs:SI (match_operand:SI 1 "s_register_operand" "r,0,0"))))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB2"
-  "@
-   cmp\\t%0, #0\;it\\tgt\;rsbgt\\t%0, %0, #0
-   eor%?\\t%0, %1, %1, asr #31\;rsb%?\\t%0, %0, %1, asr #31"
-  [(set_attr "conds" "clob,*")
+  "#"
+   ; eor%?\\t%0, %1, %1, asr #31\;rsb%?\\t%0, %0, %1, asr #31
+   ; cmp\\t%0, #0\;it\\tgt\;rsbgt\\t%0, %0, #0
+   ; cmp\\t%0, #0\;it\\tgt\;rsbgt\\t%0, %0, #0
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    if (REGNO(operands[0]) == REGNO(operands[1]))
+      {
+       rtx cc_reg = gen_rtx_REG (CCmode, CC_REGNUM);
+
+       emit_insn (gen_rtx_SET (VOIDmode,
+                               cc_reg,
+                               gen_rtx_COMPARE (CCmode, operands[0], const0_rtx)));
+       emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                    (gen_rtx_GT (SImode,
+                                                 cc_reg,
+                                                 const0_rtx)),
+                                    (gen_rtx_SET (VOIDmode,
+                                                  operands[0],
+                                                  (gen_rtx_MINUS (SImode,
+                                                                  const0_rtx,
+                                                                  operands[1]))))));
+      }
+    else
+      {
+        emit_insn (gen_rtx_SET (VOIDmode,
+                                operands[0],
+                                gen_rtx_XOR (SImode,
+                                             gen_rtx_ASHIFTRT (SImode,
+                                                               operands[1],
+                                                               GEN_INT (31)),
+                                             operands[1])));
+        emit_insn (gen_rtx_SET (VOIDmode,
+                                operands[0],
+                                gen_rtx_MINUS (SImode,
+                                               gen_rtx_ASHIFTRT (SImode,
+                                                                 operands[1],
+                                                                 GEN_INT (31)),
+                                               operands[0])));
+      }
+    DONE;
+  }
+  [(set_attr "conds" "*,clob,clob")
    (set_attr "shift" "1")
-   (set_attr "predicable" "no, yes")
+   (set_attr "predicable" "yes,no,no")
+   (set_attr "enabled_for_depr_it" "yes,yes,no")
+   (set_attr "predicable_short_it" "no")
    (set_attr "ce_count" "2")
-   (set_attr "length" "10,8")]
+   (set_attr "length" "8,6,10")
+   (set_attr "type" "multiple")]
 )
 
 ;; We have two alternatives here for memory loads (and similarly for stores)
@@ -167,8 +299,8 @@
 ;; regs.  The high register alternatives are not taken into account when
 ;; choosing register preferences in order to reflect their expense.
 (define_insn "*thumb2_movsi_insn"
-  [(set (match_operand:SI 0 "nonimmediate_operand" "=rk,r,r,r,l ,*hk,m,*m")
-	(match_operand:SI 1 "general_operand"	   "rk ,I,K,j,mi,*mi,l,*hk"))]
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=rk,r,l,r,r,l ,*hk,m,*m")
+	(match_operand:SI 1 "general_operand"	   "rk,I,Py,K,j,mi,*mi,l,*hk"))]
   "TARGET_THUMB2 && ! TARGET_IWMMXT
    && !(TARGET_HARD_FLOAT && TARGET_VFP)
    && (   register_operand (operands[0], SImode)
@@ -176,16 +308,19 @@
   "@
    mov%?\\t%0, %1
    mov%?\\t%0, %1
+   mov%?\\t%0, %1
    mvn%?\\t%0, #%B1
    movw%?\\t%0, %1
    ldr%?\\t%0, %1
    ldr%?\\t%0, %1
    str%?\\t%1, %0
    str%?\\t%1, %0"
-  [(set_attr "type" "*,*,simple_alu_imm,*,load1,load1,store1,store1")
+  [(set_attr "type" "mov_reg,alu_imm,alu_imm,alu_imm,mov_imm,load1,load1,store1,store1")
+   (set_attr "length" "2,4,2,4,4,4,4,4,4")
    (set_attr "predicable" "yes")
-   (set_attr "pool_range" "*,*,*,*,1018,4094,*,*")
-   (set_attr "neg_pool_range" "*,*,*,*,0,0,*,*")]
+   (set_attr "predicable_short_it" "yes,no,yes,no,no,no,no,no,no")
+   (set_attr "pool_range" "*,*,*,*,*,1018,4094,*,*")
+   (set_attr "neg_pool_range" "*,*,*,*,*,0,0,*,*")]
 )
 
 (define_insn "tls_load_dot_plus_four"
@@ -201,7 +336,8 @@
 			     INTVAL (operands[3]));
   return \"add\\t%2, %|pc\;ldr%?\\t%0, [%2]\";
   "
-  [(set_attr "length" "4,4,6,6")]
+  [(set_attr "length" "4,4,6,6")
+   (set_attr "type" "multiple")]
 )
 
 ;; Thumb-2 always has load/store halfword instructions, so we can avoid a lot
@@ -217,12 +353,27 @@
    movw%?\\t%0, %L1\\t%@ movhi
    str%(h%)\\t%1, %0\\t%@ movhi
    ldr%(h%)\\t%0, %1\\t%@ movhi"
-  [(set_attr "type" "*,*,store1,load1")
+  [(set_attr "type" "mov_imm,mov_reg,store1,load1")
    (set_attr "predicable" "yes")
    (set_attr "pool_range" "*,*,*,4094")
    (set_attr "neg_pool_range" "*,*,*,250")]
 )
 
+(define_insn "*thumb2_storewb_pairsi"
+  [(set (match_operand:SI 0 "register_operand" "=&kr")
+	(plus:SI (match_operand:SI 1 "register_operand" "0")
+		 (match_operand:SI 2 "const_int_operand" "n")))
+   (set (mem:SI (plus:SI (match_dup 0) (match_dup 2)))
+	(match_operand:SI 3 "register_operand" "r"))
+   (set (mem:SI (plus:SI (match_dup 0)
+			 (match_operand:SI 5 "const_int_operand" "n")))
+	(match_operand:SI 4 "register_operand" "r"))]
+  "TARGET_THUMB2
+   && INTVAL (operands[5]) == INTVAL (operands[2]) + 4"
+  "strd\\t%3, %4, [%0, %2]!"
+  [(set_attr "type" "store2")]
+)
+
 (define_insn "*thumb2_cmpsi_neg_shiftsi"
   [(set (reg:CC CC_REGNUM)
 	(compare:CC (match_operand:SI 0 "s_register_operand" "r")
@@ -233,58 +384,177 @@
   "cmn%?\\t%0, %1%S3"
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
-   (set_attr "type" "alu_shift")]
+   (set_attr "type" "alus_shift_imm")]
 )
 
-(define_insn "*thumb2_mov_scc"
-  [(set (match_operand:SI 0 "s_register_operand" "=r")
+(define_insn_and_split "*thumb2_mov_scc"
+  [(set (match_operand:SI 0 "s_register_operand" "=l,r")
 	(match_operator:SI 1 "arm_comparison_operator"
 	 [(match_operand 2 "cc_register" "") (const_int 0)]))]
   "TARGET_THUMB2"
-  "ite\\t%D1\;mov%D1\\t%0, #0\;mov%d1\\t%0, #1"
+  "#"   ; "ite\\t%D1\;mov%D1\\t%0, #0\;mov%d1\\t%0, #1"
+  "TARGET_THUMB2"
+  [(set (match_dup 0)
+        (if_then_else:SI (match_dup 1)
+                         (const_int 1)
+                         (const_int 0)))]
+  ""
   [(set_attr "conds" "use")
-   (set_attr "length" "10")]
+   (set_attr "enabled_for_depr_it" "yes,no")
+   (set_attr "length" "8,10")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb2_mov_negscc"
+(define_insn_and_split "*thumb2_mov_negscc"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(neg:SI (match_operator:SI 1 "arm_comparison_operator"
 		 [(match_operand 2 "cc_register" "") (const_int 0)])))]
+  "TARGET_THUMB2 && !arm_restrict_it"
+  "#"   ; "ite\\t%D1\;mov%D1\\t%0, #0\;mvn%d1\\t%0, #0"
   "TARGET_THUMB2"
-  "ite\\t%D1\;mov%D1\\t%0, #0\;mvn%d1\\t%0, #0"
+  [(set (match_dup 0)
+        (if_then_else:SI (match_dup 1)
+                         (match_dup 3)
+                         (const_int 0)))]
+  {
+    operands[3] = GEN_INT (~0);
+  }
   [(set_attr "conds" "use")
-   (set_attr "length" "10")]
+   (set_attr "length" "10")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb2_mov_notscc"
+(define_insn_and_split "*thumb2_mov_negscc_strict_it"
+  [(set (match_operand:SI 0 "low_register_operand" "=l")
+	(neg:SI (match_operator:SI 1 "arm_comparison_operator"
+		 [(match_operand 2 "cc_register" "") (const_int 0)])))]
+  "TARGET_THUMB2 && arm_restrict_it"
+  "#"   ; ";mvn\\t%0, #0 ;it\\t%D1\;mov%D1\\t%0, #0\"
+  "&& reload_completed"
+  [(set (match_dup 0)
+        (match_dup 3))
+   (cond_exec (match_dup 4)
+              (set (match_dup 0)
+                   (const_int 0)))]
+  {
+    operands[3] = GEN_INT (~0);
+    enum machine_mode mode = GET_MODE (operands[2]);
+    enum rtx_code rc = GET_CODE (operands[1]);
+
+    if (mode == CCFPmode || mode == CCFPEmode)
+      rc = reverse_condition_maybe_unordered (rc);
+    else
+      rc = reverse_condition (rc);
+    operands[4] = gen_rtx_fmt_ee (rc, VOIDmode, operands[2], const0_rtx);
+
+  }
+  [(set_attr "conds" "use")
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn_and_split "*thumb2_mov_notscc"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(not:SI (match_operator:SI 1 "arm_comparison_operator"
 		 [(match_operand 2 "cc_register" "") (const_int 0)])))]
+  "TARGET_THUMB2 && !arm_restrict_it"
+  "#"   ; "ite\\t%D1\;mvn%D1\\t%0, #0\;mvn%d1\\t%0, #1"
   "TARGET_THUMB2"
-  "ite\\t%D1\;mvn%D1\\t%0, #0\;mvn%d1\\t%0, #1"
+  [(set (match_dup 0)
+        (if_then_else:SI (match_dup 1)
+                         (match_dup 3)
+                         (match_dup 4)))]
+  {
+    operands[3] = GEN_INT (~1);
+    operands[4] = GEN_INT (~0);
+  }
   [(set_attr "conds" "use")
-   (set_attr "length" "10")]
+   (set_attr "length" "10")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb2_movsicc_insn"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r,r,r,r,r,r,r")
+(define_insn_and_split "*thumb2_mov_notscc_strict_it"
+  [(set (match_operand:SI 0 "low_register_operand" "=l")
+        (not:SI (match_operator:SI 1 "arm_comparison_operator"
+                 [(match_operand 2 "cc_register" "") (const_int 0)])))]
+  "TARGET_THUMB2 && arm_restrict_it"
+  "#"   ; "mvn %0, #0 ; it%d1 ; lsl%d1 %0, %0, #1"
+  "&& reload_completed"
+  [(set (match_dup 0)
+        (match_dup 3))
+   (cond_exec (match_dup 4)
+              (set (match_dup 0)
+                   (ashift:SI (match_dup 0)
+                              (const_int 1))))]
+  {
+    operands[3] = GEN_INT (~0);
+    operands[4] = gen_rtx_fmt_ee (GET_CODE (operands[1]),
+                                  VOIDmode, operands[2], const0_rtx);
+  }
+  [(set_attr "conds" "use")
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn_and_split "*thumb2_movsicc_insn"
+  [(set (match_operand:SI 0 "s_register_operand" "=l,l,r,r,r,r,r,r,r,r,r")
 	(if_then_else:SI
 	 (match_operator 3 "arm_comparison_operator"
 	  [(match_operand 4 "cc_register" "") (const_int 0)])
-	 (match_operand:SI 1 "arm_not_operand" "0,0,rI,K,rI,rI,K,K")
-	 (match_operand:SI 2 "arm_not_operand" "rI,K,0,0,rI,K,rI,K")))]
+	 (match_operand:SI 1 "arm_not_operand" "0 ,lPy,0 ,0,rI,K,rI,rI,K ,K,r")
+	 (match_operand:SI 2 "arm_not_operand" "lPy,0 ,rI,K,0 ,0,rI,K ,rI,K,r")))]
   "TARGET_THUMB2"
   "@
    it\\t%D3\;mov%D3\\t%0, %2
+   it\\t%d3\;mov%d3\\t%0, %1
+   it\\t%D3\;mov%D3\\t%0, %2
    it\\t%D3\;mvn%D3\\t%0, #%B2
    it\\t%d3\;mov%d3\\t%0, %1
    it\\t%d3\;mvn%d3\\t%0, #%B1
-   ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
-   ite\\t%d3\;mov%d3\\t%0, %1\;mvn%D3\\t%0, #%B2
-   ite\\t%d3\;mvn%d3\\t%0, #%B1\;mov%D3\\t%0, %2
-   ite\\t%d3\;mvn%d3\\t%0, #%B1\;mvn%D3\\t%0, #%B2"
-  [(set_attr "length" "6,6,6,6,10,10,10,10")
-   (set_attr "conds" "use")]
+   #
+   #
+   #
+   #
+   #"
+   ; alt 6: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
+   ; alt 7: ite\\t%d3\;mov%d3\\t%0, %1\;mvn%D3\\t%0, #%B2
+   ; alt 8: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mov%D3\\t%0, %2
+   ; alt 9: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mvn%D3\\t%0, #%B2
+   ; alt 10: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    enum rtx_code rev_code;
+    enum machine_mode mode;
+    rtx rev_cond;
+
+    emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                  operands[3],
+                                  gen_rtx_SET (VOIDmode,
+                                               operands[0],
+                                               operands[1])));
+    rev_code = GET_CODE (operands[3]);
+    mode = GET_MODE (operands[4]);
+    if (mode == CCFPmode || mode == CCFPEmode)
+      rev_code = reverse_condition_maybe_unordered (rev_code);
+    else
+      rev_code = reverse_condition (rev_code);
+
+    rev_cond = gen_rtx_fmt_ee (rev_code,
+                               VOIDmode,
+                               gen_rtx_REG (mode, CC_REGNUM),
+                               const0_rtx);
+    emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                  rev_cond,
+                                  gen_rtx_SET (VOIDmode,
+                                               operands[0],
+                                               operands[2])));
+    DONE;
+  }
+  [(set_attr "length" "4,4,6,6,6,6,10,10,10,10,6")
+   (set_attr "enabled_for_depr_it" "yes,yes,no,no,no,no,no,no,no,no,yes")
+   (set_attr "conds" "use")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*thumb2_movsfcc_soft_insn"
@@ -298,7 +568,8 @@
    it\\t%D3\;mov%D3\\t%0, %2
    it\\t%d3\;mov%d3\\t%0, %1"
   [(set_attr "length" "6,6")
-   (set_attr "conds" "use")]
+   (set_attr "conds" "use")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*call_reg_thumb2"
@@ -327,34 +598,84 @@
 	(match_operand:SI 0 "register_operand" "l*r"))]
   "TARGET_THUMB2"
   "bx\\t%0"
-  [(set_attr "conds" "clob")]
+  [(set_attr "conds" "clob")
+   (set_attr "type" "branch")]
 )
 ;; Don't define thumb2_load_indirect_jump because we can't guarantee label
 ;; addresses will have the thumb bit set correctly. 
 
 
-(define_insn "*thumb2_and_scc"
-  [(set (match_operand:SI 0 "s_register_operand" "=r")
+(define_insn_and_split "*thumb2_and_scc"
+  [(set (match_operand:SI 0 "s_register_operand" "=Ts")
 	(and:SI (match_operator:SI 1 "arm_comparison_operator"
-		 [(match_operand 3 "cc_register" "") (const_int 0)])
-		(match_operand:SI 2 "s_register_operand" "r")))]
+		 [(match_operand 2 "cc_register" "") (const_int 0)])
+		(match_operand:SI 3 "s_register_operand" "r")))]
   "TARGET_THUMB2"
-  "ite\\t%D1\;mov%D1\\t%0, #0\;and%d1\\t%0, %2, #1"
+  "#"   ; "and\\t%0, %3, #1\;it\\t%D1\;mov%D1\\t%0, #0"
+  "&& reload_completed"
+  [(set (match_dup 0)
+        (and:SI (match_dup 3) (const_int 1)))
+   (cond_exec (match_dup 4) (set (match_dup 0) (const_int 0)))]
+  {
+    enum machine_mode mode = GET_MODE (operands[2]);
+    enum rtx_code rc = GET_CODE (operands[1]);
+
+    if (mode == CCFPmode || mode == CCFPEmode)
+      rc = reverse_condition_maybe_unordered (rc);
+    else
+      rc = reverse_condition (rc);
+    operands[4] = gen_rtx_fmt_ee (rc, VOIDmode, operands[2], const0_rtx);
+  }
   [(set_attr "conds" "use")
-   (set_attr "length" "10")]
+   (set_attr "type" "multiple")
+   (set (attr "length") (if_then_else (match_test "arm_restrict_it")
+                                      (const_int 8)
+                                      (const_int 10)))]
 )
 
-(define_insn "*thumb2_ior_scc"
+(define_insn_and_split "*thumb2_ior_scc"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r")
+	(ior:SI (match_operator:SI 1 "arm_comparison_operator"
+		 [(match_operand 2 "cc_register" "") (const_int 0)])
+		(match_operand:SI 3 "s_register_operand" "0,?r")))]
+  "TARGET_THUMB2 && !arm_restrict_it"
+  "@
+   it\\t%d1\;orr%d1\\t%0, %3, #1
+   #"
+   ; alt 1: ite\\t%D1\;mov%D1\\t%0, %3\;orr%d1\\t%0, %3, #1
+   "&& reload_completed
+    && REGNO (operands [0]) != REGNO (operands[3])"
+   [(cond_exec (match_dup 5) (set (match_dup 0) (match_dup 3)))
+    (cond_exec (match_dup 4) (set (match_dup 0)
+                                  (ior:SI (match_dup 3) (const_int 1))))]
+  {
+    enum machine_mode mode = GET_MODE (operands[2]);
+    enum rtx_code rc = GET_CODE (operands[1]);
+
+    operands[4] = gen_rtx_fmt_ee (rc, VOIDmode, operands[2], const0_rtx);
+    if (mode == CCFPmode || mode == CCFPEmode)
+      rc = reverse_condition_maybe_unordered (rc);
+    else
+      rc = reverse_condition (rc);
+    operands[5] = gen_rtx_fmt_ee (rc, VOIDmode, operands[2], const0_rtx);
+  }
+  [(set_attr "conds" "use")
+   (set_attr "length" "6,10")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*thumb2_ior_scc_strict_it"
+  [(set (match_operand:SI 0 "s_register_operand" "=l,l")
 	(ior:SI (match_operator:SI 2 "arm_comparison_operator"
 		 [(match_operand 3 "cc_register" "") (const_int 0)])
-		(match_operand:SI 1 "s_register_operand" "0,?r")))]
-  "TARGET_THUMB2"
+		(match_operand:SI 1 "s_register_operand" "0,?l")))]
+  "TARGET_THUMB2 && arm_restrict_it"
   "@
-   it\\t%d2\;orr%d2\\t%0, %1, #1
-   ite\\t%D2\;mov%D2\\t%0, %1\;orr%d2\\t%0, %1, #1"
+   it\\t%d2\;mov%d2\\t%0, #1\;it\\t%d2\;orr%d2\\t%0, %1
+   mov\\t%0, #1\;orr\\t%0, %1\;it\\t%D2\;mov%D2\\t%0, %1"
   [(set_attr "conds" "use")
-   (set_attr "length" "6,10")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*thumb2_cond_move"
@@ -384,19 +705,27 @@
 	output_asm_insn (\"it\\t%D4\", operands);
 	break;
       case 2:
-	output_asm_insn (\"ite\\t%D4\", operands);
+	if (arm_restrict_it)
+	  output_asm_insn (\"it\\t%D4\", operands);
+	else
+	  output_asm_insn (\"ite\\t%D4\", operands);
 	break;
       default:
 	abort();
       }
     if (which_alternative != 0)
-      output_asm_insn (\"mov%D4\\t%0, %1\", operands);
+      {
+        output_asm_insn (\"mov%D4\\t%0, %1\", operands);
+        if (arm_restrict_it && which_alternative == 2)
+          output_asm_insn (\"it\\t%d4\", operands);
+      }
     if (which_alternative != 1)
       output_asm_insn (\"mov%d4\\t%0, %2\", operands);
     return \"\";
   "
   [(set_attr "conds" "use")
-   (set_attr "length" "6,6,10")]
+   (set_attr "length" "6,6,10")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*thumb2_cond_arith"
@@ -407,7 +736,7 @@
 	    (match_operand:SI 3 "arm_rhs_operand" "rI,rI")])
           (match_operand:SI 1 "s_register_operand" "0,?r")]))
    (clobber (reg:CC CC_REGNUM))]
-  "TARGET_THUMB2"
+  "TARGET_THUMB2 && !arm_restrict_it"
   "*
     if (GET_CODE (operands[4]) == LT && operands[3] == const0_rtx)
       return \"%i5\\t%0, %1, %2, lsr #31\";
@@ -433,12 +762,83 @@
     return \"%i5%d4\\t%0, %1, #1\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "14")]
+   (set_attr "length" "14")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn_and_split "*thumb2_cond_arith_strict_it"
+  [(set (match_operand:SI 0 "s_register_operand" "=l")
+        (match_operator:SI 5 "shiftable_operator_strict_it"
+	 [(match_operator:SI 4 "arm_comparison_operator"
+           [(match_operand:SI 2 "s_register_operand" "r")
+	    (match_operand:SI 3 "arm_rhs_operand" "rI")])
+          (match_operand:SI 1 "s_register_operand" "0")]))
+   (clobber (reg:CC CC_REGNUM))]
+  "TARGET_THUMB2 && arm_restrict_it"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    if (GET_CODE (operands[4]) == LT && operands[3] == const0_rtx)
+      {
+        /*  %i5 %0, %1, %2, lsr #31  */
+        rtx shifted_op = gen_rtx_LSHIFTRT (SImode, operands[2], GEN_INT (31));
+        rtx op = NULL_RTX;
+
+        switch (GET_CODE (operands[5]))
+          {
+          case AND:
+            op = gen_rtx_AND (SImode, shifted_op, operands[1]);
+            break;
+           case PLUS:
+            op = gen_rtx_PLUS (SImode, shifted_op, operands[1]);
+            break;
+          default: gcc_unreachable ();
+          }
+        emit_insn (gen_rtx_SET (VOIDmode, operands[0], op));
+        DONE;
+      }
+
+    /*  "cmp  %2, %3"  */
+    emit_insn (gen_rtx_SET (VOIDmode,
+                               gen_rtx_REG (CCmode, CC_REGNUM),
+                               gen_rtx_COMPARE (CCmode, operands[2], operands[3])));
+
+    if (GET_CODE (operands[5]) == AND)
+      {
+        /*  %i5  %0, %1, #1
+            it%D4
+            mov%D4  %0, #0  */
+        enum rtx_code rc = reverse_condition (GET_CODE (operands[4]));
+        emit_insn (gen_rtx_SET (VOIDmode, operands[0], gen_rtx_AND (SImode, operands[1], GEN_INT (1))));
+        emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                      gen_rtx_fmt_ee (rc, VOIDmode, gen_rtx_REG (CCmode, CC_REGNUM), const0_rtx),
+                                      gen_rtx_SET (VOIDmode, operands[0], const0_rtx)));
+        DONE;
+      }
+    else
+      {
+        /*  it\\t%d4
+            %i5%d4\\t%0, %1, #1   */
+        emit_insn (gen_rtx_COND_EXEC (VOIDmode, gen_rtx_fmt_ee (GET_CODE (operands[4]),
+                                                                VOIDmode,
+                                                                gen_rtx_REG (CCmode, CC_REGNUM), const0_rtx),
+                                                gen_rtx_SET(VOIDmode, operands[0],
+                                                            gen_rtx_PLUS (SImode,
+                                                                          operands[1],
+                                                                          GEN_INT (1)))));
+        DONE;
+      }
+     FAIL;
+  }
+  [(set_attr "conds" "clob")
+   (set_attr "length" "12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*thumb2_cond_sub"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r")
-        (minus:SI (match_operand:SI 1 "s_register_operand" "0,?r")
+  [(set (match_operand:SI 0 "s_register_operand" "=Ts,Ts")
+        (minus:SI (match_operand:SI 1 "s_register_operand" "0,?Ts")
 		  (match_operator:SI 4 "arm_comparison_operator"
                    [(match_operand:SI 2 "s_register_operand" "r,r")
 		    (match_operand:SI 3 "arm_rhs_operand" "rI,rI")])))
@@ -448,48 +848,103 @@
     output_asm_insn (\"cmp\\t%2, %3\", operands);
     if (which_alternative != 0)
       {
-	output_asm_insn (\"ite\\t%D4\", operands);
-	output_asm_insn (\"mov%D4\\t%0, %1\", operands);
+	if (arm_restrict_it)
+	  {
+	    output_asm_insn (\"mov\\t%0, %1\", operands);
+	    output_asm_insn (\"it\\t%d4\", operands);
+	  }
+	else
+	{
+	  output_asm_insn (\"ite\\t%D4\", operands);
+	  output_asm_insn (\"mov%D4\\t%0, %1\", operands);
+	}
       }
     else
       output_asm_insn (\"it\\t%d4\", operands);
     return \"sub%d4\\t%0, %1, #1\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "10,14")]
+   (set_attr "length" "10,14")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb2_negscc"
-  [(set (match_operand:SI 0 "s_register_operand" "=r")
+(define_insn_and_split "*thumb2_negscc"
+  [(set (match_operand:SI 0 "s_register_operand" "=Ts")
 	(neg:SI (match_operator 3 "arm_comparison_operator"
 		 [(match_operand:SI 1 "s_register_operand" "r")
 		  (match_operand:SI 2 "arm_rhs_operand" "rI")])))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB2"
-  "*
-  if (GET_CODE (operands[3]) == LT && operands[2] == const0_rtx)
-    return \"asr\\t%0, %1, #31\";
-
-  if (GET_CODE (operands[3]) == NE)
-    return \"subs\\t%0, %1, %2\;it\\tne\;mvnne\\t%0, #0\";
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    rtx cc_reg = gen_rtx_REG (CCmode, CC_REGNUM);
 
-  output_asm_insn (\"cmp\\t%1, %2\", operands);
-  output_asm_insn (\"ite\\t%D3\", operands);
-  output_asm_insn (\"mov%D3\\t%0, #0\", operands);
-  return \"mvn%d3\\t%0, #0\";
-  "
+    if (GET_CODE (operands[3]) == LT && operands[2] == const0_rtx)
+      {
+        /* Emit asr\\t%0, %1, #31 */
+        emit_insn (gen_rtx_SET (VOIDmode,
+                                operands[0],
+                                gen_rtx_ASHIFTRT (SImode,
+                                                  operands[1],
+                                                  GEN_INT (31))));
+        DONE;
+      }
+    else if (GET_CODE (operands[3]) == NE && !arm_restrict_it)
+      {
+        /* Emit subs\\t%0, %1, %2\;it\\tne\;mvnne\\t%0, #0 */
+        if (CONST_INT_P (operands[2]))
+          emit_insn (gen_cmpsi2_addneg (operands[0], operands[1], operands[2],
+                                        GEN_INT (- INTVAL (operands[2]))));
+        else
+          emit_insn (gen_subsi3_compare (operands[0], operands[1], operands[2]));
+
+        emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                      gen_rtx_NE (SImode,
+                                                  cc_reg,
+                                                  const0_rtx),
+                                      gen_rtx_SET (SImode,
+                                                   operands[0],
+                                                   GEN_INT (~0))));
+        DONE;
+      }
+    else
+      {
+       /* Emit:  cmp\\t%1, %2\;mvn\\t%0, #0\;it\\t%D3\;mov%D3\\t%0, #0\;*/
+       enum rtx_code rc = reverse_condition (GET_CODE (operands[3]));
+       enum machine_mode mode = SELECT_CC_MODE (rc, operands[1], operands[2]);
+       rtx tmp1 = gen_rtx_REG (mode, CC_REGNUM);
+
+       emit_insn (gen_rtx_SET (VOIDmode,
+                               cc_reg,
+                               gen_rtx_COMPARE (CCmode, operands[1], operands[2])));
+
+       emit_insn (gen_rtx_SET (VOIDmode, operands[0], GEN_INT (~0)));
+
+       emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                     gen_rtx_fmt_ee (rc,
+                                                     VOIDmode,
+                                                     tmp1,
+                                                     const0_rtx),
+                                     gen_rtx_SET (VOIDmode, operands[0], const0_rtx)));
+       DONE;
+      }
+    FAIL;
+  }
   [(set_attr "conds" "clob")
-   (set_attr "length" "14")]
+   (set_attr "length" "14")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*thumb2_movcond"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r,r")
+  [(set (match_operand:SI 0 "s_register_operand" "=Ts,Ts,Ts")
 	(if_then_else:SI
 	 (match_operator 5 "arm_comparison_operator"
 	  [(match_operand:SI 3 "s_register_operand" "r,r,r")
 	   (match_operand:SI 4 "arm_add_operand" "rIL,rIL,rIL")])
-	 (match_operand:SI 1 "arm_rhs_operand" "0,rI,?rI")
-	 (match_operand:SI 2 "arm_rhs_operand" "rI,0,rI")))
+	 (match_operand:SI 1 "arm_rhs_operand" "0,TsI,?TsI")
+	 (match_operand:SI 2 "arm_rhs_operand" "TsI,0,TsI")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB2"
   "*
@@ -544,19 +999,26 @@
       output_asm_insn (\"it\\t%d5\", operands);
       break;
     case 2:
-      output_asm_insn (\"ite\\t%d5\", operands);
+      if (arm_restrict_it)
+        {
+          output_asm_insn (\"mov\\t%0, %1\", operands);
+          output_asm_insn (\"it\\t%D5\", operands);
+        }
+      else
+        output_asm_insn (\"ite\\t%d5\", operands);
       break;
     default:
       abort();
     }
-  if (which_alternative != 0)
+  if (which_alternative != 0 && !(arm_restrict_it && which_alternative == 2))
     output_asm_insn (\"mov%d5\\t%0, %1\", operands);
   if (which_alternative != 1)
     output_asm_insn (\"mov%D5\\t%0, %2\", operands);
   return \"\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "10,10,14")]
+   (set_attr "length" "10,10,14")
+   (set_attr "type" "multiple")]
 )
 
 ;; Zero and sign extension instructions.
@@ -570,8 +1032,9 @@
   "@
    sxtb%?\\t%0, %1
    ldr%(sb%)\\t%0, %1"
-  [(set_attr "type" "simple_alu_shift,load_byte")
+  [(set_attr "type" "extend,load_byte")
    (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "pool_range" "*,4094")
    (set_attr "neg_pool_range" "*,250")]
 )
@@ -583,8 +1046,9 @@
   "@
    uxth%?\\t%0, %1
    ldr%(h%)\\t%0, %1"
-  [(set_attr "type" "simple_alu_shift,load_byte")
+  [(set_attr "type" "extend,load_byte")
    (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "pool_range" "*,4094")
    (set_attr "neg_pool_range" "*,250")]
 )
@@ -596,8 +1060,9 @@
   "@
    uxtb%(%)\\t%0, %1
    ldr%(b%)\\t%0, %1\\t%@ zero_extendqisi2"
-  [(set_attr "type" "simple_alu_shift,load_byte")
+  [(set_attr "type" "extend,load_byte")
    (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "pool_range" "*,4094")
    (set_attr "neg_pool_range" "*,250")]
 )
@@ -616,7 +1081,8 @@
   "TARGET_THUMB2 && !flag_pic"
   "* return thumb2_output_casesi(operands);"
   [(set_attr "conds" "clob")
-   (set_attr "length" "16")]
+   (set_attr "length" "16")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "thumb2_casesi_internal_pic"
@@ -634,7 +1100,8 @@
   "TARGET_THUMB2 && flag_pic"
   "* return thumb2_output_casesi(operands);"
   [(set_attr "conds" "clob")
-   (set_attr "length" "20")]
+   (set_attr "length" "20")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*thumb2_return"
@@ -671,7 +1138,8 @@
    && GET_CODE(operands[3]) != MINUS"
   "%I3%!\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "length" "2")]
+   (set_attr "length" "2")
+   (set_attr "type" "alu_reg")]
 )
 
 (define_insn "*thumb2_shiftsi3_short"
@@ -688,7 +1156,7 @@
    (set_attr "shift" "1")
    (set_attr "length" "2")
    (set (attr "type") (if_then_else (match_operand 2 "const_int_operand" "")
-		      (const_string "alu_shift")
+		      (const_string "alu_shift_imm")
 		      (const_string "alu_shift_reg")))]
 )
 
@@ -699,7 +1167,8 @@
   "TARGET_THUMB2 && reload_completed"
   "mov%!\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "length" "2")]
+   (set_attr "length" "2")
+   (set_attr "type" "mov_imm")]
 )
 
 (define_insn "*thumb2_addsi_short"
@@ -723,7 +1192,8 @@
       return \"add%!\\t%0, %1, %2\";
   "
   [(set_attr "predicable" "yes")
-   (set_attr "length" "2")]
+   (set_attr "length" "2")
+   (set_attr "type" "alu_reg")]
 )
 
 (define_insn "*thumb2_subsi_short"
@@ -734,7 +1204,8 @@
   "TARGET_THUMB2 && reload_completed"
   "sub%!\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "length" "2")]
+   (set_attr "length" "2")
+   (set_attr "type" "alu_reg")]
 )
 
 (define_peephole2
@@ -786,7 +1257,8 @@
       return \"adds\\t%0, %1, %2\";
   "
   [(set_attr "conds" "set")
-   (set_attr "length" "2,2,4")]
+   (set_attr "length" "2,2,4")
+   (set_attr "type" "alu_reg")]
 )
 
 (define_insn "*thumb2_addsi3_compare0_scratch"
@@ -811,7 +1283,7 @@
   "
   [(set_attr "conds" "set")
    (set_attr "length" "2,2,4,4")
-   (set_attr "type"   "simple_alu_imm,*,simple_alu_imm,*")]
+   (set_attr "type" "alus_imm,alus_reg,alus_imm,alus_reg")]
 )
 
 (define_insn "*thumb2_mulsi_short"
@@ -823,7 +1295,7 @@
   "mul%!\\t%0, %2, %0"
   [(set_attr "predicable" "yes")
    (set_attr "length" "2")
-   (set_attr "insn" "muls")])
+   (set_attr "type" "muls")])
 
 (define_insn "*thumb2_mulsi_short_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
@@ -836,7 +1308,7 @@
   "TARGET_THUMB2 && optimize_size"
   "muls\\t%0, %2, %0"
   [(set_attr "length" "2")
-   (set_attr "insn" "muls")])
+   (set_attr "type" "muls")])
 
 (define_insn "*thumb2_mulsi_short_compare0_scratch"
   [(set (reg:CC_NOOV CC_REGNUM)
@@ -848,7 +1320,7 @@
   "TARGET_THUMB2 && optimize_size"
   "muls\\t%0, %2, %0"
   [(set_attr "length" "2")
-   (set_attr "insn" "muls")])
+   (set_attr "type" "muls")])
 
 (define_insn "*thumb2_cbz"
   [(set (pc) (if_then_else
@@ -870,7 +1342,8 @@
 	         (le (minus (match_dup 1) (pc)) (const_int 128))
 	         (not (match_test "which_alternative")))
 	    (const_int 2)
-	    (const_int 8)))]
+	    (const_int 8)))
+   (set_attr "type" "branch,multiple")]
 )
 
 (define_insn "*thumb2_cbnz"
@@ -893,7 +1366,8 @@
 	         (le (minus (match_dup 1) (pc)) (const_int 128))
 	         (not (match_test "which_alternative")))
 	    (const_int 2)
-	    (const_int 8)))]
+	    (const_int 8)))
+   (set_attr "type" "branch,multiple")]
 )
 
 (define_insn "*thumb2_one_cmplsi2_short"
@@ -903,7 +1377,8 @@
   "TARGET_THUMB2 && reload_completed"
   "mvn%!\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "length" "2")]
+   (set_attr "length" "2")
+   (set_attr "type" "mvn_reg")]
 )
 
 (define_insn "*thumb2_negsi2_short"
@@ -913,7 +1388,8 @@
   "TARGET_THUMB2 && reload_completed"
   "neg%!\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "length" "2")]
+   (set_attr "length" "2")
+   (set_attr "type" "alu_reg")]
 )
 
 (define_insn "*orsi_notsi_si"
@@ -922,7 +1398,9 @@
 		(match_operand:SI 1 "s_register_operand" "r")))]
   "TARGET_THUMB2"
   "orn%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "logic_reg")]
 )
 
 (define_insn "*orsi_not_shiftsi_si"
@@ -934,8 +1412,9 @@
   "TARGET_THUMB2"
   "orn%?\\t%0, %1, %2%S4"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "shift" "2")
-   (set_attr "type" "alu_shift")]
+   (set_attr "type" "alu_shift_imm")]
 )
 
 (define_peephole2
Index: b/src/gcc/config/arm/arm.c
===================================================================
--- a/src/gcc/config/arm/arm.c
+++ b/src/gcc/config/arm/arm.c
@@ -172,6 +172,7 @@ static rtx arm_expand_builtin (tree, rtx
 static tree arm_builtin_decl (unsigned, bool);
 static void emit_constant_insn (rtx cond, rtx pattern);
 static rtx emit_set_insn (rtx, rtx);
+static rtx emit_multi_reg_push (unsigned long);
 static int arm_arg_partial_bytes (cumulative_args_t, enum machine_mode,
 				  tree, bool);
 static rtx arm_function_arg (cumulative_args_t, enum machine_mode,
@@ -278,6 +279,7 @@ static unsigned arm_add_stmt_cost (void
 
 static void arm_canonicalize_comparison (int *code, rtx *op0, rtx *op1,
 					 bool op0_preserve_value);
+static unsigned HOST_WIDE_INT arm_asan_shadow_offset (void);
 
 /* Table of machine attributes.  */
 static const struct attribute_spec arm_attribute_table[] =
@@ -618,6 +620,13 @@ static const struct attribute_spec arm_a
 #undef TARGET_CLASS_LIKELY_SPILLED_P
 #define TARGET_CLASS_LIKELY_SPILLED_P arm_class_likely_spilled_p
 
+#undef TARGET_VECTORIZE_BUILTINS
+#define TARGET_VECTORIZE_BUILTINS
+
+#undef TARGET_VECTORIZE_BUILTIN_VECTORIZED_FUNCTION
+#define TARGET_VECTORIZE_BUILTIN_VECTORIZED_FUNCTION \
+  arm_builtin_vectorized_function
+
 #undef TARGET_VECTOR_ALIGNMENT
 #define TARGET_VECTOR_ALIGNMENT arm_vector_alignment
 
@@ -647,6 +656,13 @@ static const struct attribute_spec arm_a
 #define TARGET_CANONICALIZE_COMPARISON \
   arm_canonicalize_comparison
 
+#undef TARGET_ASAN_SHADOW_OFFSET
+#define TARGET_ASAN_SHADOW_OFFSET arm_asan_shadow_offset
+
+#undef MAX_INSN_PER_IT_BLOCK
+#define MAX_INSN_PER_IT_BLOCK (arm_restrict_it ? 1 : 4)
+
+
 struct gcc_target targetm = TARGET_INITIALIZER;
 
 /* Obstack for minipool constant handling.  */
@@ -708,6 +724,7 @@ static int thumb_call_reg_needed;
 #define FL_ARCH7      (1 << 22)       /* Architecture 7.  */
 #define FL_ARM_DIV    (1 << 23)	      /* Hardware divide (ARM mode).  */
 #define FL_ARCH8      (1 << 24)       /* Architecture 8.  */
+#define FL_CRC32      (1 << 25)	      /* ARMv8 CRC32 instructions.  */
 
 #define FL_IWMMXT     (1 << 29)	      /* XScale v2 or "Intel Wireless MMX technology".  */
 #define FL_IWMMXT2    (1 << 30)       /* "Intel Wireless MMX2 technology".  */
@@ -837,6 +854,10 @@ int arm_arch_thumb2;
 int arm_arch_arm_hwdiv;
 int arm_arch_thumb_hwdiv;
 
+/* Nonzero if we should use Neon to handle 64-bits operations rather
+   than core registers.  */
+int prefer_neon_for_64bits = 0;
+
 /* In case of a PRE_INC, POST_INC, PRE_DEC, POST_DEC memory reference,
    we must report the mode of the memory reference from
    TARGET_PRINT_OPERAND to TARGET_PRINT_OPERAND_ADDRESS.  */
@@ -866,6 +887,9 @@ int arm_condexec_mask = 0;
 /* The number of bits used in arm_condexec_mask.  */
 int arm_condexec_masklen = 0;
 
+/* Nonzero if chip supports the ARMv8 CRC instructions.  */
+int arm_arch_crc = 0;
+
 /* The condition codes of the ARM, and the inverse function.  */
 static const char * const arm_condition_codes[] =
 {
@@ -934,6 +958,7 @@ const struct tune_params arm_slowmul_tun
   false,					/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 const struct tune_params arm_fastmul_tune =
@@ -948,6 +973,7 @@ const struct tune_params arm_fastmul_tun
   false,					/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 /* StrongARM has early execution of branches, so a sequence that is worth
@@ -965,6 +991,7 @@ const struct tune_params arm_strongarm_t
   false,					/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 const struct tune_params arm_xscale_tune =
@@ -979,6 +1006,7 @@ const struct tune_params arm_xscale_tune
   false,					/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 const struct tune_params arm_9e_tune =
@@ -993,6 +1021,7 @@ const struct tune_params arm_9e_tune =
   false,					/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 const struct tune_params arm_v6t2_tune =
@@ -1007,6 +1036,7 @@ const struct tune_params arm_v6t2_tune =
   false,					/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 /* Generic Cortex tuning.  Use more specific tunings if appropriate.  */
@@ -1022,6 +1052,7 @@ const struct tune_params arm_cortex_tune
   false,					/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 const struct tune_params arm_cortex_a15_tune =
@@ -1029,13 +1060,14 @@ const struct tune_params arm_cortex_a15_
   arm_9e_rtx_costs,
   NULL,
   1,						/* Constant limit.  */
-  5,						/* Max cond insns.  */
+  2,						/* Max cond insns.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
   false,					/* Prefer constant pool.  */
   arm_default_branch_cost,
   true,						/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 /* Branches can be dual-issued on Cortex-A5, so conditional execution is
@@ -1053,6 +1085,7 @@ const struct tune_params arm_cortex_a5_t
   false,					/* Prefer LDRD/STRD.  */
   {false, false},				/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 const struct tune_params arm_cortex_a9_tune =
@@ -1067,6 +1100,7 @@ const struct tune_params arm_cortex_a9_t
   false,					/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 /* The arm_v6m_tune is duplicated from arm_cortex_tune, rather than
@@ -1083,6 +1117,7 @@ const struct tune_params arm_v6m_tune =
   false,					/* Prefer LDRD/STRD.  */
   {false, false},				/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 const struct tune_params arm_fa726te_tune =
@@ -1097,6 +1132,7 @@ const struct tune_params arm_fa726te_tun
   false,					/* Prefer LDRD/STRD.  */
   {true, true},					/* Prefer non short circuit.  */
   &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
 
@@ -1840,6 +1876,12 @@ arm_option_override (void)
   arm_arch_thumb_hwdiv = (insn_flags & FL_THUMB_DIV) != 0;
   arm_arch_arm_hwdiv = (insn_flags & FL_ARM_DIV) != 0;
   arm_tune_cortex_a9 = (arm_tune == cortexa9) != 0;
+  arm_arch_crc = (insn_flags & FL_CRC32) != 0;
+  if (arm_restrict_it == 2)
+    arm_restrict_it = arm_arch8 && TARGET_THUMB2;
+
+  if (!TARGET_THUMB2)
+    arm_restrict_it = 0;
 
   /* If we are not using the default (ARM mode) section anchor offset
      ranges, then set the correct ranges now.  */
@@ -2127,11 +2169,25 @@ arm_option_override (void)
                            global_options.x_param_values,
                            global_options_set.x_param_values);
 
+  /* Use Neon to perform 64-bits operations rather than core
+     registers.  */
+  prefer_neon_for_64bits = current_tune->prefer_neon_for_64bits;
+  if (use_neon_for_64bits == 1)
+     prefer_neon_for_64bits = true;
+
   /* Use the alternative scheduling-pressure algorithm by default.  */
   maybe_set_param_value (PARAM_SCHED_PRESSURE_ALGORITHM, 2,
                          global_options.x_param_values,
                          global_options_set.x_param_values);
 
+  /* Disable shrink-wrap when optimizing function for size, since it tends to
+     generate additional returns.  */
+  if (optimize_function_for_size_p (cfun) && TARGET_THUMB2)
+    flag_shrink_wrap = false;
+  /* TBD: Dwarf info for apcs frame is not handled yet.  */
+  if (TARGET_APCS_FRAME)
+    flag_shrink_wrap = false;
+
   /* Register global variables with the garbage collector.  */
   arm_add_gc_roots ();
 }
@@ -2380,6 +2436,10 @@ use_return_insn (int iscond, rtx sibling
   if (IS_INTERRUPT (func_type) && (frame_pointer_needed || TARGET_THUMB))
     return 0;
 
+  if (TARGET_LDRD && current_tune->prefer_ldrd_strd
+      && !optimize_function_for_size_p (cfun))
+    return 0;
+
   offsets = arm_get_frame_offsets ();
   stack_adjust = offsets->outgoing_args - offsets->saved_regs;
 
@@ -2477,6 +2537,18 @@ use_return_insn (int iscond, rtx sibling
   return 1;
 }
 
+/* Return TRUE if we should try to use a simple_return insn, i.e. perform
+   shrink-wrapping if possible.  This is the case if we need to emit a
+   prologue, which we can test by looking at the offsets.  */
+bool
+use_simple_return_p (void)
+{
+  arm_stack_offsets *offsets;
+
+  offsets = arm_get_frame_offsets ();
+  return offsets->outgoing_args != 0;
+}
+
 /* Return TRUE if int I is a valid immediate ARM constant.  */
 
 int
@@ -2615,6 +2687,11 @@ const_ok_for_dimode_op (HOST_WIDE_INT i,
 
   switch (code)
     {
+    case AND:
+    case IOR:
+    case XOR:
+      return (const_ok_for_op (hi_val, code) || hi_val == 0xFFFFFFFF)
+              && (const_ok_for_op (lo_val, code) || lo_val == 0xFFFFFFFF);
     case PLUS:
       return arm_not_operand (hi, SImode) && arm_add_operand (lo, SImode);
 
@@ -5335,9 +5412,8 @@ arm_function_ok_for_sibcall (tree decl,
   if (cfun->machine->sibcall_blocked)
     return false;
 
-  /* Never tailcall something for which we have no decl, or if we
-     are generating code for Thumb-1.  */
-  if (decl == NULL || TARGET_THUMB1)
+  /* Never tailcall something if we are generating code for Thumb-1.  */
+  if (TARGET_THUMB1)
     return false;
 
   /* The PIC register is live on entry to VxWorks PLT entries, so we
@@ -5347,13 +5423,14 @@ arm_function_ok_for_sibcall (tree decl,
 
   /* Cannot tail-call to long calls, since these are out of range of
      a branch instruction.  */
-  if (arm_is_long_call_p (decl))
+  if (decl && arm_is_long_call_p (decl))
     return false;
 
   /* If we are interworking and the function is not declared static
      then we can't tail-call it unless we know that it exists in this
      compilation unit (since it might be a Thumb routine).  */
-  if (TARGET_INTERWORK && TREE_PUBLIC (decl) && !TREE_ASM_WRITTEN (decl))
+  if (TARGET_INTERWORK && decl && TREE_PUBLIC (decl)
+      && !TREE_ASM_WRITTEN (decl))
     return false;
 
   func_type = arm_current_func_type ();
@@ -5385,6 +5462,7 @@ arm_function_ok_for_sibcall (tree decl,
      sibling calls.  */
   if (TARGET_AAPCS_BASED
       && arm_abi == ARM_ABI_AAPCS
+      && decl
       && DECL_WEAK (decl))
     return false;
 
@@ -8596,7 +8674,18 @@ xscale_sched_adjust_cost (rtx insn, rtx
 	 instruction we depend on is another ALU instruction, then we may
 	 have to account for an additional stall.  */
       if (shift_opnum != 0
-	  && (attr_type == TYPE_ALU_SHIFT || attr_type == TYPE_ALU_SHIFT_REG))
+	  && (attr_type == TYPE_ALU_SHIFT_IMM
+	      || attr_type == TYPE_ALUS_SHIFT_IMM
+	      || attr_type == TYPE_LOGIC_SHIFT_IMM
+	      || attr_type == TYPE_LOGICS_SHIFT_IMM
+	      || attr_type == TYPE_ALU_SHIFT_REG
+	      || attr_type == TYPE_ALUS_SHIFT_REG
+	      || attr_type == TYPE_LOGIC_SHIFT_REG
+	      || attr_type == TYPE_LOGICS_SHIFT_REG
+	      || attr_type == TYPE_MOV_SHIFT
+	      || attr_type == TYPE_MVN_SHIFT
+	      || attr_type == TYPE_MOV_SHIFT_REG
+	      || attr_type == TYPE_MVN_SHIFT_REG))
 	{
 	  rtx shifted_operand;
 	  int opno;
@@ -8877,12 +8966,20 @@ cortexa7_older_only (rtx insn)
   if (recog_memoized (insn) < 0)
     return false;
 
-  if (get_attr_insn (insn) == INSN_MOV)
-    return false;
-
   switch (get_attr_type (insn))
     {
     case TYPE_ALU_REG:
+    case TYPE_ALUS_REG:
+    case TYPE_LOGIC_REG:
+    case TYPE_LOGICS_REG:
+    case TYPE_ADC_REG:
+    case TYPE_ADCS_REG:
+    case TYPE_ADR:
+    case TYPE_BFM:
+    case TYPE_REV:
+    case TYPE_MVN_REG:
+    case TYPE_SHIFT_IMM:
+    case TYPE_SHIFT_REG:
     case TYPE_LOAD_BYTE:
     case TYPE_LOAD1:
     case TYPE_STORE1:
@@ -8890,7 +8987,7 @@ cortexa7_older_only (rtx insn)
     case TYPE_FADDS:
     case TYPE_FFARITHD:
     case TYPE_FADDD:
-    case TYPE_FCPYS:
+    case TYPE_FMOV:
     case TYPE_F_CVT:
     case TYPE_FCMPS:
     case TYPE_FCMPD:
@@ -8902,7 +8999,8 @@ cortexa7_older_only (rtx insn)
     case TYPE_FMACD:
     case TYPE_FDIVS:
     case TYPE_FDIVD:
-    case TYPE_F_2_R:
+    case TYPE_F_MRC:
+    case TYPE_F_MRRC:
     case TYPE_F_FLAG:
     case TYPE_F_LOADS:
     case TYPE_F_STORES:
@@ -8923,13 +9021,18 @@ cortexa7_younger (FILE *file, int verbos
       return false;
     }
 
-  if (get_attr_insn (insn) == INSN_MOV)
-    return true;
-
   switch (get_attr_type (insn))
     {
-    case TYPE_SIMPLE_ALU_IMM:
-    case TYPE_SIMPLE_ALU_SHIFT:
+    case TYPE_ALU_IMM:
+    case TYPE_ALUS_IMM:
+    case TYPE_LOGIC_IMM:
+    case TYPE_LOGICS_IMM:
+    case TYPE_EXTEND:
+    case TYPE_MVN_IMM:
+    case TYPE_MOV_IMM:
+    case TYPE_MOV_REG:
+    case TYPE_MOV_SHIFT:
+    case TYPE_MOV_SHIFT_REG:
     case TYPE_BRANCH:
     case TYPE_CALL:
       return true;
@@ -9088,6 +9191,12 @@ arm_adjust_cost (rtx insn, rtx link, rtx
   return cost;
 }
 
+int
+arm_max_conditional_execute (void)
+{
+  return max_insns_skipped;
+}
+
 static int
 arm_default_branch_cost (bool speed_p, bool predictable_p ATTRIBUTE_UNUSED)
 {
@@ -11843,6 +11952,142 @@ arm_gen_movmemqi (rtx *operands)
   return 1;
 }
 
+/* Helper for gen_movmem_ldrd_strd. Increase the address of memory rtx
+by mode size.  */
+inline static rtx
+next_consecutive_mem (rtx mem)
+{
+  enum machine_mode mode = GET_MODE (mem);
+  HOST_WIDE_INT offset = GET_MODE_SIZE (mode);
+  rtx addr = plus_constant (Pmode, XEXP (mem, 0), offset);
+
+  return adjust_automodify_address (mem, mode, addr, offset);
+}
+
+/* Copy using LDRD/STRD instructions whenever possible.
+   Returns true upon success. */
+bool
+gen_movmem_ldrd_strd (rtx *operands)
+{
+  unsigned HOST_WIDE_INT len;
+  HOST_WIDE_INT align;
+  rtx src, dst, base;
+  rtx reg0;
+  bool src_aligned, dst_aligned;
+  bool src_volatile, dst_volatile;
+
+  gcc_assert (CONST_INT_P (operands[2]));
+  gcc_assert (CONST_INT_P (operands[3]));
+
+  len = UINTVAL (operands[2]);
+  if (len > 64)
+    return false;
+
+  /* Maximum alignment we can assume for both src and dst buffers.  */
+  align = INTVAL (operands[3]);
+
+  if ((!unaligned_access) && (len >= 4) && ((align & 3) != 0))
+    return false;
+
+  /* Place src and dst addresses in registers
+     and update the corresponding mem rtx.  */
+  dst = operands[0];
+  dst_volatile = MEM_VOLATILE_P (dst);
+  dst_aligned = MEM_ALIGN (dst) >= BITS_PER_WORD;
+  base = copy_to_mode_reg (SImode, XEXP (dst, 0));
+  dst = adjust_automodify_address (dst, VOIDmode, base, 0);
+
+  src = operands[1];
+  src_volatile = MEM_VOLATILE_P (src);
+  src_aligned = MEM_ALIGN (src) >= BITS_PER_WORD;
+  base = copy_to_mode_reg (SImode, XEXP (src, 0));
+  src = adjust_automodify_address (src, VOIDmode, base, 0);
+
+  if (!unaligned_access && !(src_aligned && dst_aligned))
+    return false;
+
+  if (src_volatile || dst_volatile)
+    return false;
+
+  /* If we cannot generate any LDRD/STRD, try to generate LDM/STM.  */
+  if (!(dst_aligned || src_aligned))
+    return arm_gen_movmemqi (operands);
+
+  src = adjust_address (src, DImode, 0);
+  dst = adjust_address (dst, DImode, 0);
+  while (len >= 8)
+    {
+      len -= 8;
+      reg0 = gen_reg_rtx (DImode);
+      if (src_aligned)
+        emit_move_insn (reg0, src);
+      else
+        emit_insn (gen_unaligned_loaddi (reg0, src));
+
+      if (dst_aligned)
+        emit_move_insn (dst, reg0);
+      else
+        emit_insn (gen_unaligned_storedi (dst, reg0));
+
+      src = next_consecutive_mem (src);
+      dst = next_consecutive_mem (dst);
+    }
+
+  gcc_assert (len < 8);
+  if (len >= 4)
+    {
+      /* More than a word but less than a double-word to copy.  Copy a word.  */
+      reg0 = gen_reg_rtx (SImode);
+      src = adjust_address (src, SImode, 0);
+      dst = adjust_address (dst, SImode, 0);
+      if (src_aligned)
+        emit_move_insn (reg0, src);
+      else
+        emit_insn (gen_unaligned_loadsi (reg0, src));
+
+      if (dst_aligned)
+        emit_move_insn (dst, reg0);
+      else
+        emit_insn (gen_unaligned_storesi (dst, reg0));
+
+      src = next_consecutive_mem (src);
+      dst = next_consecutive_mem (dst);
+      len -= 4;
+    }
+
+  if (len == 0)
+    return true;
+
+  /* Copy the remaining bytes.  */
+  if (len >= 2)
+    {
+      dst = adjust_address (dst, HImode, 0);
+      src = adjust_address (src, HImode, 0);
+      reg0 = gen_reg_rtx (SImode);
+      if (src_aligned)
+        emit_insn (gen_zero_extendhisi2 (reg0, src));
+      else
+        emit_insn (gen_unaligned_loadhiu (reg0, src));
+
+      if (dst_aligned)
+        emit_insn (gen_movhi (dst, gen_lowpart(HImode, reg0)));
+      else
+        emit_insn (gen_unaligned_storehi (dst, gen_lowpart (HImode, reg0)));
+
+      src = next_consecutive_mem (src);
+      dst = next_consecutive_mem (dst);
+      if (len == 2)
+        return true;
+    }
+
+  dst = adjust_address (dst, QImode, 0);
+  src = adjust_address (src, QImode, 0);
+  reg0 = gen_reg_rtx (QImode);
+  emit_move_insn (reg0, src);
+  emit_move_insn (dst, reg0);
+  return true;
+}
+
 /* Select a dominance comparison mode if possible for a test of the general
    form (OP (COND_OR (X) (Y)) (const_int 0)).  We support three forms.
    COND_OR == DOM_CC_X_AND_Y => (X && Y)
@@ -12643,6 +12888,286 @@ operands_ok_ldrd_strd (rtx rt, rtx rt2,
   return true;
 }
 
+/* Helper for gen_operands_ldrd_strd.  Returns true iff the memory
+   operand MEM's address contains an immediate offset from the base
+   register and has no side effects, in which case it sets BASE and
+   OFFSET accordingly.  */
+static bool
+mem_ok_for_ldrd_strd (rtx mem, rtx *base, rtx *offset)
+{
+  rtx addr;
+
+  gcc_assert (base != NULL && offset != NULL);
+
+  /* TODO: Handle more general memory operand patterns, such as
+     PRE_DEC and PRE_INC.  */
+
+  if (side_effects_p (mem))
+    return false;
+
+  /* Can't deal with subregs.  */
+  if (GET_CODE (mem) == SUBREG)
+    return false;
+
+  gcc_assert (MEM_P (mem));
+
+  *offset = const0_rtx;
+
+  addr = XEXP (mem, 0);
+
+  /* If addr isn't valid for DImode, then we can't handle it.  */
+  if (!arm_legitimate_address_p (DImode, addr,
+				 reload_in_progress || reload_completed))
+    return false;
+
+  if (REG_P (addr))
+    {
+      *base = addr;
+      return true;
+    }
+  else if (GET_CODE (addr) == PLUS || GET_CODE (addr) == MINUS)
+    {
+      *base = XEXP (addr, 0);
+      *offset = XEXP (addr, 1);
+      return (REG_P (*base) && CONST_INT_P (*offset));
+    }
+
+  return false;
+}
+
+#define SWAP_RTX(x,y) do { rtx tmp = x; x = y; y = tmp; } while (0)
+
+/* Called from a peephole2 to replace two word-size accesses with a
+   single LDRD/STRD instruction.  Returns true iff we can generate a
+   new instruction sequence.  That is, both accesses use the same base
+   register and the gap between constant offsets is 4.  This function
+   may reorder its operands to match ldrd/strd RTL templates.
+   OPERANDS are the operands found by the peephole matcher;
+   OPERANDS[0,1] are register operands, and OPERANDS[2,3] are the
+   corresponding memory operands.  LOAD indicaates whether the access
+   is load or store.  CONST_STORE indicates a store of constant
+   integer values held in OPERANDS[4,5] and assumes that the pattern
+   is of length 4 insn, for the purpose of checking dead registers.
+   COMMUTE indicates that register operands may be reordered.  */
+bool
+gen_operands_ldrd_strd (rtx *operands, bool load,
+                        bool const_store, bool commute)
+{
+  int nops = 2;
+  HOST_WIDE_INT offsets[2], offset;
+  rtx base = NULL_RTX;
+  rtx cur_base, cur_offset, tmp;
+  int i, gap;
+  HARD_REG_SET regset;
+
+  gcc_assert (!const_store || !load);
+  /* Check that the memory references are immediate offsets from the
+     same base register.  Extract the base register, the destination
+     registers, and the corresponding memory offsets.  */
+  for (i = 0; i < nops; i++)
+    {
+      if (!mem_ok_for_ldrd_strd (operands[nops+i], &cur_base, &cur_offset))
+        return false;
+
+      if (i == 0)
+        base = cur_base;
+      else if (REGNO (base) != REGNO (cur_base))
+        return false;
+
+      offsets[i] = INTVAL (cur_offset);
+      if (GET_CODE (operands[i]) == SUBREG)
+        {
+          tmp = SUBREG_REG (operands[i]);
+          gcc_assert (GET_MODE (operands[i]) == GET_MODE (tmp));
+          operands[i] = tmp;
+        }
+    }
+
+  /* Make sure there is no dependency between the individual loads.  */
+  if (load && REGNO (operands[0]) == REGNO (base))
+    return false; /* RAW */
+
+  if (load && REGNO (operands[0]) == REGNO (operands[1]))
+    return false; /* WAW */
+
+  /* If the same input register is used in both stores
+     when storing different constants, try to find a free register.
+     For example, the code
+        mov r0, 0
+        str r0, [r2]
+        mov r0, 1
+        str r0, [r2, #4]
+     can be transformed into
+        mov r1, 0
+        strd r1, r0, [r2]
+     in Thumb mode assuming that r1 is free.  */
+  if (const_store
+      && REGNO (operands[0]) == REGNO (operands[1])
+      && INTVAL (operands[4]) != INTVAL (operands[5]))
+    {
+    if (TARGET_THUMB2)
+      {
+        CLEAR_HARD_REG_SET (regset);
+        tmp = peep2_find_free_register (0, 4, "r", SImode, &regset);
+        if (tmp == NULL_RTX)
+          return false;
+
+        /* Use the new register in the first load to ensure that
+           if the original input register is not dead after peephole,
+           then it will have the correct constant value.  */
+        operands[0] = tmp;
+      }
+    else if (TARGET_ARM)
+      {
+        return false;
+        int regno = REGNO (operands[0]);
+        if (!peep2_reg_dead_p (4, operands[0]))
+          {
+            /* When the input register is even and is not dead after the
+               pattern, it has to hold the second constant but we cannot
+               form a legal STRD in ARM mode with this register as the second
+               register.  */
+            if (regno % 2 == 0)
+              return false;
+
+            /* Is regno-1 free? */
+            SET_HARD_REG_SET (regset);
+            CLEAR_HARD_REG_BIT(regset, regno - 1);
+            tmp = peep2_find_free_register (0, 4, "r", SImode, &regset);
+            if (tmp == NULL_RTX)
+              return false;
+
+            operands[0] = tmp;
+          }
+        else
+          {
+            /* Find a DImode register.  */
+            CLEAR_HARD_REG_SET (regset);
+            tmp = peep2_find_free_register (0, 4, "r", DImode, &regset);
+            if (tmp != NULL_RTX)
+              {
+                operands[0] = simplify_gen_subreg (SImode, tmp, DImode, 0);
+                operands[1] = simplify_gen_subreg (SImode, tmp, DImode, 4);
+              }
+            else
+              {
+                /* Can we use the input register to form a DI register?  */
+                SET_HARD_REG_SET (regset);
+                CLEAR_HARD_REG_BIT(regset,
+                                   regno % 2 == 0 ? regno + 1 : regno - 1);
+                tmp = peep2_find_free_register (0, 4, "r", SImode, &regset);
+                if (tmp == NULL_RTX)
+                  return false;
+                operands[regno % 2 == 1 ? 0 : 1] = tmp;
+              }
+          }
+
+        gcc_assert (operands[0] != NULL_RTX);
+        gcc_assert (operands[1] != NULL_RTX);
+        gcc_assert (REGNO (operands[0]) % 2 == 0);
+        gcc_assert (REGNO (operands[1]) == REGNO (operands[0]) + 1);
+      }
+    }
+
+  /* Make sure the instructions are ordered with lower memory access first.  */
+  if (offsets[0] > offsets[1])
+    {
+      gap = offsets[0] - offsets[1];
+      offset = offsets[1];
+
+      /* Swap the instructions such that lower memory is accessed first.  */
+      SWAP_RTX (operands[0], operands[1]);
+      SWAP_RTX (operands[2], operands[3]);
+      if (const_store)
+        SWAP_RTX (operands[4], operands[5]);
+    }
+  else
+    {
+      gap = offsets[1] - offsets[0];
+      offset = offsets[0];
+    }
+
+  /* Make sure accesses are to consecutive memory locations.  */
+  if (gap != 4)
+    return false;
+
+  /* Make sure we generate legal instructions.  */
+  if (operands_ok_ldrd_strd (operands[0], operands[1], base, offset,
+                             false, load))
+    return true;
+
+  /* In Thumb state, where registers are almost unconstrained, there
+     is little hope to fix it.  */
+  if (TARGET_THUMB2)
+    return false;
+
+  if (load && commute)
+    {
+      /* Try reordering registers.  */
+      SWAP_RTX (operands[0], operands[1]);
+      if (operands_ok_ldrd_strd (operands[0], operands[1], base, offset,
+                                 false, load))
+        return true;
+    }
+
+  if (const_store)
+    {
+      /* If input registers are dead after this pattern, they can be
+         reordered or replaced by other registers that are free in the
+         current pattern.  */
+      if (!peep2_reg_dead_p (4, operands[0])
+          || !peep2_reg_dead_p (4, operands[1]))
+        return false;
+
+      /* Try to reorder the input registers.  */
+      /* For example, the code
+           mov r0, 0
+           mov r1, 1
+           str r1, [r2]
+           str r0, [r2, #4]
+         can be transformed into
+           mov r1, 0
+           mov r0, 1
+           strd r0, [r2]
+      */
+      if (operands_ok_ldrd_strd (operands[1], operands[0], base, offset,
+                                  false, false))
+        {
+          SWAP_RTX (operands[0], operands[1]);
+          return true;
+        }
+
+      /* Try to find a free DI register.  */
+      CLEAR_HARD_REG_SET (regset);
+      add_to_hard_reg_set (&regset, SImode, REGNO (operands[0]));
+      add_to_hard_reg_set (&regset, SImode, REGNO (operands[1]));
+      while (true)
+        {
+          tmp = peep2_find_free_register (0, 4, "r", DImode, &regset);
+          if (tmp == NULL_RTX)
+            return false;
+
+          /* DREG must be an even-numbered register in DImode.
+             Split it into SI registers.  */
+          operands[0] = simplify_gen_subreg (SImode, tmp, DImode, 0);
+          operands[1] = simplify_gen_subreg (SImode, tmp, DImode, 4);
+          gcc_assert (operands[0] != NULL_RTX);
+          gcc_assert (operands[1] != NULL_RTX);
+          gcc_assert (REGNO (operands[0]) % 2 == 0);
+          gcc_assert (REGNO (operands[0]) + 1 == REGNO (operands[1]));
+
+          return (operands_ok_ldrd_strd (operands[0], operands[1],
+                                         base, offset,
+                                         false, load));
+        }
+    }
+
+  return false;
+}
+#undef SWAP_RTX
+
+
+
 
 /* Print a symbolic form of X to the debug file, F.  */
 static void
@@ -13876,6 +14401,16 @@ thumb2_reorg (void)
 				   && IN_RANGE (INTVAL (op1), -7, 7))
 			    action = CONV;
 			}
+		      /* ADCS <Rd>, <Rn>  */
+		      else if (GET_CODE (XEXP (src, 0)) == PLUS
+			      && rtx_equal_p (XEXP (XEXP (src, 0), 0), dst)
+			      && low_register_operand (XEXP (XEXP (src, 0), 1),
+						       SImode)
+			      && COMPARISON_P (op1)
+			      && cc_register (XEXP (op1, 0), VOIDmode)
+			      && maybe_get_arm_condition_code (op1) == ARM_CS
+			      && XEXP (op1, 1) == const0_rtx)
+		        action = CONV;
 		      break;
 
 		    case MINUS:
@@ -14834,7 +15369,8 @@ output_move_double (rtx *operands, bool
     {
       /* Constraints should ensure this.  */
       gcc_assert (code0 == MEM && code1 == REG);
-      gcc_assert (REGNO (operands[1]) != IP_REGNUM);
+      gcc_assert ((REGNO (operands[1]) != IP_REGNUM)
+                  || (TARGET_ARM && TARGET_LDRD));
 
       switch (GET_CODE (XEXP (operands[0], 0)))
         {
@@ -16315,140 +16851,324 @@ arm_output_function_epilogue (FILE *file
     }
 }
 
-/* Generate and emit a pattern that will be recognized as STRD pattern.  If even
-   number of registers are being pushed, multiple STRD patterns are created for
-   all register pairs.  If odd number of registers are pushed, emit a
-   combination of STRDs and STR for the prologue saves.  */
+/* Generate and emit a sequence of insns equivalent to PUSH, but using
+   STR and STRD.  If an even number of registers are being pushed, one
+   or more STRD patterns are created for each register pair.  If an
+   odd number of registers are pushed, emit an initial STR followed by
+   as many STRD instructions as are needed.  This works best when the
+   stack is initially 64-bit aligned (the normal case), since it
+   ensures that each STRD is also 64-bit aligned.  */
 static void
 thumb2_emit_strd_push (unsigned long saved_regs_mask)
 {
   int num_regs = 0;
-  int i, j;
+  int i;
+  int regno;
   rtx par = NULL_RTX;
-  rtx insn = NULL_RTX;
   rtx dwarf = NULL_RTX;
-  rtx tmp, reg, tmp1;
-
-  for (i = 0; i <= LAST_ARM_REGNUM; i++)
-    if (saved_regs_mask & (1 << i))
-      num_regs++;
+  rtx tmp;
+  bool first = true;
 
-  gcc_assert (num_regs && num_regs <= 16);
+  num_regs = bit_count (saved_regs_mask);
 
-  /* Pre-decrement the stack pointer, based on there being num_regs 4-byte
-     registers to push.  */
-  tmp = gen_rtx_SET (VOIDmode,
-                     stack_pointer_rtx,
-                     plus_constant (Pmode, stack_pointer_rtx, -4 * num_regs));
-  RTX_FRAME_RELATED_P (tmp) = 1;
-  insn = emit_insn (tmp);
+  /* Must be at least one register to save, and can't save SP or PC.  */
+  gcc_assert (num_regs > 0 && num_regs <= 14);
+  gcc_assert (!(saved_regs_mask & (1 << SP_REGNUM)));
+  gcc_assert (!(saved_regs_mask & (1 << PC_REGNUM)));
 
-  /* Create sequence for DWARF info.  */
+  /* Create sequence for DWARF info.  All the frame-related data for
+     debugging is held in this wrapper.  */
   dwarf = gen_rtx_SEQUENCE (VOIDmode, rtvec_alloc (num_regs + 1));
 
-  /* RTLs cannot be shared, hence create new copy for dwarf.  */
-  tmp1 = gen_rtx_SET (VOIDmode,
-                     stack_pointer_rtx,
-                     plus_constant (Pmode, stack_pointer_rtx, -4 * num_regs));
-  RTX_FRAME_RELATED_P (tmp1) = 1;
-  XVECEXP (dwarf, 0, 0) = tmp1;
+  /* Describe the stack adjustment.  */
+  tmp = gen_rtx_SET (VOIDmode,
+		      stack_pointer_rtx,
+		      plus_constant (Pmode, stack_pointer_rtx, -4 * num_regs));
+  RTX_FRAME_RELATED_P (tmp) = 1;
+  XVECEXP (dwarf, 0, 0) = tmp;
 
-  gcc_assert (!(saved_regs_mask & (1 << SP_REGNUM)));
-  gcc_assert (!(saved_regs_mask & (1 << PC_REGNUM)));
+  /* Find the first register.  */
+  for (regno = 0; (saved_regs_mask & (1 << regno)) == 0; regno++)
+    ;
 
-  /* Var j iterates over all the registers to gather all the registers in
-     saved_regs_mask.  Var i gives index of register R_j in stack frame.
-     A PARALLEL RTX of register-pair is created here, so that pattern for
-     STRD can be matched.  If num_regs is odd, 1st register will be pushed
-     using STR and remaining registers will be pushed with STRD in pairs.
-     If num_regs is even, all registers are pushed with STRD in pairs.
-     Hence, skip first element for odd num_regs.  */
-  for (i = num_regs - 1, j = LAST_ARM_REGNUM; i >= (num_regs % 2); j--)
-    if (saved_regs_mask & (1 << j))
-      {
-        /* Create RTX for store.  New RTX is created for dwarf as
-           they are not sharable.  */
-        reg = gen_rtx_REG (SImode, j);
-        tmp = gen_rtx_SET (SImode,
-                           gen_frame_mem
-                           (SImode,
-                            plus_constant (Pmode, stack_pointer_rtx, 4 * i)),
-                           reg);
+  i = 0;
 
-        tmp1 = gen_rtx_SET (SImode,
-                           gen_frame_mem
-                           (SImode,
-                            plus_constant (Pmode, stack_pointer_rtx, 4 * i)),
-                           reg);
-        RTX_FRAME_RELATED_P (tmp) = 1;
-        RTX_FRAME_RELATED_P (tmp1) = 1;
+  /* If there's an odd number of registers to push.  Start off by
+     pushing a single register.  This ensures that subsequent strd
+     operations are dword aligned (assuming that SP was originally
+     64-bit aligned).  */
+  if ((num_regs & 1) != 0)
+    {
+      rtx reg, mem, insn;
 
-        if (((i - (num_regs % 2)) % 2) == 1)
-          /* When (i - (num_regs % 2)) is odd, the RTX to be emitted is yet to
-             be created.  Hence create it first.  The STRD pattern we are
-             generating is :
-             [ (SET (MEM (PLUS (SP) (NUM))) (reg_t1))
-               (SET (MEM (PLUS (SP) (NUM + 4))) (reg_t2)) ]
-             where the target registers need not be consecutive.  */
-          par = gen_rtx_PARALLEL (VOIDmode, rtvec_alloc (2));
+      reg = gen_rtx_REG (SImode, regno);
+      if (num_regs == 1)
+	mem = gen_frame_mem (Pmode, gen_rtx_PRE_DEC (Pmode,
+						     stack_pointer_rtx));
+      else
+	mem = gen_frame_mem (Pmode,
+			     gen_rtx_PRE_MODIFY
+			     (Pmode, stack_pointer_rtx,
+			      plus_constant (Pmode, stack_pointer_rtx,
+					     -4 * num_regs)));
 
-        /* Register R_j is added in PARALLEL RTX.  If (i - (num_regs % 2)) is
-           even, the reg_j is added as 0th element and if it is odd, reg_i is
-           added as 1st element of STRD pattern shown above.  */
-        XVECEXP (par, 0, ((i - (num_regs % 2)) % 2)) = tmp;
-        XVECEXP (dwarf, 0, (i + 1)) = tmp1;
-
-        if (((i - (num_regs % 2)) % 2) == 0)
-          /* When (i - (num_regs % 2)) is even, RTXs for both the registers
-             to be loaded are generated in above given STRD pattern, and the
-             pattern can be emitted now.  */
-          emit_insn (par);
+      tmp = gen_rtx_SET (VOIDmode, mem, reg);
+      RTX_FRAME_RELATED_P (tmp) = 1;
+      insn = emit_insn (tmp);
+      RTX_FRAME_RELATED_P (insn) = 1;
+      add_reg_note (insn, REG_FRAME_RELATED_EXPR, dwarf);
+      tmp = gen_rtx_SET (VOIDmode, gen_frame_mem (Pmode, stack_pointer_rtx),
+			 reg);
+      RTX_FRAME_RELATED_P (tmp) = 1;
+      i++;
+      regno++;
+      XVECEXP (dwarf, 0, i) = tmp;
+      first = false;
+    }
 
-        i--;
-      }
+  while (i < num_regs)
+    if (saved_regs_mask & (1 << regno))
+      {
+	rtx reg1, reg2, mem1, mem2;
+	rtx tmp0, tmp1, tmp2;
+	int regno2;
+
+	/* Find the register to pair with this one.  */
+	for (regno2 = regno + 1; (saved_regs_mask & (1 << regno2)) == 0;
+	     regno2++)
+	  ;
 
-  if ((num_regs % 2) == 1)
-    {
-      /* If odd number of registers are pushed, generate STR pattern to store
-         lone register.  */
-      for (; (saved_regs_mask & (1 << j)) == 0; j--);
+	reg1 = gen_rtx_REG (SImode, regno);
+	reg2 = gen_rtx_REG (SImode, regno2);
 
-      tmp1 = gen_frame_mem (SImode, plus_constant (Pmode,
-                                                   stack_pointer_rtx, 4 * i));
-      reg = gen_rtx_REG (SImode, j);
-      tmp = gen_rtx_SET (SImode, tmp1, reg);
-      RTX_FRAME_RELATED_P (tmp) = 1;
+	if (first)
+	  {
+	    rtx insn;
 
-      emit_insn (tmp);
+	    first = false;
+	    mem1 = gen_frame_mem (Pmode, plus_constant (Pmode,
+							stack_pointer_rtx,
+							-4 * num_regs));
+	    mem2 = gen_frame_mem (Pmode, plus_constant (Pmode,
+							stack_pointer_rtx,
+							-4 * (num_regs - 1)));
+	    tmp0 = gen_rtx_SET (VOIDmode, stack_pointer_rtx,
+				plus_constant (Pmode, stack_pointer_rtx,
+					       -4 * (num_regs)));
+	    tmp1 = gen_rtx_SET (VOIDmode, mem1, reg1);
+	    tmp2 = gen_rtx_SET (VOIDmode, mem2, reg2);
+	    RTX_FRAME_RELATED_P (tmp0) = 1;
+	    RTX_FRAME_RELATED_P (tmp1) = 1;
+	    RTX_FRAME_RELATED_P (tmp2) = 1;
+	    par = gen_rtx_PARALLEL (VOIDmode, rtvec_alloc (3));
+	    XVECEXP (par, 0, 0) = tmp0;
+	    XVECEXP (par, 0, 1) = tmp1;
+	    XVECEXP (par, 0, 2) = tmp2;
+	    insn = emit_insn (par);
+	    RTX_FRAME_RELATED_P (insn) = 1;
+	    add_reg_note (insn, REG_FRAME_RELATED_EXPR, dwarf);
+	  }
+	else
+	  {
+	    mem1 = gen_frame_mem (Pmode, plus_constant (Pmode,
+							stack_pointer_rtx,
+							4 * i));
+	    mem2 = gen_frame_mem (Pmode, plus_constant (Pmode,
+							stack_pointer_rtx,
+							4 * (i + 1)));
+	    tmp1 = gen_rtx_SET (VOIDmode, mem1, reg1);
+	    tmp2 = gen_rtx_SET (VOIDmode, mem2, reg2);
+	    RTX_FRAME_RELATED_P (tmp1) = 1;
+	    RTX_FRAME_RELATED_P (tmp2) = 1;
+	    par = gen_rtx_PARALLEL (VOIDmode, rtvec_alloc (2));
+	    XVECEXP (par, 0, 0) = tmp1;
+	    XVECEXP (par, 0, 1) = tmp2;
+	    emit_insn (par);
+	  }
 
-      tmp1 = gen_rtx_SET (SImode,
-                         gen_frame_mem
-                         (SImode,
-                          plus_constant (Pmode, stack_pointer_rtx, 4 * i)),
-                          reg);
-      RTX_FRAME_RELATED_P (tmp1) = 1;
-      XVECEXP (dwarf, 0, (i + 1)) = tmp1;
-    }
+	/* Create unwind information.  This is an approximation.  */
+	tmp1 = gen_rtx_SET (VOIDmode,
+			    gen_frame_mem (Pmode,
+					   plus_constant (Pmode,
+							  stack_pointer_rtx,
+							  4 * i)),
+			    reg1);
+	tmp2 = gen_rtx_SET (VOIDmode,
+			    gen_frame_mem (Pmode,
+					   plus_constant (Pmode,
+							  stack_pointer_rtx,
+							  4 * (i + 1))),
+			    reg2);
+
+	RTX_FRAME_RELATED_P (tmp1) = 1;
+	RTX_FRAME_RELATED_P (tmp2) = 1;
+	XVECEXP (dwarf, 0, i + 1) = tmp1;
+	XVECEXP (dwarf, 0, i + 2) = tmp2;
+	i += 2;
+	regno = regno2 + 1;
+      }
+    else
+      regno++;
 
-  add_reg_note (insn, REG_FRAME_RELATED_EXPR, dwarf);
-  RTX_FRAME_RELATED_P (insn) = 1;
   return;
 }
 
-/* Generate and emit an insn that we will recognize as a push_multi.
-   Unfortunately, since this insn does not reflect very well the actual
-   semantics of the operation, we need to annotate the insn for the benefit
-   of DWARF2 frame unwind information.  */
-static rtx
-emit_multi_reg_push (unsigned long mask)
+/* STRD in ARM mode requires consecutive registers.  This function emits STRD
+   whenever possible, otherwise it emits single-word stores.  The first store
+   also allocates stack space for all saved registers, using writeback with
+   post-addressing mode.  All other stores use offset addressing.  If no STRD
+   can be emitted, this function emits a sequence of single-word stores,
+   and not an STM as before, because single-word stores provide more freedom
+   scheduling and can be turned into an STM by peephole optimizations.  */
+static void
+arm_emit_strd_push (unsigned long saved_regs_mask)
 {
   int num_regs = 0;
-  int num_dwarf_regs;
-  int i, j;
-  rtx par;
-  rtx dwarf;
-  int dwarf_par_index;
-  rtx tmp, reg;
+  int i, j, dwarf_index  = 0;
+  int offset = 0;
+  rtx dwarf = NULL_RTX;
+  rtx insn = NULL_RTX;
+  rtx tmp, mem;
+
+  /* TODO: A more efficient code can be emitted by changing the
+     layout, e.g., first push all pairs that can use STRD to keep the
+     stack aligned, and then push all other registers.  */
+  for (i = 0; i <= LAST_ARM_REGNUM; i++)
+    if (saved_regs_mask & (1 << i))
+      num_regs++;
+
+  gcc_assert (!(saved_regs_mask & (1 << SP_REGNUM)));
+  gcc_assert (!(saved_regs_mask & (1 << PC_REGNUM)));
+  gcc_assert (num_regs > 0);
+
+  /* Create sequence for DWARF info.  */
+  dwarf = gen_rtx_SEQUENCE (VOIDmode, rtvec_alloc (num_regs + 1));
+
+  /* For dwarf info, we generate explicit stack update.  */
+  tmp = gen_rtx_SET (VOIDmode,
+                     stack_pointer_rtx,
+                     plus_constant (Pmode, stack_pointer_rtx, -4 * num_regs));
+  RTX_FRAME_RELATED_P (tmp) = 1;
+  XVECEXP (dwarf, 0, dwarf_index++) = tmp;
+
+  /* Save registers.  */
+  offset = - 4 * num_regs;
+  j = 0;
+  while (j <= LAST_ARM_REGNUM)
+    if (saved_regs_mask & (1 << j))
+      {
+        if ((j % 2 == 0)
+            && (saved_regs_mask & (1 << (j + 1))))
+          {
+            /* Current register and previous register form register pair for
+               which STRD can be generated.  */
+            if (offset < 0)
+              {
+                /* Allocate stack space for all saved registers.  */
+                tmp = plus_constant (Pmode, stack_pointer_rtx, offset);
+                tmp = gen_rtx_PRE_MODIFY (Pmode, stack_pointer_rtx, tmp);
+                mem = gen_frame_mem (DImode, tmp);
+                offset = 0;
+              }
+            else if (offset > 0)
+              mem = gen_frame_mem (DImode,
+                                   plus_constant (Pmode,
+                                                  stack_pointer_rtx,
+                                                  offset));
+            else
+              mem = gen_frame_mem (DImode, stack_pointer_rtx);
+
+            tmp = gen_rtx_SET (DImode, mem, gen_rtx_REG (DImode, j));
+            RTX_FRAME_RELATED_P (tmp) = 1;
+            tmp = emit_insn (tmp);
+
+            /* Record the first store insn.  */
+            if (dwarf_index == 1)
+              insn = tmp;
+
+            /* Generate dwarf info.  */
+            mem = gen_frame_mem (SImode,
+                                 plus_constant (Pmode,
+                                                stack_pointer_rtx,
+                                                offset));
+            tmp = gen_rtx_SET (SImode, mem, gen_rtx_REG (SImode, j));
+            RTX_FRAME_RELATED_P (tmp) = 1;
+            XVECEXP (dwarf, 0, dwarf_index++) = tmp;
+
+            mem = gen_frame_mem (SImode,
+                                 plus_constant (Pmode,
+                                                stack_pointer_rtx,
+                                                offset + 4));
+            tmp = gen_rtx_SET (SImode, mem, gen_rtx_REG (SImode, j + 1));
+            RTX_FRAME_RELATED_P (tmp) = 1;
+            XVECEXP (dwarf, 0, dwarf_index++) = tmp;
+
+            offset += 8;
+            j += 2;
+          }
+        else
+          {
+            /* Emit a single word store.  */
+            if (offset < 0)
+              {
+                /* Allocate stack space for all saved registers.  */
+                tmp = plus_constant (Pmode, stack_pointer_rtx, offset);
+                tmp = gen_rtx_PRE_MODIFY (Pmode, stack_pointer_rtx, tmp);
+                mem = gen_frame_mem (SImode, tmp);
+                offset = 0;
+              }
+            else if (offset > 0)
+              mem = gen_frame_mem (SImode,
+                                   plus_constant (Pmode,
+                                                  stack_pointer_rtx,
+                                                  offset));
+            else
+              mem = gen_frame_mem (SImode, stack_pointer_rtx);
+
+            tmp = gen_rtx_SET (SImode, mem, gen_rtx_REG (SImode, j));
+            RTX_FRAME_RELATED_P (tmp) = 1;
+            tmp = emit_insn (tmp);
+
+            /* Record the first store insn.  */
+            if (dwarf_index == 1)
+              insn = tmp;
+
+            /* Generate dwarf info.  */
+            mem = gen_frame_mem (SImode,
+                                 plus_constant(Pmode,
+                                               stack_pointer_rtx,
+                                               offset));
+            tmp = gen_rtx_SET (SImode, mem, gen_rtx_REG (SImode, j));
+            RTX_FRAME_RELATED_P (tmp) = 1;
+            XVECEXP (dwarf, 0, dwarf_index++) = tmp;
+
+            offset += 4;
+            j += 1;
+          }
+      }
+    else
+      j++;
+
+  /* Attach dwarf info to the first insn we generate.  */
+  gcc_assert (insn != NULL_RTX);
+  add_reg_note (insn, REG_FRAME_RELATED_EXPR, dwarf);
+  RTX_FRAME_RELATED_P (insn) = 1;
+}
+
+/* Generate and emit an insn that we will recognize as a push_multi.
+   Unfortunately, since this insn does not reflect very well the actual
+   semantics of the operation, we need to annotate the insn for the benefit
+   of DWARF2 frame unwind information.  */
+static rtx
+emit_multi_reg_push (unsigned long mask)
+{
+  int num_regs = 0;
+  int num_dwarf_regs;
+  int i, j;
+  rtx par;
+  rtx dwarf;
+  int dwarf_par_index;
+  rtx tmp, reg;
 
   for (i = 0; i <= LAST_ARM_REGNUM; i++)
     if (mask & (1 << i))
@@ -16577,6 +17297,19 @@ emit_multi_reg_push (unsigned long mask)
   return par;
 }
 
+/* Add a REG_CFA_ADJUST_CFA REG note to INSN.
+   SIZE is the offset to be adjusted.
+   DEST and SRC might be stack_pointer_rtx or hard_frame_pointer_rtx.  */
+static void
+arm_add_cfa_adjust_cfa_note (rtx insn, int size, rtx dest, rtx src)
+{
+  rtx dwarf;
+
+  RTX_FRAME_RELATED_P (insn) = 1;
+  dwarf = gen_rtx_SET (VOIDmode, dest, plus_constant (Pmode, src, size));
+  add_reg_note (insn, REG_CFA_ADJUST_CFA, dwarf);
+}
+
 /* Generate and emit an insn pattern that we will recognize as a pop_multi.
    SAVED_REGS_MASK shows which registers need to be restored.
 
@@ -16634,6 +17367,17 @@ arm_emit_multi_reg_pop (unsigned long sa
     if (saved_regs_mask & (1 << i))
       {
         reg = gen_rtx_REG (SImode, i);
+        if ((num_regs == 1) && emit_update && !return_in_pc)
+          {
+            /* Emit single load with writeback.  */
+            tmp = gen_frame_mem (SImode,
+                                 gen_rtx_POST_INC (Pmode,
+                                                   stack_pointer_rtx));
+            tmp = emit_insn (gen_rtx_SET (VOIDmode, reg, tmp));
+            REG_NOTES (tmp) = alloc_reg_note (REG_CFA_RESTORE, reg, dwarf);
+            return;
+          }
+
         tmp = gen_rtx_SET (VOIDmode,
                            reg,
                            gen_frame_mem
@@ -16656,6 +17400,9 @@ arm_emit_multi_reg_pop (unsigned long sa
     par = emit_insn (par);
 
   REG_NOTES (par) = dwarf;
+  if (!return_in_pc)
+    arm_add_cfa_adjust_cfa_note (par, UNITS_PER_WORD * num_regs,
+				 stack_pointer_rtx, stack_pointer_rtx);
 }
 
 /* Generate and emit an insn pattern that we will recognize as a pop_multi
@@ -16726,6 +17473,16 @@ arm_emit_vfp_multi_reg_pop (int first_re
 
   par = emit_insn (par);
   REG_NOTES (par) = dwarf;
+
+  /* Make sure cfa doesn't leave with IP_REGNUM to allow unwinding fron FP.  */
+  if (TARGET_VFP && REGNO (base_reg) == IP_REGNUM)
+    {
+      RTX_FRAME_RELATED_P (par) = 1;
+      add_reg_note (par, REG_CFA_DEF_CFA, hard_frame_pointer_rtx);
+    }
+  else
+    arm_add_cfa_adjust_cfa_note (par, 2 * UNITS_PER_WORD * num_regs,
+				 base_reg, base_reg);
 }
 
 /* Generate and emit a pattern that will be recognized as LDRD pattern.  If even
@@ -16801,6 +17558,7 @@ thumb2_emit_ldrd_pop (unsigned long save
                pattern can be emitted now.  */
             par = emit_insn (par);
             REG_NOTES (par) = dwarf;
+	    RTX_FRAME_RELATED_P (par) = 1;
           }
 
         i++;
@@ -16817,7 +17575,12 @@ thumb2_emit_ldrd_pop (unsigned long save
                      stack_pointer_rtx,
                      plus_constant (Pmode, stack_pointer_rtx, 4 * i));
   RTX_FRAME_RELATED_P (tmp) = 1;
-  emit_insn (tmp);
+  tmp = emit_insn (tmp);
+  if (!return_in_pc)
+    {
+      arm_add_cfa_adjust_cfa_note (tmp, UNITS_PER_WORD * i,
+				   stack_pointer_rtx, stack_pointer_rtx);
+    }
 
   dwarf = NULL_RTX;
 
@@ -16851,9 +17614,11 @@ thumb2_emit_ldrd_pop (unsigned long save
       else
         {
           par = emit_insn (tmp);
+	  REG_NOTES (par) = dwarf;
+	  arm_add_cfa_adjust_cfa_note (par, UNITS_PER_WORD,
+				       stack_pointer_rtx, stack_pointer_rtx);
         }
 
-      REG_NOTES (par) = dwarf;
     }
   else if ((num_regs % 2) == 1 && return_in_pc)
     {
@@ -16865,6 +17630,132 @@ thumb2_emit_ldrd_pop (unsigned long save
   return;
 }
 
+/* LDRD in ARM mode needs consecutive registers as operands.  This function
+   emits LDRD whenever possible, otherwise it emits single-word loads. It uses
+   offset addressing and then generates one separate stack udpate. This provides
+   more scheduling freedom, compared to writeback on every load.  However,
+   if the function returns using load into PC directly
+   (i.e., if PC is in SAVED_REGS_MASK), the stack needs to be updated
+   before the last load.  TODO: Add a peephole optimization to recognize
+   the new epilogue sequence as an LDM instruction whenever possible.  TODO: Add
+   peephole optimization to merge the load at stack-offset zero
+   with the stack update instruction using load with writeback
+   in post-index addressing mode.  */
+static void
+arm_emit_ldrd_pop (unsigned long saved_regs_mask)
+{
+  int j = 0;
+  int offset = 0;
+  rtx par = NULL_RTX;
+  rtx dwarf = NULL_RTX;
+  rtx tmp, mem;
+
+  /* Restore saved registers.  */
+  gcc_assert (!((saved_regs_mask & (1 << SP_REGNUM))));
+  j = 0;
+  while (j <= LAST_ARM_REGNUM)
+    if (saved_regs_mask & (1 << j))
+      {
+        if ((j % 2) == 0
+            && (saved_regs_mask & (1 << (j + 1)))
+            && (j + 1) != PC_REGNUM)
+          {
+            /* Current register and next register form register pair for which
+               LDRD can be generated. PC is always the last register popped, and
+               we handle it separately.  */
+            if (offset > 0)
+              mem = gen_frame_mem (DImode,
+                                   plus_constant (Pmode,
+                                                  stack_pointer_rtx,
+                                                  offset));
+            else
+              mem = gen_frame_mem (DImode, stack_pointer_rtx);
+
+            tmp = gen_rtx_SET (DImode, gen_rtx_REG (DImode, j), mem);
+            tmp = emit_insn (tmp);
+	    RTX_FRAME_RELATED_P (tmp) = 1;
+
+            /* Generate dwarf info.  */
+
+            dwarf = alloc_reg_note (REG_CFA_RESTORE,
+                                    gen_rtx_REG (SImode, j),
+                                    NULL_RTX);
+            dwarf = alloc_reg_note (REG_CFA_RESTORE,
+                                    gen_rtx_REG (SImode, j + 1),
+                                    dwarf);
+
+            REG_NOTES (tmp) = dwarf;
+
+            offset += 8;
+            j += 2;
+          }
+        else if (j != PC_REGNUM)
+          {
+            /* Emit a single word load.  */
+            if (offset > 0)
+              mem = gen_frame_mem (SImode,
+                                   plus_constant (Pmode,
+                                                  stack_pointer_rtx,
+                                                  offset));
+            else
+              mem = gen_frame_mem (SImode, stack_pointer_rtx);
+
+            tmp = gen_rtx_SET (SImode, gen_rtx_REG (SImode, j), mem);
+            tmp = emit_insn (tmp);
+	    RTX_FRAME_RELATED_P (tmp) = 1;
+
+            /* Generate dwarf info.  */
+            REG_NOTES (tmp) = alloc_reg_note (REG_CFA_RESTORE,
+                                              gen_rtx_REG (SImode, j),
+                                              NULL_RTX);
+
+            offset += 4;
+            j += 1;
+          }
+        else /* j == PC_REGNUM */
+          j++;
+      }
+    else
+      j++;
+
+  /* Update the stack.  */
+  if (offset > 0)
+    {
+      tmp = gen_rtx_SET (Pmode,
+                         stack_pointer_rtx,
+                         plus_constant (Pmode,
+                                        stack_pointer_rtx,
+                                        offset));
+      tmp = emit_insn (tmp);
+      arm_add_cfa_adjust_cfa_note (tmp, offset,
+				   stack_pointer_rtx, stack_pointer_rtx);
+      offset = 0;
+    }
+
+  if (saved_regs_mask & (1 << PC_REGNUM))
+    {
+      /* Only PC is to be popped.  */
+      par = gen_rtx_PARALLEL (VOIDmode, rtvec_alloc (2));
+      XVECEXP (par, 0, 0) = ret_rtx;
+      tmp = gen_rtx_SET (SImode,
+                         gen_rtx_REG (SImode, PC_REGNUM),
+                         gen_frame_mem (SImode,
+                                        gen_rtx_POST_INC (SImode,
+                                                          stack_pointer_rtx)));
+      RTX_FRAME_RELATED_P (tmp) = 1;
+      XVECEXP (par, 0, 1) = tmp;
+      par = emit_jump_insn (par);
+
+      /* Generate dwarf info.  */
+      dwarf = alloc_reg_note (REG_CFA_RESTORE,
+                              gen_rtx_REG (SImode, PC_REGNUM),
+                              NULL_RTX);
+      REG_NOTES (par) = dwarf;
+      arm_add_cfa_adjust_cfa_note (par, UNITS_PER_WORD,
+				   stack_pointer_rtx, stack_pointer_rtx);
+    }
+}
+
 /* Calculate the size of the return value that is passed in registers.  */
 static unsigned
 arm_size_return_regs (void)
@@ -16889,11 +17780,27 @@ thumb_force_lr_save (void)
 	     || df_regs_ever_live_p (LR_REGNUM));
 }
 
+/* We do not know if r3 will be available because
+   we do have an indirect tailcall happening in this
+   particular case.  */
+static bool
+is_indirect_tailcall_p (rtx call)
+{
+  rtx pat = PATTERN (call);
+
+  /* Indirect tail call.  */
+  pat = XVECEXP (pat, 0, 0);
+  if (GET_CODE (pat) == SET)
+    pat = SET_SRC (pat);
+
+  pat = XEXP (XEXP (pat, 0), 0);
+  return REG_P (pat);
+}
 
 /* Return true if r3 is used by any of the tail call insns in the
    current function.  */
 static bool
-any_sibcall_uses_r3 (void)
+any_sibcall_could_use_r3 (void)
 {
   edge_iterator ei;
   edge e;
@@ -16907,7 +17814,8 @@ any_sibcall_uses_r3 (void)
 	if (!CALL_P (call))
 	  call = prev_nonnote_nondebug_insn (call);
 	gcc_assert (CALL_P (call) && SIBLING_CALL_P (call));
-	if (find_regno_fusage (call, USE, 3))
+	if (find_regno_fusage (call, USE, 3)
+	    || is_indirect_tailcall_p (call))
 	  return true;
       }
   return false;
@@ -17074,9 +17982,11 @@ arm_get_frame_offsets (void)
 	  /* If it is safe to use r3, then do so.  This sometimes
 	     generates better code on Thumb-2 by avoiding the need to
 	     use 32-bit push/pop instructions.  */
- 	  if (! any_sibcall_uses_r3 ()
+          if (! any_sibcall_could_use_r3 ()
 	      && arm_size_return_regs () <= 12
-	      && (offsets->saved_regs_mask & (1 << 3)) == 0)
+	      && (offsets->saved_regs_mask & (1 << 3)) == 0
+              && (TARGET_THUMB2
+		  || !(TARGET_LDRD && current_tune->prefer_ldrd_strd)))
 	    {
 	      reg = 3;
 	    }
@@ -17509,6 +18419,12 @@ arm_expand_prologue (void)
             {
               thumb2_emit_strd_push (live_regs_mask);
             }
+          else if (TARGET_ARM
+                   && !TARGET_APCS_FRAME
+                   && !IS_INTERRUPT (func_type))
+            {
+              arm_emit_strd_push (live_regs_mask);
+            }
           else
             {
               insn = emit_multi_reg_push (live_regs_mask);
@@ -18786,6 +19702,13 @@ thumb2_final_prescan_insn (rtx insn)
   enum arm_cond_code code;
   int n;
   int mask;
+  int max;
+
+  /* Maximum number of conditionally executed instructions in a block
+     is minimum of the two max values: maximum allowed in an IT block
+     and maximum that is beneficial according to the cost model and tune.  */
+  max = (max_insns_skipped < MAX_INSN_PER_IT_BLOCK) ?
+    max_insns_skipped : MAX_INSN_PER_IT_BLOCK;
 
   /* Remove the previous insn from the count of insns to be output.  */
   if (arm_condexec_count)
@@ -18828,9 +19751,9 @@ thumb2_final_prescan_insn (rtx insn)
       /* ??? Recognize conditional jumps, and combine them with IT blocks.  */
       if (GET_CODE (body) != COND_EXEC)
 	break;
-      /* Allow up to 4 conditionally executed instructions in a block.  */
+      /* Maximum number of conditionally executed instructions in a block.  */
       n = get_attr_ce_count (insn);
-      if (arm_condexec_masklen + n > 4)
+      if (arm_condexec_masklen + n > max)
 	break;
 
       predicate = COND_EXEC_TEST (body);
@@ -19388,6 +20311,7 @@ arm_debugger_arg_offset (int value, rtx
 typedef enum {
   T_V8QI,
   T_V4HI,
+  T_V4HF,
   T_V2SI,
   T_V2SF,
   T_DI,
@@ -19405,14 +20329,15 @@ typedef enum {
 #define TYPE_MODE_BIT(X) (1 << (X))
 
 #define TB_DREG (TYPE_MODE_BIT (T_V8QI) | TYPE_MODE_BIT (T_V4HI)	\
-		 | TYPE_MODE_BIT (T_V2SI) | TYPE_MODE_BIT (T_V2SF)	\
-		 | TYPE_MODE_BIT (T_DI))
+		 | TYPE_MODE_BIT (T_V4HF) | TYPE_MODE_BIT (T_V2SI)	\
+		 | TYPE_MODE_BIT (T_V2SF) | TYPE_MODE_BIT (T_DI))
 #define TB_QREG (TYPE_MODE_BIT (T_V16QI) | TYPE_MODE_BIT (T_V8HI)	\
 		 | TYPE_MODE_BIT (T_V4SI) | TYPE_MODE_BIT (T_V4SF)	\
 		 | TYPE_MODE_BIT (T_V2DI) | TYPE_MODE_BIT (T_TI))
 
 #define v8qi_UP  T_V8QI
 #define v4hi_UP  T_V4HI
+#define v4hf_UP  T_V4HF
 #define v2si_UP  T_V2SI
 #define v2sf_UP  T_V2SF
 #define di_UP    T_DI
@@ -19448,6 +20373,8 @@ typedef enum {
   NEON_SCALARMULH,
   NEON_SCALARMAC,
   NEON_CONVERT,
+  NEON_FLOAT_WIDEN,
+  NEON_FLOAT_NARROW,
   NEON_FIXCONV,
   NEON_SELECT,
   NEON_RESULTPAIR,
@@ -19508,7 +20435,8 @@ typedef struct {
   VAR9 (T, N, A, B, C, D, E, F, G, H, I), \
   {#N, NEON_##T, UP (J), CF (N, J), 0}
 
-/* The mode entries in the following table correspond to the "key" type of the
+/* The NEON builtin data can be found in arm_neon_builtins.def.
+   The mode entries in the following table correspond to the "key" type of the
    instruction variant, i.e. equivalent to that which would be specified after
    the assembler mnemonic, which usually refers to the last vector operand.
    (Signed/unsigned/polynomial types are not differentiated between though, and
@@ -19518,196 +20446,7 @@ typedef struct {
 
 static neon_builtin_datum neon_builtin_data[] =
 {
-  VAR10 (BINOP, vadd,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR3 (BINOP, vaddl, v8qi, v4hi, v2si),
-  VAR3 (BINOP, vaddw, v8qi, v4hi, v2si),
-  VAR6 (BINOP, vhadd, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR8 (BINOP, vqadd, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR3 (BINOP, vaddhn, v8hi, v4si, v2di),
-  VAR8 (BINOP, vmul, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR8 (TERNOP, vmla, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR3 (TERNOP, vmlal, v8qi, v4hi, v2si),
-  VAR2 (TERNOP, vfma, v2sf, v4sf),
-  VAR2 (TERNOP, vfms, v2sf, v4sf),
-  VAR8 (TERNOP, vmls, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR3 (TERNOP, vmlsl, v8qi, v4hi, v2si),
-  VAR4 (BINOP, vqdmulh, v4hi, v2si, v8hi, v4si),
-  VAR2 (TERNOP, vqdmlal, v4hi, v2si),
-  VAR2 (TERNOP, vqdmlsl, v4hi, v2si),
-  VAR3 (BINOP, vmull, v8qi, v4hi, v2si),
-  VAR2 (SCALARMULL, vmull_n, v4hi, v2si),
-  VAR2 (LANEMULL, vmull_lane, v4hi, v2si),
-  VAR2 (SCALARMULL, vqdmull_n, v4hi, v2si),
-  VAR2 (LANEMULL, vqdmull_lane, v4hi, v2si),
-  VAR4 (SCALARMULH, vqdmulh_n, v4hi, v2si, v8hi, v4si),
-  VAR4 (LANEMULH, vqdmulh_lane, v4hi, v2si, v8hi, v4si),
-  VAR2 (BINOP, vqdmull, v4hi, v2si),
-  VAR8 (BINOP, vshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR8 (BINOP, vqshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR8 (SHIFTIMM, vshr_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR3 (SHIFTIMM, vshrn_n, v8hi, v4si, v2di),
-  VAR3 (SHIFTIMM, vqshrn_n, v8hi, v4si, v2di),
-  VAR3 (SHIFTIMM, vqshrun_n, v8hi, v4si, v2di),
-  VAR8 (SHIFTIMM, vshl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR8 (SHIFTIMM, vqshl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR8 (SHIFTIMM, vqshlu_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR3 (SHIFTIMM, vshll_n, v8qi, v4hi, v2si),
-  VAR8 (SHIFTACC, vsra_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR10 (BINOP, vsub,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR3 (BINOP, vsubl, v8qi, v4hi, v2si),
-  VAR3 (BINOP, vsubw, v8qi, v4hi, v2si),
-  VAR8 (BINOP, vqsub, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR6 (BINOP, vhsub, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR3 (BINOP, vsubhn, v8hi, v4si, v2di),
-  VAR8 (BINOP, vceq, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR8 (BINOP, vcge, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR6 (BINOP, vcgeu, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR8 (BINOP, vcgt, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR6 (BINOP, vcgtu, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR2 (BINOP, vcage, v2sf, v4sf),
-  VAR2 (BINOP, vcagt, v2sf, v4sf),
-  VAR6 (BINOP, vtst, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR8 (BINOP, vabd, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR3 (BINOP, vabdl, v8qi, v4hi, v2si),
-  VAR6 (TERNOP, vaba, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR3 (TERNOP, vabal, v8qi, v4hi, v2si),
-  VAR8 (BINOP, vmax, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR8 (BINOP, vmin, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR4 (BINOP, vpadd, v8qi, v4hi, v2si, v2sf),
-  VAR6 (UNOP, vpaddl, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR6 (BINOP, vpadal, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR4 (BINOP, vpmax, v8qi, v4hi, v2si, v2sf),
-  VAR4 (BINOP, vpmin, v8qi, v4hi, v2si, v2sf),
-  VAR2 (BINOP, vrecps, v2sf, v4sf),
-  VAR2 (BINOP, vrsqrts, v2sf, v4sf),
-  VAR8 (SHIFTINSERT, vsri_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR8 (SHIFTINSERT, vsli_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
-  VAR8 (UNOP, vabs, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR6 (UNOP, vqabs, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR8 (UNOP, vneg, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR6 (UNOP, vqneg, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR6 (UNOP, vcls, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR6 (UNOP, vclz, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  VAR2 (UNOP, vcnt, v8qi, v16qi),
-  VAR4 (UNOP, vrecpe, v2si, v2sf, v4si, v4sf),
-  VAR4 (UNOP, vrsqrte, v2si, v2sf, v4si, v4sf),
-  VAR6 (UNOP, vmvn, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
-  /* FIXME: vget_lane supports more variants than this!  */
-  VAR10 (GETLANE, vget_lane,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (SETLANE, vset_lane,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR5 (CREATE, vcreate, v8qi, v4hi, v2si, v2sf, di),
-  VAR10 (DUP, vdup_n,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (DUPLANE, vdup_lane,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR5 (COMBINE, vcombine, v8qi, v4hi, v2si, v2sf, di),
-  VAR5 (SPLIT, vget_high, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR5 (SPLIT, vget_low, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR3 (UNOP, vmovn, v8hi, v4si, v2di),
-  VAR3 (UNOP, vqmovn, v8hi, v4si, v2di),
-  VAR3 (UNOP, vqmovun, v8hi, v4si, v2di),
-  VAR3 (UNOP, vmovl, v8qi, v4hi, v2si),
-  VAR6 (LANEMUL, vmul_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR6 (LANEMAC, vmla_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR2 (LANEMAC, vmlal_lane, v4hi, v2si),
-  VAR2 (LANEMAC, vqdmlal_lane, v4hi, v2si),
-  VAR6 (LANEMAC, vmls_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR2 (LANEMAC, vmlsl_lane, v4hi, v2si),
-  VAR2 (LANEMAC, vqdmlsl_lane, v4hi, v2si),
-  VAR6 (SCALARMUL, vmul_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR6 (SCALARMAC, vmla_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR2 (SCALARMAC, vmlal_n, v4hi, v2si),
-  VAR2 (SCALARMAC, vqdmlal_n, v4hi, v2si),
-  VAR6 (SCALARMAC, vmls_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR2 (SCALARMAC, vmlsl_n, v4hi, v2si),
-  VAR2 (SCALARMAC, vqdmlsl_n, v4hi, v2si),
-  VAR10 (BINOP, vext,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR8 (UNOP, vrev64, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR4 (UNOP, vrev32, v8qi, v4hi, v16qi, v8hi),
-  VAR2 (UNOP, vrev16, v8qi, v16qi),
-  VAR4 (CONVERT, vcvt, v2si, v2sf, v4si, v4sf),
-  VAR4 (FIXCONV, vcvt_n, v2si, v2sf, v4si, v4sf),
-  VAR10 (SELECT, vbsl,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR2 (RINT, vrintn, v2sf, v4sf),
-  VAR2 (RINT, vrinta, v2sf, v4sf),
-  VAR2 (RINT, vrintp, v2sf, v4sf),
-  VAR2 (RINT, vrintm, v2sf, v4sf),
-  VAR2 (RINT, vrintz, v2sf, v4sf),
-  VAR2 (RINT, vrintx, v2sf, v4sf),
-  VAR1 (VTBL, vtbl1, v8qi),
-  VAR1 (VTBL, vtbl2, v8qi),
-  VAR1 (VTBL, vtbl3, v8qi),
-  VAR1 (VTBL, vtbl4, v8qi),
-  VAR1 (VTBX, vtbx1, v8qi),
-  VAR1 (VTBX, vtbx2, v8qi),
-  VAR1 (VTBX, vtbx3, v8qi),
-  VAR1 (VTBX, vtbx4, v8qi),
-  VAR8 (RESULTPAIR, vtrn, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR8 (RESULTPAIR, vzip, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR8 (RESULTPAIR, vuzp, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
-  VAR5 (REINTERP, vreinterpretv8qi, v8qi, v4hi, v2si, v2sf, di),
-  VAR5 (REINTERP, vreinterpretv4hi, v8qi, v4hi, v2si, v2sf, di),
-  VAR5 (REINTERP, vreinterpretv2si, v8qi, v4hi, v2si, v2sf, di),
-  VAR5 (REINTERP, vreinterpretv2sf, v8qi, v4hi, v2si, v2sf, di),
-  VAR5 (REINTERP, vreinterpretdi, v8qi, v4hi, v2si, v2sf, di),
-  VAR5 (REINTERP, vreinterpretv16qi, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR5 (REINTERP, vreinterpretv8hi, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR5 (REINTERP, vreinterpretv4si, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR5 (REINTERP, vreinterpretv4sf, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR5 (REINTERP, vreinterpretv2di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (LOAD1, vld1,
-         v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (LOAD1LANE, vld1_lane,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (LOAD1, vld1_dup,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (STORE1, vst1,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (STORE1LANE, vst1_lane,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR9 (LOADSTRUCT,
-	vld2, v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
-  VAR7 (LOADSTRUCTLANE, vld2_lane,
-	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR5 (LOADSTRUCT, vld2_dup, v8qi, v4hi, v2si, v2sf, di),
-  VAR9 (STORESTRUCT, vst2,
-	v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
-  VAR7 (STORESTRUCTLANE, vst2_lane,
-	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR9 (LOADSTRUCT,
-	vld3, v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
-  VAR7 (LOADSTRUCTLANE, vld3_lane,
-	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR5 (LOADSTRUCT, vld3_dup, v8qi, v4hi, v2si, v2sf, di),
-  VAR9 (STORESTRUCT, vst3,
-	v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
-  VAR7 (STORESTRUCTLANE, vst3_lane,
-	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR9 (LOADSTRUCT, vld4,
-	v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
-  VAR7 (LOADSTRUCTLANE, vld4_lane,
-	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR5 (LOADSTRUCT, vld4_dup, v8qi, v4hi, v2si, v2sf, di),
-  VAR9 (STORESTRUCT, vst4,
-	v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
-  VAR7 (STORESTRUCTLANE, vst4_lane,
-	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
-  VAR10 (LOGICBINOP, vand,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (LOGICBINOP, vorr,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (BINOP, veor,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (LOGICBINOP, vbic,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
-  VAR10 (LOGICBINOP, vorn,
-	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di)
+#include "arm_neon_builtins.def"
 };
 
 #undef CF
@@ -19722,9 +20461,36 @@ static neon_builtin_datum neon_builtin_d
 #undef VAR9
 #undef VAR10
 
-/* Neon defines builtins from ARM_BUILTIN_MAX upwards, though they don't have
-   symbolic names defined here (which would require too much duplication).
-   FIXME?  */
+#define CF(N,X) ARM_BUILTIN_NEON_##N##X
+#define VAR1(T, N, A) \
+  CF (N, A)
+#define VAR2(T, N, A, B) \
+  VAR1 (T, N, A), \
+  CF (N, B)
+#define VAR3(T, N, A, B, C) \
+  VAR2 (T, N, A, B), \
+  CF (N, C)
+#define VAR4(T, N, A, B, C, D) \
+  VAR3 (T, N, A, B, C), \
+  CF (N, D)
+#define VAR5(T, N, A, B, C, D, E) \
+  VAR4 (T, N, A, B, C, D), \
+  CF (N, E)
+#define VAR6(T, N, A, B, C, D, E, F) \
+  VAR5 (T, N, A, B, C, D, E), \
+  CF (N, F)
+#define VAR7(T, N, A, B, C, D, E, F, G) \
+  VAR6 (T, N, A, B, C, D, E, F), \
+  CF (N, G)
+#define VAR8(T, N, A, B, C, D, E, F, G, H) \
+  VAR7 (T, N, A, B, C, D, E, F, G), \
+  CF (N, H)
+#define VAR9(T, N, A, B, C, D, E, F, G, H, I) \
+  VAR8 (T, N, A, B, C, D, E, F, G, H), \
+  CF (N, I)
+#define VAR10(T, N, A, B, C, D, E, F, G, H, I, J) \
+  VAR9 (T, N, A, B, C, D, E, F, G, H, I), \
+  CF (N, J)
 enum arm_builtins
 {
   ARM_BUILTIN_GETWCGR0,
@@ -19973,13 +20739,54 @@ enum arm_builtins
 
   ARM_BUILTIN_WMERGE,
 
-  ARM_BUILTIN_NEON_BASE,
+  ARM_BUILTIN_CRC32B,
+  ARM_BUILTIN_CRC32H,
+  ARM_BUILTIN_CRC32W,
+  ARM_BUILTIN_CRC32CB,
+  ARM_BUILTIN_CRC32CH,
+  ARM_BUILTIN_CRC32CW,
+
+#undef CRYPTO1
+#undef CRYPTO2
+#undef CRYPTO3
+
+#define CRYPTO1(L, U, M1, M2) \
+  ARM_BUILTIN_CRYPTO_##U,
+#define CRYPTO2(L, U, M1, M2, M3) \
+  ARM_BUILTIN_CRYPTO_##U,
+#define CRYPTO3(L, U, M1, M2, M3, M4) \
+  ARM_BUILTIN_CRYPTO_##U,
+
+#include "crypto.def"
+
+#undef CRYPTO1
+#undef CRYPTO2
+#undef CRYPTO3
 
-  ARM_BUILTIN_MAX = ARM_BUILTIN_NEON_BASE + ARRAY_SIZE (neon_builtin_data)
+#include "arm_neon_builtins.def"
+
+  ,ARM_BUILTIN_MAX
 };
 
+#define ARM_BUILTIN_NEON_BASE (ARM_BUILTIN_MAX - ARRAY_SIZE (neon_builtin_data))
+
+#undef CF
+#undef VAR1
+#undef VAR2
+#undef VAR3
+#undef VAR4
+#undef VAR5
+#undef VAR6
+#undef VAR7
+#undef VAR8
+#undef VAR9
+#undef VAR10
+
 static GTY(()) tree arm_builtin_decls[ARM_BUILTIN_MAX];
 
+#define NUM_DREG_TYPES 5
+#define NUM_QREG_TYPES 6
+
 static void
 arm_init_neon_builtins (void)
 {
@@ -19988,10 +20795,12 @@ arm_init_neon_builtins (void)
 
   tree neon_intQI_type_node;
   tree neon_intHI_type_node;
+  tree neon_floatHF_type_node;
   tree neon_polyQI_type_node;
   tree neon_polyHI_type_node;
   tree neon_intSI_type_node;
   tree neon_intDI_type_node;
+  tree neon_intUTI_type_node;
   tree neon_float_type_node;
 
   tree intQI_pointer_node;
@@ -20014,6 +20823,7 @@ arm_init_neon_builtins (void)
 
   tree V8QI_type_node;
   tree V4HI_type_node;
+  tree V4HF_type_node;
   tree V2SI_type_node;
   tree V2SF_type_node;
   tree V16QI_type_node;
@@ -20053,9 +20863,9 @@ arm_init_neon_builtins (void)
   tree void_ftype_pv4sf_v4sf_v4sf;
   tree void_ftype_pv2di_v2di_v2di;
 
-  tree reinterp_ftype_dreg[5][5];
-  tree reinterp_ftype_qreg[5][5];
-  tree dreg_types[5], qreg_types[5];
+  tree reinterp_ftype_dreg[NUM_DREG_TYPES][NUM_DREG_TYPES];
+  tree reinterp_ftype_qreg[NUM_QREG_TYPES][NUM_QREG_TYPES];
+  tree dreg_types[NUM_DREG_TYPES], qreg_types[NUM_QREG_TYPES];
 
   /* Create distinguished type nodes for NEON vector element types,
      and pointers to values of such types, so we can detect them later.  */
@@ -20068,6 +20878,9 @@ arm_init_neon_builtins (void)
   neon_float_type_node = make_node (REAL_TYPE);
   TYPE_PRECISION (neon_float_type_node) = FLOAT_TYPE_SIZE;
   layout_type (neon_float_type_node);
+  neon_floatHF_type_node = make_node (REAL_TYPE);
+  TYPE_PRECISION (neon_floatHF_type_node) = GET_MODE_PRECISION (HFmode);
+  layout_type (neon_floatHF_type_node);
 
   /* Define typedefs which exactly correspond to the modes we are basing vector
      types on.  If you change these names you'll need to change
@@ -20076,6 +20889,8 @@ arm_init_neon_builtins (void)
 					     "__builtin_neon_qi");
   (*lang_hooks.types.register_builtin_type) (neon_intHI_type_node,
 					     "__builtin_neon_hi");
+  (*lang_hooks.types.register_builtin_type) (neon_floatHF_type_node,
+					     "__builtin_neon_hf");
   (*lang_hooks.types.register_builtin_type) (neon_intSI_type_node,
 					     "__builtin_neon_si");
   (*lang_hooks.types.register_builtin_type) (neon_float_type_node,
@@ -20117,6 +20932,8 @@ arm_init_neon_builtins (void)
     build_vector_type_for_mode (neon_intQI_type_node, V8QImode);
   V4HI_type_node =
     build_vector_type_for_mode (neon_intHI_type_node, V4HImode);
+  V4HF_type_node =
+    build_vector_type_for_mode (neon_floatHF_type_node, V4HFmode);
   V2SI_type_node =
     build_vector_type_for_mode (neon_intSI_type_node, V2SImode);
   V2SF_type_node =
@@ -20138,6 +20955,8 @@ arm_init_neon_builtins (void)
   intUHI_type_node = make_unsigned_type (GET_MODE_PRECISION (HImode));
   intUSI_type_node = make_unsigned_type (GET_MODE_PRECISION (SImode));
   intUDI_type_node = make_unsigned_type (GET_MODE_PRECISION (DImode));
+  neon_intUTI_type_node = make_unsigned_type (GET_MODE_PRECISION (TImode));
+
 
   (*lang_hooks.types.register_builtin_type) (intUQI_type_node,
 					     "__builtin_neon_uqi");
@@ -20147,6 +20966,10 @@ arm_init_neon_builtins (void)
 					     "__builtin_neon_usi");
   (*lang_hooks.types.register_builtin_type) (intUDI_type_node,
 					     "__builtin_neon_udi");
+  (*lang_hooks.types.register_builtin_type) (intUDI_type_node,
+					     "__builtin_neon_poly64");
+  (*lang_hooks.types.register_builtin_type) (neon_intUTI_type_node,
+					     "__builtin_neon_poly128");
 
   /* Opaque integer types for structures of vectors.  */
   intEI_type_node = make_signed_type (GET_MODE_PRECISION (EImode));
@@ -20208,6 +21031,80 @@ arm_init_neon_builtins (void)
     build_function_type_list (void_type_node, V2DI_pointer_node, V2DI_type_node,
 			      V2DI_type_node, NULL);
 
+  if (TARGET_CRYPTO && TARGET_HARD_FLOAT)
+  {
+    tree V4USI_type_node =
+      build_vector_type_for_mode (intUSI_type_node, V4SImode);
+
+    tree V16UQI_type_node =
+      build_vector_type_for_mode (intUQI_type_node, V16QImode);
+
+    tree v16uqi_ftype_v16uqi
+      = build_function_type_list (V16UQI_type_node, V16UQI_type_node, NULL_TREE);
+
+    tree v16uqi_ftype_v16uqi_v16uqi
+      = build_function_type_list (V16UQI_type_node, V16UQI_type_node,
+                                  V16UQI_type_node, NULL_TREE);
+
+    tree v4usi_ftype_v4usi
+      = build_function_type_list (V4USI_type_node, V4USI_type_node, NULL_TREE);
+
+    tree v4usi_ftype_v4usi_v4usi
+      = build_function_type_list (V4USI_type_node, V4USI_type_node,
+                                  V4USI_type_node, NULL_TREE);
+
+    tree v4usi_ftype_v4usi_v4usi_v4usi
+      = build_function_type_list (V4USI_type_node, V4USI_type_node,
+                                  V4USI_type_node, V4USI_type_node, NULL_TREE);
+
+    tree uti_ftype_udi_udi
+      = build_function_type_list (neon_intUTI_type_node, intUDI_type_node,
+                                  intUDI_type_node, NULL_TREE);
+
+    #undef CRYPTO1
+    #undef CRYPTO2
+    #undef CRYPTO3
+    #undef C
+    #undef N
+    #undef CF
+    #undef FT1
+    #undef FT2
+    #undef FT3
+
+    #define C(U) \
+      ARM_BUILTIN_CRYPTO_##U
+    #define N(L) \
+      "__builtin_arm_crypto_"#L
+    #define FT1(R, A) \
+      R##_ftype_##A
+    #define FT2(R, A1, A2) \
+      R##_ftype_##A1##_##A2
+    #define FT3(R, A1, A2, A3) \
+      R##_ftype_##A1##_##A2##_##A3
+    #define CRYPTO1(L, U, R, A) \
+      arm_builtin_decls[C (U)] = add_builtin_function (N (L), FT1 (R, A), \
+                                                       C (U), BUILT_IN_MD, \
+                                                       NULL, NULL_TREE);
+    #define CRYPTO2(L, U, R, A1, A2) \
+      arm_builtin_decls[C (U)] = add_builtin_function (N (L), FT2 (R, A1, A2), \
+                                                       C (U), BUILT_IN_MD, \
+                                                       NULL, NULL_TREE);
+
+    #define CRYPTO3(L, U, R, A1, A2, A3) \
+      arm_builtin_decls[C (U)] = add_builtin_function (N (L), FT3 (R, A1, A2, A3), \
+                                                       C (U), BUILT_IN_MD, \
+                                                       NULL, NULL_TREE);
+    #include "crypto.def"
+
+    #undef CRYPTO1
+    #undef CRYPTO2
+    #undef CRYPTO3
+    #undef C
+    #undef N
+    #undef FT1
+    #undef FT2
+    #undef FT3
+  }
   dreg_types[0] = V8QI_type_node;
   dreg_types[1] = V4HI_type_node;
   dreg_types[2] = V2SI_type_node;
@@ -20219,14 +21116,17 @@ arm_init_neon_builtins (void)
   qreg_types[2] = V4SI_type_node;
   qreg_types[3] = V4SF_type_node;
   qreg_types[4] = V2DI_type_node;
+  qreg_types[5] = neon_intUTI_type_node;
 
-  for (i = 0; i < 5; i++)
+  for (i = 0; i < NUM_QREG_TYPES; i++)
     {
       int j;
-      for (j = 0; j < 5; j++)
+      for (j = 0; j < NUM_QREG_TYPES; j++)
         {
-          reinterp_ftype_dreg[i][j]
-            = build_function_type_list (dreg_types[i], dreg_types[j], NULL);
+          if (i < NUM_DREG_TYPES && j < NUM_DREG_TYPES)
+            reinterp_ftype_dreg[i][j]
+              = build_function_type_list (dreg_types[i], dreg_types[j], NULL);
+
           reinterp_ftype_qreg[i][j]
             = build_function_type_list (qreg_types[i], qreg_types[j], NULL);
         }
@@ -20239,7 +21139,7 @@ arm_init_neon_builtins (void)
       neon_builtin_datum *d = &neon_builtin_data[i];
 
       const char* const modenames[] = {
-	"v8qi", "v4hi", "v2si", "v2sf", "di",
+	"v8qi", "v4hi", "v4hf", "v2si", "v2sf", "di",
 	"v16qi", "v8hi", "v4si", "v4sf", "v2di",
 	"ti", "ei", "oi"
       };
@@ -20441,9 +21341,14 @@ arm_init_neon_builtins (void)
 
 	case NEON_REINTERP:
 	  {
-	    /* We iterate over 5 doubleword types, then 5 quadword
-	       types.  */
-	    int rhs = d->mode % 5;
+	    /* We iterate over NUM_DREG_TYPES doubleword types,
+	       then NUM_QREG_TYPES quadword  types.
+	       V4HF is not a type used in reinterpret, so we translate
+	       d->mode to the correct index in reinterp_ftype_dreg.  */
+	    bool qreg_p
+	      = GET_MODE_SIZE (insn_data[d->code].operand[0].mode) > 8;
+	    int rhs = (d->mode - ((!qreg_p && (d->mode > T_V4HF)) ? 1 : 0))
+	              % NUM_QREG_TYPES;
 	    switch (insn_data[d->code].operand[0].mode)
 	      {
 	      case V8QImode: ftype = reinterp_ftype_dreg[0][rhs]; break;
@@ -20456,11 +21361,43 @@ arm_init_neon_builtins (void)
 	      case V4SImode: ftype = reinterp_ftype_qreg[2][rhs]; break;
 	      case V4SFmode: ftype = reinterp_ftype_qreg[3][rhs]; break;
 	      case V2DImode: ftype = reinterp_ftype_qreg[4][rhs]; break;
+	      case TImode: ftype = reinterp_ftype_qreg[5][rhs]; break;
 	      default: gcc_unreachable ();
 	      }
 	  }
 	  break;
+	case NEON_FLOAT_WIDEN:
+	  {
+	    tree eltype = NULL_TREE;
+	    tree return_type = NULL_TREE;
+
+	    switch (insn_data[d->code].operand[1].mode)
+	    {
+	      case V4HFmode:
+	        eltype = V4HF_type_node;
+	        return_type = V4SF_type_node;
+	        break;
+	      default: gcc_unreachable ();
+	    }
+	    ftype = build_function_type_list (return_type, eltype, NULL);
+	    break;
+	  }
+	case NEON_FLOAT_NARROW:
+	  {
+	    tree eltype = NULL_TREE;
+	    tree return_type = NULL_TREE;
 
+	    switch (insn_data[d->code].operand[1].mode)
+	    {
+	      case V4SFmode:
+	        eltype = V4SF_type_node;
+	        return_type = V4HF_type_node;
+	        break;
+	      default: gcc_unreachable ();
+	    }
+	    ftype = build_function_type_list (return_type, eltype, NULL);
+	    break;
+	  }
 	default:
 	  gcc_unreachable ();
 	}
@@ -20475,6 +21412,9 @@ arm_init_neon_builtins (void)
     }
 }
 
+#undef NUM_DREG_TYPES
+#undef NUM_QREG_TYPES
+
 #define def_mbuiltin(MASK, NAME, TYPE, CODE)				\
   do									\
     {									\
@@ -20497,7 +21437,7 @@ struct builtin_description
   const enum rtx_code      comparison;
   const unsigned int       flag;
 };
-  
+
 static const struct builtin_description bdesc_2arg[] =
 {
 #define IWMMXT_BUILTIN(code, string, builtin) \
@@ -20603,6 +21543,33 @@ static const struct builtin_description
   IWMMXT_BUILTIN2 (iwmmxt_wpackdus, WPACKDUS)
   IWMMXT_BUILTIN2 (iwmmxt_wmacuz, WMACUZ)
   IWMMXT_BUILTIN2 (iwmmxt_wmacsz, WMACSZ)
+
+#define CRC32_BUILTIN(L, U) \
+  {0, CODE_FOR_##L, "__builtin_arm_"#L, ARM_BUILTIN_##U, \
+   UNKNOWN, 0},
+   CRC32_BUILTIN (crc32b, CRC32B)
+   CRC32_BUILTIN (crc32h, CRC32H)
+   CRC32_BUILTIN (crc32w, CRC32W)
+   CRC32_BUILTIN (crc32cb, CRC32CB)
+   CRC32_BUILTIN (crc32ch, CRC32CH)
+   CRC32_BUILTIN (crc32cw, CRC32CW)
+#undef CRC32_BUILTIN
+
+
+#define CRYPTO_BUILTIN(L, U) \
+  {0, CODE_FOR_crypto_##L, "__builtin_arm_crypto_"#L, ARM_BUILTIN_CRYPTO_##U, \
+   UNKNOWN, 0},
+#undef CRYPTO1
+#undef CRYPTO2
+#undef CRYPTO3
+#define CRYPTO2(L, U, R, A1, A2) CRYPTO_BUILTIN (L, U)
+#define CRYPTO1(L, U, R, A)
+#define CRYPTO3(L, U, R, A1, A2, A3)
+#include "crypto.def"
+#undef CRYPTO1
+#undef CRYPTO2
+#undef CRYPTO3
+
 };
 
 static const struct builtin_description bdesc_1arg[] =
@@ -20631,8 +21598,28 @@ static const struct builtin_description
   IWMMXT_BUILTIN (tbcstv8qi, "tbcstb", TBCSTB)
   IWMMXT_BUILTIN (tbcstv4hi, "tbcsth", TBCSTH)
   IWMMXT_BUILTIN (tbcstv2si, "tbcstw", TBCSTW)
+
+#define CRYPTO1(L, U, R, A) CRYPTO_BUILTIN (L, U)
+#define CRYPTO2(L, U, R, A1, A2)
+#define CRYPTO3(L, U, R, A1, A2, A3)
+#include "crypto.def"
+#undef CRYPTO1
+#undef CRYPTO2
+#undef CRYPTO3
 };
 
+static const struct builtin_description bdesc_3arg[] =
+{
+#define CRYPTO3(L, U, R, A1, A2, A3) CRYPTO_BUILTIN (L, U)
+#define CRYPTO1(L, U, R, A)
+#define CRYPTO2(L, U, R, A1, A2)
+#include "crypto.def"
+#undef CRYPTO1
+#undef CRYPTO2
+#undef CRYPTO3
+ };
+#undef CRYPTO_BUILTIN
+
 /* Set up all the iWMMXt builtins.  This is not called if
    TARGET_IWMMXT is zero.  */
 
@@ -20827,7 +21814,7 @@ arm_init_iwmmxt_builtins (void)
       enum machine_mode mode;
       tree type;
 
-      if (d->name == 0)
+      if (d->name == 0 || !(d->mask == FL_IWMMXT || d->mask == FL_IWMMXT2))
 	continue;
 
       mode = insn_data[d->icode].operand[1].mode;
@@ -21022,6 +22009,42 @@ arm_init_fp16_builtins (void)
 }
 
 static void
+arm_init_crc32_builtins ()
+{
+  tree si_ftype_si_qi
+    = build_function_type_list (unsigned_intSI_type_node,
+                                unsigned_intSI_type_node,
+                                unsigned_intQI_type_node, NULL_TREE);
+  tree si_ftype_si_hi
+    = build_function_type_list (unsigned_intSI_type_node,
+                                unsigned_intSI_type_node,
+                                unsigned_intHI_type_node, NULL_TREE);
+  tree si_ftype_si_si
+    = build_function_type_list (unsigned_intSI_type_node,
+                                unsigned_intSI_type_node,
+                                unsigned_intSI_type_node, NULL_TREE);
+
+  arm_builtin_decls[ARM_BUILTIN_CRC32B]
+    = add_builtin_function ("__builtin_arm_crc32b", si_ftype_si_qi,
+                            ARM_BUILTIN_CRC32B, BUILT_IN_MD, NULL, NULL_TREE);
+  arm_builtin_decls[ARM_BUILTIN_CRC32H]
+    = add_builtin_function ("__builtin_arm_crc32h", si_ftype_si_hi,
+                            ARM_BUILTIN_CRC32H, BUILT_IN_MD, NULL, NULL_TREE);
+  arm_builtin_decls[ARM_BUILTIN_CRC32W]
+    = add_builtin_function ("__builtin_arm_crc32w", si_ftype_si_si,
+                            ARM_BUILTIN_CRC32W, BUILT_IN_MD, NULL, NULL_TREE);
+  arm_builtin_decls[ARM_BUILTIN_CRC32CB]
+    = add_builtin_function ("__builtin_arm_crc32cb", si_ftype_si_qi,
+                            ARM_BUILTIN_CRC32CB, BUILT_IN_MD, NULL, NULL_TREE);
+  arm_builtin_decls[ARM_BUILTIN_CRC32CH]
+    = add_builtin_function ("__builtin_arm_crc32ch", si_ftype_si_hi,
+                            ARM_BUILTIN_CRC32CH, BUILT_IN_MD, NULL, NULL_TREE);
+  arm_builtin_decls[ARM_BUILTIN_CRC32CW]
+    = add_builtin_function ("__builtin_arm_crc32cw", si_ftype_si_si,
+                            ARM_BUILTIN_CRC32CW, BUILT_IN_MD, NULL, NULL_TREE);
+}
+
+static void
 arm_init_builtins (void)
 {
   if (TARGET_REALLY_IWMMXT)
@@ -21032,6 +22055,9 @@ arm_init_builtins (void)
 
   if (arm_fp16_format)
     arm_init_fp16_builtins ();
+
+  if (TARGET_CRC32)
+    arm_init_crc32_builtins ();
 }
 
 /* Return the ARM builtin for CODE.  */
@@ -21125,6 +22151,73 @@ safe_vector_operand (rtx x, enum machine
   return x;
 }
 
+/* Function to expand ternary builtins.  */
+static rtx
+arm_expand_ternop_builtin (enum insn_code icode,
+                           tree exp, rtx target)
+{
+  rtx pat;
+  tree arg0 = CALL_EXPR_ARG (exp, 0);
+  tree arg1 = CALL_EXPR_ARG (exp, 1);
+  tree arg2 = CALL_EXPR_ARG (exp, 2);
+
+  rtx op0 = expand_normal (arg0);
+  rtx op1 = expand_normal (arg1);
+  rtx op2 = expand_normal (arg2);
+  rtx op3 = NULL_RTX;
+
+  /* The sha1c, sha1p, sha1m crypto builtins require a different vec_select
+     lane operand depending on endianness.  */
+  bool builtin_sha1cpm_p = false;
+
+  if (insn_data[icode].n_operands == 5)
+    {
+      gcc_assert (icode == CODE_FOR_crypto_sha1c
+                  || icode == CODE_FOR_crypto_sha1p
+                  || icode == CODE_FOR_crypto_sha1m);
+      builtin_sha1cpm_p = true;
+    }
+  enum machine_mode tmode = insn_data[icode].operand[0].mode;
+  enum machine_mode mode0 = insn_data[icode].operand[1].mode;
+  enum machine_mode mode1 = insn_data[icode].operand[2].mode;
+  enum machine_mode mode2 = insn_data[icode].operand[3].mode;
+
+
+  if (VECTOR_MODE_P (mode0))
+    op0 = safe_vector_operand (op0, mode0);
+  if (VECTOR_MODE_P (mode1))
+    op1 = safe_vector_operand (op1, mode1);
+  if (VECTOR_MODE_P (mode2))
+    op2 = safe_vector_operand (op2, mode2);
+
+  if (! target
+      || GET_MODE (target) != tmode
+      || ! (*insn_data[icode].operand[0].predicate) (target, tmode))
+    target = gen_reg_rtx (tmode);
+
+  gcc_assert ((GET_MODE (op0) == mode0 || GET_MODE (op0) == VOIDmode)
+	      && (GET_MODE (op1) == mode1 || GET_MODE (op1) == VOIDmode)
+	      && (GET_MODE (op2) == mode2 || GET_MODE (op2) == VOIDmode));
+
+  if (! (*insn_data[icode].operand[1].predicate) (op0, mode0))
+    op0 = copy_to_mode_reg (mode0, op0);
+  if (! (*insn_data[icode].operand[2].predicate) (op1, mode1))
+    op1 = copy_to_mode_reg (mode1, op1);
+  if (! (*insn_data[icode].operand[3].predicate) (op2, mode2))
+    op2 = copy_to_mode_reg (mode2, op2);
+  if (builtin_sha1cpm_p)
+    op3 = GEN_INT (TARGET_BIG_END ? 1 : 0);
+
+  if (builtin_sha1cpm_p)
+    pat = GEN_FCN (icode) (target, op0, op1, op2, op3);
+  else
+    pat = GEN_FCN (icode) (target, op0, op1, op2);
+  if (! pat)
+    return 0;
+  emit_insn (pat);
+  return target;
+}
+
 /* Subroutine of arm_expand_builtin to take care of binop insns.  */
 
 static rtx
@@ -21174,8 +22267,16 @@ arm_expand_unop_builtin (enum insn_code
   rtx pat;
   tree arg0 = CALL_EXPR_ARG (exp, 0);
   rtx op0 = expand_normal (arg0);
+  rtx op1 = NULL_RTX;
   enum machine_mode tmode = insn_data[icode].operand[0].mode;
   enum machine_mode mode0 = insn_data[icode].operand[1].mode;
+  bool builtin_sha1h_p = false;
+
+  if (insn_data[icode].n_operands == 3)
+    {
+      gcc_assert (icode == CODE_FOR_crypto_sha1h);
+      builtin_sha1h_p = true;
+    }
 
   if (! target
       || GET_MODE (target) != tmode
@@ -21191,8 +22292,13 @@ arm_expand_unop_builtin (enum insn_code
       if (! (*insn_data[icode].operand[1].predicate) (op0, mode0))
 	op0 = copy_to_mode_reg (mode0, op0);
     }
+  if (builtin_sha1h_p)
+    op1 = GEN_INT (TARGET_BIG_END ? 1 : 0);
 
-  pat = GEN_FCN (icode) (target, op0);
+  if (builtin_sha1h_p)
+    pat = GEN_FCN (icode) (target, op0, op1);
+  else
+    pat = GEN_FCN (icode) (target, op0);
   if (! pat)
     return 0;
   emit_insn (pat);
@@ -21464,6 +22570,8 @@ arm_expand_neon_builtin (int fcode, tree
     case NEON_DUP:
     case NEON_RINT:
     case NEON_SPLIT:
+    case NEON_FLOAT_WIDEN:
+    case NEON_FLOAT_NARROW:
     case NEON_REINTERP:
       return arm_expand_neon_args (target, icode, 1, type_mode, exp, fcode,
         NEON_ARG_COPY_TO_REG, NEON_ARG_STOP);
@@ -21661,7 +22769,7 @@ arm_expand_builtin (tree exp,
   rtx               op1;
   rtx               op2;
   rtx               pat;
-  int               fcode = DECL_FUNCTION_CODE (fndecl);
+  unsigned int      fcode = DECL_FUNCTION_CODE (fndecl);
   size_t            i;
   enum machine_mode tmode;
   enum machine_mode mode0;
@@ -22155,6 +23263,10 @@ arm_expand_builtin (tree exp,
     if (d->code == (const enum arm_builtins) fcode)
       return arm_expand_unop_builtin (d->icode, exp, target, 0);
 
+  for (i = 0, d = bdesc_3arg; i < ARRAY_SIZE (bdesc_3arg); i++, d++)
+    if (d->code == (const enum arm_builtins) fcode)
+      return arm_expand_ternop_builtin (d->icode, exp, target);
+
   /* @@@ Should really do something sensible here.  */
   return NULL_RTX;
 }
@@ -23378,7 +24490,7 @@ thumb1_expand_prologue (void)
    all we really need to check here is if single register is to be
    returned, or multiple register return.  */
 void
-thumb2_expand_return (void)
+thumb2_expand_return (bool simple_return)
 {
   int i, num_regs;
   unsigned long saved_regs_mask;
@@ -23391,7 +24503,7 @@ thumb2_expand_return (void)
     if (saved_regs_mask & (1 << i))
       num_regs++;
 
-  if (saved_regs_mask)
+  if (!simple_return && saved_regs_mask)
     {
       if (num_regs == 1)
         {
@@ -23507,15 +24619,19 @@ arm_expand_epilogue_apcs_frame (bool rea
   if (TARGET_HARD_FLOAT && TARGET_VFP)
     {
       int start_reg;
+      rtx ip_rtx = gen_rtx_REG (SImode, IP_REGNUM);
 
       /* The offset is from IP_REGNUM.  */
       int saved_size = arm_get_vfp_saved_size ();
       if (saved_size > 0)
         {
+	  rtx insn;
           floats_from_frame += saved_size;
-          emit_insn (gen_addsi3 (gen_rtx_REG (SImode, IP_REGNUM),
-                                 hard_frame_pointer_rtx,
-                                 GEN_INT (-floats_from_frame)));
+          insn = emit_insn (gen_addsi3 (ip_rtx,
+					hard_frame_pointer_rtx,
+					GEN_INT (-floats_from_frame)));
+	  arm_add_cfa_adjust_cfa_note (insn, -floats_from_frame,
+				       ip_rtx, hard_frame_pointer_rtx);
         }
 
       /* Generate VFP register multi-pop.  */
@@ -23588,11 +24704,15 @@ arm_expand_epilogue_apcs_frame (bool rea
   num_regs = bit_count (saved_regs_mask);
   if ((offsets->outgoing_args != (1 + num_regs)) || cfun->calls_alloca)
     {
+      rtx insn;
       emit_insn (gen_blockage ());
       /* Unwind the stack to just below the saved registers.  */
-      emit_insn (gen_addsi3 (stack_pointer_rtx,
-                             hard_frame_pointer_rtx,
-                             GEN_INT (- 4 * num_regs)));
+      insn = emit_insn (gen_addsi3 (stack_pointer_rtx,
+				    hard_frame_pointer_rtx,
+				    GEN_INT (- 4 * num_regs)));
+
+      arm_add_cfa_adjust_cfa_note (insn, - 4 * num_regs,
+				   stack_pointer_rtx, hard_frame_pointer_rtx);
     }
 
   arm_emit_multi_reg_pop (saved_regs_mask);
@@ -23670,6 +24790,7 @@ arm_expand_epilogue (bool really_return)
 
   if (frame_pointer_needed)
     {
+      rtx insn;
       /* Restore stack pointer if necessary.  */
       if (TARGET_ARM)
         {
@@ -23680,9 +24801,12 @@ arm_expand_epilogue (bool really_return)
           /* Force out any pending memory operations that reference stacked data
              before stack de-allocation occurs.  */
           emit_insn (gen_blockage ());
-          emit_insn (gen_addsi3 (stack_pointer_rtx,
-                                 hard_frame_pointer_rtx,
-                                 GEN_INT (amount)));
+	  insn = emit_insn (gen_addsi3 (stack_pointer_rtx,
+			    hard_frame_pointer_rtx,
+			    GEN_INT (amount)));
+	  arm_add_cfa_adjust_cfa_note (insn, amount,
+				       stack_pointer_rtx,
+				       hard_frame_pointer_rtx);
 
           /* Emit USE(stack_pointer_rtx) to ensure that stack adjustment is not
              deleted.  */
@@ -23692,16 +24816,25 @@ arm_expand_epilogue (bool really_return)
         {
           /* In Thumb-2 mode, the frame pointer points to the last saved
              register.  */
-          amount = offsets->locals_base - offsets->saved_regs;
-          if (amount)
-            emit_insn (gen_addsi3 (hard_frame_pointer_rtx,
-                                   hard_frame_pointer_rtx,
-                                   GEN_INT (amount)));
+	  amount = offsets->locals_base - offsets->saved_regs;
+	  if (amount)
+	    {
+	      insn = emit_insn (gen_addsi3 (hard_frame_pointer_rtx,
+				hard_frame_pointer_rtx,
+				GEN_INT (amount)));
+	      arm_add_cfa_adjust_cfa_note (insn, amount,
+					   hard_frame_pointer_rtx,
+					   hard_frame_pointer_rtx);
+	    }
 
           /* Force out any pending memory operations that reference stacked data
              before stack de-allocation occurs.  */
           emit_insn (gen_blockage ());
-          emit_insn (gen_movsi (stack_pointer_rtx, hard_frame_pointer_rtx));
+	  insn = emit_insn (gen_movsi (stack_pointer_rtx,
+				       hard_frame_pointer_rtx));
+	  arm_add_cfa_adjust_cfa_note (insn, 0,
+				       stack_pointer_rtx,
+				       hard_frame_pointer_rtx);
           /* Emit USE(stack_pointer_rtx) to ensure that stack adjustment is not
              deleted.  */
           emit_insn (gen_force_register_use (stack_pointer_rtx));
@@ -23714,12 +24847,15 @@ arm_expand_epilogue (bool really_return)
       amount = offsets->outgoing_args - offsets->saved_regs;
       if (amount)
         {
+	  rtx tmp;
           /* Force out any pending memory operations that reference stacked data
              before stack de-allocation occurs.  */
           emit_insn (gen_blockage ());
-          emit_insn (gen_addsi3 (stack_pointer_rtx,
-                                 stack_pointer_rtx,
-                                 GEN_INT (amount)));
+	  tmp = emit_insn (gen_addsi3 (stack_pointer_rtx,
+				       stack_pointer_rtx,
+				       GEN_INT (amount)));
+	  arm_add_cfa_adjust_cfa_note (tmp, amount,
+				       stack_pointer_rtx, stack_pointer_rtx);
           /* Emit USE(stack_pointer_rtx) to ensure that stack adjustment is
              not deleted.  */
           emit_insn (gen_force_register_use (stack_pointer_rtx));
@@ -23772,6 +24908,8 @@ arm_expand_epilogue (bool really_return)
           REG_NOTES (insn) = alloc_reg_note (REG_CFA_RESTORE,
                                              gen_rtx_REG (V2SImode, i),
                                              NULL_RTX);
+	  arm_add_cfa_adjust_cfa_note (insn, UNITS_PER_WORD,
+				       stack_pointer_rtx, stack_pointer_rtx);
         }
 
   if (saved_regs_mask)
@@ -23819,6 +24957,9 @@ arm_expand_epilogue (bool really_return)
                     REG_NOTES (insn) = alloc_reg_note (REG_CFA_RESTORE,
                                                        gen_rtx_REG (SImode, i),
                                                        NULL_RTX);
+		    arm_add_cfa_adjust_cfa_note (insn, UNITS_PER_WORD,
+						 stack_pointer_rtx,
+						 stack_pointer_rtx);
                   }
               }
         }
@@ -23830,6 +24971,8 @@ arm_expand_epilogue (bool really_return)
             {
               if (TARGET_THUMB2)
                 thumb2_emit_ldrd_pop (saved_regs_mask);
+              else if (TARGET_ARM && !IS_INTERRUPT (func_type))
+                arm_emit_ldrd_pop (saved_regs_mask);
               else
                 arm_emit_multi_reg_pop (saved_regs_mask);
             }
@@ -23842,9 +24985,33 @@ arm_expand_epilogue (bool really_return)
     }
 
   if (crtl->args.pretend_args_size)
-    emit_insn (gen_addsi3 (stack_pointer_rtx,
-                           stack_pointer_rtx,
-                           GEN_INT (crtl->args.pretend_args_size)));
+    {
+      int i, j;
+      rtx dwarf = NULL_RTX;
+      rtx tmp = emit_insn (gen_addsi3 (stack_pointer_rtx,
+			   stack_pointer_rtx,
+			   GEN_INT (crtl->args.pretend_args_size)));
+
+      RTX_FRAME_RELATED_P (tmp) = 1;
+
+      if (cfun->machine->uses_anonymous_args)
+	{
+	  /* Restore pretend args.  Refer arm_expand_prologue on how to save
+	     pretend_args in stack.  */
+	  int num_regs = crtl->args.pretend_args_size / 4;
+	  saved_regs_mask = (0xf0 >> num_regs) & 0xf;
+	  for (j = 0, i = 0; j < num_regs; i++)
+	    if (saved_regs_mask & (1 << i))
+	      {
+		rtx reg = gen_rtx_REG (SImode, i);
+		dwarf = alloc_reg_note (REG_CFA_RESTORE, reg, dwarf);
+		j++;
+	      }
+	  REG_NOTES (tmp) = dwarf;
+	}
+      arm_add_cfa_adjust_cfa_note (tmp, crtl->args.pretend_args_size,
+				   stack_pointer_rtx, stack_pointer_rtx);
+    }
 
   if (!really_return)
     return;
@@ -24241,7 +25408,22 @@ arm_file_start (void)
     {
       const char *fpu_name;
       if (arm_selected_arch)
-	asm_fprintf (asm_out_file, "\t.arch %s\n", arm_selected_arch->name);
+        {
+          const char* pos = strchr (arm_selected_arch->name, '+');
+	  if (pos)
+	    {
+	      char buf[15];
+	      gcc_assert (strlen (arm_selected_arch->name)
+	                  <= sizeof (buf) / sizeof (*pos));
+	      strncpy (buf, arm_selected_arch->name,
+	                    (pos - arm_selected_arch->name) * sizeof (*pos));
+	      buf[pos - arm_selected_arch->name] = '\0';
+	      asm_fprintf (asm_out_file, "\t.arch %s\n", buf);
+	      asm_fprintf (asm_out_file, "\t.arch_extension %s\n", pos + 1);
+	    }
+	  else
+	    asm_fprintf (asm_out_file, "\t.arch %s\n", arm_selected_arch->name);
+        }
       else if (strncmp (arm_selected_cpu->name, "generic", 7) == 0)
 	asm_fprintf (asm_out_file, "\t.arch %s\n", arm_selected_cpu->name + 8);
       else
@@ -24620,163 +25802,6 @@ arm_setup_incoming_varargs (cumulative_a
     *pretend_size = (NUM_ARG_REGS - nregs) * UNITS_PER_WORD;
 }
 
-/* Return nonzero if the CONSUMER instruction (a store) does not need
-   PRODUCER's value to calculate the address.  */
-
-int
-arm_no_early_store_addr_dep (rtx producer, rtx consumer)
-{
-  rtx value = PATTERN (producer);
-  rtx addr = PATTERN (consumer);
-
-  if (GET_CODE (value) == COND_EXEC)
-    value = COND_EXEC_CODE (value);
-  if (GET_CODE (value) == PARALLEL)
-    value = XVECEXP (value, 0, 0);
-  value = XEXP (value, 0);
-  if (GET_CODE (addr) == COND_EXEC)
-    addr = COND_EXEC_CODE (addr);
-  if (GET_CODE (addr) == PARALLEL)
-    addr = XVECEXP (addr, 0, 0);
-  addr = XEXP (addr, 0);
-
-  return !reg_overlap_mentioned_p (value, addr);
-}
-
-/* Return nonzero if the CONSUMER instruction (a store) does need
-   PRODUCER's value to calculate the address.  */
-
-int
-arm_early_store_addr_dep (rtx producer, rtx consumer)
-{
-  return !arm_no_early_store_addr_dep (producer, consumer);
-}
-
-/* Return nonzero if the CONSUMER instruction (a load) does need
-   PRODUCER's value to calculate the address.  */
-
-int
-arm_early_load_addr_dep (rtx producer, rtx consumer)
-{
-  rtx value = PATTERN (producer);
-  rtx addr = PATTERN (consumer);
-
-  if (GET_CODE (value) == COND_EXEC)
-    value = COND_EXEC_CODE (value);
-  if (GET_CODE (value) == PARALLEL)
-    value = XVECEXP (value, 0, 0);
-  value = XEXP (value, 0);
-  if (GET_CODE (addr) == COND_EXEC)
-    addr = COND_EXEC_CODE (addr);
-  if (GET_CODE (addr) == PARALLEL)
-    {
-      if (GET_CODE (XVECEXP (addr, 0, 0)) == RETURN)
-        addr = XVECEXP (addr, 0, 1);
-      else
-        addr = XVECEXP (addr, 0, 0);
-    }
-  addr = XEXP (addr, 1);
-
-  return reg_overlap_mentioned_p (value, addr);
-}
-
-/* Return nonzero if the CONSUMER instruction (an ALU op) does not
-   have an early register shift value or amount dependency on the
-   result of PRODUCER.  */
-
-int
-arm_no_early_alu_shift_dep (rtx producer, rtx consumer)
-{
-  rtx value = PATTERN (producer);
-  rtx op = PATTERN (consumer);
-  rtx early_op;
-
-  if (GET_CODE (value) == COND_EXEC)
-    value = COND_EXEC_CODE (value);
-  if (GET_CODE (value) == PARALLEL)
-    value = XVECEXP (value, 0, 0);
-  value = XEXP (value, 0);
-  if (GET_CODE (op) == COND_EXEC)
-    op = COND_EXEC_CODE (op);
-  if (GET_CODE (op) == PARALLEL)
-    op = XVECEXP (op, 0, 0);
-  op = XEXP (op, 1);
-
-  early_op = XEXP (op, 0);
-  /* This is either an actual independent shift, or a shift applied to
-     the first operand of another operation.  We want the whole shift
-     operation.  */
-  if (REG_P (early_op))
-    early_op = op;
-
-  return !reg_overlap_mentioned_p (value, early_op);
-}
-
-/* Return nonzero if the CONSUMER instruction (an ALU op) does not
-   have an early register shift value dependency on the result of
-   PRODUCER.  */
-
-int
-arm_no_early_alu_shift_value_dep (rtx producer, rtx consumer)
-{
-  rtx value = PATTERN (producer);
-  rtx op = PATTERN (consumer);
-  rtx early_op;
-
-  if (GET_CODE (value) == COND_EXEC)
-    value = COND_EXEC_CODE (value);
-  if (GET_CODE (value) == PARALLEL)
-    value = XVECEXP (value, 0, 0);
-  value = XEXP (value, 0);
-  if (GET_CODE (op) == COND_EXEC)
-    op = COND_EXEC_CODE (op);
-  if (GET_CODE (op) == PARALLEL)
-    op = XVECEXP (op, 0, 0);
-  op = XEXP (op, 1);
-
-  early_op = XEXP (op, 0);
-
-  /* This is either an actual independent shift, or a shift applied to
-     the first operand of another operation.  We want the value being
-     shifted, in either case.  */
-  if (!REG_P (early_op))
-    early_op = XEXP (early_op, 0);
-
-  return !reg_overlap_mentioned_p (value, early_op);
-}
-
-/* Return nonzero if the CONSUMER (a mul or mac op) does not
-   have an early register mult dependency on the result of
-   PRODUCER.  */
-
-int
-arm_no_early_mul_dep (rtx producer, rtx consumer)
-{
-  rtx value = PATTERN (producer);
-  rtx op = PATTERN (consumer);
-
-  if (GET_CODE (value) == COND_EXEC)
-    value = COND_EXEC_CODE (value);
-  if (GET_CODE (value) == PARALLEL)
-    value = XVECEXP (value, 0, 0);
-  value = XEXP (value, 0);
-  if (GET_CODE (op) == COND_EXEC)
-    op = COND_EXEC_CODE (op);
-  if (GET_CODE (op) == PARALLEL)
-    op = XVECEXP (op, 0, 0);
-  op = XEXP (op, 1);
-
-  if (GET_CODE (op) == PLUS || GET_CODE (op) == MINUS)
-    {
-      if (GET_CODE (XEXP (op, 0)) == MULT)
-	return !reg_overlap_mentioned_p (value, XEXP (op, 0));
-      else
-	return !reg_overlap_mentioned_p (value, XEXP (op, 1));
-    }
-
-  return 0;
-}
-
 /* We can't rely on the caller doing the proper promotion when
    using APCS or ATPCS.  */
 
@@ -24826,95 +25851,6 @@ arm_cxx_guard_type (void)
   return TARGET_AAPCS_BASED ? integer_type_node : long_long_integer_type_node;
 }
 
-/* Return non-zero iff the consumer (a multiply-accumulate or a
-   multiple-subtract instruction) has an accumulator dependency on the
-   result of the producer and no other dependency on that result.  It
-   does not check if the producer is multiply-accumulate instruction.  */
-int
-arm_mac_accumulator_is_result (rtx producer, rtx consumer)
-{
-  rtx result;
-  rtx op0, op1, acc;
-
-  producer = PATTERN (producer);
-  consumer = PATTERN (consumer);
-
-  if (GET_CODE (producer) == COND_EXEC)
-    producer = COND_EXEC_CODE (producer);
-  if (GET_CODE (consumer) == COND_EXEC)
-    consumer = COND_EXEC_CODE (consumer);
-
-  if (GET_CODE (producer) != SET)
-    return 0;
-
-  result = XEXP (producer, 0);
-
-  if (GET_CODE (consumer) != SET)
-    return 0;
-
-  /* Check that the consumer is of the form
-     (set (...) (plus (mult ...) (...)))
-     or
-     (set (...) (minus (...) (mult ...))).  */
-  if (GET_CODE (XEXP (consumer, 1)) == PLUS)
-    {
-      if (GET_CODE (XEXP (XEXP (consumer, 1), 0)) != MULT)
-        return 0;
-
-      op0 = XEXP (XEXP (XEXP (consumer, 1), 0), 0);
-      op1 = XEXP (XEXP (XEXP (consumer, 1), 0), 1);
-      acc = XEXP (XEXP (consumer, 1), 1);
-    }
-  else if (GET_CODE (XEXP (consumer, 1)) == MINUS)
-    {
-      if (GET_CODE (XEXP (XEXP (consumer, 1), 1)) != MULT)
-        return 0;
-
-      op0 = XEXP (XEXP (XEXP (consumer, 1), 1), 0);
-      op1 = XEXP (XEXP (XEXP (consumer, 1), 1), 1);
-      acc = XEXP (XEXP (consumer, 1), 0);
-    }
-  else
-    return 0;
-
-  return (reg_overlap_mentioned_p (result, acc)
-          && !reg_overlap_mentioned_p (result, op0)
-          && !reg_overlap_mentioned_p (result, op1));
-}
-
-/* Return non-zero if the consumer (a multiply-accumulate instruction)
-   has an accumulator dependency on the result of the producer (a
-   multiplication instruction) and no other dependency on that result.  */
-int
-arm_mac_accumulator_is_mul_result (rtx producer, rtx consumer)
-{
-  rtx mul = PATTERN (producer);
-  rtx mac = PATTERN (consumer);
-  rtx mul_result;
-  rtx mac_op0, mac_op1, mac_acc;
-
-  if (GET_CODE (mul) == COND_EXEC)
-    mul = COND_EXEC_CODE (mul);
-  if (GET_CODE (mac) == COND_EXEC)
-    mac = COND_EXEC_CODE (mac);
-
-  /* Check that mul is of the form (set (...) (mult ...))
-     and mla is of the form (set (...) (plus (mult ...) (...))).  */
-  if ((GET_CODE (mul) != SET || GET_CODE (XEXP (mul, 1)) != MULT)
-      || (GET_CODE (mac) != SET || GET_CODE (XEXP (mac, 1)) != PLUS
-          || GET_CODE (XEXP (XEXP (mac, 1), 0)) != MULT))
-    return 0;
-
-  mul_result = XEXP (mul, 0);
-  mac_op0 = XEXP (XEXP (XEXP (mac, 1), 0), 0);
-  mac_op1 = XEXP (XEXP (XEXP (mac, 1), 0), 1);
-  mac_acc = XEXP (XEXP (mac, 1), 1);
-
-  return (reg_overlap_mentioned_p (mul_result, mac_acc)
-          && !reg_overlap_mentioned_p (mul_result, mac_op0)
-          && !reg_overlap_mentioned_p (mul_result, mac_op1));
-}
-
 
 /* The EABI says test the least significant bit of a guard variable.  */
 
@@ -25102,7 +26038,7 @@ arm_vector_mode_supported_p (enum machin
 {
   /* Neon also supports V2SImode, etc. listed in the clause below.  */
   if (TARGET_NEON && (mode == V2SFmode || mode == V4SImode || mode == V8HImode
-      || mode == V16QImode || mode == V4SFmode || mode == V2DImode))
+      || mode == V4HFmode || mode == V16QImode || mode == V4SFmode || mode == V2DImode))
     return true;
 
   if ((TARGET_NEON || TARGET_IWMMXT)
@@ -25265,9 +26201,8 @@ arm_dwarf_register_span (rtx rtl)
 
   nregs = GET_MODE_SIZE (GET_MODE (rtl)) / 8;
   p = gen_rtx_PARALLEL (VOIDmode, rtvec_alloc (nregs));
-  regno = (regno - FIRST_VFP_REGNUM) / 2;
   for (i = 0; i < nregs; i++)
-    XVECEXP (p, 0, i) = gen_rtx_REG (DImode, 256 + regno + i);
+    XVECEXP (p, 0, i) = gen_rtx_REG (DImode, regno + i);
 
   return p;
 }
@@ -25517,9 +26452,17 @@ arm_unwind_emit (FILE * asm_out_file, rt
 	  handled_one = true;
 	  break;
 
+	/* The INSN is generated in epilogue.  It is set as RTX_FRAME_RELATED_P
+	   to get correct dwarf information for shrink-wrap.  We should not
+	   emit unwind information for it because these are used either for
+	   pretend arguments or notes to adjust sp and restore registers from
+	   stack.  */
 	case REG_CFA_DEF_CFA:
-	case REG_CFA_EXPRESSION:
 	case REG_CFA_ADJUST_CFA:
+	case REG_CFA_RESTORE:
+	  return;
+
+	case REG_CFA_EXPRESSION:
 	case REG_CFA_OFFSET:
 	  /* ??? Only handling here what we actually emit.  */
 	  gcc_unreachable ();
@@ -25917,6 +26860,7 @@ arm_issue_rate (void)
     case cortexa7:
     case cortexa8:
     case cortexa9:
+    case cortexa53:
     case fa726te:
     case marvell_pj4:
       return 2;
@@ -25945,11 +26889,13 @@ static arm_mangle_map_entry arm_mangle_m
   { V8QImode,  "__builtin_neon_uqi",    "16__simd64_uint8_t" },
   { V4HImode,  "__builtin_neon_hi",     "16__simd64_int16_t" },
   { V4HImode,  "__builtin_neon_uhi",    "17__simd64_uint16_t" },
+  { V4HFmode,  "__builtin_neon_hf",     "18__simd64_float16_t" },
   { V2SImode,  "__builtin_neon_si",     "16__simd64_int32_t" },
   { V2SImode,  "__builtin_neon_usi",    "17__simd64_uint32_t" },
   { V2SFmode,  "__builtin_neon_sf",     "18__simd64_float32_t" },
   { V8QImode,  "__builtin_neon_poly8",  "16__simd64_poly8_t" },
   { V4HImode,  "__builtin_neon_poly16", "17__simd64_poly16_t" },
+
   /* 128-bit containerized types.  */
   { V16QImode, "__builtin_neon_qi",     "16__simd128_int8_t" },
   { V16QImode, "__builtin_neon_uqi",    "17__simd128_uint8_t" },
@@ -26043,6 +26989,60 @@ arm_have_conditional_execution (void)
   return !TARGET_THUMB1;
 }
 
+tree
+arm_builtin_vectorized_function (tree fndecl, tree type_out, tree type_in)
+{
+  enum machine_mode in_mode, out_mode;
+  int in_n, out_n;
+
+  if (TREE_CODE (type_out) != VECTOR_TYPE
+      || TREE_CODE (type_in) != VECTOR_TYPE
+      || !(TARGET_NEON && TARGET_FPU_ARMV8 && flag_unsafe_math_optimizations))
+    return NULL_TREE;
+
+  out_mode = TYPE_MODE (TREE_TYPE (type_out));
+  out_n = TYPE_VECTOR_SUBPARTS (type_out);
+  in_mode = TYPE_MODE (TREE_TYPE (type_in));
+  in_n = TYPE_VECTOR_SUBPARTS (type_in);
+
+/* ARM_CHECK_BUILTIN_MODE and ARM_FIND_VRINT_VARIANT are used to find the
+   decl of the vectorized builtin for the appropriate vector mode.
+   NULL_TREE is returned if no such builtin is available.  */
+#undef ARM_CHECK_BUILTIN_MODE
+#define ARM_CHECK_BUILTIN_MODE(C) \
+  (out_mode == SFmode && out_n == C \
+   && in_mode == SFmode && in_n == C)
+
+#undef ARM_FIND_VRINT_VARIANT
+#define ARM_FIND_VRINT_VARIANT(N) \
+  (ARM_CHECK_BUILTIN_MODE (2) \
+    ? arm_builtin_decl(ARM_BUILTIN_NEON_##N##v2sf, false) \
+    : (ARM_CHECK_BUILTIN_MODE (4) \
+      ? arm_builtin_decl(ARM_BUILTIN_NEON_##N##v4sf, false) \
+      : NULL_TREE))
+
+  if (DECL_BUILT_IN_CLASS (fndecl) == BUILT_IN_NORMAL)
+    {
+      enum built_in_function fn = DECL_FUNCTION_CODE (fndecl);
+      switch (fn)
+        {
+          case BUILT_IN_FLOORF:
+            return ARM_FIND_VRINT_VARIANT (vrintm);
+          case BUILT_IN_CEILF:
+            return ARM_FIND_VRINT_VARIANT (vrintp);
+          case BUILT_IN_TRUNCF:
+            return ARM_FIND_VRINT_VARIANT (vrintz);
+          case BUILT_IN_ROUNDF:
+            return ARM_FIND_VRINT_VARIANT (vrinta);
+          default:
+            return NULL_TREE;
+        }
+    }
+  return NULL_TREE;
+}
+#undef ARM_CHECK_BUILTIN_MODE
+#undef ARM_FIND_VRINT_VARIANT
+
 /* The AAPCS sets the maximum alignment of a vector to 64 bits.  */
 static HOST_WIDE_INT
 arm_vector_alignment (const_tree type)
@@ -26273,39 +27273,71 @@ arm_post_atomic_barrier (enum memmodel m
     emit_insn (gen_memory_barrier ());
 }
 
-/* Emit the load-exclusive and store-exclusive instructions.  */
+/* Emit the load-exclusive and store-exclusive instructions.
+   Use acquire and release versions if necessary.  */
 
 static void
-arm_emit_load_exclusive (enum machine_mode mode, rtx rval, rtx mem)
+arm_emit_load_exclusive (enum machine_mode mode, rtx rval, rtx mem, bool acq)
 {
   rtx (*gen) (rtx, rtx);
 
-  switch (mode)
+  if (acq)
     {
-    case QImode: gen = gen_arm_load_exclusiveqi; break;
-    case HImode: gen = gen_arm_load_exclusivehi; break;
-    case SImode: gen = gen_arm_load_exclusivesi; break;
-    case DImode: gen = gen_arm_load_exclusivedi; break;
-    default:
-      gcc_unreachable ();
+      switch (mode)
+        {
+        case QImode: gen = gen_arm_load_acquire_exclusiveqi; break;
+        case HImode: gen = gen_arm_load_acquire_exclusivehi; break;
+        case SImode: gen = gen_arm_load_acquire_exclusivesi; break;
+        case DImode: gen = gen_arm_load_acquire_exclusivedi; break;
+        default:
+          gcc_unreachable ();
+        }
+    }
+  else
+    {
+      switch (mode)
+        {
+        case QImode: gen = gen_arm_load_exclusiveqi; break;
+        case HImode: gen = gen_arm_load_exclusivehi; break;
+        case SImode: gen = gen_arm_load_exclusivesi; break;
+        case DImode: gen = gen_arm_load_exclusivedi; break;
+        default:
+          gcc_unreachable ();
+        }
     }
 
   emit_insn (gen (rval, mem));
 }
 
 static void
-arm_emit_store_exclusive (enum machine_mode mode, rtx bval, rtx rval, rtx mem)
+arm_emit_store_exclusive (enum machine_mode mode, rtx bval, rtx rval,
+                          rtx mem, bool rel)
 {
   rtx (*gen) (rtx, rtx, rtx);
 
-  switch (mode)
+  if (rel)
     {
-    case QImode: gen = gen_arm_store_exclusiveqi; break;
-    case HImode: gen = gen_arm_store_exclusivehi; break;
-    case SImode: gen = gen_arm_store_exclusivesi; break;
-    case DImode: gen = gen_arm_store_exclusivedi; break;
-    default:
-      gcc_unreachable ();
+      switch (mode)
+        {
+        case QImode: gen = gen_arm_store_release_exclusiveqi; break;
+        case HImode: gen = gen_arm_store_release_exclusivehi; break;
+        case SImode: gen = gen_arm_store_release_exclusivesi; break;
+        case DImode: gen = gen_arm_store_release_exclusivedi; break;
+        default:
+          gcc_unreachable ();
+        }
+    }
+  else
+    {
+      switch (mode)
+        {
+        case QImode: gen = gen_arm_store_exclusiveqi; break;
+        case HImode: gen = gen_arm_store_exclusivehi; break;
+        case SImode: gen = gen_arm_store_exclusivesi; break;
+        case DImode: gen = gen_arm_store_exclusivedi; break;
+        default:
+          gcc_unreachable ();
+        }
     }
 
   emit_insn (gen (bval, rval, mem));
@@ -26341,6 +27373,15 @@ arm_expand_compare_and_swap (rtx operand
   mod_f = operands[7];
   mode = GET_MODE (mem);
 
+  /* Normally the succ memory model must be stronger than fail, but in the
+     unlikely event of fail being ACQUIRE and succ being RELEASE we need to
+     promote succ to ACQ_REL so that we don't lose the acquire semantics.  */
+
+  if (TARGET_HAVE_LDACQ
+      && INTVAL (mod_f) == MEMMODEL_ACQUIRE
+      && INTVAL (mod_s) == MEMMODEL_RELEASE)
+    mod_s = GEN_INT (MEMMODEL_ACQ_REL);
+
   switch (mode)
     {
     case QImode:
@@ -26415,7 +27456,19 @@ arm_split_compare_and_swap (rtx operands
   scratch = operands[7];
   mode = GET_MODE (mem);
 
-  arm_pre_atomic_barrier (mod_s);
+  bool use_acquire = TARGET_HAVE_LDACQ
+                     && !(mod_s == MEMMODEL_RELAXED
+                          || mod_s == MEMMODEL_CONSUME
+                          || mod_s == MEMMODEL_RELEASE);
+
+  bool use_release = TARGET_HAVE_LDACQ
+                     && !(mod_s == MEMMODEL_RELAXED
+                          || mod_s == MEMMODEL_CONSUME
+                          || mod_s == MEMMODEL_ACQUIRE);
+
+  /* Checks whether a barrier is needed and emits one accordingly.  */
+  if (!(use_acquire || use_release))
+    arm_pre_atomic_barrier (mod_s);
 
   label1 = NULL_RTX;
   if (!is_weak)
@@ -26425,7 +27478,7 @@ arm_split_compare_and_swap (rtx operands
     }
   label2 = gen_label_rtx ();
 
-  arm_emit_load_exclusive (mode, rval, mem);
+  arm_emit_load_exclusive (mode, rval, mem, use_acquire);
 
   cond = arm_gen_compare_reg (NE, rval, oldval, scratch);
   x = gen_rtx_NE (VOIDmode, cond, const0_rtx);
@@ -26433,7 +27486,7 @@ arm_split_compare_and_swap (rtx operands
 			    gen_rtx_LABEL_REF (Pmode, label2), pc_rtx);
   emit_unlikely_jump (gen_rtx_SET (VOIDmode, pc_rtx, x));
 
-  arm_emit_store_exclusive (mode, scratch, mem, newval);
+  arm_emit_store_exclusive (mode, scratch, mem, newval, use_release);
 
   /* Weak or strong, we want EQ to be true for success, so that we
      match the flags that we got from the compare above.  */
@@ -26452,7 +27505,9 @@ arm_split_compare_and_swap (rtx operands
   if (mod_f != MEMMODEL_RELAXED)
     emit_label (label2);
 
-  arm_post_atomic_barrier (mod_s);
+  /* Checks whether a barrier is needed and emits one accordingly.  */
+  if (!(use_acquire || use_release))
+    arm_post_atomic_barrier (mod_s);
 
   if (mod_f == MEMMODEL_RELAXED)
     emit_label (label2);
@@ -26467,7 +27522,19 @@ arm_split_atomic_op (enum rtx_code code,
   enum machine_mode wmode = (mode == DImode ? DImode : SImode);
   rtx label, x;
 
-  arm_pre_atomic_barrier (model);
+  bool use_acquire = TARGET_HAVE_LDACQ
+                     && !(model == MEMMODEL_RELAXED
+                          || model == MEMMODEL_CONSUME
+                          || model == MEMMODEL_RELEASE);
+
+  bool use_release = TARGET_HAVE_LDACQ
+                     && !(model == MEMMODEL_RELAXED
+                          || model == MEMMODEL_CONSUME
+                          || model == MEMMODEL_ACQUIRE);
+
+  /* Checks whether a barrier is needed and emits one accordingly.  */
+  if (!(use_acquire || use_release))
+    arm_pre_atomic_barrier (model);
 
   label = gen_label_rtx ();
   emit_label (label);
@@ -26480,7 +27547,7 @@ arm_split_atomic_op (enum rtx_code code,
     old_out = new_out;
   value = simplify_gen_subreg (wmode, value, mode, 0);
 
-  arm_emit_load_exclusive (mode, old_out, mem);
+  arm_emit_load_exclusive (mode, old_out, mem, use_acquire);
 
   switch (code)
     {
@@ -26528,12 +27595,15 @@ arm_split_atomic_op (enum rtx_code code,
       break;
     }
 
-  arm_emit_store_exclusive (mode, cond, mem, gen_lowpart (mode, new_out));
+  arm_emit_store_exclusive (mode, cond, mem, gen_lowpart (mode, new_out),
+                            use_release);
 
   x = gen_rtx_NE (VOIDmode, cond, const0_rtx);
   emit_unlikely_jump (gen_cbranchsi4 (x, cond, const0_rtx, label));
 
-  arm_post_atomic_barrier (model);
+  /* Checks whether a barrier is needed and emits one accordingly.  */
+  if (!(use_acquire || use_release))
+    arm_post_atomic_barrier (model);
 }
 
 #define MAX_VECT_LEN 16
@@ -27473,6 +28543,14 @@ arm_validize_comparison (rtx *comparison
 
 }
 
+/* Implement the TARGET_ASAN_SHADOW_OFFSET hook.  */
+
+static unsigned HOST_WIDE_INT
+arm_asan_shadow_offset (void)
+{
+  return (unsigned HOST_WIDE_INT) 1 << 29;
+}
+
 /* return TRUE if x is a reference to a value in a constant pool */
 extern bool
 arm_is_constant_pool_ref (rtx x)
Index: b/src/gcc/config/arm/t-aprofile
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/t-aprofile
@@ -0,0 +1,177 @@
+# Copyright (C) 2012-2013 Free Software Foundation, Inc.
+#
+# This file is part of GCC.
+#
+# GCC is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3, or (at your option)
+# any later version.
+#
+# GCC is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with GCC; see the file COPYING3.  If not see
+# <http://www.gnu.org/licenses/>.
+
+# This is a target makefile fragment that attempts to get
+# multilibs built for the range of CPU's, FPU's and ABI's that
+# are relevant for the A-profile architecture.  It should
+# not be used in conjunction with another make file fragment and
+# assumes --with-arch, --with-cpu, --with-fpu, --with-float, --with-mode
+# have their default values during the configure step.  We enforce
+# this during the top-level configury.
+
+MULTILIB_OPTIONS     =
+MULTILIB_DIRNAMES    =
+MULTILIB_EXCEPTIONS  =
+MULTILIB_MATCHES     =
+MULTILIB_REUSE	     =
+
+# We have the following hierachy:
+#   ISA: A32 (.) or T32 (thumb)
+#   Architecture: ARMv7-A (v7-a), ARMv7VE (v7ve), or ARMv8-A (v8-a).
+#   FPU: VFPv3-D16 (fpv3), NEONv1 (simdv1), VFPv4-D16 (fpv4),
+#        NEON-VFPV4 (simdvfpv4), NEON for ARMv8 (simdv8), or None (.).
+#   Float-abi: Soft (.), softfp (softfp), or hard (hardfp).
+
+# We use the option -mcpu=cortex-a7 because we do not yet have march=armv7ve
+# or march=armv7a+virt as a command line option for the compiler.
+MULTILIB_OPTIONS       += mthumb
+MULTILIB_DIRNAMES      += thumb
+
+MULTILIB_OPTIONS       += march=armv7-a/mcpu=cortex-a7/march=armv8-a
+MULTILIB_DIRNAMES      += v7-a v7ve v8-a
+
+MULTILIB_OPTIONS       += mfpu=vfpv3-d16/mfpu=neon/mfpu=vfpv4-d16/mfpu=neon-vfpv4/mfpu=neon-fp-armv8
+MULTILIB_DIRNAMES      += fpv3 simdv1 fpv4 simdvfpv4 simdv8
+
+MULTILIB_OPTIONS       += mfloat-abi=softfp/mfloat-abi=hard
+MULTILIB_DIRNAMES      += softfp hard
+
+# We don't build no-float libraries with an FPU.
+MULTILIB_EXCEPTIONS    += *mfpu=vfpv3-d16
+MULTILIB_EXCEPTIONS    += *mfpu=neon
+MULTILIB_EXCEPTIONS    += *mfpu=vfpv4-d16
+MULTILIB_EXCEPTIONS    += *mfpu=neon-vfpv4
+MULTILIB_EXCEPTIONS    += *mfpu=neon-fp-armv8
+
+# We don't build libraries requiring an FPU at the CPU/Arch/ISA level.
+MULTILIB_EXCEPTIONS    += mfloat-abi=*
+MULTILIB_EXCEPTIONS    += mfpu=*
+MULTILIB_EXCEPTIONS    += mthumb/mfloat-abi=*
+MULTILIB_EXCEPTIONS    += mthumb/mfpu=*
+MULTILIB_EXCEPTIONS    += *march=armv7-a/mfloat-abi=*
+MULTILIB_EXCEPTIONS    += *mcpu=cortex-a7/mfloat-abi=*
+MULTILIB_EXCEPTIONS    += *march=armv8-a/mfloat-abi=*
+
+# Ensure the correct FPU variants apply to the correct base architectures.
+MULTILIB_EXCEPTIONS    += *mcpu=cortex-a7/*mfpu=vfpv3-d16*
+MULTILIB_EXCEPTIONS    += *mcpu=cortex-a7/*mfpu=neon/*
+MULTILIB_EXCEPTIONS    += *march=armv8-a/*mfpu=vfpv3-d16*
+MULTILIB_EXCEPTIONS    += *march=armv8-a/*mfpu=neon/*
+MULTILIB_EXCEPTIONS    += *march=armv7-a/*mfpu=vfpv4-d16*
+MULTILIB_EXCEPTIONS    += *march=armv7-a/*mfpu=neon-vfpv4*
+MULTILIB_EXCEPTIONS    += *march=armv8-a/*mfpu=vfpv4-d16*
+MULTILIB_EXCEPTIONS    += *march=armv8-a/*mfpu=neon-vfpv4*
+MULTILIB_EXCEPTIONS    += *march=armv7-a/*mfpu=neon-fp-armv8*
+MULTILIB_EXCEPTIONS    += *mcpu=cortex-a7/*mfpu=neon-fp-armv8*
+
+# CPU Matches
+MULTILIB_MATCHES       += march?armv7-a=mcpu?cortex-a8
+MULTILIB_MATCHES       += march?armv7-a=mcpu?cortex-a9
+MULTILIB_MATCHES       += march?armv7-a=mcpu?cortex-a5
+MULTILIB_MATCHES       += mcpu?cortex-a7=mcpu?cortex-a15
+MULTILIB_MATCHES       += march?armv8-a=mcpu?cortex-a53
+
+# FPU matches
+MULTILIB_MATCHES       += mfpu?vfpv3-d16=mfpu?vfpv3
+MULTILIB_MATCHES       += mfpu?vfpv3-d16=mfpu?vfpv3-fp16
+MULTILIB_MATCHES       += mfpu?vfpv3-d16=mfpu?vfpv3-fp16-d16
+MULTILIB_MATCHES       += mfpu?vfpv4-d16=mfpu?vfpv4
+MULTILIB_MATCHES       += mfpu?neon-fp-armv8=mfpu?crypto-neon-fp-armv8
+
+
+# Map all requests for vfpv3 with a later CPU to vfpv3-d16 v7-a.
+# So if new CPUs are added above at the newer architecture levels,
+# do something to map them below here.
+# We take the approach of mapping down to v7-a regardless of what
+# the fp option is if the integer architecture brings things down.
+# This applies to any similar combination at the v7ve and v8-a arch
+# levels.
+
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=mcpu.cortex-a7/mfpu.vfpv3-d16/mfloat-abi.hard
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=mcpu.cortex-a7/mfpu.vfpv3-d16/mfloat-abi.softfp
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=march.armv8-a/mfpu.vfpv3-d16/mfloat-abi.hard
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=march.armv8-a/mfpu.vfpv3-d16/mfloat-abi.softfp
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=march.armv7-a/mfpu.vfpv4-d16/mfloat-abi.hard
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=march.armv7-a/mfpu.vfpv4-d16/mfloat-abi.softfp
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=march.armv7-a/mfpu.fp-armv8/mfloat-abi.hard
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=march.armv7-a/mfpu.fp-armv8/mfloat-abi.softfp
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=march.armv7-a/mfpu.vfpv4/mfloat-abi.hard
+MULTILIB_REUSE	      += march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=march.armv7-a/mfpu.vfpv4/mfloat-abi.softfp
+
+
+MULTILIB_REUSE	      += march.armv7-a/mfpu.neon/mfloat-abi.hard=mcpu.cortex-a7/mfpu.neon/mfloat-abi.hard
+MULTILIB_REUSE	      += march.armv7-a/mfpu.neon/mfloat-abi.softfp=mcpu.cortex-a7/mfpu.neon/mfloat-abi.softfp
+MULTILIB_REUSE	      += march.armv7-a/mfpu.neon/mfloat-abi.hard=march.armv8-a/mfpu.neon/mfloat-abi.hard
+MULTILIB_REUSE	      += march.armv7-a/mfpu.neon/mfloat-abi.softfp=march.armv8-a/mfpu.neon/mfloat-abi.softfp
+MULTILIB_REUSE	      += march.armv7-a/mfpu.neon/mfloat-abi.hard=march.armv7-a/mfpu.neon-vfpv4/mfloat-abi.hard
+MULTILIB_REUSE	      += march.armv7-a/mfpu.neon/mfloat-abi.softfp=march.armv7-a/mfpu.neon-vfpv4/mfloat-abi.softfp
+MULTILIB_REUSE	      += march.armv7-a/mfpu.neon/mfloat-abi.hard=march.armv7-a/mfpu.neon-fp-armv8/mfloat-abi.hard
+MULTILIB_REUSE	      += march.armv7-a/mfpu.neon/mfloat-abi.softfp=march.armv7-a/mfpu.neon-fp-armv8/mfloat-abi.softfp
+
+
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.hard=mcpu.cortex-a7/mfpu.fp-armv8/mfloat-abi.hard
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.softfp=mcpu.cortex-a7/mfpu.fp-armv8/mfloat-abi.softfp
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.hard=march.armv8-a/mfpu.vfpv4/mfloat-abi.hard
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.softfp=march.armv8-a/mfpu.vfpv4/mfloat-abi.softfp
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.hard=march.armv8-a/mfpu.vfpv4-d16/mfloat-abi.hard
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.softfp=march.armv8-a/mfpu.vfpv4-d16/mfloat-abi.softfp
+
+
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.neon-vfpv4/mfloat-abi.hard=march.armv8-a/mfpu.neon-vfpv4/mfloat-abi.hard
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.neon-vfpv4/mfloat-abi.softfp=march.armv8-a/mfpu.neon-vfpv4/mfloat-abi.softfp
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.neon-vfpv4/mfloat-abi.hard=mcpu.cortex-a7/mfpu.neon-fp-armv8/mfloat-abi.hard
+MULTILIB_REUSE	      += mcpu.cortex-a7/mfpu.neon-vfpv4/mfloat-abi.softfp=mcpu.cortex-a7/mfpu.neon-fp-armv8/mfloat-abi.softfp
+
+
+
+# And again for mthumb.
+
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=mthumb/mcpu.cortex-a7/mfpu.vfpv3-d16/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=mthumb/mcpu.cortex-a7/mfpu.vfpv3-d16/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=mthumb/march.armv8-a/mfpu.vfpv3-d16/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=mthumb/march.armv8-a/mfpu.vfpv3-d16/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=mthumb/march.armv7-a/mfpu.vfpv4-d16/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=mthumb/march.armv7-a/mfpu.vfpv4-d16/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=mthumb/march.armv7-a/mfpu.fp-armv8/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=mthumb/march.armv7-a/mfpu.fp-armv8/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.hard=mthumb/march.armv7-a/mfpu.vfpv4/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.vfpv3-d16/mfloat-abi.softfp=mthumb/march.armv7-a/mfpu.vfpv4/mfloat-abi.softfp
+
+
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.neon/mfloat-abi.hard=mthumb/mcpu.cortex-a7/mfpu.neon/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.neon/mfloat-abi.softfp=mthumb/mcpu.cortex-a7/mfpu.neon/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.neon/mfloat-abi.hard=mthumb/march.armv8-a/mfpu.neon/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.neon/mfloat-abi.softfp=mthumb/march.armv8-a/mfpu.neon/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.neon/mfloat-abi.hard=mthumb/march.armv7-a/mfpu.neon-vfpv4/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.neon/mfloat-abi.softfp=mthumb/march.armv7-a/mfpu.neon-vfpv4/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.neon/mfloat-abi.hard=mthumb/march.armv7-a/mfpu.neon-fp-armv8/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/march.armv7-a/mfpu.neon/mfloat-abi.softfp=mthumb/march.armv7-a/mfpu.neon-fp-armv8/mfloat-abi.softfp
+
+
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.hard=mthumb/mcpu.cortex-a7/mfpu.fp-armv8/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.softfp=mthumb/mcpu.cortex-a7/mfpu.fp-armv8/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.hard=mthumb/march.armv8-a/mfpu.vfpv4/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.softfp=mthumb/march.armv8-a/mfpu.vfpv4/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.hard=mthumb/march.armv8-a/mfpu.vfpv4-d16/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.vfpv4-d16/mfloat-abi.softfp=mthumb/march.armv8-a/mfpu.vfpv4-d16/mfloat-abi.softfp
+
+
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.neon-vfpv4/mfloat-abi.hard=mthumb/march.armv8-a/mfpu.neon-vfpv4/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.neon-vfpv4/mfloat-abi.softfp=mthumb/march.armv8-a/mfpu.neon-vfpv4/mfloat-abi.softfp
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.neon-vfpv4/mfloat-abi.hard=mthumb/mcpu.cortex-a7/mfpu.neon-fp-armv8/mfloat-abi.hard
+MULTILIB_REUSE	      += mthumb/mcpu.cortex-a7/mfpu.neon-vfpv4/mfloat-abi.softfp=mthumb/mcpu.cortex-a7/mfpu.neon-fp-armv8/mfloat-abi.softfp
Index: b/src/gcc/config/arm/arm.h
===================================================================
--- a/src/gcc/config/arm/arm.h
+++ b/src/gcc/config/arm/arm.h
@@ -49,8 +49,14 @@ extern char arm_arch_name[];
            builtin_define ("__ARM_FEATURE_QBIT");	\
         if (TARGET_ARM_SAT)				\
            builtin_define ("__ARM_FEATURE_SAT");	\
+        if (TARGET_CRYPTO)				\
+	   builtin_define ("__ARM_FEATURE_CRYPTO");	\
 	if (unaligned_access)				\
 	  builtin_define ("__ARM_FEATURE_UNALIGNED");	\
+	if (TARGET_CRC32)				\
+	  builtin_define ("__ARM_FEATURE_CRC32");	\
+	if (TARGET_32BIT)				\
+	  builtin_define ("__ARM_32BIT_STATE");		\
 	if (TARGET_ARM_FEATURE_LDREX)				\
 	  builtin_define_with_int_value (			\
 	    "__ARM_FEATURE_LDREX", TARGET_ARM_FEATURE_LDREX);	\
@@ -183,6 +189,11 @@ extern arm_cc arm_current_cc;
 
 #define ARM_INVERSE_CONDITION_CODE(X)  ((arm_cc) (((int)X) ^ 1))
 
+/* The maximaum number of instructions that is beneficial to
+   conditionally execute. */
+#undef MAX_CONDITIONAL_EXECUTE
+#define MAX_CONDITIONAL_EXECUTE arm_max_conditional_execute ()
+
 extern int arm_target_label;
 extern int arm_ccfsm_state;
 extern GTY(()) rtx arm_target_insn;
@@ -269,6 +280,8 @@ extern void (*arm_lang_output_object_att
 #define TARGET_LDRD			(arm_arch5e && ARM_DOUBLEWORD_ALIGN \
                                          && !TARGET_THUMB1)
 
+#define TARGET_CRC32			(arm_arch_crc)
+
 /* The following two macros concern the ability to execute coprocessor
    instructions for VFPv3 or NEON.  TARGET_VFP3/TARGET_VFPD32 are currently
    only ever tested when we know we are generating for VFP hardware; we need
@@ -350,10 +363,16 @@ extern void (*arm_lang_output_object_att
 #define TARGET_HAVE_LDREXD	(((arm_arch6k && TARGET_ARM) || arm_arch7) \
 				 && arm_arch_notm)
 
+/* Nonzero if this chip supports load-acquire and store-release.  */
+#define TARGET_HAVE_LDACQ	(TARGET_ARM_ARCH >= 8)
+
 /* Nonzero if integer division instructions supported.  */
 #define TARGET_IDIV		((TARGET_ARM && arm_arch_arm_hwdiv) \
 				 || (TARGET_THUMB2 && arm_arch_thumb_hwdiv))
 
+/* Should NEON be used for 64-bits bitops.  */
+#define TARGET_PREFER_NEON_64BITS (prefer_neon_for_64bits)
+
 /* True iff the full BPABI is being used.  If TARGET_BPABI is true,
    then TARGET_AAPCS_BASED must be true -- but the converse does not
    hold.  TARGET_BPABI implies the use of the BPABI runtime library,
@@ -539,6 +558,13 @@ extern int arm_arch_arm_hwdiv;
 /* Nonzero if chip supports integer division instruction in Thumb mode.  */
 extern int arm_arch_thumb_hwdiv;
 
+/* Nonzero if we should use Neon to handle 64-bits operations rather
+   than core registers.  */
+extern int prefer_neon_for_64bits;
+
+/* Nonzero if chip supports the ARMv8 CRC instructions.  */
+extern int arm_arch_crc;
+
 #ifndef TARGET_DEFAULT
 #define TARGET_DEFAULT  (MASK_APCS_FRAME)
 #endif
@@ -630,6 +656,8 @@ extern int arm_arch_thumb_hwdiv;
 
 #define BIGGEST_ALIGNMENT (ARM_DOUBLEWORD_ALIGN ? DOUBLEWORD_ALIGNMENT : 32)
 
+#define MALLOC_ABI_ALIGNMENT  BIGGEST_ALIGNMENT
+
 /* XXX Blah -- this macro is used directly by libobjc.  Since it
    supports no vector modes, cut out the complexity and fall back
    on BIGGEST_FIELD_ALIGNMENT.  */
@@ -1040,7 +1068,7 @@ extern int arm_arch_thumb_hwdiv;
 /* Modes valid for Neon D registers.  */
 #define VALID_NEON_DREG_MODE(MODE) \
   ((MODE) == V2SImode || (MODE) == V4HImode || (MODE) == V8QImode \
-   || (MODE) == V2SFmode || (MODE) == DImode)
+   || (MODE) == V4HFmode || (MODE) == V2SFmode || (MODE) == DImode)
 
 /* Modes valid for Neon Q registers.  */
 #define VALID_NEON_QREG_MODE(MODE) \
@@ -1130,6 +1158,7 @@ enum reg_class
   STACK_REG,
   BASE_REGS,
   HI_REGS,
+  CALLER_SAVE_REGS,
   GENERAL_REGS,
   CORE_REGS,
   VFP_D0_D7_REGS,
@@ -1156,6 +1185,7 @@ enum reg_class
   "STACK_REG",		\
   "BASE_REGS",		\
   "HI_REGS",		\
+  "CALLER_SAVE_REGS",	\
   "GENERAL_REGS",	\
   "CORE_REGS",		\
   "VFP_D0_D7_REGS",	\
@@ -1181,6 +1211,7 @@ enum reg_class
   { 0x00002000, 0x00000000, 0x00000000, 0x00000000 }, /* STACK_REG */	\
   { 0x000020FF, 0x00000000, 0x00000000, 0x00000000 }, /* BASE_REGS */	\
   { 0x00005F00, 0x00000000, 0x00000000, 0x00000000 }, /* HI_REGS */	\
+  { 0x0000100F, 0x00000000, 0x00000000, 0x00000000 }, /* CALLER_SAVE_REGS */ \
   { 0x00005FFF, 0x00000000, 0x00000000, 0x00000000 }, /* GENERAL_REGS */ \
   { 0x00007FFF, 0x00000000, 0x00000000, 0x00000000 }, /* CORE_REGS */	\
   { 0xFFFF0000, 0x00000000, 0x00000000, 0x00000000 }, /* VFP_D0_D7_REGS  */ \
@@ -1643,7 +1674,7 @@ typedef struct
    frame.  */
 #define EXIT_IGNORE_STACK 1
 
-#define EPILOGUE_USES(REGNO) ((REGNO) == LR_REGNUM)
+#define EPILOGUE_USES(REGNO) (epilogue_completed && (REGNO) == LR_REGNUM)
 
 /* Determine if the epilogue should be output as RTL.
    You should override this if you define FUNCTION_EXTRA_EPILOGUE.  */
Index: b/src/gcc/config/arm/cortex-a8-neon.md
===================================================================
--- a/src/gcc/config/arm/cortex-a8-neon.md
+++ b/src/gcc/config/arm/cortex-a8-neon.md
@@ -18,6 +18,221 @@
 ;; along with GCC; see the file COPYING3.  If not see
 ;; <http://www.gnu.org/licenses/>.
 
+(define_attr "cortex_a8_neon_type"
+   "neon_int_1,neon_int_2,neon_int_3,neon_int_4,neon_int_5,neon_vqneg_vqabs,
+   neon_bit_ops_q,
+   neon_vaba,neon_vaba_qqq, neon_vmov,
+   neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,neon_mul_qqq_8_16_32_ddd_32,
+   neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar,
+   neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,neon_mla_qqq_8_16,
+   neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long,
+   neon_mla_qqq_32_qqd_32_scalar,neon_mul_ddd_16_scalar_32_16_long_scalar,
+   neon_mul_qqd_32_scalar,neon_mla_ddd_16_scalar_qdd_32_16_long_scalar,
+   neon_shift_1,neon_shift_2,neon_shift_3,
+   neon_vqshl_vrshl_vqrshl_qqq,neon_vsra_vrsra,neon_fp_vadd_ddd_vabs_dd,
+   neon_fp_vadd_qqq_vabs_qq,neon_fp_vsum,neon_fp_vmul_ddd,neon_fp_vmul_qqd,
+   neon_fp_vmla_ddd,neon_fp_vmla_qqq,neon_fp_vmla_ddd_scalar,
+   neon_fp_vmla_qqq_scalar,neon_fp_vrecps_vrsqrts_ddd,
+   neon_fp_vrecps_vrsqrts_qqq,neon_bp_simple,neon_bp_2cycle,neon_bp_3cycle,
+   neon_ldr,neon_str,neon_vld1_1_2_regs,neon_vld1_3_4_regs,
+   neon_vld2_2_regs_vld1_vld2_all_lanes,neon_vld2_4_regs,neon_vld3_vld4,
+   neon_vst1_1_2_regs_vst2_2_regs,neon_vst1_3_4_regs,
+   neon_vst2_4_regs_vst3_vst4,neon_vld1_vld2_lane,
+   neon_vld3_vld4_lane,neon_vst1_vst2_lane,neon_vst3_vst4_lane,
+   neon_vld3_vld4_all_lanes,neon_mcr,neon_mcr_2_mcrr,neon_mrc,neon_mrrc,
+   neon_ldm_2,neon_stm_2,none,unknown"
+  (cond [
+          (eq_attr "type" "neon_logic, neon_logic_q,\
+                           neon_bsl, neon_cls, neon_cnt,\
+                           neon_add, neon_add_q")
+                          (const_string "neon_int_1")
+          (eq_attr "type" "neon_add_widen, neon_sub_widen,\
+                           neon_sub, neon_sub_q")
+                          (const_string "neon_int_2")
+          (eq_attr "type" "neon_neg, neon_neg_q,\
+                           neon_reduc_add, neon_reduc_add_q,\
+                           neon_reduc_add_long,\
+                           neon_add_long, neon_sub_long")
+                          (const_string "neon_int_3")
+          (eq_attr "type" "neon_abs, neon_abs_q,
+                           neon_compare_zero, neon_compare_zero_q,\
+                           neon_add_halve_narrow_q,\
+                           neon_sub_halve_narrow_q,\
+                           neon_add_halve, neon_add_halve_q,\
+                           neon_qadd, neon_qadd_q,\
+                           neon_tst, neon_tst_q")
+                          (const_string "neon_int_4")
+          (eq_attr "type" "neon_abd_long, neon_sub_halve, neon_sub_halve_q,\
+                           neon_qsub, neon_qsub_q,\
+                           neon_abd, neon_abd_q,\
+                           neon_compare, neon_compare_q,\
+                           neon_minmax, neon_minmax_q, neon_reduc_minmax,\
+                           neon_reduc_minmax_q")
+                          (const_string "neon_int_5")
+          (eq_attr "type" "neon_qneg, neon_qneg_q, neon_qabs, neon_qabs_q")
+                           (const_string "neon_vqneg_vqabs")
+          (eq_attr "type" "neon_move, neon_move_q")
+                           (const_string "neon_vmov")
+          (eq_attr "type" "neon_bsl_q, neon_cls_q, neon_cnt_q")
+                           (const_string "neon_bit_ops_q")
+          (eq_attr "type" "neon_arith_acc, neon_reduc_add_acc")
+                          (const_string "neon_vaba")
+          (eq_attr "type" "neon_arith_acc_q")
+                          (const_string "neon_vaba_qqq")
+          (eq_attr "type" "neon_shift_imm, neon_shift_imm_q,\
+                           neon_shift_imm_long, neon_shift_imm_narrow_q,\
+                           neon_shift_reg")
+                           (const_string "neon_shift_1")
+          (eq_attr "type" "neon_sat_shift_imm, neon_sat_shift_imm_q,
+                           neon_sat_shift_imm_narrow_q,\
+                           neon_sat_shift_reg")
+                           (const_string "neon_shift_2")
+          (eq_attr "type" "neon_shift_reg_q")
+                           (const_string "neon_shift_3")
+          (eq_attr "type" "neon_sat_shift_reg_q")
+                           (const_string "neon_vqshl_vrshl_vqrshl_qqq")
+          (eq_attr "type" "neon_shift_acc, neon_shift_acc_q")
+                           (const_string "neon_vsra_vrsra")
+          (eq_attr "type" "neon_mul_b, neon_mul_h,\
+                           neon_mul_b_long, neon_mul_h_long,\
+                           neon_sat_mul_b, neon_sat_mul_h,\
+                           neon_sat_mul_b_long, neon_sat_mul_h_long")
+                           (const_string
+                            "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long")
+          (eq_attr "type" "neon_mul_b_q, neon_mul_h_q,\
+                           neon_sat_mul_b_q, neon_sat_mul_h_q")
+                           (const_string "neon_mul_qqq_8_16_32_ddd_32")
+          (eq_attr "type" "neon_mul_s, neon_mul_s_long,\
+                           neon_sat_mul_s, neon_sat_mul_s_long,\
+                           neon_mul_h_scalar_q, neon_sat_mul_h_scalar_q,\
+                           neon_mul_s_scalar, neon_sat_mul_s_scalar,\
+                           neon_mul_s_scalar_long,\
+                           neon_sat_mul_s_scalar_long")
+                           (const_string
+             "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")
+          (eq_attr "type" "neon_mla_b, neon_mla_h,\
+                           neon_mla_b_long, neon_mla_h_long,\
+                           neon_sat_mla_b_long, neon_sat_mla_h_long,\
+                           neon_sat_mla_h_scalar_long")
+                           (const_string
+                             "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
+          (eq_attr "type" "neon_mla_b_q, neon_mla_h_q")
+                           (const_string "neon_mla_qqq_8_16")
+          (eq_attr "type" "neon_mla_s, neon_mla_s_long,\
+                           neon_sat_mla_s_long,\
+                           neon_mla_h_scalar_q, neon_mla_s_scalar,\
+                           neon_mla_s_scalar_long,\
+                           neon_sat_mla_s_scalar_long")
+                           (const_string
+ "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")
+          (eq_attr "type" "neon_mla_s_q, neon_mla_s_scalar_q")
+                           (const_string "neon_mla_qqq_32_qqd_32_scalar")
+          (eq_attr "type" "neon_mul_h_scalar, neon_sat_mul_h_scalar,\
+                           neon_mul_h_scalar_long,\
+                           neon_sat_mul_h_scalar_long")
+                          (const_string
+                            "neon_mul_ddd_16_scalar_32_16_long_scalar")
+          (eq_attr "type" "neon_mul_s_q, neon_sat_mul_s_q,\
+                           neon_mul_s_scalar_q")
+                           (const_string "neon_mul_qqd_32_scalar")
+          (eq_attr "type" "neon_mla_h_scalar, neon_mla_h_scalar_long")
+                           (const_string
+                             "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar")
+          (eq_attr "type" "neon_fp_abd_s, neon_fp_abs_s, neon_fp_neg_s,\
+                           neon_fp_addsub_s, neon_fp_compare_s,\
+                           neon_fp_minmax_s, neon_fp_mul_s,\
+                           neon_fp_recpe_s, neon_fp_rsqrte_s,\
+                           neon_fp_to_int_s, neon_int_to_fp_s")
+                           (const_string "neon_fp_vadd_ddd_vabs_dd")
+          (eq_attr "type" "neon_fp_abd_s_q, neon_fp_abs_s_q,\
+                           neon_fp_neg_s_q,\
+                           neon_fp_addsub_s_q, neon_fp_compare_s_q,\
+                           neon_fp_minmax_s_q, neon_fp_mul_s_q,\
+                           neon_fp_recpe_s_q, neon_fp_rsqrte_s_q,\
+                           neon_fp_to_int_s_q, neon_int_to_fp_s_q")
+                           (const_string "neon_fp_vadd_qqq_vabs_qq")
+          (eq_attr "type" "neon_fp_reduc_add_s, neon_fp_reduc_minmax_s,\
+                           neon_fp_reduc_add_s_q, neon_fp_reduc_minmax_s_q")
+                           (const_string "neon_fp_vsum")
+          (eq_attr "type" "neon_fp_mul_s_scalar")
+                           (const_string "neon_fp_vmul_ddd")
+          (eq_attr "type" "neon_fp_mul_s_scalar_q")
+                           (const_string "neon_fp_vmul_qqd")
+          (eq_attr "type" "neon_fp_mla_s")
+                           (const_string "neon_fp_vmla_ddd")
+          (eq_attr "type" "neon_fp_mla_s_q")
+                           (const_string "neon_fp_vmla_qqq")
+          (eq_attr "type" "neon_fp_mla_s_scalar")
+                           (const_string "neon_fp_vmla_ddd_scalar")
+          (eq_attr "type" "neon_fp_mla_s_scalar_q")
+                           (const_string "neon_fp_vmla_qqq_scalar")
+          (eq_attr "type" "neon_fp_recps_s, neon_fp_rsqrts_s")
+                           (const_string "neon_fp_vrecps_vrsqrts_ddd")
+          (eq_attr "type" "neon_fp_recps_s_q, neon_fp_rsqrts_s_q")
+                           (const_string "neon_fp_vrecps_vrsqrts_qqq")
+          (eq_attr "type" "neon_move_narrow_q, neon_dup,\
+                           neon_dup_q, neon_permute, neon_zip,\
+                           neon_ext, neon_rev, neon_rev_q")
+                           (const_string "neon_bp_simple")
+          (eq_attr "type" "neon_permute_q, neon_ext_q, neon_tbl1, neon_tbl2")
+                           (const_string "neon_bp_2cycle")
+          (eq_attr "type" "neon_zip_q, neon_tbl3, neon_tbl4")
+                           (const_string "neon_bp_3cycle")
+          (eq_attr "type" "neon_ldr")
+                           (const_string "neon_ldr")
+          (eq_attr "type" "neon_str")
+                           (const_string "neon_str")
+          (eq_attr "type" "neon_load1_1reg, neon_load1_1reg_q,\
+                           neon_load1_2reg, neon_load1_2reg_q,\
+                           neon_load2_2reg, neon_load2_2reg_q")
+                           (const_string "neon_vld1_1_2_regs")
+          (eq_attr "type" "neon_load1_3reg, neon_load1_3reg_q,\
+                           neon_load1_4reg, neon_load1_4reg_q")
+                           (const_string "neon_vld1_3_4_regs")
+          (eq_attr "type" "neon_load1_all_lanes, neon_load1_all_lanes_q,\
+                           neon_load2_all_lanes, neon_load2_all_lanes_q")
+                           (const_string
+                              "neon_vld2_2_regs_vld1_vld2_all_lanes")
+          (eq_attr "type" "neon_load3_all_lanes, neon_load3_all_lanes_q,\
+                           neon_load4_all_lanes, neon_load4_all_lanes_q,\
+                           neon_load2_4reg, neon_load2_4reg_q")
+                           (const_string "neon_vld2_4_regs")
+          (eq_attr "type" "neon_load3_3reg, neon_load3_3reg_q,\
+                           neon_load4_4reg, neon_load4_4reg_q")
+                           (const_string "neon_vld3_vld4")
+          (eq_attr "type" "f_loads, f_loadd, f_stores, f_stored,\
+                           neon_load1_one_lane, neon_load1_one_lane_q,\
+                           neon_load2_one_lane, neon_load2_one_lane_q")
+                           (const_string "neon_vld1_vld2_lane")
+          (eq_attr "type" "neon_load3_one_lane, neon_load3_one_lane_q,\
+                           neon_load4_one_lane, neon_load4_one_lane_q")
+                           (const_string "neon_vld3_vld4_lane")
+          (eq_attr "type" "neon_store1_1reg, neon_store1_1reg_q,\
+                           neon_store1_2reg, neon_store1_2reg_q,\
+                           neon_store2_2reg, neon_store2_2reg_q")
+                           (const_string "neon_vst1_1_2_regs_vst2_2_regs")
+          (eq_attr "type" "neon_store1_3reg, neon_store1_3reg_q,\
+                           neon_store1_4reg, neon_store1_4reg_q")
+                           (const_string "neon_vst1_3_4_regs")
+          (eq_attr "type" "neon_store2_4reg, neon_store2_4reg_q,\
+                           neon_store3_3reg, neon_store3_3reg_q,\
+                           neon_store4_4reg, neon_store4_4reg_q")
+                           (const_string "neon_vst2_4_regs_vst3_vst4")
+          (eq_attr "type" "neon_store1_one_lane, neon_store1_one_lane_q,\
+                           neon_store2_one_lane, neon_store2_one_lane_q")
+                           (const_string "neon_vst1_vst2_lane")
+          (eq_attr "type" "neon_store3_one_lane, neon_store3_one_lane_q,\
+                           neon_store4_one_lane, neon_store4_one_lane_q")
+                           (const_string "neon_vst3_vst4_lane")
+          (eq_attr "type" "neon_from_gp, f_mcr")
+                           (const_string "neon_mcr")
+          (eq_attr "type" "neon_from_gp_q, f_mcrr")
+                           (const_string "neon_mcr_2_mcrr")
+          (eq_attr "type" "neon_to_gp, f_mrc")
+                           (const_string "neon_mrc")
+          (eq_attr "type" "neon_to_gp_q, f_mrrc")
+                           (const_string "neon_mrrc")]
+          (const_string "unknown")))
 
 (define_automaton "cortex_a8_neon")
 
@@ -159,12 +374,12 @@
 
 (define_insn_reservation "cortex_a8_vfp_divs" 37
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "type" "fdivs"))
+       (eq_attr "type" "fdivs, fsqrts"))
   "cortex_a8_vfp,cortex_a8_vfplite*36")
 
 (define_insn_reservation "cortex_a8_vfp_divd" 65
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "type" "fdivd"))
+       (eq_attr "type" "fdivd, fsqrtd"))
   "cortex_a8_vfp,cortex_a8_vfplite*64")
 
 ;; Comparisons can actually take 7 cycles sometimes instead of four,
@@ -172,74 +387,74 @@
 ;; take four cycles, we pick that latency.
 (define_insn_reservation "cortex_a8_vfp_farith" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "type" "fcpys,ffariths,ffarithd,fconsts,fconstd,fcmps,fcmpd"))
+       (eq_attr "type" "fmov,ffariths,ffarithd,fconsts,fconstd,fcmps,fcmpd"))
   "cortex_a8_vfp,cortex_a8_vfplite*3")
 
 (define_insn_reservation "cortex_a8_vfp_cvt" 7
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "type" "f_cvt"))
+       (eq_attr "type" "f_cvt,f_cvtf2i,f_cvti2f"))
   "cortex_a8_vfp,cortex_a8_vfplite*6")
 
 ;; NEON -> core transfers.
 
 (define_insn_reservation "cortex_a8_neon_mrc" 20
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mrc"))
+       (eq_attr "cortex_a8_neon_type" "neon_mrc"))
   "cortex_a8_neon_ls")
 
 (define_insn_reservation "cortex_a8_neon_mrrc" 21
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mrrc"))
+       (eq_attr "cortex_a8_neon_type" "neon_mrrc"))
   "cortex_a8_neon_ls_2")
 
-;; The remainder of this file is auto-generated by neon-schedgen.
+;; Arithmetic Operations
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N3.
 (define_insn_reservation "cortex_a8_neon_int_1" 3
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_int_1"))
+       (eq_attr "cortex_a8_neon_type" "neon_int_1"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)m operands at N1,
 ;; their (D|Q)n operands at N2, and produce a result at N3.
 (define_insn_reservation "cortex_a8_neon_int_2" 3
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_int_2"))
+       (eq_attr "cortex_a8_neon_type" "neon_int_2"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N3.
 (define_insn_reservation "cortex_a8_neon_int_3" 3
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_int_3"))
+       (eq_attr "cortex_a8_neon_type" "neon_int_3"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N4.
 (define_insn_reservation "cortex_a8_neon_int_4" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_int_4"))
+       (eq_attr "cortex_a8_neon_type" "neon_int_4"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)m operands at N1,
 ;; their (D|Q)n operands at N2, and produce a result at N4.
 (define_insn_reservation "cortex_a8_neon_int_5" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_int_5"))
+       (eq_attr "cortex_a8_neon_type" "neon_int_5"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N4.
 (define_insn_reservation "cortex_a8_neon_vqneg_vqabs" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vqneg_vqabs"))
+       (eq_attr "cortex_a8_neon_type" "neon_vqneg_vqabs"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation produce a result at N3.
 (define_insn_reservation "cortex_a8_neon_vmov" 3
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vmov"))
+       (eq_attr "cortex_a8_neon_type" "neon_vmov"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -247,7 +462,7 @@
 ;; produce a result at N6.
 (define_insn_reservation "cortex_a8_neon_vaba" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vaba"))
+       (eq_attr "cortex_a8_neon_type" "neon_vaba"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -255,35 +470,39 @@
 ;; produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_vaba_qqq" 7
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vaba_qqq"))
+       (eq_attr "cortex_a8_neon_type" "neon_vaba_qqq"))
   "cortex_a8_neon_dp_2")
 
-;; Instructions using this reservation read their (D|Q)m operands at N1,
-;; their (D|Q)d operands at N3, and produce a result at N6.
-(define_insn_reservation "cortex_a8_neon_vsma" 6
+;; Instructions using this reservation read their source operands at N2, and
+;; produce a result at N3 on cycle 2.
+(define_insn_reservation "cortex_a8_neon_bit_ops_q" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vsma"))
-  "cortex_a8_neon_dp")
+       (eq_attr "cortex_a8_neon_type" "neon_bit_ops_q"))
+  "cortex_a8_neon_dp_2")
+
+;; Integer Multiply/Accumulate Operations
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N6.
 (define_insn_reservation "cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long"))
+       (eq_attr "cortex_a8_neon_type"
+         "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_mul_qqq_8_16_32_ddd_32" 7
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mul_qqq_8_16_32_ddd_32"))
+       (eq_attr "cortex_a8_neon_type" "neon_mul_qqq_8_16_32_ddd_32"))
   "cortex_a8_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar" 7
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar"))
+       (eq_attr "cortex_a8_neon_type"
+            "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar"))
   "cortex_a8_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -291,7 +510,8 @@
 ;; produce a result at N6.
 (define_insn_reservation "cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long"))
+       (eq_attr "cortex_a8_neon_type"
+                  "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -299,7 +519,7 @@
 ;; produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_mla_qqq_8_16" 7
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mla_qqq_8_16"))
+       (eq_attr "cortex_a8_neon_type" "neon_mla_qqq_8_16"))
   "cortex_a8_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -307,7 +527,8 @@
 ;; produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long" 7
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))
+       (eq_attr "cortex_a8_neon_type"
+ "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))
   "cortex_a8_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -315,21 +536,22 @@
 ;; produce a result at N6 on cycle 4.
 (define_insn_reservation "cortex_a8_neon_mla_qqq_32_qqd_32_scalar" 9
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mla_qqq_32_qqd_32_scalar"))
+       (eq_attr "cortex_a8_neon_type" "neon_mla_qqq_32_qqd_32_scalar"))
   "cortex_a8_neon_dp_4")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N6.
 (define_insn_reservation "cortex_a8_neon_mul_ddd_16_scalar_32_16_long_scalar" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mul_ddd_16_scalar_32_16_long_scalar"))
+       (eq_attr "cortex_a8_neon_type"
+                  "neon_mul_ddd_16_scalar_32_16_long_scalar"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N6 on cycle 4.
 (define_insn_reservation "cortex_a8_neon_mul_qqd_32_scalar" 9
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mul_qqd_32_scalar"))
+       (eq_attr "cortex_a8_neon_type" "neon_mul_qqd_32_scalar"))
   "cortex_a8_neon_dp_4")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -337,84 +559,82 @@
 ;; produce a result at N6.
 (define_insn_reservation "cortex_a8_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar"))
+       (eq_attr "cortex_a8_neon_type"
+                  "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar"))
   "cortex_a8_neon_dp")
 
+;; Shift Operations
+
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N3.
 (define_insn_reservation "cortex_a8_neon_shift_1" 3
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_shift_1"))
+       (eq_attr "cortex_a8_neon_type" "neon_shift_1"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N4.
 (define_insn_reservation "cortex_a8_neon_shift_2" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_shift_2"))
+       (eq_attr "cortex_a8_neon_type" "neon_shift_2"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N3 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_shift_3" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_shift_3"))
+       (eq_attr "cortex_a8_neon_type" "neon_shift_3"))
   "cortex_a8_neon_dp_2")
 
 ;; Instructions using this reservation read their source operands at N1, and
-;; produce a result at N1.
-(define_insn_reservation "cortex_a8_neon_vshl_ddd" 1
-  (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vshl_ddd"))
-  "cortex_a8_neon_dp")
-
-;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N4 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_vqshl_vrshl_vqrshl_qqq" 5
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vqshl_vrshl_vqrshl_qqq"))
+       (eq_attr "cortex_a8_neon_type" "neon_vqshl_vrshl_vqrshl_qqq"))
   "cortex_a8_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)m operands at N1,
 ;; their (D|Q)d operands at N3, and produce a result at N6.
 (define_insn_reservation "cortex_a8_neon_vsra_vrsra" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vsra_vrsra"))
+       (eq_attr "cortex_a8_neon_type" "neon_vsra_vrsra"))
   "cortex_a8_neon_dp")
 
+;; Floating point Operations
+
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N5.
 (define_insn_reservation "cortex_a8_neon_fp_vadd_ddd_vabs_dd" 5
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vadd_ddd_vabs_dd"))
-  "cortex_a8_neon_fadd")
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vadd_ddd_vabs_dd"))
+ "cortex_a8_neon_fadd")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N5 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_fp_vadd_qqq_vabs_qq" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vadd_qqq_vabs_qq"))
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vadd_qqq_vabs_qq"))
   "cortex_a8_neon_fadd_2")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N5.
 (define_insn_reservation "cortex_a8_neon_fp_vsum" 5
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vsum"))
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vsum"))
   "cortex_a8_neon_fadd")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N5.
 (define_insn_reservation "cortex_a8_neon_fp_vmul_ddd" 5
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vmul_ddd"))
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vmul_ddd"))
   "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N5 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_fp_vmul_qqd" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vmul_qqd"))
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vmul_qqd"))
   "cortex_a8_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -422,7 +642,7 @@
 ;; produce a result at N9.
 (define_insn_reservation "cortex_a8_neon_fp_vmla_ddd" 9
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vmla_ddd"))
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vmla_ddd"))
   "cortex_a8_neon_fmul_then_fadd")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -430,7 +650,7 @@
 ;; produce a result at N9 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_fp_vmla_qqq" 10
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vmla_qqq"))
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vmla_qqq"))
   "cortex_a8_neon_fmul_then_fadd_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -438,7 +658,7 @@
 ;; produce a result at N9.
 (define_insn_reservation "cortex_a8_neon_fp_vmla_ddd_scalar" 9
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vmla_ddd_scalar"))
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vmla_ddd_scalar"))
   "cortex_a8_neon_fmul_then_fadd")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -446,152 +666,148 @@
 ;; produce a result at N9 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_fp_vmla_qqq_scalar" 10
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vmla_qqq_scalar"))
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vmla_qqq_scalar"))
   "cortex_a8_neon_fmul_then_fadd_2")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N9.
 (define_insn_reservation "cortex_a8_neon_fp_vrecps_vrsqrts_ddd" 9
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vrecps_vrsqrts_ddd"))
+       (eq_attr "cortex_a8_neon_type" "neon_fp_vrecps_vrsqrts_ddd"))
   "cortex_a8_neon_fmul_then_fadd")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N9 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_fp_vrecps_vrsqrts_qqq" 10
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_fp_vrecps_vrsqrts_qqq"))
+       (eq_attr "type" "neon_fp_recps_s_q, neon_fp_rsqrts_s_q"))
   "cortex_a8_neon_fmul_then_fadd_2")
 
+;; Permute operations.
+
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2.
 (define_insn_reservation "cortex_a8_neon_bp_simple" 2
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_bp_simple"))
+       (eq_attr "cortex_a8_neon_type" "neon_bp_simple"))
   "cortex_a8_neon_perm")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_bp_2cycle" 3
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_bp_2cycle"))
+       (eq_attr "cortex_a8_neon_type" "neon_bp_2cycle"))
   "cortex_a8_neon_perm_2")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2 on cycle 3.
 (define_insn_reservation "cortex_a8_neon_bp_3cycle" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_bp_3cycle"))
+       (eq_attr "cortex_a8_neon_type" "neon_bp_3cycle"))
   "cortex_a8_neon_perm_3")
 
+;; Load Operations.
+
 ;; Instructions using this reservation produce a result at N1.
 (define_insn_reservation "cortex_a8_neon_ldr" 1
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_ldr"))
+       (eq_attr "cortex_a8_neon_type" "neon_ldr"))
   "cortex_a8_neon_ls")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a8_neon_str" 0
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_str"))
+       (eq_attr "cortex_a8_neon_type" "neon_str"))
   "cortex_a8_neon_ls")
 
 ;; Instructions using this reservation produce a result at N1 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_vld1_1_2_regs" 2
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vld1_1_2_regs"))
+       (eq_attr "cortex_a8_neon_type" "neon_vld1_1_2_regs"))
   "cortex_a8_neon_ls_2")
 
 ;; Instructions using this reservation produce a result at N1 on cycle 3.
 (define_insn_reservation "cortex_a8_neon_vld1_3_4_regs" 3
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vld1_3_4_regs"))
+       (eq_attr "cortex_a8_neon_type" "neon_vld1_3_4_regs"))
   "cortex_a8_neon_ls_3")
 
 ;; Instructions using this reservation produce a result at N2 on cycle 2.
 (define_insn_reservation "cortex_a8_neon_vld2_2_regs_vld1_vld2_all_lanes" 3
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vld2_2_regs_vld1_vld2_all_lanes"))
+       (eq_attr "cortex_a8_neon_type" "neon_vld2_2_regs_vld1_vld2_all_lanes"))
   "cortex_a8_neon_ls_2")
 
 ;; Instructions using this reservation produce a result at N2 on cycle 3.
 (define_insn_reservation "cortex_a8_neon_vld2_4_regs" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vld2_4_regs"))
+       (eq_attr "cortex_a8_neon_type" "neon_vld2_4_regs"))
   "cortex_a8_neon_ls_3")
 
 ;; Instructions using this reservation produce a result at N2 on cycle 4.
 (define_insn_reservation "cortex_a8_neon_vld3_vld4" 5
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vld3_vld4"))
+       (eq_attr "cortex_a8_neon_type" "neon_vld3_vld4"))
   "cortex_a8_neon_ls_4")
 
+;; Store operations.
+
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a8_neon_vst1_1_2_regs_vst2_2_regs" 0
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vst1_1_2_regs_vst2_2_regs"))
+       (eq_attr "cortex_a8_neon_type" "neon_vst1_1_2_regs_vst2_2_regs"))
   "cortex_a8_neon_ls_2")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a8_neon_vst1_3_4_regs" 0
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vst1_3_4_regs"))
+       (eq_attr "cortex_a8_neon_type" "neon_vst1_3_4_regs"))
   "cortex_a8_neon_ls_3")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a8_neon_vst2_4_regs_vst3_vst4" 0
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vst2_4_regs_vst3_vst4"))
-  "cortex_a8_neon_ls_4")
-
-;; Instructions using this reservation read their source operands at N1.
-(define_insn_reservation "cortex_a8_neon_vst3_vst4" 0
-  (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vst3_vst4"))
+       (eq_attr "cortex_a8_neon_type" "neon_vst2_4_regs_vst3_vst4"))
   "cortex_a8_neon_ls_4")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2 on cycle 3.
 (define_insn_reservation "cortex_a8_neon_vld1_vld2_lane" 4
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vld1_vld2_lane"))
+       (eq_attr "cortex_a8_neon_type" "neon_vld1_vld2_lane"))
   "cortex_a8_neon_ls_3")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2 on cycle 5.
 (define_insn_reservation "cortex_a8_neon_vld3_vld4_lane" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vld3_vld4_lane"))
+       (eq_attr "cortex_a8_neon_type" "neon_vld3_vld4_lane"))
   "cortex_a8_neon_ls_5")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a8_neon_vst1_vst2_lane" 0
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vst1_vst2_lane"))
+       (eq_attr "cortex_a8_neon_type" "neon_vst1_vst2_lane"))
   "cortex_a8_neon_ls_2")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a8_neon_vst3_vst4_lane" 0
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vst3_vst4_lane"))
+       (eq_attr "cortex_a8_neon_type" "neon_vst3_vst4_lane"))
   "cortex_a8_neon_ls_3")
 
-;; Instructions using this reservation produce a result at N2 on cycle 2.
-(define_insn_reservation "cortex_a8_neon_vld3_vld4_all_lanes" 3
-  (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_vld3_vld4_all_lanes"))
-  "cortex_a8_neon_ls_3")
+;; Register Transfer Operations
 
 ;; Instructions using this reservation produce a result at N2.
 (define_insn_reservation "cortex_a8_neon_mcr" 2
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mcr"))
+       (eq_attr "cortex_a8_neon_type" "neon_mcr"))
   "cortex_a8_neon_perm")
 
 ;; Instructions using this reservation produce a result at N2.
 (define_insn_reservation "cortex_a8_neon_mcr_2_mcrr" 2
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "neon_type" "neon_mcr_2_mcrr"))
+       (eq_attr "cortex_a8_neon_type" "neon_mcr_2_mcrr"))
   "cortex_a8_neon_perm_2")
 
 ;; Exceptions to the default latencies.
@@ -599,6 +815,7 @@
 (define_bypass 1 "cortex_a8_neon_mcr_2_mcrr"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -613,20 +830,7 @@
 (define_bypass 1 "cortex_a8_neon_mcr"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
-               cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a8_neon_mla_qqq_8_16,\
-               cortex_a8_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a8_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a8_neon_fp_vmla_ddd,\
-               cortex_a8_neon_fp_vmla_qqq,\
-               cortex_a8_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a8_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 2 "cortex_a8_neon_vld3_vld4_all_lanes"
-               "cortex_a8_neon_int_1,\
-               cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -641,6 +845,7 @@
 (define_bypass 5 "cortex_a8_neon_vld3_vld4_lane"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -655,6 +860,7 @@
 (define_bypass 3 "cortex_a8_neon_vld1_vld2_lane"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -669,6 +875,7 @@
 (define_bypass 4 "cortex_a8_neon_vld3_vld4"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -683,6 +890,7 @@
 (define_bypass 3 "cortex_a8_neon_vld2_4_regs"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -697,6 +905,7 @@
 (define_bypass 2 "cortex_a8_neon_vld2_2_regs_vld1_vld2_all_lanes"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -711,6 +920,7 @@
 (define_bypass 2 "cortex_a8_neon_vld1_3_4_regs"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -725,6 +935,7 @@
 (define_bypass 1 "cortex_a8_neon_vld1_1_2_regs"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -739,6 +950,7 @@
 (define_bypass 0 "cortex_a8_neon_ldr"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -753,6 +965,7 @@
 (define_bypass 3 "cortex_a8_neon_bp_3cycle"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -767,6 +980,7 @@
 (define_bypass 2 "cortex_a8_neon_bp_2cycle"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -781,6 +995,7 @@
 (define_bypass 1 "cortex_a8_neon_bp_simple"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -795,6 +1010,7 @@
 (define_bypass 9 "cortex_a8_neon_fp_vrecps_vrsqrts_qqq"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -809,6 +1025,7 @@
 (define_bypass 8 "cortex_a8_neon_fp_vrecps_vrsqrts_ddd"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -823,6 +1040,7 @@
 (define_bypass 9 "cortex_a8_neon_fp_vmla_qqq_scalar"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -837,6 +1055,7 @@
 (define_bypass 8 "cortex_a8_neon_fp_vmla_ddd_scalar"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -851,6 +1070,7 @@
 (define_bypass 9 "cortex_a8_neon_fp_vmla_qqq"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -865,6 +1085,7 @@
 (define_bypass 8 "cortex_a8_neon_fp_vmla_ddd"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -879,6 +1100,7 @@
 (define_bypass 5 "cortex_a8_neon_fp_vmul_qqd"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -893,6 +1115,7 @@
 (define_bypass 4 "cortex_a8_neon_fp_vmul_ddd"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -907,6 +1130,7 @@
 (define_bypass 4 "cortex_a8_neon_fp_vsum"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -921,6 +1145,7 @@
 (define_bypass 5 "cortex_a8_neon_fp_vadd_qqq_vabs_qq"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -935,6 +1160,7 @@
 (define_bypass 4 "cortex_a8_neon_fp_vadd_ddd_vabs_dd"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -949,6 +1175,7 @@
 (define_bypass 5 "cortex_a8_neon_vsra_vrsra"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -963,20 +1190,7 @@
 (define_bypass 4 "cortex_a8_neon_vqshl_vrshl_vqrshl_qqq"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
-               cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a8_neon_mla_qqq_8_16,\
-               cortex_a8_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a8_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a8_neon_fp_vmla_ddd,\
-               cortex_a8_neon_fp_vmla_qqq,\
-               cortex_a8_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a8_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 0 "cortex_a8_neon_vshl_ddd"
-               "cortex_a8_neon_int_1,\
-               cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -991,6 +1205,7 @@
 (define_bypass 3 "cortex_a8_neon_shift_3"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1005,6 +1220,7 @@
 (define_bypass 3 "cortex_a8_neon_shift_2"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1019,6 +1235,7 @@
 (define_bypass 2 "cortex_a8_neon_shift_1"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1033,6 +1250,7 @@
 (define_bypass 5 "cortex_a8_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1047,6 +1265,7 @@
 (define_bypass 8 "cortex_a8_neon_mul_qqd_32_scalar"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1061,6 +1280,7 @@
 (define_bypass 5 "cortex_a8_neon_mul_ddd_16_scalar_32_16_long_scalar"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1075,6 +1295,7 @@
 (define_bypass 8 "cortex_a8_neon_mla_qqq_32_qqd_32_scalar"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1089,6 +1310,7 @@
 (define_bypass 6 "cortex_a8_neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1103,6 +1325,7 @@
 (define_bypass 6 "cortex_a8_neon_mla_qqq_8_16"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1117,6 +1340,7 @@
 (define_bypass 5 "cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1131,6 +1355,7 @@
 (define_bypass 6 "cortex_a8_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1145,6 +1370,7 @@
 (define_bypass 6 "cortex_a8_neon_mul_qqq_8_16_32_ddd_32"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1159,20 +1385,7 @@
 (define_bypass 5 "cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
-               cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a8_neon_mla_qqq_8_16,\
-               cortex_a8_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a8_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a8_neon_fp_vmla_ddd,\
-               cortex_a8_neon_fp_vmla_qqq,\
-               cortex_a8_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a8_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a8_neon_vsma"
-               "cortex_a8_neon_int_1,\
-               cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1187,6 +1400,7 @@
 (define_bypass 6 "cortex_a8_neon_vaba_qqq"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1201,6 +1415,7 @@
 (define_bypass 5 "cortex_a8_neon_vaba"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1212,9 +1427,10 @@
                cortex_a8_neon_fp_vrecps_vrsqrts_ddd,\
                cortex_a8_neon_fp_vrecps_vrsqrts_qqq")
 
-(define_bypass 2 "cortex_a8_neon_vmov"
+(define_bypass 3 "cortex_a8_neon_bit_ops_q"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1229,6 +1445,7 @@
 (define_bypass 3 "cortex_a8_neon_vqneg_vqabs"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1243,6 +1460,7 @@
 (define_bypass 3 "cortex_a8_neon_int_5"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1257,6 +1475,7 @@
 (define_bypass 3 "cortex_a8_neon_int_4"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1271,6 +1490,7 @@
 (define_bypass 2 "cortex_a8_neon_int_3"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1285,6 +1505,7 @@
 (define_bypass 2 "cortex_a8_neon_int_2"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1299,6 +1520,7 @@
 (define_bypass 2 "cortex_a8_neon_int_1"
                "cortex_a8_neon_int_1,\
                cortex_a8_neon_int_4,\
+               cortex_a8_neon_bit_ops_q,\
                cortex_a8_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a8_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a8_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
Index: b/src/gcc/config/arm/cortex-a9-neon.md
===================================================================
--- a/src/gcc/config/arm/cortex-a9-neon.md
+++ b/src/gcc/config/arm/cortex-a9-neon.md
@@ -19,6 +19,220 @@
 ;; along with GCC; see the file COPYING3.  If not see
 ;; <http://www.gnu.org/licenses/>.
 
+(define_attr "cortex_a9_neon_type"
+   "neon_int_1,neon_int_2,neon_int_3,neon_int_4,neon_int_5,neon_vqneg_vqabs,
+   neon_bit_ops_q,
+   neon_vaba,neon_vaba_qqq, neon_vmov,
+   neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,neon_mul_qqq_8_16_32_ddd_32,
+   neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar,
+   neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,neon_mla_qqq_8_16,
+   neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long,
+   neon_mla_qqq_32_qqd_32_scalar,neon_mul_ddd_16_scalar_32_16_long_scalar,
+   neon_mul_qqd_32_scalar,neon_mla_ddd_16_scalar_qdd_32_16_long_scalar,
+   neon_shift_1,neon_shift_2,neon_shift_3,
+   neon_vqshl_vrshl_vqrshl_qqq,neon_vsra_vrsra,neon_fp_vadd_ddd_vabs_dd,
+   neon_fp_vadd_qqq_vabs_qq,neon_fp_vsum,neon_fp_vmul_ddd,neon_fp_vmul_qqd,
+   neon_fp_vmla_ddd,neon_fp_vmla_qqq,neon_fp_vmla_ddd_scalar,
+   neon_fp_vmla_qqq_scalar,neon_fp_vrecps_vrsqrts_ddd,
+   neon_fp_vrecps_vrsqrts_qqq,neon_bp_simple,neon_bp_2cycle,neon_bp_3cycle,
+   neon_ldr,neon_str,neon_vld1_1_2_regs,neon_vld1_3_4_regs,
+   neon_vld2_2_regs_vld1_vld2_all_lanes,neon_vld2_4_regs,neon_vld3_vld4,
+   neon_vst1_1_2_regs_vst2_2_regs,neon_vst1_3_4_regs,
+   neon_vst2_4_regs_vst3_vst4,neon_vld1_vld2_lane,
+   neon_vld3_vld4_lane,neon_vst1_vst2_lane,neon_vst3_vst4_lane,
+   neon_vld3_vld4_all_lanes,neon_mcr,neon_mcr_2_mcrr,neon_mrc,neon_mrrc,
+   neon_ldm_2,neon_stm_2,none,unknown"
+  (cond [
+          (eq_attr "type" "neon_logic, neon_logic_q,\
+                           neon_bsl, neon_cls, neon_cnt,\
+                           neon_add, neon_add_q")
+                          (const_string "neon_int_1")
+          (eq_attr "type" "neon_add_widen, neon_sub_widen,\
+                           neon_sub, neon_sub_q")
+                          (const_string "neon_int_2")
+          (eq_attr "type" "neon_neg, neon_neg_q,\
+                           neon_reduc_add, neon_reduc_add_q,\
+                           neon_reduc_add_long,\
+                           neon_add_long, neon_sub_long")
+                          (const_string "neon_int_3")
+          (eq_attr "type" "neon_abs, neon_abs_q,
+                           neon_compare_zero, neon_compare_zero_q,\
+                           neon_add_halve_narrow_q,\
+                           neon_sub_halve_narrow_q,\
+                           neon_add_halve, neon_add_halve_q,\
+                           neon_qadd, neon_qadd_q,\
+                           neon_tst, neon_tst_q")
+                          (const_string "neon_int_4")
+          (eq_attr "type" "neon_abd_long, neon_sub_halve, neon_sub_halve_q,\
+                           neon_qsub, neon_qsub_q,\
+                           neon_abd, neon_abd_q,\
+                           neon_compare, neon_compare_q,\
+                           neon_minmax, neon_minmax_q, neon_reduc_minmax,\
+                           neon_reduc_minmax_q")
+                          (const_string "neon_int_5")
+          (eq_attr "type" "neon_qneg, neon_qneg_q, neon_qabs, neon_qabs_q")
+                           (const_string "neon_vqneg_vqabs")
+          (eq_attr "type" "neon_move, neon_move_q")
+                           (const_string "neon_vmov")
+          (eq_attr "type" "neon_bsl_q, neon_cls_q, neon_cnt_q")
+                           (const_string "neon_bit_ops_q")
+          (eq_attr "type" "neon_arith_acc, neon_reduc_add_acc")
+                          (const_string "neon_vaba")
+          (eq_attr "type" "neon_arith_acc_q")
+                          (const_string "neon_vaba_qqq")
+          (eq_attr "type" "neon_shift_imm, neon_shift_imm_q,\
+                           neon_shift_imm_long, neon_shift_imm_narrow_q,\
+                           neon_shift_reg")
+                           (const_string "neon_shift_1")
+          (eq_attr "type" "neon_sat_shift_imm, neon_sat_shift_imm_q,
+                           neon_sat_shift_imm_narrow_q,\
+                           neon_sat_shift_reg")
+                           (const_string "neon_shift_2")
+          (eq_attr "type" "neon_shift_reg_q")
+                           (const_string "neon_shift_3")
+          (eq_attr "type" "neon_sat_shift_reg_q")
+                           (const_string "neon_vqshl_vrshl_vqrshl_qqq")
+          (eq_attr "type" "neon_shift_acc, neon_shift_acc_q")
+                           (const_string "neon_vsra_vrsra")
+          (eq_attr "type" "neon_mul_b, neon_mul_h,\
+                           neon_mul_b_long, neon_mul_h_long,\
+                           neon_sat_mul_b, neon_sat_mul_h,\
+                           neon_sat_mul_b_long, neon_sat_mul_h_long")
+                           (const_string
+                            "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long")
+          (eq_attr "type" "neon_mul_b_q, neon_mul_h_q,\
+                           neon_sat_mul_b_q, neon_sat_mul_h_q")
+                           (const_string "neon_mul_qqq_8_16_32_ddd_32")
+          (eq_attr "type" "neon_mul_s, neon_mul_s_long,\
+                           neon_sat_mul_s, neon_sat_mul_s_long,\
+                           neon_mul_h_scalar_q, neon_sat_mul_h_scalar_q,\
+                           neon_mul_s_scalar, neon_sat_mul_s_scalar,\
+                           neon_mul_s_scalar_long,\
+                           neon_sat_mul_s_scalar_long")
+                           (const_string
+             "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")
+          (eq_attr "type" "neon_mla_b, neon_mla_h,\
+                           neon_mla_b_long, neon_mla_h_long,\
+                           neon_sat_mla_b_long, neon_sat_mla_h_long,\
+                           neon_sat_mla_h_scalar_long")
+                           (const_string
+                             "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
+          (eq_attr "type" "neon_mla_b_q, neon_mla_h_q")
+                           (const_string "neon_mla_qqq_8_16")
+          (eq_attr "type" "neon_mla_s, neon_mla_s_long,\
+                           neon_sat_mla_s_long,\
+                           neon_mla_h_scalar_q, neon_mla_s_scalar,\
+                           neon_mla_s_scalar_long,\
+                           neon_sat_mla_s_scalar_long")
+                           (const_string
+ "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")
+          (eq_attr "type" "neon_mla_s_q, neon_mla_s_scalar_q")
+                           (const_string "neon_mla_qqq_32_qqd_32_scalar")
+          (eq_attr "type" "neon_mul_h_scalar, neon_sat_mul_h_scalar,\
+                           neon_mul_h_scalar_long,\
+                           neon_sat_mul_h_scalar_long")
+                          (const_string
+                            "neon_mul_ddd_16_scalar_32_16_long_scalar")
+          (eq_attr "type" "neon_mul_s_q, neon_sat_mul_s_q,\
+                           neon_mul_s_scalar_q")
+                           (const_string "neon_mul_qqd_32_scalar")
+          (eq_attr "type" "neon_mla_h_scalar, neon_mla_h_scalar_long")
+                           (const_string
+                             "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar")
+          (eq_attr "type" "neon_fp_abd_s, neon_fp_abs_s, neon_fp_neg_s,\
+                           neon_fp_addsub_s, neon_fp_compare_s,\
+                           neon_fp_minmax_s, neon_fp_mul_s,\
+                           neon_fp_recpe_s, neon_fp_rsqrte_s,\
+                           neon_fp_to_int_s, neon_int_to_fp_s")
+                           (const_string "neon_fp_vadd_ddd_vabs_dd")
+          (eq_attr "type" "neon_fp_abd_s_q, neon_fp_abs_s_q,\
+                           neon_fp_neg_s_q,\
+                           neon_fp_addsub_s_q, neon_fp_compare_s_q,\
+                           neon_fp_minmax_s_q, neon_fp_mul_s_q,\
+                           neon_fp_recpe_s_q, neon_fp_rsqrte_s_q,\
+                           neon_fp_to_int_s_q, neon_int_to_fp_s_q")
+                           (const_string "neon_fp_vadd_qqq_vabs_qq")
+          (eq_attr "type" "neon_fp_reduc_add_s, neon_fp_reduc_minmax_s,\
+                           neon_fp_reduc_add_s_q, neon_fp_reduc_minmax_s_q")
+                           (const_string "neon_fp_vsum")
+          (eq_attr "type" "neon_fp_mul_s_scalar")
+                           (const_string "neon_fp_vmul_ddd")
+          (eq_attr "type" "neon_fp_mul_s_scalar_q")
+                           (const_string "neon_fp_vmul_qqd")
+          (eq_attr "type" "neon_fp_mla_s")
+                           (const_string "neon_fp_vmla_ddd")
+          (eq_attr "type" "neon_fp_mla_s_q")
+                           (const_string "neon_fp_vmla_qqq")
+          (eq_attr "type" "neon_fp_mla_s_scalar")
+                           (const_string "neon_fp_vmla_ddd_scalar")
+          (eq_attr "type" "neon_fp_mla_s_scalar_q")
+                           (const_string "neon_fp_vmla_qqq_scalar")
+          (eq_attr "type" "neon_fp_recps_s, neon_fp_rsqrts_s")
+                           (const_string "neon_fp_vrecps_vrsqrts_ddd")
+          (eq_attr "type" "neon_fp_recps_s_q, neon_fp_rsqrts_s_q")
+                           (const_string "neon_fp_vrecps_vrsqrts_qqq")
+          (eq_attr "type" "neon_move_narrow_q, neon_dup,\
+                           neon_dup_q, neon_permute, neon_zip,\
+                           neon_ext, neon_rev, neon_rev_q")
+                           (const_string "neon_bp_simple")
+          (eq_attr "type" "neon_permute_q, neon_ext_q, neon_tbl1, neon_tbl2")
+                           (const_string "neon_bp_2cycle")
+          (eq_attr "type" "neon_zip_q, neon_tbl3, neon_tbl4")
+                           (const_string "neon_bp_3cycle")
+          (eq_attr "type" "neon_ldr")
+                           (const_string "neon_ldr")
+          (eq_attr "type" "neon_str")
+                           (const_string "neon_str")
+          (eq_attr "type" "neon_load1_1reg, neon_load1_1reg_q,\
+                           neon_load1_2reg, neon_load1_2reg_q,\
+                           neon_load2_2reg, neon_load2_2reg_q")
+                           (const_string "neon_vld1_1_2_regs")
+          (eq_attr "type" "neon_load1_3reg, neon_load1_3reg_q,\
+                           neon_load1_4reg, neon_load1_4reg_q")
+                           (const_string "neon_vld1_3_4_regs")
+          (eq_attr "type" "neon_load1_all_lanes, neon_load1_all_lanes_q,\
+                           neon_load2_all_lanes, neon_load2_all_lanes_q")
+                           (const_string
+                              "neon_vld2_2_regs_vld1_vld2_all_lanes")
+          (eq_attr "type" "neon_load3_all_lanes, neon_load3_all_lanes_q,\
+                           neon_load4_all_lanes, neon_load4_all_lanes_q,\
+                           neon_load2_4reg, neon_load2_4reg_q")
+                           (const_string "neon_vld2_4_regs")
+          (eq_attr "type" "neon_load3_3reg, neon_load3_3reg_q,\
+                           neon_load4_4reg, neon_load4_4reg_q")
+                           (const_string "neon_vld3_vld4")
+          (eq_attr "type" "neon_load1_one_lane, neon_load1_one_lane_q,\
+                           neon_load2_one_lane, neon_load2_one_lane_q")
+                           (const_string "neon_vld1_vld2_lane")
+          (eq_attr "type" "neon_load3_one_lane, neon_load3_one_lane_q,\
+                           neon_load4_one_lane, neon_load4_one_lane_q")
+                           (const_string "neon_vld3_vld4_lane")
+          (eq_attr "type" "neon_store1_1reg, neon_store1_1reg_q,\
+                           neon_store1_2reg, neon_store1_2reg_q,\
+                           neon_store2_2reg, neon_store2_2reg_q")
+                           (const_string "neon_vst1_1_2_regs_vst2_2_regs")
+          (eq_attr "type" "neon_store1_3reg, neon_store1_3reg_q,\
+                           neon_store1_4reg, neon_store1_4reg_q")
+                           (const_string "neon_vst1_3_4_regs")
+          (eq_attr "type" "neon_store2_4reg, neon_store2_4reg_q,\
+                           neon_store3_3reg, neon_store3_3reg_q,\
+                           neon_store4_4reg, neon_store4_4reg_q")
+                           (const_string "neon_vst2_4_regs_vst3_vst4")
+          (eq_attr "type" "neon_store1_one_lane, neon_store1_one_lane_q,\
+                           neon_store2_one_lane, neon_store2_one_lane_q")
+                           (const_string "neon_vst1_vst2_lane")
+          (eq_attr "type" "neon_store3_one_lane, neon_store3_one_lane_q,\
+                           neon_store4_one_lane, neon_store4_one_lane_q")
+                           (const_string "neon_vst3_vst4_lane")
+          (eq_attr "type" "neon_from_gp")
+                           (const_string "neon_mcr")
+          (eq_attr "type" "neon_from_gp_q")
+                           (const_string "neon_mcr_2_mcrr")
+          (eq_attr "type" "neon_to_gp")
+                           (const_string "neon_mrc")
+          (eq_attr "type" "neon_to_gp_q")
+                           (const_string "neon_mrrc")]
+          (const_string "unknown")))
 
 (define_automaton "cortex_a9_neon")
 
@@ -105,74 +319,71 @@
 		     cortex_a9_neon_issue_fadd,\
 		     cortex_a9_neon_issue_fadd")
 
-
 ;; NEON -> core transfers.
 (define_insn_reservation "ca9_neon_mrc" 1
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mrc"))
+       (eq_attr "cortex_a9_neon_type" "neon_mrc"))
   "ca9_issue_vfp_neon + cortex_a9_neon_mcr")
 
 (define_insn_reservation "ca9_neon_mrrc" 1
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mrrc"))
+       (eq_attr "cortex_a9_neon_type" "neon_mrrc"))
   "ca9_issue_vfp_neon + cortex_a9_neon_mcr")
 
-;; The remainder of this file is auto-generated by neon-schedgen.
-
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N3.
 (define_insn_reservation "cortex_a9_neon_int_1" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_int_1"))
+       (eq_attr "cortex_a9_neon_type" "neon_int_1"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)m operands at N1,
 ;; their (D|Q)n operands at N2, and produce a result at N3.
 (define_insn_reservation "cortex_a9_neon_int_2" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_int_2"))
+       (eq_attr "cortex_a9_neon_type" "neon_int_2"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N3.
 (define_insn_reservation "cortex_a9_neon_int_3" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_int_3"))
-  "cortex_a9_neon_dp")
+       (eq_attr "cortex_a9_neon_type" "neon_int_3"))
+   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N4.
 (define_insn_reservation "cortex_a9_neon_int_4" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_int_4"))
+       (eq_attr "cortex_a9_neon_type" "neon_int_4"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)m operands at N1,
 ;; their (D|Q)n operands at N2, and produce a result at N4.
 (define_insn_reservation "cortex_a9_neon_int_5" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_int_5"))
+       (eq_attr "cortex_a9_neon_type" "neon_int_5"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N4.
 (define_insn_reservation "cortex_a9_neon_vqneg_vqabs" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vqneg_vqabs"))
-  "cortex_a9_neon_dp")
+       (eq_attr "cortex_a9_neon_type" "neon_vqneg_vqabs"))
+   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation produce a result at N3.
 (define_insn_reservation "cortex_a9_neon_vmov" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vmov"))
-  "cortex_a9_neon_dp")
+       (eq_attr "cortex_a9_neon_type" "neon_vmov"))
+  "cortex_a8_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, their (D|Q)d operands at N3, and
 ;; produce a result at N6.
 (define_insn_reservation "cortex_a9_neon_vaba" 6
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vaba"))
+       (eq_attr "cortex_a9_neon_type" "neon_vaba"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -180,35 +391,35 @@
 ;; produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_vaba_qqq" 7
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vaba_qqq"))
+       (eq_attr "cortex_a9_neon_type" "neon_vaba_qqq"))
   "cortex_a9_neon_dp_2")
 
-;; Instructions using this reservation read their (D|Q)m operands at N1,
-;; their (D|Q)d operands at N3, and produce a result at N6.
-(define_insn_reservation "cortex_a9_neon_vsma" 6
+;; Instructions using this reservation read their source operands at N2, and
+;; produce a result at N3 on cycle 2.
+(define_insn_reservation "cortex_a9_neon_bit_ops_q" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vsma"))
-  "cortex_a9_neon_dp")
+       (eq_attr "cortex_a9_neon_type" "neon_bit_ops_q"))
+  "cortex_a9_neon_dp_2")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N6.
 (define_insn_reservation "cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long" 6
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long"))
+       (eq_attr "cortex_a9_neon_type" "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_mul_qqq_8_16_32_ddd_32" 7
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mul_qqq_8_16_32_ddd_32"))
+       (eq_attr "cortex_a9_neon_type" "neon_mul_qqq_8_16_32_ddd_32"))
   "cortex_a9_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar" 7
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar"))
+       (eq_attr "cortex_a9_neon_type" "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar"))
   "cortex_a9_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -216,7 +427,7 @@
 ;; produce a result at N6.
 (define_insn_reservation "cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long" 6
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long"))
+       (eq_attr "cortex_a9_neon_type" "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -224,7 +435,7 @@
 ;; produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_mla_qqq_8_16" 7
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mla_qqq_8_16"))
+       (eq_attr "cortex_a9_neon_type" "neon_mla_qqq_8_16"))
   "cortex_a9_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -232,7 +443,7 @@
 ;; produce a result at N6 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long" 7
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))
+       (eq_attr "cortex_a9_neon_type" "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))
   "cortex_a9_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -240,21 +451,21 @@
 ;; produce a result at N6 on cycle 4.
 (define_insn_reservation "cortex_a9_neon_mla_qqq_32_qqd_32_scalar" 9
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mla_qqq_32_qqd_32_scalar"))
+       (eq_attr "cortex_a9_neon_type" "neon_mla_qqq_32_qqd_32_scalar"))
   "cortex_a9_neon_dp_4")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N6.
 (define_insn_reservation "cortex_a9_neon_mul_ddd_16_scalar_32_16_long_scalar" 6
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mul_ddd_16_scalar_32_16_long_scalar"))
+       (eq_attr "cortex_a9_neon_type" "neon_mul_ddd_16_scalar_32_16_long_scalar"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N6 on cycle 4.
 (define_insn_reservation "cortex_a9_neon_mul_qqd_32_scalar" 9
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mul_qqd_32_scalar"))
+       (eq_attr "cortex_a9_neon_type" "neon_mul_qqd_32_scalar"))
   "cortex_a9_neon_dp_4")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -262,84 +473,77 @@
 ;; produce a result at N6.
 (define_insn_reservation "cortex_a9_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar" 6
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar"))
+       (eq_attr "cortex_a9_neon_type" "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N3.
 (define_insn_reservation "cortex_a9_neon_shift_1" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_shift_1"))
+       (eq_attr "cortex_a9_neon_type" "neon_shift_1"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N4.
 (define_insn_reservation "cortex_a9_neon_shift_2" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_shift_2"))
+       (eq_attr "cortex_a9_neon_type" "neon_shift_2"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N3 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_shift_3" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_shift_3"))
+       (eq_attr "cortex_a9_neon_type" "neon_shift_3"))
   "cortex_a9_neon_dp_2")
 
 ;; Instructions using this reservation read their source operands at N1, and
-;; produce a result at N1.
-(define_insn_reservation "cortex_a9_neon_vshl_ddd" 1
-  (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vshl_ddd"))
-  "cortex_a9_neon_dp")
-
-;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N4 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_vqshl_vrshl_vqrshl_qqq" 5
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vqshl_vrshl_vqrshl_qqq"))
+       (eq_attr "cortex_a9_neon_type" "neon_vqshl_vrshl_vqrshl_qqq"))
   "cortex_a9_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)m operands at N1,
 ;; their (D|Q)d operands at N3, and produce a result at N6.
 (define_insn_reservation "cortex_a9_neon_vsra_vrsra" 6
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vsra_vrsra"))
+       (eq_attr "cortex_a9_neon_type" "neon_vsra_vrsra"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N5.
 (define_insn_reservation "cortex_a9_neon_fp_vadd_ddd_vabs_dd" 5
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vadd_ddd_vabs_dd"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vadd_ddd_vabs_dd"))
   "cortex_a9_neon_fadd")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N5 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_fp_vadd_qqq_vabs_qq" 6
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vadd_qqq_vabs_qq"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vadd_qqq_vabs_qq"))
   "cortex_a9_neon_fadd_2")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N5.
 (define_insn_reservation "cortex_a9_neon_fp_vsum" 5
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vsum"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vsum"))
   "cortex_a9_neon_fadd")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N5.
 (define_insn_reservation "cortex_a9_neon_fp_vmul_ddd" 5
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vmul_ddd"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vmul_ddd"))
   "cortex_a9_neon_dp")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
 ;; their (D|Q)m operands at N1, and produce a result at N5 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_fp_vmul_qqd" 6
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vmul_qqd"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vmul_qqd"))
   "cortex_a9_neon_dp_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -347,7 +551,7 @@
 ;; produce a result at N9.
 (define_insn_reservation "cortex_a9_neon_fp_vmla_ddd" 9
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vmla_ddd"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vmla_ddd"))
   "cortex_a9_neon_fmul_then_fadd")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -355,7 +559,7 @@
 ;; produce a result at N9 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_fp_vmla_qqq" 10
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vmla_qqq"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vmla_qqq"))
   "cortex_a9_neon_fmul_then_fadd_2")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -363,7 +567,7 @@
 ;; produce a result at N9.
 (define_insn_reservation "cortex_a9_neon_fp_vmla_ddd_scalar" 9
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vmla_ddd_scalar"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vmla_ddd_scalar"))
   "cortex_a9_neon_fmul_then_fadd")
 
 ;; Instructions using this reservation read their (D|Q)n operands at N2,
@@ -371,152 +575,146 @@
 ;; produce a result at N9 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_fp_vmla_qqq_scalar" 10
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vmla_qqq_scalar"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vmla_qqq_scalar"))
   "cortex_a9_neon_fmul_then_fadd_2")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N9.
 (define_insn_reservation "cortex_a9_neon_fp_vrecps_vrsqrts_ddd" 9
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vrecps_vrsqrts_ddd"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vrecps_vrsqrts_ddd"))
   "cortex_a9_neon_fmul_then_fadd")
 
 ;; Instructions using this reservation read their source operands at N2, and
 ;; produce a result at N9 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_fp_vrecps_vrsqrts_qqq" 10
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_fp_vrecps_vrsqrts_qqq"))
+       (eq_attr "cortex_a9_neon_type" "neon_fp_vrecps_vrsqrts_qqq"))
   "cortex_a9_neon_fmul_then_fadd_2")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2.
 (define_insn_reservation "cortex_a9_neon_bp_simple" 2
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_bp_simple"))
+       (eq_attr "cortex_a9_neon_type" "neon_bp_simple"))
   "cortex_a9_neon_perm")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_bp_2cycle" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_bp_2cycle"))
+       (eq_attr "cortex_a9_neon_type" "neon_bp_2cycle"))
   "cortex_a9_neon_perm_2")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2 on cycle 3.
 (define_insn_reservation "cortex_a9_neon_bp_3cycle" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_bp_3cycle"))
+       (eq_attr "cortex_a9_neon_type" "neon_bp_3cycle"))
   "cortex_a9_neon_perm_3")
 
 ;; Instructions using this reservation produce a result at N1.
 (define_insn_reservation "cortex_a9_neon_ldr" 1
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_ldr"))
+       (eq_attr "cortex_a9_neon_type" "neon_ldr"))
   "cortex_a9_neon_ls")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a9_neon_str" 0
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_str"))
+       (eq_attr "cortex_a9_neon_type" "neon_str"))
   "cortex_a9_neon_ls")
 
 ;; Instructions using this reservation produce a result at N1 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_vld1_1_2_regs" 2
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vld1_1_2_regs"))
+       (eq_attr "cortex_a9_neon_type" "neon_vld1_1_2_regs"))
   "cortex_a9_neon_ls_2")
 
 ;; Instructions using this reservation produce a result at N1 on cycle 3.
 (define_insn_reservation "cortex_a9_neon_vld1_3_4_regs" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vld1_3_4_regs"))
+       (eq_attr "cortex_a9_neon_type" "neon_vld1_3_4_regs"))
   "cortex_a9_neon_ls_3")
 
 ;; Instructions using this reservation produce a result at N2 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_vld2_2_regs_vld1_vld2_all_lanes" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vld2_2_regs_vld1_vld2_all_lanes"))
+       (eq_attr "cortex_a9_neon_type" "neon_vld2_2_regs_vld1_vld2_all_lanes"))
   "cortex_a9_neon_ls_2")
 
 ;; Instructions using this reservation produce a result at N2 on cycle 3.
 (define_insn_reservation "cortex_a9_neon_vld2_4_regs" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vld2_4_regs"))
+       (eq_attr "cortex_a9_neon_type" "neon_vld2_4_regs"))
   "cortex_a9_neon_ls_3")
 
 ;; Instructions using this reservation produce a result at N2 on cycle 4.
 (define_insn_reservation "cortex_a9_neon_vld3_vld4" 5
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vld3_vld4"))
+       (eq_attr "cortex_a9_neon_type" "neon_vld3_vld4"))
   "cortex_a9_neon_ls_4")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a9_neon_vst1_1_2_regs_vst2_2_regs" 0
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vst1_1_2_regs_vst2_2_regs"))
+       (eq_attr "cortex_a9_neon_type" "neon_vst1_1_2_regs_vst2_2_regs"))
   "cortex_a9_neon_ls_2")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a9_neon_vst1_3_4_regs" 0
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vst1_3_4_regs"))
+       (eq_attr "cortex_a9_neon_type" "neon_vst1_3_4_regs"))
   "cortex_a9_neon_ls_3")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a9_neon_vst2_4_regs_vst3_vst4" 0
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vst2_4_regs_vst3_vst4"))
-  "cortex_a9_neon_ls_4")
-
-;; Instructions using this reservation read their source operands at N1.
-(define_insn_reservation "cortex_a9_neon_vst3_vst4" 0
-  (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vst3_vst4"))
+       (eq_attr "cortex_a9_neon_type" "neon_vst2_4_regs_vst3_vst4"))
   "cortex_a9_neon_ls_4")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2 on cycle 3.
 (define_insn_reservation "cortex_a9_neon_vld1_vld2_lane" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vld1_vld2_lane"))
+       (eq_attr "cortex_a9_neon_type" "neon_vld1_vld2_lane"))
   "cortex_a9_neon_ls_3")
 
 ;; Instructions using this reservation read their source operands at N1, and
 ;; produce a result at N2 on cycle 5.
 (define_insn_reservation "cortex_a9_neon_vld3_vld4_lane" 6
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vld3_vld4_lane"))
+       (eq_attr "cortex_a9_neon_type" "neon_vld3_vld4_lane"))
   "cortex_a9_neon_ls_5")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a9_neon_vst1_vst2_lane" 0
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vst1_vst2_lane"))
+       (eq_attr "cortex_a9_neon_type" "neon_vst1_vst2_lane"))
   "cortex_a9_neon_ls_2")
 
 ;; Instructions using this reservation read their source operands at N1.
 (define_insn_reservation "cortex_a9_neon_vst3_vst4_lane" 0
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vst3_vst4_lane"))
+       (eq_attr "cortex_a9_neon_type" "neon_vst3_vst4_lane"))
   "cortex_a9_neon_ls_3")
 
 ;; Instructions using this reservation produce a result at N2 on cycle 2.
 (define_insn_reservation "cortex_a9_neon_vld3_vld4_all_lanes" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_vld3_vld4_all_lanes"))
+       (eq_attr "cortex_a9_neon_type" "neon_vld3_vld4_all_lanes"))
   "cortex_a9_neon_ls_3")
 
 ;; Instructions using this reservation produce a result at N2.
 (define_insn_reservation "cortex_a9_neon_mcr" 2
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mcr"))
+       (eq_attr "cortex_a9_neon_type" "neon_mcr"))
   "cortex_a9_neon_perm")
 
 ;; Instructions using this reservation produce a result at N2.
 (define_insn_reservation "cortex_a9_neon_mcr_2_mcrr" 2
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "neon_type" "neon_mcr_2_mcrr"))
+       (eq_attr "cortex_a9_neon_type" "neon_mcr_2_mcrr"))
   "cortex_a9_neon_perm_2")
 
 ;; Exceptions to the default latencies.
@@ -524,6 +722,7 @@
 (define_bypass 1 "cortex_a9_neon_mcr_2_mcrr"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -538,6 +737,7 @@
 (define_bypass 1 "cortex_a9_neon_mcr"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -552,6 +752,7 @@
 (define_bypass 2 "cortex_a9_neon_vld3_vld4_all_lanes"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -566,6 +767,7 @@
 (define_bypass 5 "cortex_a9_neon_vld3_vld4_lane"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -580,6 +782,7 @@
 (define_bypass 3 "cortex_a9_neon_vld1_vld2_lane"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -594,6 +797,7 @@
 (define_bypass 4 "cortex_a9_neon_vld3_vld4"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -608,6 +812,7 @@
 (define_bypass 3 "cortex_a9_neon_vld2_4_regs"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -622,6 +827,7 @@
 (define_bypass 2 "cortex_a9_neon_vld2_2_regs_vld1_vld2_all_lanes"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -636,6 +842,7 @@
 (define_bypass 2 "cortex_a9_neon_vld1_3_4_regs"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -650,6 +857,7 @@
 (define_bypass 1 "cortex_a9_neon_vld1_1_2_regs"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -664,6 +872,7 @@
 (define_bypass 0 "cortex_a9_neon_ldr"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -678,6 +887,7 @@
 (define_bypass 3 "cortex_a9_neon_bp_3cycle"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -692,6 +902,7 @@
 (define_bypass 2 "cortex_a9_neon_bp_2cycle"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -706,6 +917,7 @@
 (define_bypass 1 "cortex_a9_neon_bp_simple"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -720,6 +932,7 @@
 (define_bypass 9 "cortex_a9_neon_fp_vrecps_vrsqrts_qqq"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -734,6 +947,7 @@
 (define_bypass 8 "cortex_a9_neon_fp_vrecps_vrsqrts_ddd"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -748,6 +962,7 @@
 (define_bypass 9 "cortex_a9_neon_fp_vmla_qqq_scalar"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -762,6 +977,7 @@
 (define_bypass 8 "cortex_a9_neon_fp_vmla_ddd_scalar"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -776,6 +992,7 @@
 (define_bypass 9 "cortex_a9_neon_fp_vmla_qqq"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -790,6 +1007,7 @@
 (define_bypass 8 "cortex_a9_neon_fp_vmla_ddd"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -804,6 +1022,7 @@
 (define_bypass 5 "cortex_a9_neon_fp_vmul_qqd"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -818,6 +1037,7 @@
 (define_bypass 4 "cortex_a9_neon_fp_vmul_ddd"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -832,6 +1052,7 @@
 (define_bypass 4 "cortex_a9_neon_fp_vsum"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -846,6 +1067,7 @@
 (define_bypass 5 "cortex_a9_neon_fp_vadd_qqq_vabs_qq"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -860,6 +1082,7 @@
 (define_bypass 4 "cortex_a9_neon_fp_vadd_ddd_vabs_dd"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -874,6 +1097,7 @@
 (define_bypass 5 "cortex_a9_neon_vsra_vrsra"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -888,20 +1112,7 @@
 (define_bypass 4 "cortex_a9_neon_vqshl_vrshl_vqrshl_qqq"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
-               cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a9_neon_mla_qqq_8_16,\
-               cortex_a9_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a9_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a9_neon_fp_vmla_ddd,\
-               cortex_a9_neon_fp_vmla_qqq,\
-               cortex_a9_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a9_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 0 "cortex_a9_neon_vshl_ddd"
-               "cortex_a9_neon_int_1,\
-               cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -916,6 +1127,7 @@
 (define_bypass 3 "cortex_a9_neon_shift_3"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -930,6 +1142,7 @@
 (define_bypass 3 "cortex_a9_neon_shift_2"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -944,6 +1157,7 @@
 (define_bypass 2 "cortex_a9_neon_shift_1"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -958,6 +1172,7 @@
 (define_bypass 5 "cortex_a9_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -972,6 +1187,7 @@
 (define_bypass 8 "cortex_a9_neon_mul_qqd_32_scalar"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -986,6 +1202,7 @@
 (define_bypass 5 "cortex_a9_neon_mul_ddd_16_scalar_32_16_long_scalar"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1000,6 +1217,7 @@
 (define_bypass 8 "cortex_a9_neon_mla_qqq_32_qqd_32_scalar"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1014,6 +1232,7 @@
 (define_bypass 6 "cortex_a9_neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1028,6 +1247,7 @@
 (define_bypass 6 "cortex_a9_neon_mla_qqq_8_16"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1042,6 +1262,7 @@
 (define_bypass 5 "cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1056,6 +1277,7 @@
 (define_bypass 6 "cortex_a9_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1070,6 +1292,7 @@
 (define_bypass 6 "cortex_a9_neon_mul_qqq_8_16_32_ddd_32"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1084,6 +1307,7 @@
 (define_bypass 5 "cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1095,9 +1319,10 @@
                cortex_a9_neon_fp_vrecps_vrsqrts_ddd,\
                cortex_a9_neon_fp_vrecps_vrsqrts_qqq")
 
-(define_bypass 5 "cortex_a9_neon_vsma"
+(define_bypass 6 "cortex_a9_neon_vaba_qqq"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1109,9 +1334,10 @@
                cortex_a9_neon_fp_vrecps_vrsqrts_ddd,\
                cortex_a9_neon_fp_vrecps_vrsqrts_qqq")
 
-(define_bypass 6 "cortex_a9_neon_vaba_qqq"
+(define_bypass 5 "cortex_a9_neon_vaba"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1123,9 +1349,10 @@
                cortex_a9_neon_fp_vrecps_vrsqrts_ddd,\
                cortex_a9_neon_fp_vrecps_vrsqrts_qqq")
 
-(define_bypass 5 "cortex_a9_neon_vaba"
+(define_bypass 2 "cortex_a9_neon_vmov"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1137,9 +1364,10 @@
                cortex_a9_neon_fp_vrecps_vrsqrts_ddd,\
                cortex_a9_neon_fp_vrecps_vrsqrts_qqq")
 
-(define_bypass 2 "cortex_a9_neon_vmov"
+(define_bypass 3 "cortex_a9_neon_bit_ops_q"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1154,6 +1382,7 @@
 (define_bypass 3 "cortex_a9_neon_vqneg_vqabs"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1168,6 +1397,7 @@
 (define_bypass 3 "cortex_a9_neon_int_5"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1182,6 +1412,7 @@
 (define_bypass 3 "cortex_a9_neon_int_4"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1196,6 +1427,7 @@
 (define_bypass 2 "cortex_a9_neon_int_3"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1210,6 +1442,7 @@
 (define_bypass 2 "cortex_a9_neon_int_2"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
@@ -1224,6 +1457,7 @@
 (define_bypass 2 "cortex_a9_neon_int_1"
                "cortex_a9_neon_int_1,\
                cortex_a9_neon_int_4,\
+               cortex_a9_neon_bit_ops_q,\
                cortex_a9_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
                cortex_a9_neon_mul_qqq_8_16_32_ddd_32,\
                cortex_a9_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
Index: b/src/gcc/config/arm/cortex-a8.md
===================================================================
--- a/src/gcc/config/arm/cortex-a8.md
+++ b/src/gcc/config/arm/cortex-a8.md
@@ -85,30 +85,34 @@
 ;; (source read in E2 and destination available at the end of that cycle).
 (define_insn_reservation "cortex_a8_alu" 2
   (and (eq_attr "tune" "cortexa8")
-       (ior (and (and (eq_attr "type" "alu_reg,simple_alu_imm")
-		      (eq_attr "neon_type" "none"))
-		 (not (eq_attr "insn" "mov,mvn")))
-            (eq_attr "insn" "clz")))
+       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        adr,bfm,clz,rbit,rev,\
+                        shift_imm,shift_reg,\
+                        multiple,no_insn"))
   "cortex_a8_default")
 
 (define_insn_reservation "cortex_a8_alu_shift" 2
   (and (eq_attr "tune" "cortexa8")
-       (and (eq_attr "type" "simple_alu_shift,alu_shift")
-            (not (eq_attr "insn" "mov,mvn"))))
+       (eq_attr "type" "alu_shift_imm,alus_shift_imm,\
+                        logic_shift_imm,logics_shift_imm,\
+                        extend"))
   "cortex_a8_default")
 
 (define_insn_reservation "cortex_a8_alu_shift_reg" 2
   (and (eq_attr "tune" "cortexa8")
-       (and (eq_attr "type" "alu_shift_reg")
-            (not (eq_attr "insn" "mov,mvn"))))
+       (eq_attr "type" "alu_shift_reg,alus_shift_reg,\
+                        logic_shift_reg,logics_shift_reg"))
   "cortex_a8_default")
 
 ;; Move instructions.
 
 (define_insn_reservation "cortex_a8_mov" 1
   (and (eq_attr "tune" "cortexa8")
-       (and (eq_attr "type" "alu_reg,simple_alu_imm,simple_alu_shift,alu_shift,alu_shift_reg")
-            (eq_attr "insn" "mov,mvn")))
+       (eq_attr "type" "mov_imm,mov_reg,mov_shift,mov_shift_reg,\
+                        mvn_imm,mvn_reg,mvn_shift,mvn_shift_reg,\
+                        mrs"))
   "cortex_a8_default")
 
 ;; Exceptions to the default latencies for data processing instructions.
@@ -139,22 +143,22 @@
 
 (define_insn_reservation "cortex_a8_mul" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "insn" "mul,smulxy,smmul"))
+       (eq_attr "type" "mul,smulxy,smmul"))
   "cortex_a8_multiply_2")
 
 (define_insn_reservation "cortex_a8_mla" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "insn" "mla,smlaxy,smlawy,smmla,smlad,smlsd"))
+       (eq_attr "type" "mla,smlaxy,smlawy,smmla,smlad,smlsd"))
   "cortex_a8_multiply_2")
 
 (define_insn_reservation "cortex_a8_mull" 7
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "insn" "smull,umull,smlal,umlal,umaal,smlalxy"))
+       (eq_attr "type" "smull,umull,smlal,umlal,umaal,smlalxy"))
   "cortex_a8_multiply_3")
 
 (define_insn_reservation "cortex_a8_smulwy" 5
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "insn" "smulwy,smuad,smusd"))
+       (eq_attr "type" "smulwy,smuad,smusd"))
   "cortex_a8_multiply")
 
 ;; smlald and smlsld are multiply-accumulate instructions but do not
@@ -162,7 +166,7 @@
 ;; cannot go in cortex_a8_mla above.  (See below for bypass details.)
 (define_insn_reservation "cortex_a8_smlald" 6
   (and (eq_attr "tune" "cortexa8")
-       (eq_attr "insn" "smlald,smlsld"))
+       (eq_attr "type" "smlald,smlsld"))
   "cortex_a8_multiply_2")
 
 ;; A multiply with a single-register result or an MLA, followed by an
Index: b/src/gcc/config/arm/arm-fixed.md
===================================================================
--- a/src/gcc/config/arm/arm-fixed.md
+++ b/src/gcc/config/arm/arm-fixed.md
@@ -19,12 +19,14 @@
 ;; This file contains ARM instructions that support fixed-point operations.
 
 (define_insn "add<mode>3"
-  [(set (match_operand:FIXED 0 "s_register_operand" "=r")
-	(plus:FIXED (match_operand:FIXED 1 "s_register_operand" "r")
-		    (match_operand:FIXED 2 "s_register_operand" "r")))]
+  [(set (match_operand:FIXED 0 "s_register_operand" "=l,r")
+	(plus:FIXED (match_operand:FIXED 1 "s_register_operand" "l,r")
+		    (match_operand:FIXED 2 "s_register_operand" "l,r")))]
   "TARGET_32BIT"
   "add%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "yes,no")
+   (set_attr "type" "alu_reg")])
 
 (define_insn "add<mode>3"
   [(set (match_operand:ADDSUB 0 "s_register_operand" "=r")
@@ -32,7 +34,9 @@
 		     (match_operand:ADDSUB 2 "s_register_operand" "r")))]
   "TARGET_INT_SIMD"
   "sadd<qaddsub_suf>%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "alu_reg")])
 
 (define_insn "usadd<mode>3"
   [(set (match_operand:UQADDSUB 0 "s_register_operand" "=r")
@@ -40,7 +44,9 @@
 			  (match_operand:UQADDSUB 2 "s_register_operand" "r")))]
   "TARGET_INT_SIMD"
   "uqadd<qaddsub_suf>%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "alu_reg")])
 
 (define_insn "ssadd<mode>3"
   [(set (match_operand:QADDSUB 0 "s_register_operand" "=r")
@@ -48,15 +54,19 @@
 			 (match_operand:QADDSUB 2 "s_register_operand" "r")))]
   "TARGET_INT_SIMD"
   "qadd<qaddsub_suf>%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "alu_reg")])
 
 (define_insn "sub<mode>3"
-  [(set (match_operand:FIXED 0 "s_register_operand" "=r")
-	(minus:FIXED (match_operand:FIXED 1 "s_register_operand" "r")
-		     (match_operand:FIXED 2 "s_register_operand" "r")))]
+  [(set (match_operand:FIXED 0 "s_register_operand" "=l,r")
+	(minus:FIXED (match_operand:FIXED 1 "s_register_operand" "l,r")
+		     (match_operand:FIXED 2 "s_register_operand" "l,r")))]
   "TARGET_32BIT"
   "sub%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "yes,no")
+   (set_attr "type" "alu_reg")])
 
 (define_insn "sub<mode>3"
   [(set (match_operand:ADDSUB 0 "s_register_operand" "=r")
@@ -64,7 +74,9 @@
 		      (match_operand:ADDSUB 2 "s_register_operand" "r")))]
   "TARGET_INT_SIMD"
   "ssub<qaddsub_suf>%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "alu_reg")])
 
 (define_insn "ussub<mode>3"
   [(set (match_operand:UQADDSUB 0 "s_register_operand" "=r")
@@ -73,7 +85,9 @@
 	  (match_operand:UQADDSUB 2 "s_register_operand" "r")))]
   "TARGET_INT_SIMD"
   "uqsub<qaddsub_suf>%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "alu_reg")])
 
 (define_insn "sssub<mode>3"
   [(set (match_operand:QADDSUB 0 "s_register_operand" "=r")
@@ -81,7 +95,9 @@
 			  (match_operand:QADDSUB 2 "s_register_operand" "r")))]
   "TARGET_INT_SIMD"
   "qsub<qaddsub_suf>%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "alu_reg")])
 
 ;; Fractional multiplies.
 
@@ -96,7 +112,7 @@
   rtx tmp1 = gen_reg_rtx (HImode);
   rtx tmp2 = gen_reg_rtx (HImode);
   rtx tmp3 = gen_reg_rtx (SImode);
-  
+
   emit_insn (gen_extendqihi2 (tmp1, gen_lowpart (QImode, operands[1])));
   emit_insn (gen_extendqihi2 (tmp2, gen_lowpart (QImode, operands[2])));
   emit_insn (gen_mulhisi3 (tmp3, tmp1, tmp2));
@@ -132,7 +148,7 @@
   rtx tmp1 = gen_reg_rtx (DImode);
   rtx tmp2 = gen_reg_rtx (SImode);
   rtx tmp3 = gen_reg_rtx (SImode);
-  
+
   /* s.31 * s.31 -> s.62 multiplication.  */
   emit_insn (gen_mulsidi3 (tmp1, gen_lowpart (SImode, operands[1]),
 			   gen_lowpart (SImode, operands[2])));
@@ -154,7 +170,7 @@
   rtx tmp1 = gen_reg_rtx (DImode);
   rtx tmp2 = gen_reg_rtx (SImode);
   rtx tmp3 = gen_reg_rtx (SImode);
-  
+
   emit_insn (gen_mulsidi3 (tmp1, gen_lowpart (SImode, operands[1]),
 			   gen_lowpart (SImode, operands[2])));
   emit_insn (gen_lshrsi3 (tmp2, gen_lowpart (SImode, tmp1), GEN_INT (15)));
@@ -173,13 +189,13 @@
   rtx tmp1 = gen_reg_rtx (DImode);
   rtx tmp2 = gen_reg_rtx (SImode);
   rtx tmp3 = gen_reg_rtx (SImode);
-  
+
   emit_insn (gen_umulsidi3 (tmp1, gen_lowpart (SImode, operands[1]),
 			    gen_lowpart (SImode, operands[2])));
   emit_insn (gen_lshrsi3 (tmp2, gen_lowpart (SImode, tmp1), GEN_INT (16)));
   emit_insn (gen_ashlsi3 (tmp3, gen_highpart (SImode, tmp1), GEN_INT (16)));
   emit_insn (gen_iorsi3 (gen_lowpart (SImode, operands[0]), tmp2, tmp3));
-  
+
   DONE;
 })
 
@@ -209,7 +225,7 @@
     }
 
   /* We have:
-      31  high word  0     31  low word  0 
+      31  high word  0     31  low word  0
 
     [ S i i .... i i i ] [ i f f f ... f f ]
                         |
@@ -221,17 +237,29 @@
   output_asm_insn ("ssat\\t%R3, #15, %R3", operands);
   output_asm_insn ("mrs\\t%4, APSR", operands);
   output_asm_insn ("tst\\t%4, #1<<27", operands);
-  if (TARGET_THUMB2)
-    output_asm_insn ("it\\tne", operands);
-  output_asm_insn ("mvnne\\t%Q3, %R3, asr #32", operands);
+  if (arm_restrict_it)
+    {
+      output_asm_insn ("mvn\\t%4, %R3, asr #32", operands);
+      output_asm_insn ("it\\tne", operands);
+      output_asm_insn ("movne\\t%Q3, %4", operands);
+    }
+  else
+    {
+      if (TARGET_THUMB2)
+        output_asm_insn ("it\\tne", operands);
+      output_asm_insn ("mvnne\\t%Q3, %R3, asr #32", operands);
+    }
   output_asm_insn ("mov\\t%0, %Q3, lsr #15", operands);
   output_asm_insn ("orr\\t%0, %0, %R3, asl #17", operands);
   return "";
 }
   [(set_attr "conds" "clob")
+   (set_attr "type" "multiple")
    (set (attr "length")
 	(if_then_else (eq_attr "is_thumb" "yes")
-		      (const_int 38)
+		      (if_then_else (match_test "arm_restrict_it")
+		                    (const_int 40)
+		                    (const_int 38))
 		      (const_int 32)))])
 
 ;; Same goes for this.
@@ -257,7 +285,7 @@
     }
 
   /* We have:
-      31  high word  0     31  low word  0 
+      31  high word  0     31  low word  0
 
     [ i i i .... i i i ] [ f f f f ... f f ]
                         |
@@ -269,17 +297,29 @@
   output_asm_insn ("usat\\t%R3, #16, %R3", operands);
   output_asm_insn ("mrs\\t%4, APSR", operands);
   output_asm_insn ("tst\\t%4, #1<<27", operands);
-  if (TARGET_THUMB2)
-    output_asm_insn ("it\\tne", operands);
-  output_asm_insn ("sbfxne\\t%Q3, %R3, #15, #1", operands);
+  if (arm_restrict_it)
+    {
+      output_asm_insn ("sbfx\\t%4, %R3, #15, #1", operands);
+      output_asm_insn ("it\\tne", operands);
+      output_asm_insn ("movne\\t%Q3, %4", operands);
+    }
+  else
+    {
+      if (TARGET_THUMB2)
+        output_asm_insn ("it\\tne", operands);
+      output_asm_insn ("sbfxne\\t%Q3, %R3, #15, #1", operands);
+    }
   output_asm_insn ("lsr\\t%0, %Q3, #16", operands);
   output_asm_insn ("orr\\t%0, %0, %R3, asl #16", operands);
   return "";
 }
   [(set_attr "conds" "clob")
+   (set_attr "type" "multiple")
    (set (attr "length")
 	(if_then_else (eq_attr "is_thumb" "yes")
-		      (const_int 38)
+		      (if_then_else (match_test "arm_restrict_it")
+		                    (const_int 40)
+		                    (const_int 38))
 		      (const_int 32)))])
 
 (define_expand "mulha3"
@@ -289,7 +329,7 @@
   "TARGET_DSP_MULTIPLY && arm_arch_thumb2"
 {
   rtx tmp = gen_reg_rtx (SImode);
-  
+
   emit_insn (gen_mulhisi3 (tmp, gen_lowpart (HImode, operands[1]),
 			   gen_lowpart (HImode, operands[2])));
   emit_insn (gen_extv (gen_lowpart (SImode, operands[0]), tmp, GEN_INT (16),
@@ -307,7 +347,7 @@
   rtx tmp1 = gen_reg_rtx (SImode);
   rtx tmp2 = gen_reg_rtx (SImode);
   rtx tmp3 = gen_reg_rtx (SImode);
-  
+
   /* 8.8 * 8.8 -> 16.16 multiply.  */
   emit_insn (gen_zero_extendhisi2 (tmp1, gen_lowpart (HImode, operands[1])));
   emit_insn (gen_zero_extendhisi2 (tmp2, gen_lowpart (HImode, operands[2])));
@@ -326,7 +366,7 @@
 {
   rtx tmp = gen_reg_rtx (SImode);
   rtx rshift;
-  
+
   emit_insn (gen_mulhisi3 (tmp, gen_lowpart (HImode, operands[1]),
 			   gen_lowpart (HImode, operands[2])));
 
@@ -348,12 +388,12 @@
   rtx tmp2 = gen_reg_rtx (SImode);
   rtx tmp3 = gen_reg_rtx (SImode);
   rtx rshift_tmp = gen_reg_rtx (SImode);
-  
+
   /* Note: there's no smul[bt][bt] equivalent for unsigned multiplies.  Use a
      normal 32x32->32-bit multiply instead.  */
   emit_insn (gen_zero_extendhisi2 (tmp1, gen_lowpart (HImode, operands[1])));
   emit_insn (gen_zero_extendhisi2 (tmp2, gen_lowpart (HImode, operands[2])));
-  
+
   emit_insn (gen_mulsi3 (tmp3, tmp1, tmp2));
 
   /* The operand to "usat" is signed, so we cannot use the "..., asr #8"
@@ -374,9 +414,9 @@
   "TARGET_32BIT && arm_arch6"
   "ssat%?\\t%0, #16, %2%S1"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "sat")
+   (set_attr "predicable_short_it" "no")
    (set_attr "shift" "1")
-   (set_attr "type" "alu_shift")])
+   (set_attr "type" "alu_shift_imm")])
 
 (define_insn "arm_usatsihi"
   [(set (match_operand:HI 0 "s_register_operand" "=r")
@@ -384,4 +424,6 @@
   "TARGET_INT_SIMD"
   "usat%?\\t%0, #16, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "sat")])
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "alu_imm")]
+)
Index: b/src/gcc/config/arm/crypto.def
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/crypto.def
@@ -0,0 +1,34 @@
+/* Cryptographic instruction builtin definitions.
+   Copyright (C) 2013-2014 Free Software Foundation, Inc.
+   Contributed by ARM Ltd.
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+CRYPTO2 (aesd, AESD, v16uqi, v16uqi, v16uqi)
+CRYPTO2 (aese, AESE, v16uqi, v16uqi, v16uqi)
+CRYPTO1 (aesimc, AESIMC, v16uqi, v16uqi)
+CRYPTO1 (aesmc, AESMC, v16uqi, v16uqi)
+CRYPTO1 (sha1h, SHA1H, v4usi, v4usi)
+CRYPTO2 (sha1su1, SHA1SU1, v4usi, v4usi, v4usi)
+CRYPTO2 (sha256su0, SHA256SU0, v4usi, v4usi, v4usi)
+CRYPTO3 (sha1c, SHA1C, v4usi, v4usi, v4usi, v4usi)
+CRYPTO3 (sha1m, SHA1M, v4usi, v4usi, v4usi, v4usi)
+CRYPTO3 (sha1p, SHA1P, v4usi, v4usi, v4usi, v4usi)
+CRYPTO3 (sha1su0, SHA1SU0, v4usi, v4usi, v4usi, v4usi)
+CRYPTO3 (sha256h, SHA256H, v4usi, v4usi, v4usi, v4usi)
+CRYPTO3 (sha256h2, SHA256H2, v4usi, v4usi, v4usi, v4usi)
+CRYPTO3 (sha256su1, SHA256SU1, v4usi, v4usi, v4usi, v4usi)
+CRYPTO2 (vmullp64, VMULLP64, uti, udi, udi)
Index: b/src/gcc/config/arm/unspecs.md
===================================================================
--- a/src/gcc/config/arm/unspecs.md
+++ b/src/gcc/config/arm/unspecs.md
@@ -139,12 +139,37 @@
   VUNSPEC_ATOMIC_OP	; Represent an atomic operation.
   VUNSPEC_LL		; Represent a load-register-exclusive.
   VUNSPEC_SC		; Represent a store-register-exclusive.
+  VUNSPEC_LAX		; Represent a load-register-acquire-exclusive.
+  VUNSPEC_SLX		; Represent a store-register-release-exclusive.
+  VUNSPEC_LDA		; Represent a store-register-acquire.
+  VUNSPEC_STL		; Represent a store-register-release.
 ])
 
 ;; Enumerators for NEON unspecs.
 (define_c_enum "unspec" [
   UNSPEC_ASHIFT_SIGNED
   UNSPEC_ASHIFT_UNSIGNED
+  UNSPEC_CRC32B
+  UNSPEC_CRC32H
+  UNSPEC_CRC32W
+  UNSPEC_CRC32CB
+  UNSPEC_CRC32CH
+  UNSPEC_CRC32CW
+  UNSPEC_AESD
+  UNSPEC_AESE
+  UNSPEC_AESIMC
+  UNSPEC_AESMC
+  UNSPEC_SHA1C
+  UNSPEC_SHA1M
+  UNSPEC_SHA1P
+  UNSPEC_SHA1H
+  UNSPEC_SHA1SU0
+  UNSPEC_SHA1SU1
+  UNSPEC_SHA256H
+  UNSPEC_SHA256H2
+  UNSPEC_SHA256SU0
+  UNSPEC_SHA256SU1
+  UNSPEC_VMULLP64
   UNSPEC_LOAD_COUNT
   UNSPEC_VABD
   UNSPEC_VABDL
Index: b/src/gcc/config/arm/cortex-m4.md
===================================================================
--- a/src/gcc/config/arm/cortex-m4.md
+++ b/src/gcc/config/arm/cortex-m4.md
@@ -31,7 +31,20 @@
 ;; ALU and multiply is one cycle.
 (define_insn_reservation "cortex_m4_alu" 1
   (and (eq_attr "tune" "cortexm4")
-       (eq_attr "type" "alu_reg,simple_alu_imm,simple_alu_shift,alu_shift,alu_shift_reg,mult"))
+       (ior (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                             alu_reg,alus_reg,logic_reg,logics_reg,\
+                             adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                             adr,bfm,rev,\
+                             shift_imm,shift_reg,extend,\
+                             alu_shift_imm,alus_shift_imm,\
+                             logic_shift_imm,logics_shift_imm,\
+                             alu_shift_reg,alus_shift_reg,\
+                             logic_shift_reg,logics_shift_reg,\
+                             mov_imm,mov_reg,mov_shift,mov_shift_reg,\
+                             mvn_imm,mvn_reg,mvn_shift,mvn_shift_reg,\
+                             mrs,multiple,no_insn")
+	    (ior (eq_attr "mul32" "yes")
+		 (eq_attr "mul64" "yes"))))
   "cortex_m4_ex")
 
 ;; Byte, half-word and word load is two cycles.
Index: b/src/gcc/config/arm/linux-eabi.h
===================================================================
--- a/src/gcc/config/arm/linux-eabi.h
+++ b/src/gcc/config/arm/linux-eabi.h
@@ -84,10 +84,14 @@
   LINUX_OR_ANDROID_LD (LINUX_TARGET_LINK_SPEC,				\
 		       LINUX_TARGET_LINK_SPEC " " ANDROID_LINK_SPEC)
 
+#undef  ASAN_CC1_SPEC
+#define ASAN_CC1_SPEC "%{fsanitize=*:-funwind-tables}"
+
 #undef  CC1_SPEC
 #define CC1_SPEC							\
-  LINUX_OR_ANDROID_CC (GNU_USER_TARGET_CC1_SPEC,			\
-		       GNU_USER_TARGET_CC1_SPEC " " ANDROID_CC1_SPEC)
+  LINUX_OR_ANDROID_CC (GNU_USER_TARGET_CC1_SPEC " " ASAN_CC1_SPEC,	\
+		       GNU_USER_TARGET_CC1_SPEC " " ASAN_CC1_SPEC " "	\
+		       ANDROID_CC1_SPEC)
 
 #define CC1PLUS_SPEC \
   LINUX_OR_ANDROID_CC ("", ANDROID_CC1PLUS_SPEC)
@@ -95,7 +99,7 @@
 #undef  LIB_SPEC
 #define LIB_SPEC							\
   LINUX_OR_ANDROID_LD (GNU_USER_TARGET_LIB_SPEC,			\
-		       GNU_USER_TARGET_LIB_SPEC " " ANDROID_LIB_SPEC)
+		    GNU_USER_TARGET_NO_PTHREADS_LIB_SPEC " " ANDROID_LIB_SPEC)
 
 #undef	STARTFILE_SPEC
 #define STARTFILE_SPEC \
Index: b/src/gcc/config/arm/arm-cores.def
===================================================================
--- a/src/gcc/config/arm/arm-cores.def
+++ b/src/gcc/config/arm/arm-cores.def
@@ -129,9 +129,11 @@ ARM_CORE("cortex-a7",	  cortexa7,	7A,
 ARM_CORE("cortex-a8",	  cortexa8,	7A,				 FL_LDSCHED, cortex)
 ARM_CORE("cortex-a9",	  cortexa9,	7A,				 FL_LDSCHED, cortex_a9)
 ARM_CORE("cortex-a15",	  cortexa15,	7A,				 FL_LDSCHED | FL_THUMB_DIV | FL_ARM_DIV, cortex_a15)
+ARM_CORE("cortex-a53",	  cortexa53,	8A,				 FL_LDSCHED, cortex)
 ARM_CORE("cortex-r4",	  cortexr4,	7R,				 FL_LDSCHED, cortex)
 ARM_CORE("cortex-r4f",	  cortexr4f,	7R,				 FL_LDSCHED, cortex)
 ARM_CORE("cortex-r5",	  cortexr5,	7R,				 FL_LDSCHED | FL_ARM_DIV, cortex)
+ARM_CORE("cortex-r7",	  cortexr7,	7R,				 FL_LDSCHED | FL_ARM_DIV, cortex)
 ARM_CORE("cortex-m4",	  cortexm4,	7EM,				 FL_LDSCHED, cortex)
 ARM_CORE("cortex-m3",	  cortexm3,	7M,				 FL_LDSCHED, cortex)
 ARM_CORE("cortex-m1",	  cortexm1,	6M,				 FL_LDSCHED, v6m)
Index: b/src/gcc/config/arm/cortex-r4.md
===================================================================
--- a/src/gcc/config/arm/cortex-r4.md
+++ b/src/gcc/config/arm/cortex-r4.md
@@ -78,24 +78,31 @@
 ;; for the purposes of the dual-issue constraints above.
 (define_insn_reservation "cortex_r4_alu" 2
   (and (eq_attr "tune_cortexr4" "yes")
-       (and (eq_attr "type" "alu_reg,simple_alu_imm")
-            (not (eq_attr "insn" "mov"))))
+       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        adr,bfm,rev,\
+                        shift_imm,shift_reg,mvn_imm,mvn_reg"))
   "cortex_r4_alu")
 
 (define_insn_reservation "cortex_r4_mov" 2
   (and (eq_attr "tune_cortexr4" "yes")
-       (and (eq_attr "type" "alu_reg,simple_alu_imm")
-            (eq_attr "insn" "mov")))
+       (eq_attr "type" "mov_imm,mov_reg"))
   "cortex_r4_mov")
 
 (define_insn_reservation "cortex_r4_alu_shift" 2
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "type" "simple_alu_shift,alu_shift"))
+       (eq_attr "type" "alu_shift_imm,alus_shift_imm,\
+                        logic_shift_imm,logics_shift_imm,\
+                        extend,mov_shift,mvn_shift"))
   "cortex_r4_alu")
 
 (define_insn_reservation "cortex_r4_alu_shift_reg" 2
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "type" "alu_shift_reg"))
+       (eq_attr "type" "alu_shift_reg,alus_shift_reg,\
+                       logic_shift_reg,logics_shift_reg,\
+                       mov_shift_reg,mvn_shift_reg,\
+                       mrs,multiple,no_insn"))
   "cortex_r4_alu_shift_reg")
 
 ;; An ALU instruction followed by an ALU instruction with no early dep.
@@ -128,32 +135,32 @@
 
 (define_insn_reservation "cortex_r4_mul_4" 4
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "insn" "mul,smmul"))
+       (eq_attr "type" "mul,smmul"))
   "cortex_r4_mul_2")
 
 (define_insn_reservation "cortex_r4_mul_3" 3
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "insn" "smulxy,smulwy,smuad,smusd"))
+       (eq_attr "type" "smulxy,smulwy,smuad,smusd"))
   "cortex_r4_mul")
 
 (define_insn_reservation "cortex_r4_mla_4" 4
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "insn" "mla,smmla"))
+       (eq_attr "type" "mla,smmla"))
   "cortex_r4_mul_2")
 
 (define_insn_reservation "cortex_r4_mla_3" 3
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "insn" "smlaxy,smlawy,smlad,smlsd"))
+       (eq_attr "type" "smlaxy,smlawy,smlad,smlsd"))
   "cortex_r4_mul")
 
 (define_insn_reservation "cortex_r4_smlald" 3
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "insn" "smlald,smlsld"))
+       (eq_attr "type" "smlald,smlsld"))
   "cortex_r4_mul")
 
 (define_insn_reservation "cortex_r4_mull" 4
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "insn" "smull,umull,umlal,umaal"))
+       (eq_attr "type" "smull,umull,umlal,umaal"))
   "cortex_r4_mul_2")
 
 ;; A multiply or an MLA with a single-register result, followed by an
@@ -196,12 +203,12 @@
 ;; This gives a latency of nine for udiv and ten for sdiv.
 (define_insn_reservation "cortex_r4_udiv" 9
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "insn" "udiv"))
+       (eq_attr "type" "udiv"))
   "cortex_r4_div_9")
 
 (define_insn_reservation "cortex_r4_sdiv" 10
   (and (eq_attr "tune_cortexr4" "yes")
-       (eq_attr "insn" "sdiv"))
+       (eq_attr "type" "sdiv"))
   "cortex_r4_div_10")
 
 ;; Branches.  We assume correct prediction.
Index: b/src/gcc/config/arm/arm-tune.md
===================================================================
--- a/src/gcc/config/arm/arm-tune.md
+++ b/src/gcc/config/arm/arm-tune.md
@@ -1,5 +1,5 @@
 ;; -*- buffer-read-only: t -*-
 ;; Generated automatically by gentune.sh from arm-cores.def
 (define_attr "tune"
-	"arm2,arm250,arm3,arm6,arm60,arm600,arm610,arm620,arm7,arm7d,arm7di,arm70,arm700,arm700i,arm710,arm720,arm710c,arm7100,arm7500,arm7500fe,arm7m,arm7dm,arm7dmi,arm8,arm810,strongarm,strongarm110,strongarm1100,strongarm1110,fa526,fa626,arm7tdmi,arm7tdmis,arm710t,arm720t,arm740t,arm9,arm9tdmi,arm920,arm920t,arm922t,arm940t,ep9312,arm10tdmi,arm1020t,arm9e,arm946es,arm966es,arm968es,arm10e,arm1020e,arm1022e,xscale,iwmmxt,iwmmxt2,fa606te,fa626te,fmp626,fa726te,arm926ejs,arm1026ejs,arm1136js,arm1136jfs,arm1176jzs,arm1176jzfs,mpcorenovfp,mpcore,arm1156t2s,arm1156t2fs,genericv7a,cortexa5,cortexa7,cortexa8,cortexa9,cortexa15,cortexr4,cortexr4f,cortexr5,cortexm4,cortexm3,cortexm1,cortexm0,cortexm0plus,marvell_pj4"
+	"arm2,arm250,arm3,arm6,arm60,arm600,arm610,arm620,arm7,arm7d,arm7di,arm70,arm700,arm700i,arm710,arm720,arm710c,arm7100,arm7500,arm7500fe,arm7m,arm7dm,arm7dmi,arm8,arm810,strongarm,strongarm110,strongarm1100,strongarm1110,fa526,fa626,arm7tdmi,arm7tdmis,arm710t,arm720t,arm740t,arm9,arm9tdmi,arm920,arm920t,arm922t,arm940t,ep9312,arm10tdmi,arm1020t,arm9e,arm946es,arm966es,arm968es,arm10e,arm1020e,arm1022e,xscale,iwmmxt,iwmmxt2,fa606te,fa626te,fmp626,fa726te,arm926ejs,arm1026ejs,arm1136js,arm1136jfs,arm1176jzs,arm1176jzfs,mpcorenovfp,mpcore,arm1156t2s,arm1156t2fs,genericv7a,cortexa5,cortexa7,cortexa8,cortexa9,cortexa15,cortexa53,cortexr4,cortexr4f,cortexr5,cortexr7,cortexm4,cortexm3,cortexm1,cortexm0,cortexm0plus,marvell_pj4"
 	(const (symbol_ref "((enum attr_tune) arm_tune)")))
Index: b/src/gcc/config/arm/arm_acle.h
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/arm_acle.h
@@ -0,0 +1,100 @@
+/* ARM Non-NEON ACLE intrinsics include file.
+
+   Copyright (C) 2013-2014 Free Software Foundation, Inc.
+   Contributed by ARM Ltd.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   Under Section 7 of GPL version 3, you are granted additional
+   permissions described in the GCC Runtime Library Exception, version
+   3.1, as published by the Free Software Foundation.
+
+   You should have received a copy of the GNU General Public License and
+   a copy of the GCC Runtime Library Exception along with this program;
+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+   <http://www.gnu.org/licenses/>.  */
+
+#ifndef _GCC_ARM_ACLE_H
+#define _GCC_ARM_ACLE_H
+
+#include <stdint.h>
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#ifdef __ARM_FEATURE_CRC32
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+__crc32b (uint32_t __a, uint8_t __b)
+{
+  return __builtin_arm_crc32b (__a, __b);
+}
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+__crc32h (uint32_t __a, uint16_t __b)
+{
+  return __builtin_arm_crc32h (__a, __b);
+}
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+__crc32w (uint32_t __a, uint32_t __b)
+{
+  return __builtin_arm_crc32w (__a, __b);
+}
+
+#ifdef __ARM_32BIT_STATE
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+__crc32d (uint32_t __a, uint64_t __b)
+{
+  uint32_t __d;
+
+  __d = __crc32w (__crc32w (__a, __b & 0xffffffffULL), __b >> 32);
+  return __d;
+}
+#endif
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+__crc32cb (uint32_t __a, uint8_t __b)
+{
+  return __builtin_arm_crc32cb (__a, __b);
+}
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+__crc32ch (uint32_t __a, uint16_t __b)
+{
+  return __builtin_arm_crc32ch (__a, __b);
+}
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+__crc32cw (uint32_t __a, uint32_t __b)
+{
+  return __builtin_arm_crc32cw (__a, __b);
+}
+
+#ifdef __ARM_32BIT_STATE
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+__crc32cd (uint32_t __a, uint64_t __b)
+{
+  uint32_t __d;
+
+  __d = __crc32cw (__crc32cw (__a, __b & 0xffffffffULL), __b >> 32);
+  return __d;
+}
+#endif
+
+#endif
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
Index: b/src/gcc/config/arm/arm-protos.h
===================================================================
--- a/src/gcc/config/arm/arm-protos.h
+++ b/src/gcc/config/arm/arm-protos.h
@@ -24,12 +24,13 @@
 
 extern enum unwind_info_type arm_except_unwind_info (struct gcc_options *);
 extern int use_return_insn (int, rtx);
+extern bool use_simple_return_p (void);
 extern enum reg_class arm_regno_class (int);
 extern void arm_load_pic_register (unsigned long);
 extern int arm_volatile_func (void);
 extern void arm_expand_prologue (void);
 extern void arm_expand_epilogue (bool);
-extern void thumb2_expand_return (void);
+extern void thumb2_expand_return (bool);
 extern const char *arm_strip_name_encoding (const char *);
 extern void arm_asm_output_labelref (FILE *, const char *);
 extern void thumb2_asm_output_opcode (FILE *);
@@ -79,6 +80,7 @@ extern char *neon_output_shift_immediate
 extern void neon_pairwise_reduce (rtx, rtx, enum machine_mode,
 				  rtx (*) (rtx, rtx, rtx));
 extern rtx neon_make_constant (rtx);
+extern tree arm_builtin_vectorized_function (tree, tree, tree);
 extern void neon_expand_vector_init (rtx, rtx);
 extern void neon_lane_bounds (rtx, HOST_WIDE_INT, HOST_WIDE_INT);
 extern void neon_const_bounds (rtx, HOST_WIDE_INT, HOST_WIDE_INT);
@@ -96,14 +98,6 @@ extern bool arm_tls_referenced_p (rtx);
 extern int arm_coproc_mem_operand (rtx, bool);
 extern int neon_vector_mem_operand (rtx, int);
 extern int neon_struct_mem_operand (rtx);
-extern int arm_no_early_store_addr_dep (rtx, rtx);
-extern int arm_early_store_addr_dep (rtx, rtx);
-extern int arm_early_load_addr_dep (rtx, rtx);
-extern int arm_no_early_alu_shift_dep (rtx, rtx);
-extern int arm_no_early_alu_shift_value_dep (rtx, rtx);
-extern int arm_no_early_mul_dep (rtx, rtx);
-extern int arm_mac_accumulator_is_result (rtx, rtx);
-extern int arm_mac_accumulator_is_mul_result (rtx, rtx);
 
 extern int tls_mentioned_p (rtx);
 extern int symbol_mentioned_p (rtx);
@@ -118,7 +112,9 @@ extern rtx arm_gen_load_multiple (int *,
 extern rtx arm_gen_store_multiple (int *, int, rtx, int, rtx, HOST_WIDE_INT *);
 extern bool offset_ok_for_ldrd_strd (HOST_WIDE_INT);
 extern bool operands_ok_ldrd_strd (rtx, rtx, rtx, HOST_WIDE_INT, bool, bool);
+extern bool gen_operands_ldrd_strd (rtx *, bool, bool, bool);
 extern int arm_gen_movmemqi (rtx *);
+extern bool gen_movmem_ldrd_strd (rtx *);
 extern enum machine_mode arm_select_cc_mode (RTX_CODE, rtx, rtx);
 extern enum machine_mode arm_select_dominance_cc_mode (rtx, rtx,
 						       HOST_WIDE_INT);
@@ -225,6 +221,8 @@ extern const char *arm_mangle_type (cons
 
 extern void arm_order_regs_for_local_alloc (void);
 
+extern int arm_max_conditional_execute ();
+
 /* Vectorizer cost model implementation.  */
 struct cpu_vec_costs {
   const int scalar_stmt_cost;   /* Cost of any scalar operation, excluding
@@ -254,8 +252,7 @@ struct tune_params
   bool (*rtx_costs) (rtx, RTX_CODE, RTX_CODE, int *, bool);
   bool (*sched_adjust_cost) (rtx, rtx, rtx, int *);
   int constant_limit;
-  /* Maximum number of instructions to conditionalise in
-     arm_final_prescan_insn.  */
+  /* Maximum number of instructions to conditionalise.  */
   int max_insns_skipped;
   int num_prefetch_slots;
   int l1_cache_size;
@@ -270,6 +267,8 @@ struct tune_params
   bool logical_op_non_short_circuit[2];
   /* Vectorizer costs.  */
   const struct cpu_vec_costs* vec_costs;
+  /* Prefer Neon for 64-bit bitops.  */
+  bool prefer_neon_for_64bits;
 };
 
 extern const struct tune_params *current_tune;
Index: b/src/gcc/config/arm/vfp.md
===================================================================
--- a/src/gcc/config/arm/vfp.md
+++ b/src/gcc/config/arm/vfp.md
@@ -18,31 +18,6 @@
 ;; along with GCC; see the file COPYING3.  If not see
 ;; <http://www.gnu.org/licenses/>.  */
 
-;; The VFP "type" attributes differ from those used in the FPA model.
-;; fcpys	Single precision cpy.
-;; ffariths	Single precision abs, neg.
-;; ffarithd	Double precision abs, neg, cpy.
-;; fadds	Single precision add/sub.
-;; faddd	Double precision add/sub.
-;; fconsts	Single precision load immediate.
-;; fconstd	Double precision load immediate.
-;; fcmps	Single precision comparison.
-;; fcmpd	Double precision comparison.
-;; fmuls	Single precision multiply.
-;; fmuld	Double precision multiply.
-;; fmacs	Single precision multiply-accumulate.
-;; fmacd	Double precision multiply-accumulate.
-;; ffmas	Single precision fused multiply-accumulate.
-;; ffmad	Double precision fused multiply-accumulate.
-;; fdivs	Single precision sqrt or division.
-;; fdivd	Double precision sqrt or division.
-;; f_flag	fmstat operation
-;; f_load[sd]	Floating point load from memory.
-;; f_store[sd]	Floating point store to memory.
-;; f_2_r	Transfer vfp to arm reg.
-;; r_2_f	Transfer arm to vfp reg.
-;; f_cvt	Convert floating<->integral
-
 ;; SImode moves
 ;; ??? For now do not allow loading constants into vfp regs.  This causes
 ;; problems because small constants get converted into adds.
@@ -78,62 +53,65 @@
     }
   "
   [(set_attr "predicable" "yes")
-   (set_attr "type" "*,*,simple_alu_imm,simple_alu_imm,load1,store1,r_2_f,f_2_r,fcpys,f_loads,f_stores")
-   (set_attr "neon_type" "*,*,*,*,*,*,neon_mcr,neon_mrc,neon_vmov,*,*")
-   (set_attr "insn" "mov,mov,mvn,mov,*,*,*,*,*,*,*")
+   (set_attr "type" "mov_reg,mov_reg,mvn_imm,mov_imm,load1,store1,f_mcr,f_mrc,fmov,f_loads,f_stores")
    (set_attr "pool_range"     "*,*,*,*,4096,*,*,*,*,1020,*")
    (set_attr "neg_pool_range" "*,*,*,*,4084,*,*,*,*,1008,*")]
 )
 
 ;; See thumb2.md:thumb2_movsi_insn for an explanation of the split
 ;; high/low register alternatives for loads and stores here.
+;; The l/Py alternative should come after r/I to ensure that the short variant
+;; is chosen with length 2 when the instruction is predicated for
+;; arm_restrict_it.
 (define_insn "*thumb2_movsi_vfp"
-  [(set (match_operand:SI 0 "nonimmediate_operand" "=rk,r,r,r, l,*hk,m, *m,*t, r,*t,*t,  *Uv")
-	(match_operand:SI 1 "general_operand"	   "rk, I,K,j,mi,*mi,l,*hk, r,*t,*t,*Uvi,*t"))]
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=rk,r,l,r,r, l,*hk,m, *m,*t, r,*t,*t,  *Uv")
+	(match_operand:SI 1 "general_operand"	   "rk,I,Py,K,j,mi,*mi,l,*hk, r,*t,*t,*Uvi,*t"))]
   "TARGET_THUMB2 && TARGET_VFP && TARGET_HARD_FLOAT
    && (   s_register_operand (operands[0], SImode)
        || s_register_operand (operands[1], SImode))"
   "*
   switch (which_alternative)
     {
-    case 0: case 1:
-      return \"mov%?\\t%0, %1\";
+    case 0:
+    case 1:
     case 2:
-      return \"mvn%?\\t%0, #%B1\";
+      return \"mov%?\\t%0, %1\";
     case 3:
-      return \"movw%?\\t%0, %1\";
+      return \"mvn%?\\t%0, #%B1\";
     case 4:
+      return \"movw%?\\t%0, %1\";
     case 5:
-      return \"ldr%?\\t%0, %1\";
     case 6:
+      return \"ldr%?\\t%0, %1\";
     case 7:
-      return \"str%?\\t%1, %0\";
     case 8:
-      return \"fmsr%?\\t%0, %1\\t%@ int\";
+      return \"str%?\\t%1, %0\";
     case 9:
-      return \"fmrs%?\\t%0, %1\\t%@ int\";
+      return \"fmsr%?\\t%0, %1\\t%@ int\";
     case 10:
+      return \"fmrs%?\\t%0, %1\\t%@ int\";
+    case 11:
       return \"fcpys%?\\t%0, %1\\t%@ int\";
-    case 11: case 12:
+    case 12: case 13:
       return output_move_vfp (operands);
     default:
       gcc_unreachable ();
     }
   "
   [(set_attr "predicable" "yes")
-   (set_attr "type" "*,*,*,*,load1,load1,store1,store1,r_2_f,f_2_r,fcpys,f_loads,f_stores")
-   (set_attr "neon_type" "*,*,*,*,*,*,*,*,neon_mcr,neon_mrc,neon_vmov,*,*")
-   (set_attr "insn" "mov,mov,mvn,mov,*,*,*,*,*,*,*,*,*")
-   (set_attr "pool_range"     "*,*,*,*,1018,4094,*,*,*,*,*,1018,*")
-   (set_attr "neg_pool_range" "*,*,*,*,   0,   0,*,*,*,*,*,1008,*")]
+   (set_attr "predicable_short_it" "yes,no,yes,no,no,no,no,no,no,no,no,no,no,no")
+   (set_attr "type" "mov_reg,mov_reg,mov_reg,mvn_reg,mov_reg,load1,load1,store1,store1,f_mcr,f_mrc,fmov,f_loads,f_stores")
+   (set_attr "length" "2,4,2,4,4,4,4,4,4,4,4,4,4,4")
+   (set_attr "pool_range"     "*,*,*,*,*,1018,4094,*,*,*,*,*,1018,*")
+   (set_attr "neg_pool_range" "*,*,*,*,*,   0,   0,*,*,*,*,*,1008,*")]
 )
 
 
 ;; DImode moves
 
 (define_insn "*movdi_vfp"
-  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r,r,r,r,r,r,m,w,r,w,w, Uv")
-       (match_operand:DI 1 "di_operand"              "r,rDa,Db,Dc,mi,mi,r,r,w,w,Uvi,w"))]
+  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r,r,r,r,q,q,m,w,r,w,w, Uv")
+       (match_operand:DI 1 "di_operand"              "r,rDa,Db,Dc,mi,mi,q,r,w,w,Uvi,w"))]
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP && arm_tune != cortexa8
    && (   register_operand (operands[0], DImode)
        || register_operand (operands[1], DImode))
@@ -166,8 +144,7 @@
       gcc_unreachable ();
     }
   "
-  [(set_attr "type" "*,*,*,*,load2,load2,store2,r_2_f,f_2_r,ffarithd,f_loadd,f_stored")
-   (set_attr "neon_type" "*,*,*,*,*,*,*,neon_mcr_2_mcrr,neon_mrrc,neon_vmov,*,*")
+  [(set_attr "type" "multiple,multiple,multiple,multiple,load2,load2,store2,f_mcrr,f_mrrc,ffarithd,f_loadd,f_stored")
    (set (attr "length") (cond [(eq_attr "alternative" "1,4,5,6") (const_int 8)
                               (eq_attr "alternative" "2") (const_int 12)
                               (eq_attr "alternative" "3") (const_int 16)
@@ -215,8 +192,7 @@
       gcc_unreachable ();
     }
   "
-  [(set_attr "type" "*,*,*,*,load2,load2,store2,r_2_f,f_2_r,ffarithd,f_loadd,f_stored")
-   (set_attr "neon_type" "*,*,*,*,*,*,*,neon_mcr_2_mcrr,neon_mrrc,neon_vmov,*,*")
+  [(set_attr "type" "multiple,multiple,multiple,multiple,load2,load2,store2,f_mcrr,f_mrrc,ffarithd,f_loadd,f_stored")
    (set (attr "length") (cond [(eq_attr "alternative" "1") (const_int 8)
                                (eq_attr "alternative" "2") (const_int 12)
                                (eq_attr "alternative" "3") (const_int 16)
@@ -284,8 +260,8 @@
     }
   "
   [(set_attr "conds" "unconditional")
-   (set_attr "type" "*,*,load1,store1,fcpys,*,r_2_f,f_2_r,*")
-   (set_attr "neon_type" "neon_vld1_1_2_regs,neon_vst1_1_2_regs_vst2_2_regs,*,*,*,*,*,*,*")
+   (set_attr "type" "neon_load1_1reg,neon_store1_1reg,\
+                     load1,store1,fmov,mov_reg,f_mcr,f_mrc,multiple")
    (set_attr "length" "4,4,4,4,4,4,4,4,8")]
 )
 
@@ -335,7 +311,7 @@
     }
   "
   [(set_attr "conds" "unconditional")
-   (set_attr "type" "load1,store1,fcpys,*,r_2_f,f_2_r,*")
+   (set_attr "type" "load1,store1,fmov,mov_reg,f_mcr,f_mrc,multiple")
    (set_attr "length" "4,4,4,4,4,4,8")]
 )
 
@@ -375,9 +351,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "type"
-     "r_2_f,f_2_r,fconsts,f_loads,f_stores,load1,store1,fcpys,*")
-   (set_attr "neon_type" "neon_mcr,neon_mrc,*,*,*,*,*,neon_vmov,*")
-   (set_attr "insn" "*,*,*,*,*,*,*,*,mov")
+     "f_mcr,f_mrc,fconsts,f_loads,f_stores,load1,store1,fmov,mov_reg")
    (set_attr "pool_range" "*,*,*,1020,*,4096,*,*,*")
    (set_attr "neg_pool_range" "*,*,*,1008,*,4080,*,*,*")]
 )
@@ -412,15 +386,13 @@
     }
   "
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type"
-     "r_2_f,f_2_r,fconsts,f_loads,f_stores,load1,store1,fcpys,*")
-   (set_attr "neon_type" "neon_mcr,neon_mrc,*,*,*,*,*,neon_vmov,*")
-   (set_attr "insn" "*,*,*,*,*,*,*,*,mov")
+     "f_mcr,f_mrc,fconsts,f_loads,f_stores,load1,store1,fmov,mov_reg")
    (set_attr "pool_range" "*,*,*,1018,*,4090,*,*,*")
    (set_attr "neg_pool_range" "*,*,*,1008,*,0,*,*,*")]
 )
 
-
 ;; DFmode moves
 
 (define_insn "*movdf_vfp"
@@ -456,9 +428,8 @@
       }
     }
   "
-  [(set_attr "type"
-     "r_2_f,f_2_r,fconstd,f_loadd,f_stored,load2,store2,ffarithd,*")
-   (set_attr "neon_type" "neon_mcr_2_mcrr,neon_mrrc,*,*,*,*,*,neon_vmov,*")
+  [(set_attr "type" "f_mcrr,f_mrrc,fconstd,f_loadd,f_stored,\
+                     load2,store2,ffarithd,multiple")
    (set (attr "length") (cond [(eq_attr "alternative" "5,6,8") (const_int 8)
 			       (eq_attr "alternative" "7")
 				(if_then_else
@@ -502,9 +473,8 @@
       }
     }
   "
-  [(set_attr "type"
-     "r_2_f,f_2_r,fconstd,f_loadd,f_stored,load2,store2,ffarithd,*")
-   (set_attr "neon_type" "neon_mcr_2_mcrr,neon_mrrc,*,*,*,*,*,neon_vmov,*")
+  [(set_attr "type" "f_mcrr,f_mrrc,fconstd,f_loadd,\
+                     f_stored,load2,store2,ffarithd,multiple")
    (set (attr "length") (cond [(eq_attr "alternative" "5,6,8") (const_int 8)
 			       (eq_attr "alternative" "7")
 				(if_then_else
@@ -539,8 +509,7 @@
    fmrs%D3\\t%0, %2\;fmrs%d3\\t%0, %1"
    [(set_attr "conds" "use")
     (set_attr "length" "4,4,8,4,4,8,4,4,8")
-    (set_attr "type" "fcpys,fcpys,fcpys,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")
-    (set_attr "neon_type" "neon_vmov,neon_vmov,neon_vmov,neon_mcr,neon_mcr,neon_mcr,neon_mrc,neon_mrc,neon_mrc")]
+    (set_attr "type" "fmov,fmov,fmov,f_mcr,f_mcr,f_mcr,f_mrc,f_mrc,f_mrc")]
 )
 
 (define_insn "*thumb2_movsfcc_vfp"
@@ -550,7 +519,7 @@
 	    [(match_operand 4 "cc_register" "") (const_int 0)])
 	  (match_operand:SF 1 "s_register_operand" "0,t,t,0,?r,?r,0,t,t")
 	  (match_operand:SF 2 "s_register_operand" "t,0,t,?r,0,?r,t,0,t")))]
-  "TARGET_THUMB2 && TARGET_HARD_FLOAT && TARGET_VFP"
+  "TARGET_THUMB2 && TARGET_HARD_FLOAT && TARGET_VFP && !arm_restrict_it"
   "@
    it\\t%D3\;fcpys%D3\\t%0, %2
    it\\t%d3\;fcpys%d3\\t%0, %1
@@ -563,8 +532,7 @@
    ite\\t%D3\;fmrs%D3\\t%0, %2\;fmrs%d3\\t%0, %1"
    [(set_attr "conds" "use")
     (set_attr "length" "6,6,10,6,6,10,6,6,10")
-    (set_attr "type" "fcpys,fcpys,fcpys,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")
-    (set_attr "neon_type" "neon_vmov,neon_vmov,neon_vmov,neon_mcr,neon_mcr,neon_mcr,neon_mrc,neon_mrc,neon_mrc")]
+    (set_attr "type" "fmov,fmov,fmov,f_mcr,f_mcr,f_mcr,f_mrc,f_mrc,f_mrc")]
 )
 
 (define_insn "*movdfcc_vfp"
@@ -587,8 +555,7 @@
    fmrrd%D3\\t%Q0, %R0, %P2\;fmrrd%d3\\t%Q0, %R0, %P1"
    [(set_attr "conds" "use")
     (set_attr "length" "4,4,8,4,4,8,4,4,8")
-    (set_attr "type" "ffarithd,ffarithd,ffarithd,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")
-    (set_attr "neon_type" "neon_vmov,neon_vmov,neon_vmov,neon_mcr_2_mcrr,neon_mcr_2_mcrr,neon_mcr_2_mcrr,neon_mrrc,neon_mrrc,neon_mrrc")]
+    (set_attr "type" "ffarithd,ffarithd,ffarithd,f_mcr,f_mcr,f_mcr,f_mrrc,f_mrrc,f_mrrc")]
 )
 
 (define_insn "*thumb2_movdfcc_vfp"
@@ -598,7 +565,7 @@
 	    [(match_operand 4 "cc_register" "") (const_int 0)])
 	  (match_operand:DF 1 "s_register_operand" "0,w,w,0,?r,?r,0,w,w")
 	  (match_operand:DF 2 "s_register_operand" "w,0,w,?r,0,?r,w,0,w")))]
-  "TARGET_THUMB2 && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
+  "TARGET_THUMB2 && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE && !arm_restrict_it"
   "@
    it\\t%D3\;fcpyd%D3\\t%P0, %P2
    it\\t%d3\;fcpyd%d3\\t%P0, %P1
@@ -611,8 +578,7 @@
    ite\\t%D3\;fmrrd%D3\\t%Q0, %R0, %P2\;fmrrd%d3\\t%Q0, %R0, %P1"
    [(set_attr "conds" "use")
     (set_attr "length" "6,6,10,6,6,10,6,6,10")
-    (set_attr "type" "ffarithd,ffarithd,ffarithd,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")
-    (set_attr "neon_type" "neon_vmov,neon_vmov,neon_vmov,neon_mcr_2_mcrr,neon_mcr_2_mcrr,neon_mcr_2_mcrr,neon_mrrc,neon_mrrc,neon_mrrc")]
+    (set_attr "type" "ffarithd,ffarithd,ffarithd,f_mcr,f_mcr,f_mcrr,f_mrrc,f_mrrc,f_mrrc")]
 )
 
 
@@ -624,6 +590,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fabss%?\\t%0, %1"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "ffariths")]
 )
 
@@ -633,6 +600,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fabsd%?\\t%P0, %P1"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "ffarithd")]
 )
 
@@ -644,6 +612,7 @@
    fnegs%?\\t%0, %1
    eor%?\\t%0, %1, #-2147483648"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "ffariths")]
 )
 
@@ -689,6 +658,7 @@
     }
   "
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "length" "4,4,8")
    (set_attr "type" "ffarithd")]
 )
@@ -703,6 +673,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fadds%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fadds")]
 )
 
@@ -713,6 +684,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "faddd%?\\t%P0, %P1, %P2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "faddd")]
 )
 
@@ -724,6 +696,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fsubs%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fadds")]
 )
 
@@ -734,6 +707,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fsubd%?\\t%P0, %P1, %P2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "faddd")]
 )
 
@@ -747,6 +721,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fdivs%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fdivs")]
 )
 
@@ -757,6 +732,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fdivd%?\\t%P0, %P1, %P2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fdivd")]
 )
 
@@ -770,6 +746,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fmuls%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmuls")]
 )
 
@@ -780,6 +757,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fmuld%?\\t%P0, %P1, %P2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmuld")]
 )
 
@@ -790,6 +768,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fnmuls%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmuls")]
 )
 
@@ -800,6 +779,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fnmuld%?\\t%P0, %P1, %P2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmuld")]
 )
 
@@ -815,6 +795,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fmacs%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmacs")]
 )
 
@@ -826,6 +807,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fmacd%?\\t%P0, %P2, %P3"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmacd")]
 )
 
@@ -838,6 +820,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fmscs%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmacs")]
 )
 
@@ -849,6 +832,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fmscd%?\\t%P0, %P2, %P3"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmacd")]
 )
 
@@ -861,6 +845,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fnmacs%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmacs")]
 )
 
@@ -872,6 +857,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fnmacd%?\\t%P0, %P2, %P3"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmacd")]
 )
 
@@ -886,6 +872,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fnmscs%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmacs")]
 )
 
@@ -898,6 +885,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fnmscd%?\\t%P0, %P2, %P3"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fmacd")]
 )
 
@@ -911,6 +899,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_FMA"
   "vfma%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "ffma<vfp_type>")]
 )
 
@@ -923,6 +912,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_FMA"
   "vfms%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "ffma<vfp_type>")]
 )
 
@@ -934,6 +924,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_FMA"
   "vfnms%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "ffma<vfp_type>")]
 )
 
@@ -946,6 +937,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_FMA"
   "vfnma%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "ffma<vfp_type>")]
 )
 
@@ -958,6 +950,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fcvtds%?\\t%P0, %1"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "f_cvt")]
 )
 
@@ -967,6 +960,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fcvtsd%?\\t%0, %P1"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "f_cvt")]
 )
 
@@ -976,6 +970,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_FP16"
   "vcvtb%?.f32.f16\\t%0, %1"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "f_cvt")]
 )
 
@@ -985,6 +980,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_FP16"
   "vcvtb%?.f16.f32\\t%0, %1"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "f_cvt")]
 )
 
@@ -994,7 +990,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "ftosizs%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "f_cvt")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "f_cvtf2i")]
 )
 
 (define_insn "*truncsidf2_vfp"
@@ -1003,7 +1000,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "ftosizd%?\\t%0, %P1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "f_cvt")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "f_cvtf2i")]
 )
 
 
@@ -1013,7 +1011,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "ftouizs%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "f_cvt")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "f_cvtf2i")]
 )
 
 (define_insn "fixuns_truncdfsi2"
@@ -1022,7 +1021,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "ftouizd%?\\t%0, %P1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "f_cvt")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "f_cvtf2i")]
 )
 
 
@@ -1032,7 +1032,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fsitos%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "f_cvt")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "f_cvti2f")]
 )
 
 (define_insn "*floatsidf2_vfp"
@@ -1041,7 +1042,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fsitod%?\\t%P0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "f_cvt")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "f_cvti2f")]
 )
 
 
@@ -1051,7 +1053,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fuitos%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "f_cvt")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "f_cvti2f")]
 )
 
 (define_insn "floatunssidf2"
@@ -1060,7 +1063,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fuitod%?\\t%P0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "f_cvt")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "f_cvti2f")]
 )
 
 
@@ -1072,7 +1076,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fsqrts%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "fdivs")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "fsqrts")]
 )
 
 (define_insn "*sqrtdf2_vfp"
@@ -1081,7 +1086,8 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fsqrtd%?\\t%P0, %P1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "fdivd")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "fsqrtd")]
 )
 
 
@@ -1168,6 +1174,7 @@
    fcmps%?\\t%0, %1
    fcmpzs%?\\t%0"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fcmps")]
 )
 
@@ -1180,6 +1187,7 @@
    fcmpes%?\\t%0, %1
    fcmpezs%?\\t%0"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fcmps")]
 )
 
@@ -1192,6 +1200,7 @@
    fcmpd%?\\t%P0, %P1
    fcmpzd%?\\t%P0"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fcmpd")]
 )
 
@@ -1204,6 +1213,7 @@
    fcmped%?\\t%P0, %P1
    fcmpezd%?\\t%P0"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "type" "fcmpd")]
 )
 
@@ -1219,7 +1229,7 @@
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP3 && !flag_rounding_math"
   "vcvt%?.f32.<FCVTI32typename>\\t%0, %1, %v2"
  [(set_attr "predicable" "yes")
-  (set_attr "type" "f_cvt")]
+  (set_attr "type" "f_cvti2f")]
 )
 
 ;; Not the ideal way of implementing this. Ideally we would be able to split
@@ -1237,7 +1247,7 @@
   vmov%?.f64\\t%P0, %1, %1\;vcvt%?.f64.<FCVTI32typename>\\t%P0, %P0, %v2"
  [(set_attr "predicable" "yes")
   (set_attr "ce_count" "2")
-  (set_attr "type" "f_cvt")
+  (set_attr "type" "f_cvti2f")
   (set_attr "length" "8")]
 )
 
@@ -1264,6 +1274,7 @@
   "TARGET_HARD_FLOAT && TARGET_FPU_ARMV8 <vfp_double_cond>"
   "vrint<vrint_variant>%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1"
   [(set_attr "predicable" "<vrint_predicable>")
+   (set_attr "predicable_short_it" "no")
    (set_attr "conds" "<vrint_conds>")
    (set_attr "type" "f_rint<vfp_type>")]
 )
Index: b/src/gcc/config/arm/t-linux-eabi
===================================================================
--- a/src/gcc/config/arm/t-linux-eabi
+++ b/src/gcc/config/arm/t-linux-eabi
@@ -18,6 +18,8 @@
 
 # We do not build a Thumb multilib for Linux because the definition of
 # CLEAR_INSN_CACHE in linux-gas.h does not work in Thumb mode.
+# If you set MULTILIB_OPTIONS to a non-empty value you should also set
+# MULTILIB_DEFAULTS in linux-elf.h.
 MULTILIB_OPTIONS	=
 MULTILIB_DIRNAMES	=
 
Index: b/src/gcc/config/arm/neon.md
===================================================================
--- a/src/gcc/config/arm/neon.md
+++ b/src/gcc/config/arm/neon.md
@@ -20,7 +20,7 @@
 
 
 ;; Attribute used to permit string comparisons against <VQH_mnem> in
-;; neon_type attribute definitions.
+;; type attribute definitions.
 (define_attr "vqh_mnem" "vadd,vmin,vmax" (const_string "vadd"))
 
 (define_insn "*neon_mov<mode>"
@@ -60,9 +60,9 @@
     default: return output_move_double (operands, true, NULL);
     }
 }
- [(set_attr "neon_type" "neon_int_1,*,neon_vmov,*,neon_mrrc,neon_mcr_2_mcrr,*,*,*")
-  (set_attr "type" "*,f_stored,*,f_loadd,*,*,alu_reg,load2,store2")
-  (set_attr "insn" "*,*,*,*,*,*,mov,*,*")
+ [(set_attr "type" "neon_move<q>,neon_store1_1reg,neon_move<q>,\
+                    neon_load1_1reg, neon_to_gp<q>,neon_from_gp<q>,mov_reg,\
+                    neon_load1_2reg, neon_store1_2reg")
   (set_attr "length" "4,4,4,4,4,4,8,8,8")
   (set_attr "arm_pool_range"     "*,*,*,1020,*,*,*,1020,*")
   (set_attr "thumb2_pool_range"     "*,*,*,1018,*,*,*,1018,*")
@@ -105,10 +105,9 @@
     default: return output_move_quad (operands);
     }
 }
-  [(set_attr "neon_type" "neon_int_1,neon_stm_2,neon_vmov,neon_ldm_2,\
-                          neon_mrrc,neon_mcr_2_mcrr,*,*,*")
-   (set_attr "type" "*,*,*,*,*,*,alu_reg,load4,store4")
-   (set_attr "insn" "*,*,*,*,*,*,mov,*,*")
+  [(set_attr "type" "neon_move_q,neon_store2_2reg_q,neon_move_q,\
+                     neon_load2_2reg_q,neon_to_gp_q,neon_from_gp_q,\
+                     mov_reg,neon_load1_4reg,neon_store1_4reg")
    (set_attr "length" "4,8,4,8,8,8,16,8,16")
    (set_attr "arm_pool_range" "*,*,*,1020,*,*,*,1020,*")
    (set_attr "thumb2_pool_range" "*,*,*,1018,*,*,*,1018,*")
@@ -152,7 +151,7 @@
     default: gcc_unreachable ();
     }
 }
-  [(set_attr "neon_type" "neon_int_1,neon_stm_2,neon_ldm_2")
+  [(set_attr "type" "neon_move_q,neon_store2_2reg_q,neon_load2_2reg_q")
    (set (attr "length") (symbol_ref "arm_attr_length_move_neon (insn)"))])
 
 (define_split
@@ -260,7 +259,7 @@
 		    UNSPEC_MISALIGNED_ACCESS))]
   "TARGET_NEON && !BYTES_BIG_ENDIAN && unaligned_access"
   "vst1.<V_sz_elem>\t{%P1}, %A0"
-  [(set_attr "neon_type" "neon_vst1_1_2_regs_vst2_2_regs")])
+  [(set_attr "type" "neon_store1_1reg<q>")])
 
 (define_insn "*movmisalign<mode>_neon_load"
   [(set (match_operand:VDX 0 "s_register_operand"		"=w")
@@ -268,7 +267,7 @@
 		    UNSPEC_MISALIGNED_ACCESS))]
   "TARGET_NEON && !BYTES_BIG_ENDIAN && unaligned_access"
   "vld1.<V_sz_elem>\t{%P0}, %A1"
-  [(set_attr "neon_type" "neon_vld1_1_2_regs")])
+  [(set_attr "type" "neon_load1_1reg<q>")])
 
 (define_insn "*movmisalign<mode>_neon_store"
   [(set (match_operand:VQX 0 "neon_struct_operand"	       "=Um")
@@ -276,7 +275,7 @@
 		    UNSPEC_MISALIGNED_ACCESS))]
   "TARGET_NEON && !BYTES_BIG_ENDIAN && unaligned_access"
   "vst1.<V_sz_elem>\t{%q1}, %A0"
-  [(set_attr "neon_type" "neon_vst1_1_2_regs_vst2_2_regs")])
+  [(set_attr "type" "neon_store1_1reg<q>")])
 
 (define_insn "*movmisalign<mode>_neon_load"
   [(set (match_operand:VQX 0 "s_register_operand"	         "=w")
@@ -284,7 +283,7 @@
 		    UNSPEC_MISALIGNED_ACCESS))]
   "TARGET_NEON && !BYTES_BIG_ENDIAN && unaligned_access"
   "vld1.<V_sz_elem>\t{%q0}, %A1"
-  [(set_attr "neon_type" "neon_vld1_1_2_regs")])
+  [(set_attr "type" "neon_store1_1reg<q>")])
 
 (define_insn "vec_set<mode>_internal"
   [(set (match_operand:VD 0 "s_register_operand" "=w,w")
@@ -305,7 +304,7 @@
   else
     return "vmov.<V_sz_elem>\t%P0[%c2], %1";
 }
-  [(set_attr "neon_type" "neon_vld1_vld2_lane,neon_mcr")])
+  [(set_attr "type" "neon_load1_all_lanes<q>,neon_from_gp<q>")])
 
 (define_insn "vec_set<mode>_internal"
   [(set (match_operand:VQ 0 "s_register_operand" "=w,w")
@@ -333,7 +332,7 @@
   else
     return "vmov.<V_sz_elem>\t%P0[%c2], %1";
 }
-  [(set_attr "neon_type" "neon_vld1_vld2_lane,neon_mcr")]
+  [(set_attr "type" "neon_load1_all_lanes<q>,neon_from_gp<q>")]
 )
 
 (define_insn "vec_setv2di_internal"
@@ -355,7 +354,7 @@
   else
     return "vmov\t%P0, %Q1, %R1";
 }
-  [(set_attr "neon_type" "neon_vld1_1_2_regs,neon_mcr_2_mcrr")]
+  [(set_attr "type" "neon_load1_all_lanes_q,neon_from_gp_q")]
 )
 
 (define_expand "vec_set<mode>"
@@ -389,7 +388,7 @@
   else
     return "vmov.<V_uf_sclr>\t%0, %P1[%c2]";
 }
-  [(set_attr "neon_type" "neon_vst1_vst2_lane,neon_bp_simple")]
+  [(set_attr "type" "neon_store1_one_lane<q>,neon_to_gp<q>")]
 )
 
 (define_insn "vec_extract<mode>"
@@ -415,7 +414,7 @@
   else
     return "vmov.<V_uf_sclr>\t%0, %P1[%c2]";
 }
-  [(set_attr "neon_type" "neon_vst1_vst2_lane,neon_bp_simple")]
+  [(set_attr "type" "neon_store1_one_lane<q>,neon_to_gp<q>")]
 )
 
 (define_insn "vec_extractv2di"
@@ -434,7 +433,7 @@
   else
     return "vmov\t%Q0, %R0, %P1  @ v2di";
 }
-  [(set_attr "neon_type" "neon_vst1_vst2_lane,neon_int_1")]
+  [(set_attr "type" "neon_store1_one_lane_q,neon_to_gp_q")]
 )
 
 (define_expand "vec_init<mode>"
@@ -457,12 +456,10 @@
 		  (match_operand:VDQ 2 "s_register_operand" "w")))]
   "TARGET_NEON && (!<Is_float_mode> || flag_unsafe_math_optimizations)"
   "vadd.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_1")))]
+                    (const_string "neon_fp_addsub_s<q>")
+                    (const_string "neon_add<q>")))]
 )
 
 (define_insn "adddi3_neon"
@@ -484,10 +481,11 @@
     default: gcc_unreachable ();
     }
 }
-  [(set_attr "neon_type" "neon_int_1,*,*,neon_int_1,*,*,*")
+  [(set_attr "type" "neon_add,multiple,multiple,neon_add,\
+		     multiple,multiple,multiple")
    (set_attr "conds" "*,clob,clob,*,clob,clob,clob")
    (set_attr "length" "*,8,8,*,8,8,8")
-   (set_attr "arch" "nota8,*,*,onlya8,*,*,*")]
+   (set_attr "arch" "neon_for_64bits,*,*,avoid_neon_for_64bits,*,*,*")]
 )
 
 (define_insn "*sub<mode>3_neon"
@@ -496,12 +494,10 @@
                    (match_operand:VDQ 2 "s_register_operand" "w")))]
   "TARGET_NEON && (!<Is_float_mode> || flag_unsafe_math_optimizations)"
   "vsub.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_2")))]
+                    (const_string "neon_fp_addsub_s<q>")
+                    (const_string "neon_sub<q>")))]
 )
 
 (define_insn "subdi3_neon"
@@ -521,75 +517,48 @@
     default: gcc_unreachable ();
     }
 }
-  [(set_attr "neon_type" "neon_int_2,*,*,*,neon_int_2")
+  [(set_attr "type" "neon_sub,multiple,multiple,multiple,neon_sub")
    (set_attr "conds" "*,clob,clob,clob,*")
    (set_attr "length" "*,8,8,8,*")
-   (set_attr "arch" "nota8,*,*,*,onlya8")]
+   (set_attr "arch" "neon_for_64bits,*,*,*,avoid_neon_for_64bits")]
 )
 
 (define_insn "*mul<mode>3_neon"
-  [(set (match_operand:VDQ 0 "s_register_operand" "=w")
-        (mult:VDQ (match_operand:VDQ 1 "s_register_operand" "w")
-                  (match_operand:VDQ 2 "s_register_operand" "w")))]
+  [(set (match_operand:VDQW 0 "s_register_operand" "=w")
+        (mult:VDQW (match_operand:VDQW 1 "s_register_operand" "w")
+                   (match_operand:VDQW 2 "s_register_operand" "w")))]
   "TARGET_NEON && (!<Is_float_mode> || flag_unsafe_math_optimizations)"
   "vmul.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (if_then_else
-                                    (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long")
-                                    (const_string "neon_mul_qqq_8_16_32_ddd_32"))
-                                  (if_then_else (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mul_qqq_8_16_32_ddd_32")
-                                    (const_string "neon_mul_qqq_8_16_32_ddd_32")))))]
+		    (const_string "neon_fp_mul_s<q>")
+                    (const_string "neon_mul_<V_elem_ch><q>")))]
 )
 
 (define_insn "mul<mode>3add<mode>_neon"
-  [(set (match_operand:VDQ 0 "s_register_operand" "=w")
-        (plus:VDQ (mult:VDQ (match_operand:VDQ 2 "s_register_operand" "w")
-                            (match_operand:VDQ 3 "s_register_operand" "w"))
-		  (match_operand:VDQ 1 "s_register_operand" "0")))]
+  [(set (match_operand:VDQW 0 "s_register_operand" "=w")
+        (plus:VDQW (mult:VDQW (match_operand:VDQW 2 "s_register_operand" "w")
+                            (match_operand:VDQW 3 "s_register_operand" "w"))
+		  (match_operand:VDQW 1 "s_register_operand" "0")))]
   "TARGET_NEON && (!<Is_float_mode> || flag_unsafe_math_optimizations)"
   "vmla.<V_if_elem>\t%<V_reg>0, %<V_reg>2, %<V_reg>3"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vmla_ddd")
-                                  (const_string "neon_fp_vmla_qqq"))
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (if_then_else
-                                    (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-                                    (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))
-                                  (if_then_else (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mla_qqq_8_16")
-                                    (const_string "neon_mla_qqq_32_qqd_32_scalar")))))]
+		    (const_string "neon_fp_mla_s<q>")
+		    (const_string "neon_mla_<V_elem_ch><q>")))]
 )
 
 (define_insn "mul<mode>3neg<mode>add<mode>_neon"
-  [(set (match_operand:VDQ 0 "s_register_operand" "=w")
-        (minus:VDQ (match_operand:VDQ 1 "s_register_operand" "0")
-                   (mult:VDQ (match_operand:VDQ 2 "s_register_operand" "w")
-                             (match_operand:VDQ 3 "s_register_operand" "w"))))]
+  [(set (match_operand:VDQW 0 "s_register_operand" "=w")
+        (minus:VDQW (match_operand:VDQW 1 "s_register_operand" "0")
+                    (mult:VDQW (match_operand:VDQW 2 "s_register_operand" "w")
+                               (match_operand:VDQW 3 "s_register_operand" "w"))))]
   "TARGET_NEON && (!<Is_float_mode> || flag_unsafe_math_optimizations)"
   "vmls.<V_if_elem>\t%<V_reg>0, %<V_reg>2, %<V_reg>3"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vmla_ddd")
-                                  (const_string "neon_fp_vmla_qqq"))
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (if_then_else
-                                    (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-                                    (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))
-                                  (if_then_else (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mla_qqq_8_16")
-                                    (const_string "neon_mla_qqq_32_qqd_32_scalar")))))]
+		    (const_string "neon_fp_mla_s<q>")
+		    (const_string "neon_mla_<V_elem_ch><q>")))]
 )
 
 ;; Fused multiply-accumulate
@@ -604,10 +573,7 @@
 		 (match_operand:VCVTF 3 "register_operand" "0")))]
   "TARGET_NEON && TARGET_FMA && flag_unsafe_math_optimizations"
   "vfma%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-	(if_then_else (match_test "<Is_d_reg>")
-		      (const_string "neon_fp_vmla_ddd")
-		      (const_string "neon_fp_vmla_qqq")))]
+  [(set_attr "type" "neon_fp_mla_s<q>")]
 )
 
 (define_insn "fma<VCVTF:mode>4_intrinsic"
@@ -617,10 +583,7 @@
 		 (match_operand:VCVTF 3 "register_operand" "0")))]
   "TARGET_NEON && TARGET_FMA"
   "vfma%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-	(if_then_else (match_test "<Is_d_reg>")
-		      (const_string "neon_fp_vmla_ddd")
-		      (const_string "neon_fp_vmla_qqq")))]
+  [(set_attr "type" "neon_fp_mla_s<q>")]
 )
 
 (define_insn "*fmsub<VCVTF:mode>4"
@@ -630,10 +593,7 @@
 		   (match_operand:VCVTF 3 "register_operand" "0")))]
   "TARGET_NEON && TARGET_FMA && flag_unsafe_math_optimizations"
   "vfms%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-	(if_then_else (match_test "<Is_d_reg>")
-		      (const_string "neon_fp_vmla_ddd")
-		      (const_string "neon_fp_vmla_qqq")))]
+  [(set_attr "type" "neon_fp_mla_s<q>")]
 )
 
 (define_insn "fmsub<VCVTF:mode>4_intrinsic"
@@ -643,10 +603,7 @@
 		   (match_operand:VCVTF 3 "register_operand" "0")))]
   "TARGET_NEON && TARGET_FMA"
   "vfms%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-	(if_then_else (match_test "<Is_d_reg>")
-		      (const_string "neon_fp_vmla_ddd")
-		      (const_string "neon_fp_vmla_qqq")))]
+  [(set_attr "type" "neon_fp_mla_s<q>")]
 )
 
 (define_insn "neon_vrint<NEON_VRINT:nvrint_variant><VCVTF:mode>"
@@ -656,10 +613,7 @@
 		NEON_VRINT))]
   "TARGET_NEON && TARGET_FPU_ARMV8"
   "vrint<nvrint_variant>%?.f32\\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-	(if_then_else (match_test "<Is_d_reg>")
-		(const_string "neon_fp_vadd_ddd_vabs_dd")
-		(const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_fp_round_<V_elem_ch><q>")]
 )
 
 (define_insn "ior<mode>3"
@@ -676,30 +630,7 @@
     default: gcc_unreachable ();
     }
 }
-  [(set_attr "neon_type" "neon_int_1")]
-)
-
-(define_insn "iordi3_neon"
-  [(set (match_operand:DI 0 "s_register_operand" "=w,w,?&r,?&r,?w,?w")
-        (ior:DI (match_operand:DI 1 "s_register_operand" "%w,0,0,r,w,0")
-		(match_operand:DI 2 "neon_logic_op2" "w,Dl,r,r,w,Dl")))]
-  "TARGET_NEON"
-{
-  switch (which_alternative)
-    {
-    case 0: /* fall through */
-    case 4: return "vorr\t%P0, %P1, %P2";
-    case 1: /* fall through */
-    case 5: return neon_output_logic_immediate ("vorr", &operands[2],
-		     DImode, 0, VALID_NEON_QREG_MODE (DImode));
-    case 2: return "#";
-    case 3: return "#";
-    default: gcc_unreachable ();
-    }
-}
-  [(set_attr "neon_type" "neon_int_1,neon_int_1,*,*,neon_int_1,neon_int_1")
-   (set_attr "length" "*,*,8,8,*,*")
-   (set_attr "arch" "nota8,nota8,*,*,onlya8,onlya8")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 ;; The concrete forms of the Neon immediate-logic instructions are vbic and
@@ -721,30 +652,7 @@
     default: gcc_unreachable ();
     }
 }
-  [(set_attr "neon_type" "neon_int_1")]
-)
-
-(define_insn "anddi3_neon"
-  [(set (match_operand:DI 0 "s_register_operand" "=w,w,?&r,?&r,?w,?w")
-        (and:DI (match_operand:DI 1 "s_register_operand" "%w,0,0,r,w,0")
-		(match_operand:DI 2 "neon_inv_logic_op2" "w,DL,r,r,w,DL")))]
-  "TARGET_NEON"
-{
-  switch (which_alternative)
-    {
-    case 0: /* fall through */
-    case 4: return "vand\t%P0, %P1, %P2";
-    case 1: /* fall through */
-    case 5: return neon_output_logic_immediate ("vand", &operands[2],
-    		     DImode, 1, VALID_NEON_QREG_MODE (DImode));
-    case 2: return "#";
-    case 3: return "#";
-    default: gcc_unreachable ();
-    }
-}
-  [(set_attr "neon_type" "neon_int_1,neon_int_1,*,*,neon_int_1,neon_int_1")
-   (set_attr "length" "*,*,8,8,*,*")
-   (set_attr "arch" "nota8,nota8,*,*,onlya8,onlya8")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 (define_insn "orn<mode>3_neon"
@@ -753,7 +661,7 @@
 		 (match_operand:VDQ 1 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vorn\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_1")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 ;; TODO: investigate whether we should disable 
@@ -791,7 +699,7 @@
         DONE;
       }
   }"
-  [(set_attr "neon_type" "neon_int_1,*,*,*")
+  [(set_attr "type" "neon_logic,multiple,multiple,multiple")
    (set_attr "length" "*,16,8,8")
    (set_attr "arch" "any,a,t2,t2")]
 )
@@ -802,7 +710,7 @@
 		 (match_operand:VDQ 1 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vbic\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_1")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 ;; Compare to *anddi_notdi_di.
@@ -815,7 +723,7 @@
    vbic\t%P0, %P1, %P2
    #
    #"
-  [(set_attr "neon_type" "neon_int_1,*,*")
+  [(set_attr "type" "neon_logic,multiple,multiple")
    (set_attr "length" "*,8,8")]
 )
 
@@ -825,22 +733,7 @@
 		 (match_operand:VDQ 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "veor\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_1")]
-)
-
-(define_insn "xordi3_neon"
-  [(set (match_operand:DI 0 "s_register_operand" "=w,?&r,?&r,?w")
-        (xor:DI (match_operand:DI 1 "s_register_operand" "%w,0,r,w")
-	        (match_operand:DI 2 "s_register_operand" "w,r,r,w")))]
-  "TARGET_NEON"
-  "@
-   veor\t%P0, %P1, %P2
-   #
-   #
-   veor\t%P0, %P1, %P2"
-  [(set_attr "neon_type" "neon_int_1,*,*,neon_int_1")
-   (set_attr "length" "*,8,8,*")
-   (set_attr "arch" "nota8,*,*,onlya8")]
+  [(set_attr "type" "neon_logic<q>")]
 )
 
 (define_insn "one_cmpl<mode>2"
@@ -848,7 +741,7 @@
         (not:VDQ (match_operand:VDQ 1 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vmvn\t%<V_reg>0, %<V_reg>1"
-  [(set_attr "neon_type" "neon_int_1")]
+  [(set_attr "type" "neon_move<q>")]
 )
 
 (define_insn "abs<mode>2"
@@ -856,12 +749,10 @@
 	(abs:VDQW (match_operand:VDQW 1 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vabs.<V_s_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_3")))]
+                    (const_string "neon_fp_abs_s<q>")
+                    (const_string "neon_abs<q>")))]
 )
 
 (define_insn "neg<mode>2"
@@ -869,12 +760,10 @@
 	(neg:VDQW (match_operand:VDQW 1 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vneg.<V_s_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_3")))]
+                    (const_string "neon_fp_neg_s<q>")
+                    (const_string "neon_neg<q>")))]
 )
 
 (define_insn "negdi2_neon"
@@ -884,7 +773,8 @@
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_NEON"
   "#"
-  [(set_attr "length" "8")]
+  [(set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 ; Split negdi2_neon for vfp registers
@@ -922,7 +812,7 @@
 		    (match_operand:VDQIW 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vmin.<V_u_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_minmax<q>")]
 )
 
 (define_insn "*umax<mode>3_neon"
@@ -931,7 +821,7 @@
 		    (match_operand:VDQIW 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vmax.<V_u_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_minmax<q>")]
 )
 
 (define_insn "*smin<mode>3_neon"
@@ -940,10 +830,10 @@
 		   (match_operand:VDQW 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vmin.<V_s_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_vadd_ddd_vabs_dd")
-                    (const_string "neon_int_5")))]
+                    (const_string "neon_fp_minmax_s<q>")
+                    (const_string "neon_minmax<q>")))]
 )
 
 (define_insn "*smax<mode>3_neon"
@@ -952,10 +842,10 @@
 		   (match_operand:VDQW 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vmax.<V_s_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_vadd_ddd_vabs_dd")
-                    (const_string "neon_int_5")))]
+                    (const_string "neon_fp_minmax_s<q>")
+                    (const_string "neon_minmax<q>")))]
 )
 
 ; TODO: V2DI shifts are current disabled because there are bugs in the
@@ -978,10 +868,7 @@
         default: gcc_unreachable ();
       }
   }
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_vshl_ddd")
-                    (const_string "neon_shift_3")))]
+  [(set_attr "type" "neon_shift_reg<q>, neon_shift_imm<q>")]
 )
 
 (define_insn "vashr<mode>3_imm"
@@ -994,10 +881,7 @@
 					<MODE>mode, VALID_NEON_QREG_MODE (<MODE>mode),
 					false);
   }
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_vshl_ddd")
-                    (const_string "neon_shift_3")))]
+  [(set_attr "type" "neon_shift_imm<q>")]
 )
 
 (define_insn "vlshr<mode>3_imm"
@@ -1010,10 +894,7 @@
 					<MODE>mode, VALID_NEON_QREG_MODE (<MODE>mode),
 					false);
   }              
-  [(set (attr "neon_type")
-	(if_then_else (match_test "<Is_d_reg>")
-		      (const_string "neon_vshl_ddd")
-		      (const_string "neon_shift_3")))]
+  [(set_attr "type" "neon_shift_imm<q>")]
 )
 
 ; Used for implementing logical shift-right, which is a left-shift by a negative
@@ -1028,10 +909,7 @@
 		     UNSPEC_ASHIFT_SIGNED))]
   "TARGET_NEON"
   "vshl.<V_s_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_vshl_ddd")
-                    (const_string "neon_shift_3")))]
+  [(set_attr "type" "neon_shift_reg<q>")]
 )
 
 ; Used for implementing logical shift-right, which is a left-shift by a negative
@@ -1044,10 +922,7 @@
 		     UNSPEC_ASHIFT_UNSIGNED))]
   "TARGET_NEON"
   "vshl.<V_u_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_vshl_ddd")
-                    (const_string "neon_shift_3")))]
+  [(set_attr "type" "neon_shift_reg<q>")]
 )
 
 (define_expand "vashr<mode>3"
@@ -1099,7 +974,7 @@
   "@
    vld1.32\t{%P0[0]}, %A1
    vmov.32\t%P0[0], %1"
-  [(set_attr "neon_type" "neon_vld1_vld2_lane,neon_mcr")]
+  [(set_attr "type" "neon_load1_1reg,neon_from_gp")]
 )
 
 (define_insn "ashldi3_neon_noclobber"
@@ -1112,7 +987,7 @@
   "@
    vshl.u64\t%P0, %P1, %2
    vshl.u64\t%P0, %P1, %P2"
-  [(set_attr "neon_type" "neon_vshl_ddd,neon_vshl_ddd")]
+  [(set_attr "type" "neon_shift_imm, neon_shift_reg")]
 )
 
 (define_insn_and_split "ashldi3_neon"
@@ -1162,8 +1037,9 @@
       }
     DONE;
   }"
-  [(set_attr "arch" "nota8,nota8,*,*,onlya8,onlya8")
-   (set_attr "opt" "*,*,speed,speed,*,*")]
+  [(set_attr "arch" "neon_for_64bits,neon_for_64bits,*,*,avoid_neon_for_64bits,avoid_neon_for_64bits")
+   (set_attr "opt" "*,*,speed,speed,*,*")
+   (set_attr "type" "multiple")]
 )
 
 ; The shift amount needs to be negated for right-shifts
@@ -1174,7 +1050,7 @@
 		   UNSPEC_ASHIFT_SIGNED))]
   "TARGET_NEON && reload_completed"
   "vshl.s64\t%P0, %P1, %P2"
-  [(set_attr "neon_type" "neon_vshl_ddd")]
+  [(set_attr "type" "neon_shift_reg")]
 )
 
 ; The shift amount needs to be negated for right-shifts
@@ -1185,7 +1061,7 @@
 		   UNSPEC_ASHIFT_UNSIGNED))]
   "TARGET_NEON && reload_completed"
   "vshl.u64\t%P0, %P1, %P2"
-  [(set_attr "neon_type" "neon_vshl_ddd")]
+  [(set_attr "type" "neon_shift_reg")]
 )
 
 (define_insn "ashrdi3_neon_imm_noclobber"
@@ -1195,7 +1071,7 @@
   "TARGET_NEON && reload_completed
    && INTVAL (operands[2]) > 0 && INTVAL (operands[2]) <= 64"
   "vshr.s64\t%P0, %P1, %2"
-  [(set_attr "neon_type" "neon_vshl_ddd")]
+  [(set_attr "type" "neon_shift_imm")]
 )
 
 (define_insn "lshrdi3_neon_imm_noclobber"
@@ -1205,7 +1081,7 @@
   "TARGET_NEON && reload_completed
    && INTVAL (operands[2]) > 0 && INTVAL (operands[2]) <= 64"
   "vshr.u64\t%P0, %P1, %2"
-  [(set_attr "neon_type" "neon_vshl_ddd")]
+  [(set_attr "type" "neon_shift_imm")]
 )
 
 ;; ashrdi3_neon
@@ -1263,8 +1139,9 @@
 
     DONE;
   }"
-  [(set_attr "arch" "nota8,nota8,*,*,onlya8,onlya8")
-   (set_attr "opt" "*,*,speed,speed,*,*")]
+  [(set_attr "arch" "neon_for_64bits,neon_for_64bits,*,*,avoid_neon_for_64bits,avoid_neon_for_64bits")
+   (set_attr "opt" "*,*,speed,speed,*,*")
+   (set_attr "type" "multiple")]
 )
 
 ;; Widening operations
@@ -1276,7 +1153,7 @@
 		        (match_operand:<V_widen> 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vaddw.<V_s_elem>\t%q0, %q2, %P1"
-  [(set_attr "neon_type" "neon_int_3")]
+  [(set_attr "type" "neon_add_widen")]
 )
 
 (define_insn "widen_usum<mode>3"
@@ -1286,7 +1163,7 @@
 		        (match_operand:<V_widen> 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vaddw.<V_u_elem>\t%q0, %q2, %P1"
-  [(set_attr "neon_type" "neon_int_3")]
+  [(set_attr "type" "neon_add_widen")]
 )
 
 ;; VEXT can be used to synthesize coarse whole-vector shifts with 8-bit
@@ -1370,9 +1247,7 @@
   "TARGET_NEON"
   "<VQH_mnem>.<VQH_sign>32\t%P0, %e1, %f1"
   [(set_attr "vqh_mnem" "<VQH_mnem>")
-   (set (attr "neon_type")
-      (if_then_else (eq_attr "vqh_mnem" "vadd")
-                    (const_string "neon_int_1") (const_string "neon_int_5")))]
+   (set_attr "type" "neon_reduc_<VQH_type>_q")]
 )
 
 (define_insn "quad_halves_<code>v4sf"
@@ -1385,9 +1260,7 @@
   "TARGET_NEON && flag_unsafe_math_optimizations"
   "<VQH_mnem>.f32\t%P0, %e1, %f1"
   [(set_attr "vqh_mnem" "<VQH_mnem>")
-   (set (attr "neon_type")
-      (if_then_else (eq_attr "vqh_mnem" "vadd")
-                    (const_string "neon_int_1") (const_string "neon_int_5")))]
+   (set_attr "type" "neon_fp_reduc_<VQH_type>_s_q")]
 )
 
 (define_insn "quad_halves_<code>v8hi"
@@ -1402,9 +1275,7 @@
   "TARGET_NEON"
   "<VQH_mnem>.<VQH_sign>16\t%P0, %e1, %f1"
   [(set_attr "vqh_mnem" "<VQH_mnem>")
-   (set (attr "neon_type")
-      (if_then_else (eq_attr "vqh_mnem" "vadd")
-                    (const_string "neon_int_1") (const_string "neon_int_5")))]
+   (set_attr "type" "neon_reduc_<VQH_type>_q")]
 )
 
 (define_insn "quad_halves_<code>v16qi"
@@ -1423,9 +1294,7 @@
   "TARGET_NEON"
   "<VQH_mnem>.<VQH_sign>8\t%P0, %e1, %f1"
   [(set_attr "vqh_mnem" "<VQH_mnem>")
-   (set (attr "neon_type")
-      (if_then_else (eq_attr "vqh_mnem" "vadd")
-                    (const_string "neon_int_1") (const_string "neon_int_5")))]
+   (set_attr "type" "neon_reduc_<VQH_type>_q")]
 )
 
 (define_expand "move_hi_quad_<mode>"
@@ -1484,7 +1353,7 @@
 		     UNSPEC_VPADD))]
   "TARGET_NEON && !BYTES_BIG_ENDIAN"
   "vadd.i64\t%e0, %e1, %f1"
-  [(set_attr "neon_type" "neon_int_1")]
+  [(set_attr "type" "neon_add_q")]
 )
 
 ;; NEON does not distinguish between signed and unsigned addition except on
@@ -1608,12 +1477,10 @@
   "TARGET_NEON"
   "vpadd.<V_if_elem>\t%P0, %P1, %P2"
   ;; Assume this schedules like vadd.
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_1")))]
+                    (const_string "neon_fp_reduc_add_s<q>")
+                    (const_string "neon_reduc_add<q>")))]
 )
 
 (define_insn "neon_vpsmin<mode>"
@@ -1623,11 +1490,10 @@
                    UNSPEC_VPSMIN))]
   "TARGET_NEON"
   "vpmin.<V_s_elem>\t%P0, %P1, %P2"
-  ;; Assume this schedules like vmin.
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_vadd_ddd_vabs_dd")
-                    (const_string "neon_int_5")))]
+                    (const_string "neon_fp_reduc_minmax_s<q>")
+                    (const_string "neon_reduc_minmax<q>")))]
 )
 
 (define_insn "neon_vpsmax<mode>"
@@ -1637,11 +1503,10 @@
                    UNSPEC_VPSMAX))]
   "TARGET_NEON"
   "vpmax.<V_s_elem>\t%P0, %P1, %P2"
-  ;; Assume this schedules like vmax.
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_vadd_ddd_vabs_dd")
-                    (const_string "neon_int_5")))]
+                    (const_string "neon_fp_reduc_minmax_s<q>")
+                    (const_string "neon_reduc_minmax<q>")))]
 )
 
 (define_insn "neon_vpumin<mode>"
@@ -1651,8 +1516,7 @@
                    UNSPEC_VPUMIN))]
   "TARGET_NEON"
   "vpmin.<V_u_elem>\t%P0, %P1, %P2"
-  ;; Assume this schedules like umin.
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_reduc_minmax<q>")]
 )
 
 (define_insn "neon_vpumax<mode>"
@@ -1662,8 +1526,7 @@
                    UNSPEC_VPUMAX))]
   "TARGET_NEON"
   "vpmax.<V_u_elem>\t%P0, %P1, %P2"
-  ;; Assume this schedules like umax.
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_reduc_minmax<q>")]
 )
 
 ;; Saturating arithmetic
@@ -1680,7 +1543,7 @@
                    (match_operand:VD 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vqadd.<V_s_elem>\t%P0, %P1, %P2"
-  [(set_attr "neon_type" "neon_int_4")]
+  [(set_attr "type" "neon_qadd<q>")]
 )
 
 (define_insn "*us_add<mode>_neon"
@@ -1689,7 +1552,7 @@
                    (match_operand:VD 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vqadd.<V_u_elem>\t%P0, %P1, %P2"
-  [(set_attr "neon_type" "neon_int_4")]
+  [(set_attr "type" "neon_qadd<q>")]
 )
 
 (define_insn "*ss_sub<mode>_neon"
@@ -1698,7 +1561,7 @@
                     (match_operand:VD 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vqsub.<V_s_elem>\t%P0, %P1, %P2"
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_qsub<q>")]
 )
 
 (define_insn "*us_sub<mode>_neon"
@@ -1707,7 +1570,7 @@
                     (match_operand:VD 2 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vqsub.<V_u_elem>\t%P0, %P1, %P2"
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_qsub<q>")]
 )
 
 ;; Conditional instructions.  These are comparisons with conditional moves for
@@ -1999,12 +1862,10 @@
                      UNSPEC_VADD))]
   "TARGET_NEON"
   "vadd.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_1")))]
+                    (const_string "neon_fp_addsub_s<q>")
+                    (const_string "neon_add<q>")))]
 )
 
 ; operand 3 represents in bits:
@@ -2019,7 +1880,7 @@
                           UNSPEC_VADDL))]
   "TARGET_NEON"
   "vaddl.%T3%#<V_sz_elem>\t%q0, %P1, %P2"
-  [(set_attr "neon_type" "neon_int_3")]
+  [(set_attr "type" "neon_add_long")]
 )
 
 (define_insn "neon_vaddw<mode>"
@@ -2030,7 +1891,7 @@
                           UNSPEC_VADDW))]
   "TARGET_NEON"
   "vaddw.%T3%#<V_sz_elem>\t%q0, %q1, %P2"
-  [(set_attr "neon_type" "neon_int_2")]
+  [(set_attr "type" "neon_add_widen")]
 )
 
 ; vhadd and vrhadd.
@@ -2043,7 +1904,7 @@
 		      UNSPEC_VHADD))]
   "TARGET_NEON"
   "v%O3hadd.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_4")]
+  [(set_attr "type" "neon_add_halve_q")]
 )
 
 (define_insn "neon_vqadd<mode>"
@@ -2054,7 +1915,7 @@
                      UNSPEC_VQADD))]
   "TARGET_NEON"
   "vqadd.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_4")]
+  [(set_attr "type" "neon_qadd<q>")]
 )
 
 (define_insn "neon_vaddhn<mode>"
@@ -2065,7 +1926,7 @@
                            UNSPEC_VADDHN))]
   "TARGET_NEON"
   "v%O3addhn.<V_if_elem>\t%P0, %q1, %q2"
-  [(set_attr "neon_type" "neon_int_4")]
+  [(set_attr "type" "neon_add_halve_narrow_q")]
 )
 
 ;; We cannot replace this unspec with mul<mode>3 because of the odd 
@@ -2078,19 +1939,10 @@
 		     UNSPEC_VMUL))]
   "TARGET_NEON"
   "vmul.%F3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (if_then_else
-                                    (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long")
-                                    (const_string "neon_mul_qqq_8_16_32_ddd_32"))
-                                  (if_then_else (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mul_qqq_8_16_32_ddd_32")
-                                    (const_string "neon_mul_qqq_8_16_32_ddd_32")))))]
+                    (const_string "neon_fp_mul_s<q>")
+                    (const_string "neon_mul_<V_elem_ch><q>")))]
 )
 
 (define_expand "neon_vmla<mode>"
@@ -2139,26 +1991,17 @@
 ; Used for intrinsics when flag_unsafe_math_optimizations is false.
 
 (define_insn "neon_vmla<mode>_unspec"
-  [(set (match_operand:VDQ 0 "s_register_operand" "=w")
-	(unspec:VDQ [(match_operand:VDQ 1 "s_register_operand" "0")
-		     (match_operand:VDQ 2 "s_register_operand" "w")
-		     (match_operand:VDQ 3 "s_register_operand" "w")]
+  [(set (match_operand:VDQW 0 "s_register_operand" "=w")
+	(unspec:VDQW [(match_operand:VDQW 1 "s_register_operand" "0")
+		      (match_operand:VDQW 2 "s_register_operand" "w")
+		      (match_operand:VDQW 3 "s_register_operand" "w")]
 		    UNSPEC_VMLA))]
   "TARGET_NEON"
   "vmla.<V_if_elem>\t%<V_reg>0, %<V_reg>2, %<V_reg>3"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vmla_ddd")
-                                  (const_string "neon_fp_vmla_qqq"))
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (if_then_else
-                                    (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-                                    (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))
-                                  (if_then_else (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mla_qqq_8_16")
-                                    (const_string "neon_mla_qqq_32_qqd_32_scalar")))))]
+                    (const_string "neon_fp_mla_s<q>")
+                    (const_string "neon_mla_<V_elem_ch><q>")))]
 )
 
 (define_insn "neon_vmlal<mode>"
@@ -2170,10 +2013,7 @@
                           UNSPEC_VMLAL))]
   "TARGET_NEON"
   "vmlal.%T4%#<V_sz_elem>\t%q0, %P2, %P3"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-                   (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")))]
+  [(set_attr "type" "neon_mla_<V_elem_ch>_long")]
 )
 
 (define_expand "neon_vmls<mode>"
@@ -2196,27 +2036,17 @@
 ; Used for intrinsics when flag_unsafe_math_optimizations is false.
 
 (define_insn "neon_vmls<mode>_unspec"
-  [(set (match_operand:VDQ 0 "s_register_operand" "=w")
-	(unspec:VDQ [(match_operand:VDQ 1 "s_register_operand" "0")
-		     (match_operand:VDQ 2 "s_register_operand" "w")
-		     (match_operand:VDQ 3 "s_register_operand" "w")]
+  [(set (match_operand:VDQW 0 "s_register_operand" "=w")
+	(unspec:VDQW [(match_operand:VDQW 1 "s_register_operand" "0")
+		      (match_operand:VDQW 2 "s_register_operand" "w")
+		      (match_operand:VDQW 3 "s_register_operand" "w")]
 		    UNSPEC_VMLS))]
   "TARGET_NEON"
   "vmls.<V_if_elem>\t%<V_reg>0, %<V_reg>2, %<V_reg>3"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vmla_ddd")
-                                  (const_string "neon_fp_vmla_qqq"))
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (if_then_else
-                                    (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-                                    (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))
-                                  (if_then_else
-                                    (match_test "<Scalar_mul_8_16>")
-                                    (const_string "neon_mla_qqq_8_16")
-                                    (const_string "neon_mla_qqq_32_qqd_32_scalar")))))]
+                    (const_string "neon_fp_mla_s<q>")
+                    (const_string "neon_mla_<V_elem_ch><q>")))]
 )
 
 (define_insn "neon_vmlsl<mode>"
@@ -2228,10 +2058,7 @@
                           UNSPEC_VMLSL))]
   "TARGET_NEON"
   "vmlsl.%T4%#<V_sz_elem>\t%q0, %P2, %P3"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-                   (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")))]
+  [(set_attr "type" "neon_mla_<V_elem_ch>_long")]
 )
 
 (define_insn "neon_vqdmulh<mode>"
@@ -2242,14 +2069,7 @@
                       UNSPEC_VQDMULH))]
   "TARGET_NEON"
   "vq%O3dmulh.<V_s_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-        (if_then_else (match_test "<Scalar_mul_8_16>")
-                      (const_string "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long")
-                      (const_string "neon_mul_qqq_8_16_32_ddd_32"))
-        (if_then_else (match_test "<Scalar_mul_8_16>")
-                      (const_string "neon_mul_qqq_8_16_32_ddd_32")
-                      (const_string "neon_mul_qqq_8_16_32_ddd_32"))))]
+  [(set_attr "type" "neon_sat_mul_<V_elem_ch><q>")]
 )
 
 (define_insn "neon_vqdmlal<mode>"
@@ -2261,10 +2081,7 @@
                           UNSPEC_VQDMLAL))]
   "TARGET_NEON"
   "vqdmlal.<V_s_elem>\t%q0, %P2, %P3"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-                   (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")))]
+  [(set_attr "type" "neon_sat_mla_<V_elem_ch>_long")]
 )
 
 (define_insn "neon_vqdmlsl<mode>"
@@ -2276,10 +2093,7 @@
                           UNSPEC_VQDMLSL))]
   "TARGET_NEON"
   "vqdmlsl.<V_s_elem>\t%q0, %P2, %P3"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long")
-                   (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")))]
+  [(set_attr "type" "neon_sat_mla_<V_elem_ch>_long")]
 )
 
 (define_insn "neon_vmull<mode>"
@@ -2290,10 +2104,7 @@
                           UNSPEC_VMULL))]
   "TARGET_NEON"
   "vmull.%T3%#<V_sz_elem>\t%q0, %P1, %P2"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long")
-                   (const_string "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")))]
+  [(set_attr "type" "neon_mul_<V_elem_ch>_long")]
 )
 
 (define_insn "neon_vqdmull<mode>"
@@ -2304,10 +2115,7 @@
                           UNSPEC_VQDMULL))]
   "TARGET_NEON"
   "vqdmull.<V_s_elem>\t%q0, %P1, %P2"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long")
-                   (const_string "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")))]
+  [(set_attr "type" "neon_sat_mul_<V_elem_ch>_long")]
 )
 
 (define_expand "neon_vsub<mode>"
@@ -2334,12 +2142,10 @@
                      UNSPEC_VSUB))]
   "TARGET_NEON"
   "vsub.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_2")))]
+                    (const_string "neon_fp_addsub_s<q>")
+                    (const_string "neon_sub<q>")))]
 )
 
 (define_insn "neon_vsubl<mode>"
@@ -2350,7 +2156,7 @@
                           UNSPEC_VSUBL))]
   "TARGET_NEON"
   "vsubl.%T3%#<V_sz_elem>\t%q0, %P1, %P2"
-  [(set_attr "neon_type" "neon_int_2")]
+  [(set_attr "type" "neon_sub_long")]
 )
 
 (define_insn "neon_vsubw<mode>"
@@ -2361,7 +2167,7 @@
 			  UNSPEC_VSUBW))]
   "TARGET_NEON"
   "vsubw.%T3%#<V_sz_elem>\t%q0, %q1, %P2"
-  [(set_attr "neon_type" "neon_int_2")]
+  [(set_attr "type" "neon_sub_widen")]
 )
 
 (define_insn "neon_vqsub<mode>"
@@ -2372,7 +2178,7 @@
 		      UNSPEC_VQSUB))]
   "TARGET_NEON"
   "vqsub.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_qsub<q>")]
 )
 
 (define_insn "neon_vhsub<mode>"
@@ -2383,7 +2189,7 @@
 		      UNSPEC_VHSUB))]
   "TARGET_NEON"
   "vhsub.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_sub_halve<q>")]
 )
 
 (define_insn "neon_vsubhn<mode>"
@@ -2394,7 +2200,7 @@
                            UNSPEC_VSUBHN))]
   "TARGET_NEON"
   "v%O3subhn.<V_if_elem>\t%P0, %q1, %q2"
-  [(set_attr "neon_type" "neon_int_4")]
+  [(set_attr "type" "neon_sub_halve_narrow_q")]
 )
 
 (define_insn "neon_vceq<mode>"
@@ -2408,12 +2214,12 @@
   "@
   vceq.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2
   vceq.<V_if_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_5")))]
+                    (const_string "neon_fp_compare_s<q>")
+                    (if_then_else (match_operand 2 "zero_operand")
+                      (const_string "neon_compare_zero<q>")
+                      (const_string "neon_compare<q>"))))]
 )
 
 (define_insn "neon_vcge<mode>"
@@ -2427,12 +2233,12 @@
   "@
   vcge.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2
   vcge.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "neon_type")
+  [(set (attr "type")
      (if_then_else (match_test "<Is_float_mode>")
-                   (if_then_else (match_test "<Is_d_reg>")
-                                 (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                 (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                   (const_string "neon_int_5")))]
+                   (const_string "neon_fp_compare_s<q>")
+                    (if_then_else (match_operand 2 "zero_operand")
+                      (const_string "neon_compare_zero<q>")
+                      (const_string "neon_compare<q>"))))]
 )
 
 (define_insn "neon_vcgeu<mode>"
@@ -2444,7 +2250,7 @@
           UNSPEC_VCGEU))]
   "TARGET_NEON"
   "vcge.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_compare<q>")]
 )
 
 (define_insn "neon_vcgt<mode>"
@@ -2458,12 +2264,12 @@
   "@
   vcgt.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2
   vcgt.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "neon_type")
+  [(set (attr "type")
      (if_then_else (match_test "<Is_float_mode>")
-                   (if_then_else (match_test "<Is_d_reg>")
-                                 (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                 (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                   (const_string "neon_int_5")))]
+                   (const_string "neon_fp_compare_s<q>")
+                    (if_then_else (match_operand 2 "zero_operand")
+                      (const_string "neon_compare_zero<q>")
+                      (const_string "neon_compare<q>"))))]
 )
 
 (define_insn "neon_vcgtu<mode>"
@@ -2475,7 +2281,7 @@
           UNSPEC_VCGTU))]
   "TARGET_NEON"
   "vcgt.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_compare<q>")]
 )
 
 ;; VCLE and VCLT only support comparisons with immediate zero (register
@@ -2490,12 +2296,12 @@
           UNSPEC_VCLE))]
   "TARGET_NEON"
   "vcle.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_5")))]
+                    (const_string "neon_fp_compare_s<q>")
+                    (if_then_else (match_operand 2 "zero_operand")
+                      (const_string "neon_compare_zero<q>")
+                      (const_string "neon_compare<q>"))))]
 )
 
 (define_insn "neon_vclt<mode>"
@@ -2507,12 +2313,12 @@
           UNSPEC_VCLT))]
   "TARGET_NEON"
   "vclt.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (match_test "<Is_float_mode>")
-                    (if_then_else (match_test "<Is_d_reg>")
-                                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                  (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                    (const_string "neon_int_5")))]
+                    (const_string "neon_fp_compare_s<q>")
+                    (if_then_else (match_operand 2 "zero_operand")
+                      (const_string "neon_compare_zero<q>")
+                      (const_string "neon_compare<q>"))))]
 )
 
 (define_insn "neon_vcage<mode>"
@@ -2523,10 +2329,7 @@
                                UNSPEC_VCAGE))]
   "TARGET_NEON"
   "vacge.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_fp_compare_s<q>")]
 )
 
 (define_insn "neon_vcagt<mode>"
@@ -2537,10 +2340,7 @@
                                UNSPEC_VCAGT))]
   "TARGET_NEON"
   "vacgt.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_fp_compare_s<q>")]
 )
 
 (define_insn "neon_vtst<mode>"
@@ -2551,7 +2351,7 @@
 		      UNSPEC_VTST))]
   "TARGET_NEON"
   "vtst.<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "neon_type" "neon_int_4")]
+  [(set_attr "type" "neon_tst<q>")]
 )
 
 (define_insn "neon_vabd<mode>"
@@ -2562,12 +2362,10 @@
 		     UNSPEC_VABD))]
   "TARGET_NEON"
   "vabd.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
      (if_then_else (match_test "<Is_float_mode>")
-                   (if_then_else (match_test "<Is_d_reg>")
-                                 (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                 (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                   (const_string "neon_int_5")))]
+                   (const_string "neon_fp_abd_s<q>")
+                   (const_string "neon_abd<q>")))]
 )
 
 (define_insn "neon_vabdl<mode>"
@@ -2578,7 +2376,7 @@
                           UNSPEC_VABDL))]
   "TARGET_NEON"
   "vabdl.%T3%#<V_sz_elem>\t%q0, %P1, %P2"
-  [(set_attr "neon_type" "neon_int_5")]
+  [(set_attr "type" "neon_abd_long")]
 )
 
 (define_insn "neon_vaba<mode>"
@@ -2590,9 +2388,7 @@
 		    (match_operand:VDQIW 1 "s_register_operand" "0")))]
   "TARGET_NEON"
   "vaba.%T4%#<V_sz_elem>\t%<V_reg>0, %<V_reg>2, %<V_reg>3"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_vaba") (const_string "neon_vaba_qqq")))]
+  [(set_attr "type" "neon_arith_acc<q>")]
 )
 
 (define_insn "neon_vabal<mode>"
@@ -2604,7 +2400,7 @@
 			 (match_operand:<V_widen> 1 "s_register_operand" "0")))]
   "TARGET_NEON"
   "vabal.%T4%#<V_sz_elem>\t%q0, %P2, %P3"
-  [(set_attr "neon_type" "neon_vaba")]
+  [(set_attr "type" "neon_arith_acc<q>")]
 )
 
 (define_insn "neon_vmax<mode>"
@@ -2615,12 +2411,10 @@
                      UNSPEC_VMAX))]
   "TARGET_NEON"
   "vmax.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
     (if_then_else (match_test "<Is_float_mode>")
-                  (if_then_else (match_test "<Is_d_reg>")
-                                (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                  (const_string "neon_int_5")))]
+                  (const_string "neon_fp_minmax_s<q>")
+                  (const_string "neon_minmax<q>")))]
 )
 
 (define_insn "neon_vmin<mode>"
@@ -2631,12 +2425,10 @@
                      UNSPEC_VMIN))]
   "TARGET_NEON"
   "vmin.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
+  [(set (attr "type")
     (if_then_else (match_test "<Is_float_mode>")
-                  (if_then_else (match_test "<Is_d_reg>")
-                                (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                  (const_string "neon_int_5")))]
+                  (const_string "neon_fp_minmax_s<q>")
+                  (const_string "neon_minmax<q>")))]
 )
 
 (define_expand "neon_vpadd<mode>"
@@ -2658,8 +2450,7 @@
                                  UNSPEC_VPADDL))]
   "TARGET_NEON"
   "vpaddl.%T2%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1"
-  ;; Assume this schedules like vaddl.
-  [(set_attr "neon_type" "neon_int_3")]
+  [(set_attr "type" "neon_reduc_add_long")]
 )
 
 (define_insn "neon_vpadal<mode>"
@@ -2670,8 +2461,7 @@
                                  UNSPEC_VPADAL))]
   "TARGET_NEON"
   "vpadal.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>2"
-  ;; Assume this schedules like vpadd.
-  [(set_attr "neon_type" "neon_int_1")]
+  [(set_attr "type" "neon_reduc_add_acc")]
 )
 
 (define_insn "neon_vpmax<mode>"
@@ -2682,11 +2472,10 @@
                    UNSPEC_VPMAX))]
   "TARGET_NEON"
   "vpmax.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  ;; Assume this schedules like vmax.
-  [(set (attr "neon_type")
+  [(set (attr "type")
     (if_then_else (match_test "<Is_float_mode>")
-                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                  (const_string "neon_int_5")))]
+                  (const_string "neon_fp_reduc_minmax_s<q>")
+                  (const_string "neon_reduc_minmax<q>")))]
 )
 
 (define_insn "neon_vpmin<mode>"
@@ -2697,11 +2486,10 @@
                    UNSPEC_VPMIN))]
   "TARGET_NEON"
   "vpmin.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  ;; Assume this schedules like vmin.
-  [(set (attr "neon_type")
+  [(set (attr "type")
     (if_then_else (match_test "<Is_float_mode>")
-                  (const_string "neon_fp_vadd_ddd_vabs_dd")
-                  (const_string "neon_int_5")))]
+                  (const_string "neon_fp_reduc_minmax_s<q>")
+                  (const_string "neon_reduc_minmax<q>")))]
 )
 
 (define_insn "neon_vrecps<mode>"
@@ -2712,10 +2500,7 @@
                       UNSPEC_VRECPS))]
   "TARGET_NEON"
   "vrecps.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_fp_vrecps_vrsqrts_ddd")
-                    (const_string "neon_fp_vrecps_vrsqrts_qqq")))]
+  [(set_attr "type" "neon_fp_recps_s<q>")]
 )
 
 (define_insn "neon_vrsqrts<mode>"
@@ -2726,10 +2511,7 @@
                       UNSPEC_VRSQRTS))]
   "TARGET_NEON"
   "vrsqrts.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_fp_vrecps_vrsqrts_ddd")
-                    (const_string "neon_fp_vrecps_vrsqrts_qqq")))]
+  [(set_attr "type" "neon_fp_rsqrts_s<q>")]
 )
 
 (define_expand "neon_vabs<mode>"
@@ -2749,7 +2531,7 @@
 		      UNSPEC_VQABS))]
   "TARGET_NEON"
   "vqabs.<V_s_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set_attr "neon_type" "neon_vqneg_vqabs")]
+  [(set_attr "type" "neon_qabs<q>")]
 )
 
 (define_expand "neon_vneg<mode>"
@@ -2769,7 +2551,7 @@
 		      UNSPEC_VQNEG))]
   "TARGET_NEON"
   "vqneg.<V_s_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set_attr "neon_type" "neon_vqneg_vqabs")]
+  [(set_attr "type" "neon_qneg<q>")]
 )
 
 (define_insn "neon_vcls<mode>"
@@ -2779,7 +2561,7 @@
 		      UNSPEC_VCLS))]
   "TARGET_NEON"
   "vcls.<V_s_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set_attr "neon_type" "neon_int_1")]
+  [(set_attr "type" "neon_cls<q>")]
 )
 
 (define_insn "clz<mode>2"
@@ -2787,7 +2569,7 @@
         (clz:VDQIW (match_operand:VDQIW 1 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vclz.<V_if_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set_attr "neon_type" "neon_int_1")]
+  [(set_attr "type" "neon_cnt<q>")]
 )
 
 (define_expand "neon_vclz<mode>"
@@ -2805,7 +2587,7 @@
         (popcount:VE (match_operand:VE 1 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vcnt.<V_sz_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set_attr "neon_type" "neon_int_1")]
+  [(set_attr "type" "neon_cnt<q>")]
 )
 
 (define_expand "neon_vcnt<mode>"
@@ -2825,10 +2607,7 @@
                     UNSPEC_VRECPE))]
   "TARGET_NEON"
   "vrecpe.<V_u_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_fp_vadd_ddd_vabs_dd")
-                    (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_fp_recpe_s<q>")]
 )
 
 (define_insn "neon_vrsqrte<mode>"
@@ -2838,10 +2617,7 @@
                     UNSPEC_VRSQRTE))]
   "TARGET_NEON"
   "vrsqrte.<V_u_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_fp_vadd_ddd_vabs_dd")
-                    (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_fp_rsqrte_s<q>")]
 )
 
 (define_expand "neon_vmvn<mode>"
@@ -2870,7 +2646,7 @@
     }
   return "vmov.s<V_sz_elem>\t%0, %P1[%c2]";
 }
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_to_gp")]
 )
 
 (define_insn "neon_vget_lane<mode>_zext_internal"
@@ -2889,7 +2665,7 @@
     }
   return "vmov.u<V_sz_elem>\t%0, %P1[%c2]";
 }
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_to_gp")]
 )
 
 (define_insn "neon_vget_lane<mode>_sext_internal"
@@ -2916,7 +2692,7 @@
 
   return "";
 }
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_to_gp_q")]
 )
 
 (define_insn "neon_vget_lane<mode>_zext_internal"
@@ -2943,7 +2719,7 @@
 
   return "";
 }
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_to_gp_q")]
 )
 
 (define_expand "neon_vget_lane<mode>"
@@ -3075,8 +2851,7 @@
         (vec_duplicate:VX (match_operand:<V_elem> 1 "s_register_operand" "r")))]
   "TARGET_NEON"
   "vdup.<V_sz_elem>\t%<V_reg>0, %1"
-  ;; Assume this schedules like vmov.
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_from_gp<q>")]
 )
 
 (define_insn "neon_vdup_n<mode>"
@@ -3086,8 +2861,7 @@
   "@
   vdup.<V_sz_elem>\t%<V_reg>0, %1
   vdup.<V_sz_elem>\t%<V_reg>0, %y1"
-  ;; Assume this schedules like vmov.
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_from_gp<q>,neon_dup<q>")]
 )
 
 (define_expand "neon_vdup_ndi"
@@ -3108,7 +2882,7 @@
   vmov\t%e0, %Q1, %R1\;vmov\t%f0, %Q1, %R1
   vmov\t%e0, %P1\;vmov\t%f0, %P1"
   [(set_attr "length" "8")
-   (set_attr "neon_type" "neon_bp_simple")]
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "neon_vdup_lane<mode>_internal"
@@ -3130,8 +2904,7 @@
   else
     return "vdup.<V_sz_elem>\t%q0, %P1[%c2]";
 }
-  ;; Assume this schedules like vmov.
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_dup<q>")]
 )
 
 (define_expand "neon_vdup_lane<mode>"
@@ -3186,10 +2959,7 @@
    (set (match_dup 1) (match_dup 0))]
   "TARGET_NEON && reload_completed"
   "vswp\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-	(if_then_else (match_test "<Is_d_reg>")
-		      (const_string "neon_bp_simple")
-		      (const_string "neon_bp_2cycle")))]
+  [(set_attr "type" "neon_permute<q>")]
 )
 
 ;; In this insn, operand 1 should be low, and operand 2 the high part of the
@@ -3211,7 +2981,9 @@
 {
   neon_split_vcombine (operands);
   DONE;
-})
+}
+[(set_attr "type" "multiple")]
+)
 
 (define_expand "neon_vget_high<mode>"
   [(match_operand:<V_HALF> 0 "s_register_operand")
@@ -3240,10 +3012,7 @@
         (float:<V_CVTTO> (match_operand:VCVTI 1 "s_register_operand" "w")))]
   "TARGET_NEON && !flag_rounding_math"
   "vcvt.f32.s32\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_int_to_fp_<V_elem_ch><q>")]
 )
 
 (define_insn "floatuns<mode><V_cvtto>2"
@@ -3251,10 +3020,7 @@
         (unsigned_float:<V_CVTTO> (match_operand:VCVTI 1 "s_register_operand" "w")))] 
   "TARGET_NEON && !flag_rounding_math"
   "vcvt.f32.u32\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_int_to_fp_<V_elem_ch><q>")]
 )
 
 (define_insn "fix_trunc<mode><V_cvtto>2"
@@ -3262,10 +3028,7 @@
         (fix:<V_CVTTO> (match_operand:VCVTF 1 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vcvt.s32.f32\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_fp_to_int_<V_elem_ch><q>")]
 )
 
 (define_insn "fixuns_trunc<mode><V_cvtto>2"
@@ -3273,10 +3036,7 @@
         (unsigned_fix:<V_CVTTO> (match_operand:VCVTF 1 "s_register_operand" "w")))]
   "TARGET_NEON"
   "vcvt.u32.f32\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_fp_to_int_<V_elem_ch><q>")]
 )
 
 (define_insn "neon_vcvt<mode>"
@@ -3286,10 +3046,7 @@
 			  UNSPEC_VCVT))]
   "TARGET_NEON"
   "vcvt.%T2%#32.f32\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_fp_to_int_<V_elem_ch><q>")]
 )
 
 (define_insn "neon_vcvt<mode>"
@@ -3299,10 +3056,25 @@
 			  UNSPEC_VCVT))]
   "TARGET_NEON"
   "vcvt.f32.%T2%#32\t%<V_reg>0, %<V_reg>1"
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_int_to_fp_<V_elem_ch><q>")]
+)
+
+(define_insn "neon_vcvtv4sfv4hf"
+  [(set (match_operand:V4SF 0 "s_register_operand" "=w")
+	(unspec:V4SF [(match_operand:V4HF 1 "s_register_operand" "w")]
+			  UNSPEC_VCVT))]
+  "TARGET_NEON && TARGET_FP16"
+  "vcvt.f32.f16\t%q0, %P1"
+  [(set_attr "type" "neon_fp_cvt_widen_h")]
+)
+
+(define_insn "neon_vcvtv4hfv4sf"
+  [(set (match_operand:V4HF 0 "s_register_operand" "=w")
+	(unspec:V4HF [(match_operand:V4SF 1 "s_register_operand" "w")]
+			  UNSPEC_VCVT))]
+  "TARGET_NEON && TARGET_FP16"
+  "vcvt.f16.f32\t%P0, %q1"
+  [(set_attr "type" "neon_fp_cvt_narrow_s_q")]
 )
 
 (define_insn "neon_vcvt_n<mode>"
@@ -3316,10 +3088,7 @@
   neon_const_bounds (operands[2], 1, 33);
   return "vcvt.%T3%#32.f32\t%<V_reg>0, %<V_reg>1, %2";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_fp_to_int_<V_elem_ch><q>")]
 )
 
 (define_insn "neon_vcvt_n<mode>"
@@ -3333,10 +3102,7 @@
   neon_const_bounds (operands[2], 1, 33);
   return "vcvt.f32.%T3%#32\t%<V_reg>0, %<V_reg>1, %2";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Is_d_reg>")
-                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                   (const_string "neon_fp_vadd_qqq_vabs_qq")))]
+  [(set_attr "type" "neon_int_to_fp_<V_elem_ch><q>")]
 )
 
 (define_insn "neon_vmovn<mode>"
@@ -3346,7 +3112,7 @@
                            UNSPEC_VMOVN))]
   "TARGET_NEON"
   "vmovn.<V_if_elem>\t%P0, %q1"
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_shift_imm_narrow_q")]
 )
 
 (define_insn "neon_vqmovn<mode>"
@@ -3356,7 +3122,7 @@
                            UNSPEC_VQMOVN))]
   "TARGET_NEON"
   "vqmovn.%T2%#<V_sz_elem>\t%P0, %q1"
-  [(set_attr "neon_type" "neon_shift_2")]
+  [(set_attr "type" "neon_sat_shift_imm_narrow_q")]
 )
 
 (define_insn "neon_vqmovun<mode>"
@@ -3366,7 +3132,7 @@
                            UNSPEC_VQMOVUN))]
   "TARGET_NEON"
   "vqmovun.<V_s_elem>\t%P0, %q1"
-  [(set_attr "neon_type" "neon_shift_2")]
+  [(set_attr "type" "neon_sat_shift_imm_narrow_q")]
 )
 
 (define_insn "neon_vmovl<mode>"
@@ -3376,7 +3142,7 @@
                           UNSPEC_VMOVL))]
   "TARGET_NEON"
   "vmovl.%T2%#<V_sz_elem>\t%q0, %P1"
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_shift_imm_long")]
 )
 
 (define_insn "neon_vmul_lane<mode>"
@@ -3392,12 +3158,10 @@
   neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmul.<V_if_elem>\t%P0, %P1, %P2[%c3]";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
      (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_vmul_ddd")
-                   (if_then_else (match_test "<Scalar_mul_8_16>")
-                                 (const_string "neon_mul_ddd_16_scalar_32_16_long_scalar")
-                                 (const_string "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar"))))]
+                   (const_string "neon_fp_mul_s_scalar<q>")
+                   (const_string "neon_mul_<V_elem_ch>_scalar<q>")))]
 )
 
 (define_insn "neon_vmul_lane<mode>"
@@ -3413,12 +3177,10 @@
   neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<V_HALF>mode));
   return "vmul.<V_if_elem>\t%q0, %q1, %P2[%c3]";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
      (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_vmul_qqd")
-                   (if_then_else (match_test "<Scalar_mul_8_16>")
-                                 (const_string "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")
-                                 (const_string "neon_mul_qqd_32_scalar"))))]
+                   (const_string "neon_fp_mul_s_scalar<q>")
+                   (const_string "neon_mul_<V_elem_ch>_scalar<q>")))]
 )
 
 (define_insn "neon_vmull_lane<mode>"
@@ -3434,10 +3196,7 @@
   neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmull.%T4%#<V_sz_elem>\t%q0, %P1, %P2[%c3]";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mul_ddd_16_scalar_32_16_long_scalar")
-                   (const_string "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")))]
+  [(set_attr "type" "neon_mul_<V_elem_ch>_scalar_long")]
 )
 
 (define_insn "neon_vqdmull_lane<mode>"
@@ -3453,10 +3212,7 @@
   neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vqdmull.<V_s_elem>\t%q0, %P1, %P2[%c3]";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mul_ddd_16_scalar_32_16_long_scalar")
-                   (const_string "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")))]
+  [(set_attr "type" "neon_sat_mul_<V_elem_ch>_scalar_long")]
 )
 
 (define_insn "neon_vqdmulh_lane<mode>"
@@ -3472,10 +3228,7 @@
   neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vq%O4dmulh.%T4%#<V_sz_elem>\t%q0, %q1, %P2[%c3]";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")
-                   (const_string "neon_mul_qqd_32_scalar")))]
+  [(set_attr "type" "neon_sat_mul_<V_elem_ch>_scalar_q")]
 )
 
 (define_insn "neon_vqdmulh_lane<mode>"
@@ -3491,10 +3244,7 @@
   neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vq%O4dmulh.%T4%#<V_sz_elem>\t%P0, %P1, %P2[%c3]";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mul_ddd_16_scalar_32_16_long_scalar")
-                   (const_string "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar")))]
+  [(set_attr "type" "neon_sat_mul_<V_elem_ch>_scalar_q")]
 )
 
 (define_insn "neon_vmla_lane<mode>"
@@ -3511,12 +3261,10 @@
   neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmla.<V_if_elem>\t%P0, %P2, %P3[%c4]";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
      (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_vmla_ddd_scalar")
-                   (if_then_else (match_test "<Scalar_mul_8_16>")
-                                 (const_string "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar")
-                                 (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))))]
+                   (const_string "neon_fp_mla_s_scalar<q>")
+                   (const_string "neon_mla_<V_elem_ch>_scalar<q>")))]
 )
 
 (define_insn "neon_vmla_lane<mode>"
@@ -3533,12 +3281,10 @@
   neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmla.<V_if_elem>\t%q0, %q2, %P3[%c4]";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
      (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_vmla_qqq_scalar")
-                   (if_then_else (match_test "<Scalar_mul_8_16>")
-                                 (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")
-                                 (const_string "neon_mla_qqq_32_qqd_32_scalar"))))]
+                   (const_string "neon_fp_mla_s_scalar<q>")
+                   (const_string "neon_mla_<V_elem_ch>_scalar<q>")))]
 )
 
 (define_insn "neon_vmlal_lane<mode>"
@@ -3555,10 +3301,7 @@
   neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmlal.%T5%#<V_sz_elem>\t%q0, %P2, %P3[%c4]";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar")
-                   (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")))]
+  [(set_attr "type" "neon_mla_<V_elem_ch>_scalar_long")]
 )
 
 (define_insn "neon_vqdmlal_lane<mode>"
@@ -3575,10 +3318,7 @@
   neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vqdmlal.<V_s_elem>\t%q0, %P2, %P3[%c4]";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar")
-                   (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")))]
+  [(set_attr "type" "neon_sat_mla_<V_elem_ch>_scalar_long")]
 )
 
 (define_insn "neon_vmls_lane<mode>"
@@ -3595,12 +3335,10 @@
   neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmls.<V_if_elem>\t%P0, %P2, %P3[%c4]";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
      (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_vmla_ddd_scalar")
-                   (if_then_else (match_test "<Scalar_mul_8_16>")
-                                 (const_string "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar")
-                                 (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))))]
+                   (const_string "neon_fp_mla_s_scalar<q>")
+                   (const_string "neon_mla_<V_elem_ch>_scalar<q>")))]
 )
 
 (define_insn "neon_vmls_lane<mode>"
@@ -3617,12 +3355,10 @@
   neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmls.<V_if_elem>\t%q0, %q2, %P3[%c4]";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
      (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_vmla_qqq_scalar")
-                   (if_then_else (match_test "<Scalar_mul_8_16>")
-                                 (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")
-                                 (const_string "neon_mla_qqq_32_qqd_32_scalar"))))]
+                   (const_string "neon_fp_mla_s_scalar<q>")
+                   (const_string "neon_mla_<V_elem_ch>_scalar<q>")))]
 )
 
 (define_insn "neon_vmlsl_lane<mode>"
@@ -3639,10 +3375,7 @@
   neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmlsl.%T5%#<V_sz_elem>\t%q0, %P2, %P3[%c4]";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar")
-                   (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")))]
+  [(set_attr "type" "neon_mla_<V_elem_ch>_scalar_long")]
 )
 
 (define_insn "neon_vqdmlsl_lane<mode>"
@@ -3659,10 +3392,7 @@
   neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vqdmlsl.<V_s_elem>\t%q0, %P2, %P3[%c4]";
 }
-  [(set (attr "neon_type")
-     (if_then_else (match_test "<Scalar_mul_8_16>")
-                   (const_string "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar")
-                   (const_string "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long")))]
+  [(set_attr "type" "neon_sat_mla_<V_elem_ch>_scalar_long")]
 )
 
 ; FIXME: For the "_n" multiply/multiply-accumulate insns, we copy a value in a
@@ -3887,10 +3617,7 @@
   neon_const_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vext.<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2, %3";
 }
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_bp_simple")
-                    (const_string "neon_bp_2cycle")))]
+  [(set_attr "type" "neon_ext<q>")]
 )
 
 (define_insn "neon_vrev64<mode>"
@@ -3900,7 +3627,7 @@
                     UNSPEC_VREV64))]
   "TARGET_NEON"
   "vrev64.<V_sz_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_rev<q>")]
 )
 
 (define_insn "neon_vrev32<mode>"
@@ -3910,7 +3637,7 @@
                    UNSPEC_VREV32))]
   "TARGET_NEON"
   "vrev32.<V_sz_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_rev<q>")]
 )
 
 (define_insn "neon_vrev16<mode>"
@@ -3920,7 +3647,7 @@
                    UNSPEC_VREV16))]
   "TARGET_NEON"
   "vrev16.<V_sz_elem>\t%<V_reg>0, %<V_reg>1"
-  [(set_attr "neon_type" "neon_bp_simple")]
+  [(set_attr "type" "neon_rev<q>")]
 )
 
 ; vbsl_* intrinsics may compile to any of vbsl/vbif/vbit depending on register
@@ -3942,7 +3669,7 @@
   vbsl\t%<V_reg>0, %<V_reg>2, %<V_reg>3
   vbit\t%<V_reg>0, %<V_reg>2, %<V_reg>1
   vbif\t%<V_reg>0, %<V_reg>3, %<V_reg>1"
-  [(set_attr "neon_type" "neon_int_1")]
+  [(set_attr "type" "neon_bsl<q>")]
 )
 
 (define_expand "neon_vbsl<mode>"
@@ -3965,10 +3692,7 @@
                       UNSPEC_VSHL))]
   "TARGET_NEON"
   "v%O3shl.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_vshl_ddd")
-                    (const_string "neon_shift_3")))]
+  [(set_attr "type" "neon_shift_imm<q>")]
 )
 
 (define_insn "neon_vqshl<mode>"
@@ -3979,10 +3703,7 @@
                       UNSPEC_VQSHL))]
   "TARGET_NEON"
   "vq%O3shl.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_shift_2")
-                    (const_string "neon_vqshl_vrshl_vqrshl_qqq")))]
+  [(set_attr "type" "neon_sat_shift_imm<q>")]
 )
 
 (define_insn "neon_vshr_n<mode>"
@@ -3996,7 +3717,7 @@
   neon_const_bounds (operands[2], 1, neon_element_bits (<MODE>mode) + 1);
   return "v%O3shr.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %2";
 }
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_shift_imm<q>")]
 )
 
 (define_insn "neon_vshrn_n<mode>"
@@ -4010,7 +3731,7 @@
   neon_const_bounds (operands[2], 1, neon_element_bits (<MODE>mode) / 2 + 1);
   return "v%O3shrn.<V_if_elem>\t%P0, %q1, %2";
 }
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_shift_imm_narrow_q")]
 )
 
 (define_insn "neon_vqshrn_n<mode>"
@@ -4024,7 +3745,7 @@
   neon_const_bounds (operands[2], 1, neon_element_bits (<MODE>mode) / 2 + 1);
   return "vq%O3shrn.%T3%#<V_sz_elem>\t%P0, %q1, %2";
 }
-  [(set_attr "neon_type" "neon_shift_2")]
+  [(set_attr "type" "neon_sat_shift_imm_narrow_q")]
 )
 
 (define_insn "neon_vqshrun_n<mode>"
@@ -4038,7 +3759,7 @@
   neon_const_bounds (operands[2], 1, neon_element_bits (<MODE>mode) / 2 + 1);
   return "vq%O3shrun.%T3%#<V_sz_elem>\t%P0, %q1, %2";
 }
-  [(set_attr "neon_type" "neon_shift_2")]
+  [(set_attr "type" "neon_sat_shift_imm_narrow_q")]
 )
 
 (define_insn "neon_vshl_n<mode>"
@@ -4052,7 +3773,7 @@
   neon_const_bounds (operands[2], 0, neon_element_bits (<MODE>mode));
   return "vshl.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %2";
 }
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_shift_imm<q>")]
 )
 
 (define_insn "neon_vqshl_n<mode>"
@@ -4066,7 +3787,7 @@
   neon_const_bounds (operands[2], 0, neon_element_bits (<MODE>mode));
   return "vqshl.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %2";
 }
-  [(set_attr "neon_type" "neon_shift_2")]
+  [(set_attr "type" "neon_sat_shift_imm<q>")]
 )
 
 (define_insn "neon_vqshlu_n<mode>"
@@ -4080,7 +3801,7 @@
   neon_const_bounds (operands[2], 0, neon_element_bits (<MODE>mode));
   return "vqshlu.%T3%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %2";
 }
-  [(set_attr "neon_type" "neon_shift_2")]
+  [(set_attr "type" "neon_sat_shift_imm<q>")]
 )
 
 (define_insn "neon_vshll_n<mode>"
@@ -4095,7 +3816,7 @@
   neon_const_bounds (operands[2], 0, neon_element_bits (<MODE>mode) + 1);
   return "vshll.%T3%#<V_sz_elem>\t%q0, %P1, %2";
 }
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_shift_imm_long")]
 )
 
 (define_insn "neon_vsra_n<mode>"
@@ -4110,7 +3831,7 @@
   neon_const_bounds (operands[3], 1, neon_element_bits (<MODE>mode) + 1);
   return "v%O4sra.%T4%#<V_sz_elem>\t%<V_reg>0, %<V_reg>2, %3";
 }
-  [(set_attr "neon_type" "neon_vsra_vrsra")]
+  [(set_attr "type" "neon_shift_acc<q>")]
 )
 
 (define_insn "neon_vsri_n<mode>"
@@ -4124,10 +3845,7 @@
   neon_const_bounds (operands[3], 1, neon_element_bits (<MODE>mode) + 1);
   return "vsri.<V_sz_elem>\t%<V_reg>0, %<V_reg>2, %3";
 }
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_shift_1")
-                    (const_string "neon_shift_3")))]
+  [(set_attr "type" "neon_shift_reg<q>")]
 )
 
 (define_insn "neon_vsli_n<mode>"
@@ -4141,10 +3859,7 @@
   neon_const_bounds (operands[3], 0, neon_element_bits (<MODE>mode));
   return "vsli.<V_sz_elem>\t%<V_reg>0, %<V_reg>2, %3";
 }
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_shift_1")
-                    (const_string "neon_shift_3")))]
+  [(set_attr "type" "neon_shift_reg<q>")]
 )
 
 (define_insn "neon_vtbl1v8qi"
@@ -4154,7 +3869,7 @@
                      UNSPEC_VTBL))]
   "TARGET_NEON"
   "vtbl.8\t%P0, {%P1}, %P2"
-  [(set_attr "neon_type" "neon_bp_2cycle")]
+  [(set_attr "type" "neon_tbl1")]
 )
 
 (define_insn "neon_vtbl2v8qi"
@@ -4175,7 +3890,7 @@
 
   return "";
 }
-  [(set_attr "neon_type" "neon_bp_2cycle")]
+  [(set_attr "type" "neon_tbl2")]
 )
 
 (define_insn "neon_vtbl3v8qi"
@@ -4197,7 +3912,7 @@
 
   return "";
 }
-  [(set_attr "neon_type" "neon_bp_3cycle")]
+  [(set_attr "type" "neon_tbl3")]
 )
 
 (define_insn "neon_vtbl4v8qi"
@@ -4220,7 +3935,7 @@
 
   return "";
 }
-  [(set_attr "neon_type" "neon_bp_3cycle")]
+  [(set_attr "type" "neon_tbl4")]
 )
 
 ;; These three are used by the vec_perm infrastructure for V16QImode.
@@ -4251,7 +3966,9 @@
   part2 = simplify_subreg (V8QImode, op2, V16QImode, ofs);
   emit_insn (gen_neon_vtbl2v8qi (part0, op1, part2));
   DONE;
-})
+}
+  [(set_attr "type" "multiple")]
+)
 
 (define_insn_and_split "neon_vtbl2v16qi"
   [(set (match_operand:V16QI 0 "s_register_operand" "=&w")
@@ -4280,7 +3997,9 @@
   part2 = simplify_subreg (V8QImode, op2, V16QImode, ofs);
   emit_insn (gen_neon_vtbl2v8qi (part0, op1, part2));
   DONE;
-})
+}
+  [(set_attr "type" "multiple")]
+)
 
 ;; ??? Logically we should extend the regular neon_vcombine pattern to
 ;; handle quad-word input modes, producing octa-word output modes.  But
@@ -4298,7 +4017,9 @@
 {
   neon_split_vcombine (operands);
   DONE;
-})
+}
+[(set_attr "type" "multiple")]
+)
 
 (define_insn "neon_vtbx1v8qi"
   [(set (match_operand:V8QI 0 "s_register_operand" "=w")
@@ -4308,7 +4029,7 @@
                      UNSPEC_VTBX))]
   "TARGET_NEON"
   "vtbx.8\t%P0, {%P2}, %P3"
-  [(set_attr "neon_type" "neon_bp_2cycle")]
+  [(set_attr "type" "neon_tbl1")]
 )
 
 (define_insn "neon_vtbx2v8qi"
@@ -4330,7 +4051,7 @@
 
   return "";
 }
-  [(set_attr "neon_type" "neon_bp_2cycle")]
+  [(set_attr "type" "neon_tbl2")]
 )
 
 (define_insn "neon_vtbx3v8qi"
@@ -4353,7 +4074,7 @@
 
   return "";
 }
-  [(set_attr "neon_type" "neon_bp_3cycle")]
+  [(set_attr "type" "neon_tbl3")]
 )
 
 (define_insn "neon_vtbx4v8qi"
@@ -4377,7 +4098,7 @@
 
   return "";
 }
-  [(set_attr "neon_type" "neon_bp_3cycle")]
+  [(set_attr "type" "neon_tbl4")]
 )
 
 (define_expand "neon_vtrn<mode>_internal"
@@ -4403,10 +4124,7 @@
                      UNSPEC_VTRN2))]
   "TARGET_NEON"
   "vtrn.<V_sz_elem>\t%<V_reg>0, %<V_reg>2"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_bp_simple")
-                    (const_string "neon_bp_3cycle")))]
+  [(set_attr "type" "neon_permute<q>")]
 )
 
 (define_expand "neon_vtrn<mode>"
@@ -4443,10 +4161,7 @@
                      UNSPEC_VZIP2))]
   "TARGET_NEON"
   "vzip.<V_sz_elem>\t%<V_reg>0, %<V_reg>2"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_bp_simple")
-                    (const_string "neon_bp_3cycle")))]
+  [(set_attr "type" "neon_zip<q>")]
 )
 
 (define_expand "neon_vzip<mode>"
@@ -4483,10 +4198,7 @@
                      UNSPEC_VUZP2))]
   "TARGET_NEON"
   "vuzp.<V_sz_elem>\t%<V_reg>0, %<V_reg>2"
-  [(set (attr "neon_type")
-      (if_then_else (match_test "<Is_d_reg>")
-                    (const_string "neon_bp_simple")
-                    (const_string "neon_bp_3cycle")))]
+  [(set_attr "type" "neon_zip<q>")]
 )
 
 (define_expand "neon_vuzp<mode>"
@@ -4545,9 +4257,19 @@
   DONE;
 })
 
+(define_expand "neon_vreinterpretti<mode>"
+  [(match_operand:TI 0 "s_register_operand" "")
+   (match_operand:VQXMOV 1 "s_register_operand" "")]
+  "TARGET_NEON"
+{
+  neon_reinterpret (operands[0], operands[1]);
+  DONE;
+})
+
+
 (define_expand "neon_vreinterpretv16qi<mode>"
   [(match_operand:V16QI 0 "s_register_operand" "")
-   (match_operand:VQX 1 "s_register_operand" "")]
+   (match_operand:VQXMOV 1 "s_register_operand" "")]
   "TARGET_NEON"
 {
   neon_reinterpret (operands[0], operands[1]);
@@ -4556,7 +4278,7 @@
 
 (define_expand "neon_vreinterpretv8hi<mode>"
   [(match_operand:V8HI 0 "s_register_operand" "")
-   (match_operand:VQX 1 "s_register_operand" "")]
+   (match_operand:VQXMOV 1 "s_register_operand" "")]
   "TARGET_NEON"
 {
   neon_reinterpret (operands[0], operands[1]);
@@ -4565,7 +4287,7 @@
 
 (define_expand "neon_vreinterpretv4si<mode>"
   [(match_operand:V4SI 0 "s_register_operand" "")
-   (match_operand:VQX 1 "s_register_operand" "")]
+   (match_operand:VQXMOV 1 "s_register_operand" "")]
   "TARGET_NEON"
 {
   neon_reinterpret (operands[0], operands[1]);
@@ -4574,7 +4296,7 @@
 
 (define_expand "neon_vreinterpretv4sf<mode>"
   [(match_operand:V4SF 0 "s_register_operand" "")
-   (match_operand:VQX 1 "s_register_operand" "")]
+   (match_operand:VQXMOV 1 "s_register_operand" "")]
   "TARGET_NEON"
 {
   neon_reinterpret (operands[0], operands[1]);
@@ -4583,7 +4305,7 @@
 
 (define_expand "neon_vreinterpretv2di<mode>"
   [(match_operand:V2DI 0 "s_register_operand" "")
-   (match_operand:VQX 1 "s_register_operand" "")]
+   (match_operand:VQXMOV 1 "s_register_operand" "")]
   "TARGET_NEON"
 {
   neon_reinterpret (operands[0], operands[1]);
@@ -4602,7 +4324,7 @@
                     UNSPEC_VLD1))]
   "TARGET_NEON"
   "vld1.<V_sz_elem>\t%h0, %A1"
-  [(set_attr "neon_type" "neon_vld1_1_2_regs")]
+  [(set_attr "type" "neon_load1_1reg<q>")]
 )
 
 (define_insn "neon_vld1_lane<mode>"
@@ -4622,10 +4344,7 @@
   else
     return "vld1.<V_sz_elem>\t{%P0[%c3]}, %A1";
 }
-  [(set (attr "neon_type")
-      (if_then_else (eq (const_string "<V_mode_nunits>") (const_int 2))
-                    (const_string "neon_vld1_1_2_regs")
-                    (const_string "neon_vld1_vld2_lane")))]
+  [(set_attr "type" "neon_load1_one_lane<q>")]
 )
 
 (define_insn "neon_vld1_lane<mode>"
@@ -4653,26 +4372,24 @@
   else
     return "vld1.<V_sz_elem>\t{%P0[%c3]}, %A1";
 }
-  [(set (attr "neon_type")
-      (if_then_else (eq (const_string "<V_mode_nunits>") (const_int 2))
-                    (const_string "neon_vld1_1_2_regs")
-                    (const_string "neon_vld1_vld2_lane")))]
+  [(set_attr "type" "neon_load1_one_lane<q>")]
 )
 
 (define_insn "neon_vld1_dup<mode>"
-  [(set (match_operand:VDX 0 "s_register_operand" "=w")
-        (vec_duplicate:VDX (match_operand:<V_elem> 1 "neon_struct_operand" "Um")))]
+  [(set (match_operand:VD 0 "s_register_operand" "=w")
+        (vec_duplicate:VD (match_operand:<V_elem> 1 "neon_struct_operand" "Um")))]
   "TARGET_NEON"
-{
-  if (GET_MODE_NUNITS (<MODE>mode) > 1)
-    return "vld1.<V_sz_elem>\t{%P0[]}, %A1";
-  else
-    return "vld1.<V_sz_elem>\t%h0, %A1";
-}
-  [(set (attr "neon_type")
-      (if_then_else (gt (const_string "<V_mode_nunits>") (const_string "1"))
-                    (const_string "neon_vld2_2_regs_vld1_vld2_all_lanes")
-                    (const_string "neon_vld1_1_2_regs")))]
+  "vld1.<V_sz_elem>\t{%P0[]}, %A1"
+  [(set_attr "type" "neon_load1_all_lanes<q>")]
+)
+
+;; Special case for DImode.  Treat it exactly like a simple load.
+(define_expand "neon_vld1_dupdi"
+  [(set (match_operand:DI 0 "s_register_operand" "")
+        (unspec:DI [(match_operand:DI 1 "neon_struct_operand" "")]
+		   UNSPEC_VLD1))]
+  "TARGET_NEON"
+  ""
 )
 
 (define_insn "neon_vld1_dup<mode>"
@@ -4682,7 +4399,7 @@
 {
   return "vld1.<V_sz_elem>\t{%e0[], %f0[]}, %A1";
 }
-  [(set_attr "neon_type" "neon_vld2_2_regs_vld1_vld2_all_lanes")]
+  [(set_attr "type" "neon_load1_all_lanes<q>")]
 )
 
 (define_insn_and_split "neon_vld1_dupv2di"
@@ -4699,7 +4416,7 @@
     DONE;
     }
   [(set_attr "length" "8")
-   (set_attr "neon_type" "neon_vld2_2_regs_vld1_vld2_all_lanes")]
+   (set_attr "type" "neon_load1_all_lanes_q")]
 )
 
 (define_expand "vec_store_lanes<mode><mode>"
@@ -4714,7 +4431,7 @@
 		     UNSPEC_VST1))]
   "TARGET_NEON"
   "vst1.<V_sz_elem>\t%h1, %A0"
-  [(set_attr "neon_type" "neon_vst1_1_2_regs_vst2_2_regs")])
+  [(set_attr "type" "neon_store1_1reg<q>")])
 
 (define_insn "neon_vst1_lane<mode>"
   [(set (match_operand:<V_elem> 0 "neon_struct_operand" "=Um")
@@ -4733,10 +4450,8 @@
   else
     return "vst1.<V_sz_elem>\t{%P1[%c2]}, %A0";
 }
-  [(set (attr "neon_type")
-      (if_then_else (eq (const_string "<V_mode_nunits>") (const_int 1))
-                    (const_string "neon_vst1_1_2_regs_vst2_2_regs")
-                    (const_string "neon_vst1_vst2_lane")))])
+  [(set_attr "type" "neon_store1_one_lane<q>")]
+)
 
 (define_insn "neon_vst1_lane<mode>"
   [(set (match_operand:<V_elem> 0 "neon_struct_operand" "=Um")
@@ -4763,7 +4478,7 @@
   else
     return "vst1.<V_sz_elem>\t{%P1[%c2]}, %A0";
 }
-  [(set_attr "neon_type" "neon_vst1_vst2_lane")]
+  [(set_attr "type" "neon_store1_one_lane<q>")]
 )
 
 (define_expand "vec_load_lanesti<mode>"
@@ -4785,10 +4500,10 @@
   else
     return "vld2.<V_sz_elem>\t%h0, %A1";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (eq (const_string "<V_sz_elem>") (const_string "64"))
-                    (const_string "neon_vld1_1_2_regs")
-                    (const_string "neon_vld2_2_regs_vld1_vld2_all_lanes")))]
+                    (const_string "neon_load1_2reg<q>")
+                    (const_string "neon_load2_2reg<q>")))]
 )
 
 (define_expand "vec_load_lanesoi<mode>"
@@ -4805,7 +4520,7 @@
                    UNSPEC_VLD2))]
   "TARGET_NEON"
   "vld2.<V_sz_elem>\t%h0, %A1"
-  [(set_attr "neon_type" "neon_vld2_2_regs_vld1_vld2_all_lanes")])
+  [(set_attr "type" "neon_load2_2reg_q")])
 
 (define_insn "neon_vld2_lane<mode>"
   [(set (match_operand:TI 0 "s_register_operand" "=w")
@@ -4829,7 +4544,7 @@
   output_asm_insn ("vld2.<V_sz_elem>\t{%P0[%c3], %P1[%c3]}, %A2", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld1_vld2_lane")]
+  [(set_attr "type" "neon_load2_one_lane<q>")]
 )
 
 (define_insn "neon_vld2_lane<mode>"
@@ -4859,7 +4574,7 @@
   output_asm_insn ("vld2.<V_sz_elem>\t{%P0[%c3], %P1[%c3]}, %A2", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld1_vld2_lane")]
+  [(set_attr "type" "neon_load2_one_lane<q>")]
 )
 
 (define_insn "neon_vld2_dup<mode>"
@@ -4874,10 +4589,10 @@
   else
     return "vld1.<V_sz_elem>\t%h0, %A1";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (gt (const_string "<V_mode_nunits>") (const_string "1"))
-                    (const_string "neon_vld2_2_regs_vld1_vld2_all_lanes")
-                    (const_string "neon_vld1_1_2_regs")))]
+                    (const_string "neon_load2_all_lanes<q>")
+                    (const_string "neon_load1_1reg<q>")))]
 )
 
 (define_expand "vec_store_lanesti<mode>"
@@ -4899,10 +4614,10 @@
   else
     return "vst2.<V_sz_elem>\t%h1, %A0";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (eq (const_string "<V_sz_elem>") (const_string "64"))
-                    (const_string "neon_vst1_1_2_regs_vst2_2_regs")
-                    (const_string "neon_vst1_1_2_regs_vst2_2_regs")))]
+                    (const_string "neon_store1_2reg<q>")
+                    (const_string "neon_store2_one_lane<q>")))]
 )
 
 (define_expand "vec_store_lanesoi<mode>"
@@ -4919,7 +4634,7 @@
 		   UNSPEC_VST2))]
   "TARGET_NEON"
   "vst2.<V_sz_elem>\t%h1, %A0"
-  [(set_attr "neon_type" "neon_vst1_1_2_regs_vst2_2_regs")]
+  [(set_attr "type" "neon_store2_4reg<q>")]
 )
 
 (define_insn "neon_vst2_lane<mode>"
@@ -4944,7 +4659,7 @@
   output_asm_insn ("vst2.<V_sz_elem>\t{%P1[%c3], %P2[%c3]}, %A0", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vst1_vst2_lane")]
+  [(set_attr "type" "neon_store2_one_lane<q>")]
 )
 
 (define_insn "neon_vst2_lane<mode>"
@@ -4974,7 +4689,7 @@
   output_asm_insn ("vst2.<V_sz_elem>\t{%P1[%c3], %P2[%c3]}, %A0", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vst1_vst2_lane")]
+  [(set_attr "type" "neon_store2_one_lane<q>")]
 )
 
 (define_expand "vec_load_lanesei<mode>"
@@ -4996,10 +4711,10 @@
   else
     return "vld3.<V_sz_elem>\t%h0, %A1";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (eq (const_string "<V_sz_elem>") (const_string "64"))
-                    (const_string "neon_vld1_1_2_regs")
-                    (const_string "neon_vld3_vld4")))]
+                    (const_string "neon_load1_3reg<q>")
+                    (const_string "neon_load3_3reg<q>")))]
 )
 
 (define_expand "vec_load_lanesci<mode>"
@@ -5043,7 +4758,7 @@
   output_asm_insn ("vld3.<V_sz_elem>\t{%P0, %P1, %P2}, %A3", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld3_vld4")]
+  [(set_attr "type" "neon_load3_3reg<q>")]
 )
 
 (define_insn "neon_vld3qb<mode>"
@@ -5063,7 +4778,7 @@
   output_asm_insn ("vld3.<V_sz_elem>\t{%P0, %P1, %P2}, %A3", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld3_vld4")]
+  [(set_attr "type" "neon_load3_3reg<q>")]
 )
 
 (define_insn "neon_vld3_lane<mode>"
@@ -5090,7 +4805,7 @@
                    ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld3_vld4_lane")]
+  [(set_attr "type" "neon_load3_one_lane<q>")]
 )
 
 (define_insn "neon_vld3_lane<mode>"
@@ -5122,7 +4837,7 @@
                    ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld3_vld4_lane")]
+  [(set_attr "type" "neon_load3_one_lane<q>")]
 )
 
 (define_insn "neon_vld3_dup<mode>"
@@ -5146,10 +4861,10 @@
   else
     return "vld1.<V_sz_elem>\t%h0, %A1";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (gt (const_string "<V_mode_nunits>") (const_string "1"))
-                    (const_string "neon_vld3_vld4_all_lanes")
-                    (const_string "neon_vld1_1_2_regs")))])
+                    (const_string "neon_load3_all_lanes<q>")
+                    (const_string "neon_load1_1reg<q>")))])
 
 (define_expand "vec_store_lanesei<mode>"
   [(set (match_operand:EI 0 "neon_struct_operand")
@@ -5170,10 +4885,10 @@
   else
     return "vst3.<V_sz_elem>\t%h1, %A0";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (eq (const_string "<V_sz_elem>") (const_string "64"))
-                    (const_string "neon_vst1_1_2_regs_vst2_2_regs")
-                    (const_string "neon_vst2_4_regs_vst3_vst4")))])
+                    (const_string "neon_store1_3reg<q>")
+                    (const_string "neon_store3_one_lane<q>")))])
 
 (define_expand "vec_store_lanesci<mode>"
   [(match_operand:CI 0 "neon_struct_operand")
@@ -5216,7 +4931,7 @@
   output_asm_insn ("vst3.<V_sz_elem>\t{%P1, %P2, %P3}, %A0", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vst2_4_regs_vst3_vst4")]
+  [(set_attr "type" "neon_store3_3reg<q>")]
 )
 
 (define_insn "neon_vst3qb<mode>"
@@ -5235,7 +4950,7 @@
   output_asm_insn ("vst3.<V_sz_elem>\t{%P1, %P2, %P3}, %A0", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vst2_4_regs_vst3_vst4")]
+  [(set_attr "type" "neon_store3_3reg<q>")]
 )
 
 (define_insn "neon_vst3_lane<mode>"
@@ -5262,7 +4977,7 @@
                    ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vst3_vst4_lane")]
+  [(set_attr "type" "neon_store3_one_lane<q>")]
 )
 
 (define_insn "neon_vst3_lane<mode>"
@@ -5294,7 +5009,8 @@
                    ops);
   return "";
 }
-[(set_attr "neon_type" "neon_vst3_vst4_lane")])
+  [(set_attr "type" "neon_store3_one_lane<q>")]
+)
 
 (define_expand "vec_load_lanesoi<mode>"
   [(set (match_operand:OI 0 "s_register_operand")
@@ -5315,10 +5031,10 @@
   else
     return "vld4.<V_sz_elem>\t%h0, %A1";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (eq (const_string "<V_sz_elem>") (const_string "64"))
-                    (const_string "neon_vld1_1_2_regs")
-                    (const_string "neon_vld3_vld4")))]
+                    (const_string "neon_load1_4reg<q>")
+                    (const_string "neon_load4_4reg<q>")))]
 )
 
 (define_expand "vec_load_lanesxi<mode>"
@@ -5363,7 +5079,7 @@
   output_asm_insn ("vld4.<V_sz_elem>\t{%P0, %P1, %P2, %P3}, %A4", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld3_vld4")]
+  [(set_attr "type" "neon_load4_4reg<q>")]
 )
 
 (define_insn "neon_vld4qb<mode>"
@@ -5384,7 +5100,7 @@
   output_asm_insn ("vld4.<V_sz_elem>\t{%P0, %P1, %P2, %P3}, %A4", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld3_vld4")]
+  [(set_attr "type" "neon_load4_4reg<q>")]
 )
 
 (define_insn "neon_vld4_lane<mode>"
@@ -5412,7 +5128,7 @@
                    ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld3_vld4_lane")]
+  [(set_attr "type" "neon_load4_one_lane<q>")]
 )
 
 (define_insn "neon_vld4_lane<mode>"
@@ -5445,7 +5161,7 @@
                    ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vld3_vld4_lane")]
+  [(set_attr "type" "neon_load4_one_lane<q>")]
 )
 
 (define_insn "neon_vld4_dup<mode>"
@@ -5471,10 +5187,10 @@
   else
     return "vld1.<V_sz_elem>\t%h0, %A1";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (gt (const_string "<V_mode_nunits>") (const_string "1"))
-                    (const_string "neon_vld3_vld4_all_lanes")
-                    (const_string "neon_vld1_1_2_regs")))]
+                    (const_string "neon_load4_all_lanes<q>")
+                    (const_string "neon_load1_1reg<q>")))]
 )
 
 (define_expand "vec_store_lanesoi<mode>"
@@ -5496,10 +5212,10 @@
   else
     return "vst4.<V_sz_elem>\t%h1, %A0";
 }
-  [(set (attr "neon_type")
+  [(set (attr "type")
       (if_then_else (eq (const_string "<V_sz_elem>") (const_string "64"))
-                    (const_string "neon_vst1_1_2_regs_vst2_2_regs")
-                    (const_string "neon_vst2_4_regs_vst3_vst4")))]
+                    (const_string "neon_store1_4reg<q>")
+                    (const_string "neon_store4_4reg<q>")))]
 )
 
 (define_expand "vec_store_lanesxi<mode>"
@@ -5544,7 +5260,7 @@
   output_asm_insn ("vst4.<V_sz_elem>\t{%P1, %P2, %P3, %P4}, %A0", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vst2_4_regs_vst3_vst4")]
+  [(set_attr "type" "neon_store4_4reg<q>")]
 )
 
 (define_insn "neon_vst4qb<mode>"
@@ -5564,7 +5280,7 @@
   output_asm_insn ("vst4.<V_sz_elem>\t{%P1, %P2, %P3, %P4}, %A0", ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vst2_4_regs_vst3_vst4")]
+  [(set_attr "type" "neon_store4_4reg<q>")]
 )
 
 (define_insn "neon_vst4_lane<mode>"
@@ -5592,7 +5308,7 @@
                    ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vst3_vst4_lane")]
+  [(set_attr "type" "neon_store4_one_lane<q>")]
 )
 
 (define_insn "neon_vst4_lane<mode>"
@@ -5625,7 +5341,7 @@
                    ops);
   return "";
 }
-  [(set_attr "neon_type" "neon_vst3_vst4_lane")]
+  [(set_attr "type" "neon_store4_4reg<q>")]
 )
 
 (define_expand "neon_vand<mode>"
@@ -5635,7 +5351,7 @@
    (match_operand:SI 3 "immediate_operand" "")]
   "TARGET_NEON"
 {
-  emit_insn (gen_and<mode>3<V_suf64> (operands[0], operands[1], operands[2]));
+  emit_insn (gen_and<mode>3 (operands[0], operands[1], operands[2]));
   DONE;
 })
 
@@ -5646,7 +5362,7 @@
    (match_operand:SI 3 "immediate_operand" "")]
   "TARGET_NEON"
 {
-  emit_insn (gen_ior<mode>3<V_suf64> (operands[0], operands[1], operands[2]));
+  emit_insn (gen_ior<mode>3 (operands[0], operands[1], operands[2]));
   DONE;
 })
 
@@ -5657,7 +5373,7 @@
    (match_operand:SI 3 "immediate_operand" "")]
   "TARGET_NEON"
 {
-  emit_insn (gen_xor<mode>3<V_suf64> (operands[0], operands[1], operands[2]));
+  emit_insn (gen_xor<mode>3 (operands[0], operands[1], operands[2]));
   DONE;
 })
 
@@ -5690,7 +5406,7 @@
 			  (match_operand:VU 2 "vect_par_constant_low" ""))))]
   "TARGET_NEON && !BYTES_BIG_ENDIAN"
   "vmovl.<US><V_sz_elem> %q0, %e1"
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_shift_imm_long")]
 )
 
 (define_insn "neon_vec_unpack<US>_hi_<mode>"
@@ -5700,7 +5416,7 @@
 			  (match_operand:VU 2 "vect_par_constant_high" ""))))]
   "TARGET_NEON && !BYTES_BIG_ENDIAN"
   "vmovl.<US><V_sz_elem> %q0, %f1"
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_shift_imm_long")]
 )
 
 (define_expand "vec_unpack<US>_hi_<mode>"
@@ -5750,7 +5466,7 @@
                            (match_dup 2)))))]
   "TARGET_NEON && !BYTES_BIG_ENDIAN"
   "vmull.<US><V_sz_elem> %q0, %e1, %e3"
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_mul_<V_elem_ch>_long")]
 )
 
 (define_expand "vec_widen_<US>mult_lo_<mode>"
@@ -5784,7 +5500,7 @@
 			    (match_dup 2)))))]
   "TARGET_NEON && !BYTES_BIG_ENDIAN"
   "vmull.<US><V_sz_elem> %q0, %f1, %f3"
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_mul_<V_elem_ch>_long")]
 )
 
 (define_expand "vec_widen_<US>mult_hi_<mode>"
@@ -5817,7 +5533,7 @@
 {
   return "vshll.<US><V_sz_elem> %q0, %P1, %2";
 }
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_shift_imm_long")]
 )
 
 (define_expand "vec_widen_<US>shiftl_lo_<mode>"
@@ -5853,7 +5569,7 @@
        (SE:<V_widen> (match_operand:VDI 1 "register_operand" "w")))]
  "TARGET_NEON"
  "vmovl.<US><V_sz_elem> %q0, %P1"
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_move")]
 )
 
 (define_expand "vec_unpack<US>_lo_<mode>"
@@ -5890,7 +5606,7 @@
 			   (match_operand:VDI 2 "register_operand" "w"))))]
   "TARGET_NEON"
   "vmull.<US><V_sz_elem> %q0, %P1, %P2"
-  [(set_attr "neon_type" "neon_shift_1")]
+  [(set_attr "type" "neon_mul_<V_elem_ch>_long")]
 )
 
 (define_expand "vec_widen_<US>mult_hi_<mode>"
@@ -5964,7 +5680,7 @@
 			(match_operand:VN 2 "register_operand" "w"))))]
  "TARGET_NEON && !BYTES_BIG_ENDIAN"
  "vmovn.i<V_sz_elem>\t%e0, %q1\;vmovn.i<V_sz_elem>\t%f0, %q2"
- [(set_attr "neon_type" "neon_shift_1")
+ [(set_attr "type" "multiple")
   (set_attr "length" "8")]
 )
 
@@ -5974,7 +5690,7 @@
        (truncate:<V_narrow> (match_operand:VN 1 "register_operand" "w")))]
  "TARGET_NEON && !BYTES_BIG_ENDIAN"
  "vmovn.i<V_sz_elem>\t%P0, %q1"
- [(set_attr "neon_type" "neon_shift_1")]
+ [(set_attr "type" "neon_move_narrow_q")]
 )
 
 (define_expand "vec_pack_trunc_<mode>"
@@ -5997,12 +5713,10 @@
                            (match_operand:VDQ 2 "s_register_operand" "w"))))]
  "TARGET_NEON && (!<Is_float_mode> || flag_unsafe_math_optimizations)"
  "vabd.<V_s_elem> %<V_reg>0, %<V_reg>1, %<V_reg>2"
- [(set (attr "neon_type")
+ [(set (attr "type")
        (if_then_else (ne (symbol_ref "<Is_float_mode>") (const_int 0))
-                     (if_then_else (ne (symbol_ref "<Is_d_reg>") (const_int 0))
-                                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                   (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                     (const_string "neon_int_5")))]
+                     (const_string "neon_fp_abd_s<q>")
+                     (const_string "neon_abd<q>")))]
 )
 
 (define_insn "neon_vabd<mode>_3"
@@ -6012,12 +5726,10 @@
                  UNSPEC_VSUB)))]
  "TARGET_NEON && (!<Is_float_mode> || flag_unsafe_math_optimizations)"
  "vabd.<V_if_elem> %<V_reg>0, %<V_reg>1, %<V_reg>2"
- [(set (attr "neon_type")
+ [(set (attr "type")
        (if_then_else (ne (symbol_ref "<Is_float_mode>") (const_int 0))
-                     (if_then_else (ne (symbol_ref "<Is_d_reg>") (const_int 0))
-                                   (const_string "neon_fp_vadd_ddd_vabs_dd")
-                                   (const_string "neon_fp_vadd_qqq_vabs_qq"))
-                     (const_string "neon_int_5")))]
+                     (const_string "neon_fp_abd_s<q>")
+                     (const_string "neon_abd<q>")))]
 )
 
 ;; Copy from core-to-neon regs, then extend, not vice-versa
Index: b/src/gcc/config/arm/ldmstm.md
===================================================================
--- a/src/gcc/config/arm/ldmstm.md
+++ b/src/gcc/config/arm/ldmstm.md
@@ -37,7 +37,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 4"
   "ldm%(ia%)\t%5, {%1, %2, %3, %4}"
   [(set_attr "type" "load4")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb_ldm4_ia"
   [(match_parallel 0 "load_multiple_operation"
@@ -74,7 +75,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 5"
   "ldm%(ia%)\t%5!, {%1, %2, %3, %4}"
   [(set_attr "type" "load4")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb_ldm4_ia_update"
   [(match_parallel 0 "load_multiple_operation"
@@ -108,7 +110,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 4"
   "stm%(ia%)\t%5, {%1, %2, %3, %4}"
   [(set_attr "type" "store4")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*stm4_ia_update"
   [(match_parallel 0 "store_multiple_operation"
@@ -125,7 +128,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 5"
   "stm%(ia%)\t%5!, {%1, %2, %3, %4}"
   [(set_attr "type" "store4")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb_stm4_ia_update"
   [(match_parallel 0 "store_multiple_operation"
@@ -302,7 +306,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 4"
   "ldm%(db%)\t%5, {%1, %2, %3, %4}"
   [(set_attr "type" "load4")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*ldm4_db_update"
   [(match_parallel 0 "load_multiple_operation"
@@ -323,7 +328,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 5"
   "ldm%(db%)\t%5!, {%1, %2, %3, %4}"
   [(set_attr "type" "load4")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*stm4_db"
   [(match_parallel 0 "store_multiple_operation"
@@ -338,7 +344,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 4"
   "stm%(db%)\t%5, {%1, %2, %3, %4}"
   [(set_attr "type" "store4")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*stm4_db_update"
   [(match_parallel 0 "store_multiple_operation"
@@ -355,7 +362,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 5"
   "stm%(db%)\t%5!, {%1, %2, %3, %4}"
   [(set_attr "type" "store4")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_peephole2
   [(set (match_operand:SI 0 "s_register_operand" "")
@@ -477,7 +485,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 3"
   "ldm%(ia%)\t%4, {%1, %2, %3}"
   [(set_attr "type" "load3")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb_ldm3_ia"
   [(match_parallel 0 "load_multiple_operation"
@@ -508,7 +517,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 4"
   "ldm%(ia%)\t%4!, {%1, %2, %3}"
   [(set_attr "type" "load3")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb_ldm3_ia_update"
   [(match_parallel 0 "load_multiple_operation"
@@ -537,7 +547,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 3"
   "stm%(ia%)\t%4, {%1, %2, %3}"
   [(set_attr "type" "store3")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*stm3_ia_update"
   [(match_parallel 0 "store_multiple_operation"
@@ -552,7 +563,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 4"
   "stm%(ia%)\t%4!, {%1, %2, %3}"
   [(set_attr "type" "store3")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb_stm3_ia_update"
   [(match_parallel 0 "store_multiple_operation"
@@ -704,7 +716,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 3"
   "ldm%(db%)\t%4, {%1, %2, %3}"
   [(set_attr "type" "load3")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*ldm3_db_update"
   [(match_parallel 0 "load_multiple_operation"
@@ -722,7 +735,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 4"
   "ldm%(db%)\t%4!, {%1, %2, %3}"
   [(set_attr "type" "load3")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*stm3_db"
   [(match_parallel 0 "store_multiple_operation"
@@ -735,7 +749,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 3"
   "stm%(db%)\t%4, {%1, %2, %3}"
   [(set_attr "type" "store3")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*stm3_db_update"
   [(match_parallel 0 "store_multiple_operation"
@@ -750,7 +765,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 4"
   "stm%(db%)\t%4!, {%1, %2, %3}"
   [(set_attr "type" "store3")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_peephole2
   [(set (match_operand:SI 0 "s_register_operand" "")
@@ -855,7 +871,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 2"
   "ldm%(ia%)\t%3, {%1, %2}"
   [(set_attr "type" "load2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb_ldm2_ia"
   [(match_parallel 0 "load_multiple_operation"
@@ -880,7 +897,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 3"
   "ldm%(ia%)\t%3!, {%1, %2}"
   [(set_attr "type" "load2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb_ldm2_ia_update"
   [(match_parallel 0 "load_multiple_operation"
@@ -904,7 +922,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 2"
   "stm%(ia%)\t%3, {%1, %2}"
   [(set_attr "type" "store2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*stm2_ia_update"
   [(match_parallel 0 "store_multiple_operation"
@@ -917,7 +936,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 3"
   "stm%(ia%)\t%3!, {%1, %2}"
   [(set_attr "type" "store2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb_stm2_ia_update"
   [(match_parallel 0 "store_multiple_operation"
@@ -1044,7 +1064,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 2"
   "ldm%(db%)\t%3, {%1, %2}"
   [(set_attr "type" "load2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*ldm2_db_update"
   [(match_parallel 0 "load_multiple_operation"
@@ -1059,7 +1080,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 3"
   "ldm%(db%)\t%3!, {%1, %2}"
   [(set_attr "type" "load2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*stm2_db"
   [(match_parallel 0 "store_multiple_operation"
@@ -1070,7 +1092,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 2"
   "stm%(db%)\t%3, {%1, %2}"
   [(set_attr "type" "store2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*stm2_db_update"
   [(match_parallel 0 "store_multiple_operation"
@@ -1083,7 +1106,8 @@
   "TARGET_32BIT && XVECLEN (operands[0], 0) == 3"
   "stm%(db%)\t%3!, {%1, %2}"
   [(set_attr "type" "store2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_peephole2
   [(set (match_operand:SI 0 "s_register_operand" "")
Index: b/src/gcc/config/arm/types.md
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/types.md
@@ -0,0 +1,1076 @@
+;; Instruction Classification for ARM for GNU compiler.
+
+;; Copyright (C) 1991-2013 Free Software Foundation, Inc.
+;; Contributed by ARM Ltd.
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published
+;; by the Free Software Foundation; either version 3, or (at your
+;; option) any later version.
+
+;; GCC is distributed in the hope that it will be useful, but WITHOUT
+;; ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+;; or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+;; License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+; TYPE attribute is used to classify instructions for use in scheduling.
+;
+; Instruction classification:
+;
+; adc_imm            add/subtract with carry and with an immediate operand.
+; adc_reg            add/subtract with carry and no immediate operand.
+; adcs_imm           as adc_imm, setting condition flags.
+; adcs_reg           as adc_reg, setting condition flags.
+; adr                calculate address.
+; alu_ext            From ARMv8-A: any arithmetic instruction that has a
+;                    sign/zero-extended.
+;                    AArch64 Only.
+;                    source operand
+; alu_imm            any arithmetic instruction that doesn't have a shifted
+;                    operand and has an immediate operand.  This
+;                    excludes MOV, MVN and RSB(S) immediate.
+; alu_reg            any arithmetic instruction that doesn't have a shifted
+;                    or an immediate operand.  This excludes
+;                    MOV and MVN but includes MOVT.  This is also the default.
+; alu_shift_imm      any arithmetic instruction that has a source operand
+;                    shifted by a constant.  This excludes simple shifts.
+; alu_shift_reg      as alu_shift_imm, with the shift amount specified in a
+;                    register.
+; alus_ext           From ARMv8-A: as alu_ext, setting condition flags.
+;                    AArch64 Only.
+; alus_imm           as alu_imm, setting condition flags.
+; alus_reg           as alu_reg, setting condition flags.
+; alus_shift_imm     as alu_shift_imm, setting condition flags.
+; alus_shift_reg     as alu_shift_reg, setting condition flags.
+; bfm                bitfield move operation.
+; block              blockage insn, this blocks all functional units.
+; branch             branch.
+; call               subroutine call.
+; clz                count leading zeros (CLZ).
+; csel               From ARMv8-A: conditional select.
+; extend             extend instruction (SXTB, SXTH, UXTB, UXTH).
+; f_cvt              conversion between float representations.
+; f_cvtf2i           conversion between float and integral types.
+; f_cvti2f           conversion between integral and float types.
+; f_flag             transfer of co-processor flags to the CPSR.
+; f_load[d,s]        double/single load from memory.  Used for VFP unit.
+; f_mcr              transfer arm to vfp reg.
+; f_mcrr             transfer two arm regs to vfp reg.
+; f_minmax[d,s]      double/single floating point minimum/maximum.
+; f_mrc              transfer vfp to arm reg.
+; f_mrrc             transfer vfp to two arm regs.
+; f_rint[d,s]        double/single floating point rount to integral.
+; f_sel[d,s]         double/single floating byte select.
+; f_store[d,s]       double/single store to memory.  Used for VFP unit.
+; fadd[d,s]          double/single floating-point scalar addition.
+; fcmp[d,s]          double/single floating-point compare.
+; fconst[d,s]        double/single load immediate.
+; fcsel              From ARMv8-A: Floating-point conditional select.
+; fdiv[d,s]          double/single precision floating point division.
+; ffarith[d,s]       double/single floating point abs/neg/cpy.
+; ffma[d,s]          double/single floating point fused multiply-accumulate.
+; float              floating point arithmetic operation.
+; fmac[d,s]          double/single floating point multiply-accumulate.
+; fmov               floating point to floating point register move.
+; fmul[d,s]          double/single floating point multiply.
+; fsqrt[d,s]         double/single precision floating point square root.
+; load_acq           load-acquire.
+; load_byte          load byte(s) from memory to arm registers.
+; load1              load 1 word from memory to arm registers.
+; load2              load 2 words from memory to arm registers.
+; load3              load 3 words from memory to arm registers.
+; load4              load 4 words from memory to arm registers.
+; logic_imm          any logical instruction that doesn't have a shifted
+;                    operand and has an immediate operand.
+; logic_reg          any logical instruction that doesn't have a shifted
+;                    operand or an immediate operand.
+; logic_shift_imm    any logical instruction that has a source operand
+;                    shifted by a constant.  This excludes simple shifts.
+; logic_shift_reg    as logic_shift_imm, with the shift amount specified in a
+;                    register.
+; logics_imm         as logic_imm, setting condition flags.
+; logics_reg         as logic_reg, setting condition flags.
+; logics_shift_imm   as logic_shift_imm, setting condition flags.
+; logics_shift_reg   as logic_shift_reg, setting condition flags.
+; mla                integer multiply accumulate.
+; mlas               integer multiply accumulate, flag setting.
+; mov_imm            simple MOV instruction that moves an immediate to
+;                    register.  This includes MOVW, but not MOVT.
+; mov_reg            simple MOV instruction that moves a register to another
+;                    register.  This includes MOVW, but not MOVT.
+; mov_shift          simple MOV instruction, shifted operand by a constant.
+; mov_shift_reg      simple MOV instruction, shifted operand by a register.
+; mrs                system/special/co-processor register move.
+; mul                integer multiply.
+; muls               integer multiply, flag setting.
+; multiple           more than one instruction, candidate for future
+;                    splitting, or better modeling.
+; mvn_imm            inverting move instruction, immediate.
+; mvn_reg            inverting move instruction, register.
+; mvn_shift          inverting move instruction, shifted operand by a constant.
+; mvn_shift_reg      inverting move instruction, shifted operand by a register.
+; no_insn            an insn which does not represent an instruction in the
+;                    final output, thus having no impact on scheduling.
+; rbit               reverse bits.
+; rev                reverse bytes.
+; sdiv               signed division.
+; shift_imm          simple shift operation (LSL, LSR, ASR, ROR) with an
+;                    immediate.
+; shift_reg          simple shift by a register.
+; smlad              signed multiply accumulate dual.
+; smladx             signed multiply accumulate dual reverse.
+; smlal              signed multiply accumulate long.
+; smlald             signed multiply accumulate long dual.
+; smlals             signed multiply accumulate long, flag setting.
+; smlalxy            signed multiply accumulate, 16x16-bit, 64-bit accumulate.
+; smlawx             signed multiply accumulate, 32x16-bit, 32-bit accumulate.
+; smlawy             signed multiply accumulate wide, 32x16-bit,
+;                    32-bit accumulate.
+; smlaxy             signed multiply accumulate, 16x16-bit, 32-bit accumulate.
+; smlsd              signed multiply subtract dual.
+; smlsdx             signed multiply subtract dual reverse.
+; smlsld             signed multiply subtract long dual.
+; smmla              signed most significant word multiply accumulate.
+; smmul              signed most significant word multiply.
+; smmulr             signed most significant word multiply, rounded.
+; smuad              signed dual multiply add.
+; smuadx             signed dual multiply add reverse.
+; smull              signed multiply long.
+; smulls             signed multiply long, flag setting.
+; smulwy             signed multiply wide, 32x16-bit, 32-bit accumulate.
+; smulxy             signed multiply, 16x16-bit, 32-bit accumulate.
+; smusd              signed dual multiply subtract.
+; smusdx             signed dual multiply subtract reverse.
+; store_rel          store-release.
+; store1             store 1 word to memory from arm registers.
+; store2             store 2 words to memory from arm registers.
+; store3             store 3 words to memory from arm registers.
+; store4             store 4 (or more) words to memory from arm registers.
+; udiv               unsigned division.
+; umaal              unsigned multiply accumulate accumulate long.
+; umlal              unsigned multiply accumulate long.
+; umlals             unsigned multiply accumulate long, flag setting.
+; umull              unsigned multiply long.
+; umulls             unsigned multiply long, flag setting.
+; untyped            insn without type information - default, and error,
+;                    case.
+;
+; The classification below is for instructions used by the Wireless MMX
+; Technology. Each attribute value is used to classify an instruction of the
+; same name or family.
+;
+; wmmx_tandc
+; wmmx_tbcst
+; wmmx_textrc
+; wmmx_textrm
+; wmmx_tinsr
+; wmmx_tmcr
+; wmmx_tmcrr
+; wmmx_tmia
+; wmmx_tmiaph
+; wmmx_tmiaxy
+; wmmx_tmrc
+; wmmx_tmrrc
+; wmmx_tmovmsk
+; wmmx_torc
+; wmmx_torvsc
+; wmmx_wabs
+; wmmx_wdiff
+; wmmx_wacc
+; wmmx_wadd
+; wmmx_waddbhus
+; wmmx_waddsubhx
+; wmmx_waligni
+; wmmx_walignr
+; wmmx_wand
+; wmmx_wandn
+; wmmx_wavg2
+; wmmx_wavg4
+; wmmx_wcmpeq
+; wmmx_wcmpgt
+; wmmx_wmac
+; wmmx_wmadd
+; wmmx_wmax
+; wmmx_wmerge
+; wmmx_wmiawxy
+; wmmx_wmiaxy
+; wmmx_wmin
+; wmmx_wmov
+; wmmx_wmul
+; wmmx_wmulw
+; wmmx_wldr
+; wmmx_wor
+; wmmx_wpack
+; wmmx_wqmiaxy
+; wmmx_wqmulm
+; wmmx_wqmulwm
+; wmmx_wror
+; wmmx_wsad
+; wmmx_wshufh
+; wmmx_wsll
+; wmmx_wsra
+; wmmx_wsrl
+; wmmx_wstr
+; wmmx_wsub
+; wmmx_wsubaddhx
+; wmmx_wunpckeh
+; wmmx_wunpckel
+; wmmx_wunpckih
+; wmmx_wunpckil
+; wmmx_wxor
+;
+; The classification below is for NEON instructions.
+;
+; neon_add
+; neon_add_q
+; neon_add_widen
+; neon_add_long
+; neon_qadd
+; neon_qadd_q
+; neon_add_halve
+; neon_add_halve_q
+; neon_add_halve_narrow_q
+; neon_sub
+; neon_sub_q
+; neon_sub_widen
+; neon_sub_long
+; neon_qsub
+; neon_qsub_q
+; neon_sub_halve
+; neon_sub_halve_q
+; neon_sub_halve_narrow_q
+; neon_abs
+; neon_abs_q
+; neon_neg
+; neon_neg_q
+; neon_qneg
+; neon_qneg_q
+; neon_qabs
+; neon_qabs_q
+; neon_abd
+; neon_abd_q
+; neon_abd_long
+; neon_minmax
+; neon_minmax_q
+; neon_compare
+; neon_compare_q
+; neon_compare_zero
+; neon_compare_zero_q
+; neon_arith_acc
+; neon_arith_acc_q
+; neon_reduc_add
+; neon_reduc_add_q
+; neon_reduc_add_long
+; neon_reduc_add_acc
+; neon_reduc_add_acc_q
+; neon_reduc_minmax
+; neon_reduc_minmax_q
+; neon_logic
+; neon_logic_q
+; neon_tst
+; neon_tst_q
+; neon_shift_imm
+; neon_shift_imm_q
+; neon_shift_imm_narrow_q
+; neon_shift_imm_long
+; neon_shift_reg
+; neon_shift_reg_q
+; neon_shift_acc
+; neon_shift_acc_q
+; neon_sat_shift_imm
+; neon_sat_shift_imm_q
+; neon_sat_shift_imm_narrow_q
+; neon_sat_shift_reg
+; neon_sat_shift_reg_q
+; neon_ins
+; neon_ins_q
+; neon_move
+; neon_move_q
+; neon_move_narrow_q
+; neon_permute
+; neon_permute_q
+; neon_zip
+; neon_zip_q
+; neon_tbl1
+; neon_tbl1_q
+; neon_tbl2
+; neon_tbl2_q
+; neon_tbl3
+; neon_tbl3_q
+; neon_tbl4
+; neon_tbl4_q
+; neon_bsl
+; neon_bsl_q
+; neon_cls
+; neon_cls_q
+; neon_cnt
+; neon_cnt_q
+; neon_ext
+; neon_ext_q
+; neon_rbit
+; neon_rbit_q
+; neon_rev
+; neon_rev_q
+; neon_mul_b
+; neon_mul_b_q
+; neon_mul_h
+; neon_mul_h_q
+; neon_mul_s
+; neon_mul_s_q
+; neon_mul_b_long
+; neon_mul_h_long
+; neon_mul_s_long
+; neon_mul_d_long
+; neon_mul_h_scalar
+; neon_mul_h_scalar_q
+; neon_mul_s_scalar
+; neon_mul_s_scalar_q
+; neon_mul_h_scalar_long
+; neon_mul_s_scalar_long
+; neon_sat_mul_b
+; neon_sat_mul_b_q
+; neon_sat_mul_h
+; neon_sat_mul_h_q
+; neon_sat_mul_s
+; neon_sat_mul_s_q
+; neon_sat_mul_b_long
+; neon_sat_mul_h_long
+; neon_sat_mul_s_long
+; neon_sat_mul_h_scalar
+; neon_sat_mul_h_scalar_q
+; neon_sat_mul_s_scalar
+; neon_sat_mul_s_scalar_q
+; neon_sat_mul_h_scalar_long
+; neon_sat_mul_s_scalar_long
+; neon_mla_b
+; neon_mla_b_q
+; neon_mla_h
+; neon_mla_h_q
+; neon_mla_s
+; neon_mla_s_q
+; neon_mla_b_long
+; neon_mla_h_long
+; neon_mla_s_long
+; neon_mla_h_scalar
+; neon_mla_h_scalar_q
+; neon_mla_s_scalar
+; neon_mla_s_scalar_q
+; neon_mla_h_scalar_long
+; neon_mla_s_scalar_long
+; neon_sat_mla_b_long
+; neon_sat_mla_h_long
+; neon_sat_mla_s_long
+; neon_sat_mla_h_scalar_long
+; neon_sat_mla_s_scalar_long
+; neon_to_gp
+; neon_to_gp_q
+; neon_from_gp
+; neon_from_gp_q
+; neon_ldr
+; neon_load1_1reg
+; neon_load1_1reg_q
+; neon_load1_2reg
+; neon_load1_2reg_q
+; neon_load1_3reg
+; neon_load1_3reg_q
+; neon_load1_4reg
+; neon_load1_4reg_q
+; neon_load1_all_lanes
+; neon_load1_all_lanes_q
+; neon_load1_one_lane
+; neon_load1_one_lane_q
+; neon_load2_2reg
+; neon_load2_2reg_q
+; neon_load2_4reg
+; neon_load2_4reg_q
+; neon_load2_all_lanes
+; neon_load2_all_lanes_q
+; neon_load2_one_lane
+; neon_load2_one_lane_q
+; neon_load3_3reg
+; neon_load3_3reg_q
+; neon_load3_all_lanes
+; neon_load3_all_lanes_q
+; neon_load3_one_lane
+; neon_load3_one_lane_q
+; neon_load4_4reg
+; neon_load4_4reg_q
+; neon_load4_all_lanes
+; neon_load4_all_lanes_q
+; neon_load4_one_lane
+; neon_load4_one_lane_q
+; neon_str
+; neon_store1_1reg
+; neon_store1_1reg_q
+; neon_store1_2reg
+; neon_store1_2reg_q
+; neon_store1_3reg
+; neon_store1_3reg_q
+; neon_store1_4reg
+; neon_store1_4reg_q
+; neon_store1_one_lane
+; neon_store1_one_lane_q
+; neon_store2_2reg
+; neon_store2_2reg_q
+; neon_store2_4reg
+; neon_store2_4reg_q
+; neon_store2_one_lane
+; neon_store2_one_lane_q
+; neon_store3_3reg
+; neon_store3_3reg_q
+; neon_store3_one_lane
+; neon_store3_one_lane_q
+; neon_store4_4reg
+; neon_store4_4reg_q
+; neon_store4_one_lane
+; neon_store4_one_lane_q
+; neon_fp_abs_s
+; neon_fp_abs_s_q
+; neon_fp_abs_d
+; neon_fp_abs_d_q
+; neon_fp_neg_s
+; neon_fp_neg_s_q
+; neon_fp_neg_d
+; neon_fp_neg_d_q
+; neon_fp_abd_s
+; neon_fp_abd_s_q
+; neon_fp_abd_d
+; neon_fp_abd_d_q
+; neon_fp_addsub_s
+; neon_fp_addsub_s_q
+; neon_fp_addsub_d
+; neon_fp_addsub_d_q
+; neon_fp_compare_s
+; neon_fp_compare_s_q
+; neon_fp_compare_d
+; neon_fp_compare_d_q
+; neon_fp_minmax_s
+; neon_fp_minmax_s_q
+; neon_fp_minmax_d
+; neon_fp_minmax_d_q
+; neon_fp_reduc_add_s
+; neon_fp_reduc_add_s_q
+; neon_fp_reduc_add_d
+; neon_fp_reduc_add_d_q
+; neon_fp_reduc_minmax_s
+; neon_fp_reduc_minmax_s_q
+; neon_fp_reduc_minmax_d
+; neon_fp_reduc_minmax_d_q
+; neon_fp_cvt_narrow_s_q
+; neon_fp_cvt_narrow_d_q
+; neon_fp_cvt_widen_h
+; neon_fp_cvt_widen_s
+; neon_fp_to_int_s
+; neon_fp_to_int_s_q
+; neon_fp_to_int_d
+; neon_fp_to_int_d_q
+; neon_int_to_fp_s
+; neon_int_to_fp_s_q
+; neon_int_to_fp_d
+; neon_int_to_fp_d_q
+; neon_fp_round_s
+; neon_fp_round_s_q
+; neon_fp_round_d
+; neon_fp_round_d_q
+; neon_fp_recpe_s
+; neon_fp_recpe_s_q
+; neon_fp_recpe_d
+; neon_fp_recpe_d_q
+; neon_fp_recps_s
+; neon_fp_recps_s_q
+; neon_fp_recps_d
+; neon_fp_recps_d_q
+; neon_fp_recpx_s
+; neon_fp_recpx_s_q
+; neon_fp_recpx_d
+; neon_fp_recpx_d_q
+; neon_fp_rsqrte_s
+; neon_fp_rsqrte_s_q
+; neon_fp_rsqrte_d
+; neon_fp_rsqrte_d_q
+; neon_fp_rsqrts_s
+; neon_fp_rsqrts_s_q
+; neon_fp_rsqrts_d
+; neon_fp_rsqrts_d_q
+; neon_fp_mul_s
+; neon_fp_mul_s_q
+; neon_fp_mul_s_scalar
+; neon_fp_mul_s_scalar_q
+; neon_fp_mul_d
+; neon_fp_mul_d_q
+; neon_fp_mul_d_scalar_q
+; neon_fp_mla_s
+; neon_fp_mla_s_q
+; neon_fp_mla_s_scalar
+; neon_fp_mla_s_scalar_q
+; neon_fp_mla_d
+; neon_fp_mla_d_q
+; neon_fp_mla_d_scalar_q
+; neon_fp_sqrt_s
+; neon_fp_sqrt_s_q
+; neon_fp_sqrt_d
+; neon_fp_sqrt_d_q
+; neon_fp_div_s
+; neon_fp_div_s_q
+; neon_fp_div_d
+; neon_fp_div_d_q
+
+;
+; The classification below is for Crypto instructions.
+;
+; crypto_aes
+; crypto_sha1_xor
+; crypto_sha1_fast
+; crypto_sha1_slow
+; crypto_sha256_fast
+; crypto_sha256_slow
+
+(define_attr "type"
+ "adc_imm,\
+  adc_reg,\
+  adcs_imm,\
+  adcs_reg,\
+  adr,\
+  alu_ext,\
+  alu_imm,\
+  alu_reg,\
+  alu_shift_imm,\
+  alu_shift_reg,\
+  alus_ext,\
+  alus_imm,\
+  alus_reg,\
+  alus_shift_imm,\
+  alus_shift_reg,\
+  bfm,\
+  block,\
+  branch,\
+  call,\
+  clz,\
+  no_insn,\
+  csel,\
+  crc,\
+  extend,\
+  f_cvt,\
+  f_cvtf2i,\
+  f_cvti2f,\
+  f_flag,\
+  f_loadd,\
+  f_loads,\
+  f_mcr,\
+  f_mcrr,\
+  f_minmaxd,\
+  f_minmaxs,\
+  f_mrc,\
+  f_mrrc,\
+  f_rintd,\
+  f_rints,\
+  f_seld,\
+  f_sels,\
+  f_stored,\
+  f_stores,\
+  faddd,\
+  fadds,\
+  fcmpd,\
+  fcmps,\
+  fconstd,\
+  fconsts,\
+  fcsel,\
+  fdivd,\
+  fdivs,\
+  ffarithd,\
+  ffariths,\
+  ffmad,\
+  ffmas,\
+  float,\
+  fmacd,\
+  fmacs,\
+  fmov,\
+  fmuld,\
+  fmuls,\
+  fsqrts,\
+  fsqrtd,\
+  load_acq,\
+  load_byte,\
+  load1,\
+  load2,\
+  load3,\
+  load4,\
+  logic_imm,\
+  logic_reg,\
+  logic_shift_imm,\
+  logic_shift_reg,\
+  logics_imm,\
+  logics_reg,\
+  logics_shift_imm,\
+  logics_shift_reg,\
+  mla,\
+  mlas,\
+  mov_imm,\
+  mov_reg,\
+  mov_shift,\
+  mov_shift_reg,\
+  mrs,\
+  mul,\
+  muls,\
+  multiple,\
+  mvn_imm,\
+  mvn_reg,\
+  mvn_shift,\
+  mvn_shift_reg,\
+  nop,\
+  rbit,\
+  rev,\
+  sdiv,\
+  shift_imm,\
+  shift_reg,\
+  smlad,\
+  smladx,\
+  smlal,\
+  smlald,\
+  smlals,\
+  smlalxy,\
+  smlawx,\
+  smlawy,\
+  smlaxy,\
+  smlsd,\
+  smlsdx,\
+  smlsld,\
+  smmla,\
+  smmul,\
+  smmulr,\
+  smuad,\
+  smuadx,\
+  smull,\
+  smulls,\
+  smulwy,\
+  smulxy,\
+  smusd,\
+  smusdx,\
+  store_rel,\
+  store1,\
+  store2,\
+  store3,\
+  store4,\
+  udiv,\
+  umaal,\
+  umlal,\
+  umlals,\
+  umull,\
+  umulls,\
+  untyped,\
+  wmmx_tandc,\
+  wmmx_tbcst,\
+  wmmx_textrc,\
+  wmmx_textrm,\
+  wmmx_tinsr,\
+  wmmx_tmcr,\
+  wmmx_tmcrr,\
+  wmmx_tmia,\
+  wmmx_tmiaph,\
+  wmmx_tmiaxy,\
+  wmmx_tmrc,\
+  wmmx_tmrrc,\
+  wmmx_tmovmsk,\
+  wmmx_torc,\
+  wmmx_torvsc,\
+  wmmx_wabs,\
+  wmmx_wabsdiff,\
+  wmmx_wacc,\
+  wmmx_wadd,\
+  wmmx_waddbhus,\
+  wmmx_waddsubhx,\
+  wmmx_waligni,\
+  wmmx_walignr,\
+  wmmx_wand,\
+  wmmx_wandn,\
+  wmmx_wavg2,\
+  wmmx_wavg4,\
+  wmmx_wcmpeq,\
+  wmmx_wcmpgt,\
+  wmmx_wmac,\
+  wmmx_wmadd,\
+  wmmx_wmax,\
+  wmmx_wmerge,\
+  wmmx_wmiawxy,\
+  wmmx_wmiaxy,\
+  wmmx_wmin,\
+  wmmx_wmov,\
+  wmmx_wmul,\
+  wmmx_wmulw,\
+  wmmx_wldr,\
+  wmmx_wor,\
+  wmmx_wpack,\
+  wmmx_wqmiaxy,\
+  wmmx_wqmulm,\
+  wmmx_wqmulwm,\
+  wmmx_wror,\
+  wmmx_wsad,\
+  wmmx_wshufh,\
+  wmmx_wsll,\
+  wmmx_wsra,\
+  wmmx_wsrl,\
+  wmmx_wstr,\
+  wmmx_wsub,\
+  wmmx_wsubaddhx,\
+  wmmx_wunpckeh,\
+  wmmx_wunpckel,\
+  wmmx_wunpckih,\
+  wmmx_wunpckil,\
+  wmmx_wxor,\
+\
+  neon_add,\
+  neon_add_q,\
+  neon_add_widen,\
+  neon_add_long,\
+  neon_qadd,\
+  neon_qadd_q,\
+  neon_add_halve,\
+  neon_add_halve_q,\
+  neon_add_halve_narrow_q,\
+\
+  neon_sub,\
+  neon_sub_q,\
+  neon_sub_widen,\
+  neon_sub_long,\
+  neon_qsub,\
+  neon_qsub_q,\
+  neon_sub_halve,\
+  neon_sub_halve_q,\
+  neon_sub_halve_narrow_q,\
+\
+  neon_abs,\
+  neon_abs_q,\
+  neon_neg,\
+  neon_neg_q,\
+  neon_qneg,\
+  neon_qneg_q,\
+  neon_qabs,\
+  neon_qabs_q,\
+  neon_abd,\
+  neon_abd_q,\
+  neon_abd_long,\
+\
+  neon_minmax,\
+  neon_minmax_q,\
+  neon_compare,\
+  neon_compare_q,\
+  neon_compare_zero,\
+  neon_compare_zero_q,\
+\
+  neon_arith_acc,\
+  neon_arith_acc_q,\
+  neon_reduc_add,\
+  neon_reduc_add_q,\
+  neon_reduc_add_long,\
+  neon_reduc_add_acc,\
+  neon_reduc_add_acc_q,\
+  neon_reduc_minmax,\
+  neon_reduc_minmax_q,\
+  neon_logic,\
+  neon_logic_q,\
+  neon_tst,\
+  neon_tst_q,\
+\
+  neon_shift_imm,\
+  neon_shift_imm_q,\
+  neon_shift_imm_narrow_q,\
+  neon_shift_imm_long,\
+  neon_shift_reg,\
+  neon_shift_reg_q,\
+  neon_shift_acc,\
+  neon_shift_acc_q,\
+  neon_sat_shift_imm,\
+  neon_sat_shift_imm_q,\
+  neon_sat_shift_imm_narrow_q,\
+  neon_sat_shift_reg,\
+  neon_sat_shift_reg_q,\
+\
+  neon_ins,\
+  neon_ins_q,\
+  neon_move,\
+  neon_move_q,\
+  neon_move_narrow_q,\
+  neon_permute,\
+  neon_permute_q,\
+  neon_zip,\
+  neon_zip_q,\
+  neon_tbl1,\
+  neon_tbl1_q,\
+  neon_tbl2,\
+  neon_tbl2_q,\
+  neon_tbl3,\
+  neon_tbl3_q,\
+  neon_tbl4,\
+  neon_tbl4_q,\
+\
+  neon_bsl,\
+  neon_bsl_q,\
+  neon_cls,\
+  neon_cls_q,\
+  neon_cnt,\
+  neon_cnt_q,\
+  neon_dup,\
+  neon_dup_q,\
+  neon_ext,\
+  neon_ext_q,\
+  neon_rbit,\
+  neon_rbit_q,\
+  neon_rev,\
+  neon_rev_q,\
+\
+  neon_mul_b,\
+  neon_mul_b_q,\
+  neon_mul_h,\
+  neon_mul_h_q,\
+  neon_mul_s,\
+  neon_mul_s_q,\
+  neon_mul_b_long,\
+  neon_mul_h_long,\
+  neon_mul_s_long,\
+  neon_mul_d_long,\
+  neon_mul_h_scalar,\
+  neon_mul_h_scalar_q,\
+  neon_mul_s_scalar,\
+  neon_mul_s_scalar_q,\
+  neon_mul_h_scalar_long,\
+  neon_mul_s_scalar_long,\
+\
+  neon_sat_mul_b,\
+  neon_sat_mul_b_q,\
+  neon_sat_mul_h,\
+  neon_sat_mul_h_q,\
+  neon_sat_mul_s,\
+  neon_sat_mul_s_q,\
+  neon_sat_mul_b_long,\
+  neon_sat_mul_h_long,\
+  neon_sat_mul_s_long,\
+  neon_sat_mul_h_scalar,\
+  neon_sat_mul_h_scalar_q,\
+  neon_sat_mul_s_scalar,\
+  neon_sat_mul_s_scalar_q,\
+  neon_sat_mul_h_scalar_long,\
+  neon_sat_mul_s_scalar_long,\
+\
+  neon_mla_b,\
+  neon_mla_b_q,\
+  neon_mla_h,\
+  neon_mla_h_q,\
+  neon_mla_s,\
+  neon_mla_s_q,\
+  neon_mla_b_long,\
+  neon_mla_h_long,\
+  neon_mla_s_long,\
+  neon_mla_h_scalar,\
+  neon_mla_h_scalar_q,\
+  neon_mla_s_scalar,\
+  neon_mla_s_scalar_q,\
+  neon_mla_h_scalar_long,\
+  neon_mla_s_scalar_long,\
+\
+  neon_sat_mla_b_long,\
+  neon_sat_mla_h_long,\
+  neon_sat_mla_s_long,\
+  neon_sat_mla_h_scalar_long,\
+  neon_sat_mla_s_scalar_long,\
+\
+  neon_to_gp,\
+  neon_to_gp_q,\
+  neon_from_gp,\
+  neon_from_gp_q,\
+\
+  neon_ldr,\
+  neon_load1_1reg,\
+  neon_load1_1reg_q,\
+  neon_load1_2reg,\
+  neon_load1_2reg_q,\
+  neon_load1_3reg,\
+  neon_load1_3reg_q,\
+  neon_load1_4reg,\
+  neon_load1_4reg_q,\
+  neon_load1_all_lanes,\
+  neon_load1_all_lanes_q,\
+  neon_load1_one_lane,\
+  neon_load1_one_lane_q,\
+\
+  neon_load2_2reg,\
+  neon_load2_2reg_q,\
+  neon_load2_4reg,\
+  neon_load2_4reg_q,\
+  neon_load2_all_lanes,\
+  neon_load2_all_lanes_q,\
+  neon_load2_one_lane,\
+  neon_load2_one_lane_q,\
+\
+  neon_load3_3reg,\
+  neon_load3_3reg_q,\
+  neon_load3_all_lanes,\
+  neon_load3_all_lanes_q,\
+  neon_load3_one_lane,\
+  neon_load3_one_lane_q,\
+\
+  neon_load4_4reg,\
+  neon_load4_4reg_q,\
+  neon_load4_all_lanes,\
+  neon_load4_all_lanes_q,\
+  neon_load4_one_lane,\
+  neon_load4_one_lane_q,\
+\
+  neon_str,\
+  neon_store1_1reg,\
+  neon_store1_1reg_q,\
+  neon_store1_2reg,\
+  neon_store1_2reg_q,\
+  neon_store1_3reg,\
+  neon_store1_3reg_q,\
+  neon_store1_4reg,\
+  neon_store1_4reg_q,\
+  neon_store1_one_lane,\
+  neon_store1_one_lane_q,\
+\
+  neon_store2_2reg,\
+  neon_store2_2reg_q,\
+  neon_store2_4reg,\
+  neon_store2_4reg_q,\
+  neon_store2_one_lane,\
+  neon_store2_one_lane_q,\
+\
+  neon_store3_3reg,\
+  neon_store3_3reg_q,\
+  neon_store3_one_lane,\
+  neon_store3_one_lane_q,\
+\
+  neon_store4_4reg,\
+  neon_store4_4reg_q,\
+  neon_store4_one_lane,\
+  neon_store4_one_lane_q,\
+\
+  neon_fp_abs_s,\
+  neon_fp_abs_s_q,\
+  neon_fp_abs_d,\
+  neon_fp_abs_d_q,\
+  neon_fp_neg_s,\
+  neon_fp_neg_s_q,\
+  neon_fp_neg_d,\
+  neon_fp_neg_d_q,\
+\
+  neon_fp_abd_s,\
+  neon_fp_abd_s_q,\
+  neon_fp_abd_d,\
+  neon_fp_abd_d_q,\
+  neon_fp_addsub_s,\
+  neon_fp_addsub_s_q,\
+  neon_fp_addsub_d,\
+  neon_fp_addsub_d_q,\
+  neon_fp_compare_s,\
+  neon_fp_compare_s_q,\
+  neon_fp_compare_d,\
+  neon_fp_compare_d_q,\
+  neon_fp_minmax_s,\
+  neon_fp_minmax_s_q,\
+  neon_fp_minmax_d,\
+  neon_fp_minmax_d_q,\
+\
+  neon_fp_reduc_add_s,\
+  neon_fp_reduc_add_s_q,\
+  neon_fp_reduc_add_d,\
+  neon_fp_reduc_add_d_q,\
+  neon_fp_reduc_minmax_s,\
+  neon_fp_reduc_minmax_s_q,\
+  neon_fp_reduc_minmax_d,\
+  neon_fp_reduc_minmax_d_q,\
+\
+  neon_fp_cvt_narrow_s_q,\
+  neon_fp_cvt_narrow_d_q,\
+  neon_fp_cvt_widen_h,\
+  neon_fp_cvt_widen_s,\
+\
+  neon_fp_to_int_s,\
+  neon_fp_to_int_s_q,\
+  neon_fp_to_int_d,\
+  neon_fp_to_int_d_q,\
+  neon_int_to_fp_s,\
+  neon_int_to_fp_s_q,\
+  neon_int_to_fp_d,\
+  neon_int_to_fp_d_q,\
+  neon_fp_round_s,\
+  neon_fp_round_s_q,\
+  neon_fp_round_d,\
+  neon_fp_round_d_q,\
+\
+  neon_fp_recpe_s,\
+  neon_fp_recpe_s_q,\
+  neon_fp_recpe_d,\
+  neon_fp_recpe_d_q,\
+  neon_fp_recps_s,\
+  neon_fp_recps_s_q,\
+  neon_fp_recps_d,\
+  neon_fp_recps_d_q,\
+  neon_fp_recpx_s,\
+  neon_fp_recpx_s_q,\
+  neon_fp_recpx_d,\
+  neon_fp_recpx_d_q,\
+\
+  neon_fp_rsqrte_s,\
+  neon_fp_rsqrte_s_q,\
+  neon_fp_rsqrte_d,\
+  neon_fp_rsqrte_d_q,\
+  neon_fp_rsqrts_s,\
+  neon_fp_rsqrts_s_q,\
+  neon_fp_rsqrts_d,\
+  neon_fp_rsqrts_d_q,\
+\
+  neon_fp_mul_s,\
+  neon_fp_mul_s_q,\
+  neon_fp_mul_s_scalar,\
+  neon_fp_mul_s_scalar_q,\
+  neon_fp_mul_d,\
+  neon_fp_mul_d_q,\
+  neon_fp_mul_d_scalar_q,\
+\
+  neon_fp_mla_s,\
+  neon_fp_mla_s_q,\
+  neon_fp_mla_s_scalar,\
+  neon_fp_mla_s_scalar_q,\
+  neon_fp_mla_d,\
+  neon_fp_mla_d_q,\
+  neon_fp_mla_d_scalar_q,\
+\
+  neon_fp_sqrt_s,\
+  neon_fp_sqrt_s_q,\
+  neon_fp_sqrt_d,\
+  neon_fp_sqrt_d_q,\
+  neon_fp_div_s,\
+  neon_fp_div_s_q,\
+  neon_fp_div_d,\
+  neon_fp_div_d_q,\
+\
+  crypto_aes,\
+  crypto_sha1_xor,\
+  crypto_sha1_fast,\
+  crypto_sha1_slow,\
+  crypto_sha256_fast,\
+  crypto_sha256_slow"
+   (const_string "untyped"))
+
+; Is this an (integer side) multiply with a 32-bit (or smaller) result?
+(define_attr "mul32" "no,yes"
+  (if_then_else
+    (eq_attr "type"
+     "smulxy,smlaxy,smulwy,smlawx,mul,muls,mla,mlas,smlawy,smuad,smuadx,\
+      smlad,smladx,smusd,smusdx,smlsd,smlsdx,smmul,smmulr,smmla,smlald,smlsld")
+    (const_string "yes")
+    (const_string "no")))
+
+; Is this an (integer side) multiply with a 64-bit result?
+(define_attr "mul64" "no,yes"
+  (if_then_else
+    (eq_attr "type"
+     "smlalxy,umull,umulls,umaal,umlal,umlals,smull,smulls,smlal,smlals")
+    (const_string "yes")
+    (const_string "no")))
Index: b/src/gcc/config/arm/arm_neon_builtins.def
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/arm_neon_builtins.def
@@ -0,0 +1,213 @@
+/* NEON builtin definitions for ARM.
+   Copyright (C) 2013
+   Free Software Foundation, Inc.
+   Contributed by ARM Ltd.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+VAR10 (BINOP, vadd,
+	v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR3 (BINOP, vaddl, v8qi, v4hi, v2si),
+VAR3 (BINOP, vaddw, v8qi, v4hi, v2si),
+VAR6 (BINOP, vhadd, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR8 (BINOP, vqadd, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR3 (BINOP, vaddhn, v8hi, v4si, v2di),
+VAR8 (BINOP, vmul, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR8 (TERNOP, vmla, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR3 (TERNOP, vmlal, v8qi, v4hi, v2si),
+VAR2 (TERNOP, vfma, v2sf, v4sf),
+VAR2 (TERNOP, vfms, v2sf, v4sf),
+VAR8 (TERNOP, vmls, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR3 (TERNOP, vmlsl, v8qi, v4hi, v2si),
+VAR4 (BINOP, vqdmulh, v4hi, v2si, v8hi, v4si),
+VAR2 (TERNOP, vqdmlal, v4hi, v2si),
+VAR2 (TERNOP, vqdmlsl, v4hi, v2si),
+VAR3 (BINOP, vmull, v8qi, v4hi, v2si),
+VAR2 (SCALARMULL, vmull_n, v4hi, v2si),
+VAR2 (LANEMULL, vmull_lane, v4hi, v2si),
+VAR2 (SCALARMULL, vqdmull_n, v4hi, v2si),
+VAR2 (LANEMULL, vqdmull_lane, v4hi, v2si),
+VAR4 (SCALARMULH, vqdmulh_n, v4hi, v2si, v8hi, v4si),
+VAR4 (LANEMULH, vqdmulh_lane, v4hi, v2si, v8hi, v4si),
+VAR2 (BINOP, vqdmull, v4hi, v2si),
+VAR8 (BINOP, vshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR8 (BINOP, vqshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR8 (SHIFTIMM, vshr_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR3 (SHIFTIMM, vshrn_n, v8hi, v4si, v2di),
+VAR3 (SHIFTIMM, vqshrn_n, v8hi, v4si, v2di),
+VAR3 (SHIFTIMM, vqshrun_n, v8hi, v4si, v2di),
+VAR8 (SHIFTIMM, vshl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR8 (SHIFTIMM, vqshl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR8 (SHIFTIMM, vqshlu_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR3 (SHIFTIMM, vshll_n, v8qi, v4hi, v2si),
+VAR8 (SHIFTACC, vsra_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR10 (BINOP, vsub, v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR3 (BINOP, vsubl, v8qi, v4hi, v2si),
+VAR3 (BINOP, vsubw, v8qi, v4hi, v2si),
+VAR8 (BINOP, vqsub, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR6 (BINOP, vhsub, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR3 (BINOP, vsubhn, v8hi, v4si, v2di),
+VAR8 (BINOP, vceq, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR8 (BINOP, vcge, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR6 (BINOP, vcgeu, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR8 (BINOP, vcgt, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR6 (BINOP, vcgtu, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR2 (BINOP, vcage, v2sf, v4sf),
+VAR2 (BINOP, vcagt, v2sf, v4sf),
+VAR6 (BINOP, vtst, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR8 (BINOP, vabd, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR3 (BINOP, vabdl, v8qi, v4hi, v2si),
+VAR6 (TERNOP, vaba, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR3 (TERNOP, vabal, v8qi, v4hi, v2si),
+VAR8 (BINOP, vmax, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR8 (BINOP, vmin, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR4 (BINOP, vpadd, v8qi, v4hi, v2si, v2sf),
+VAR6 (UNOP, vpaddl, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR6 (BINOP, vpadal, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR4 (BINOP, vpmax, v8qi, v4hi, v2si, v2sf),
+VAR4 (BINOP, vpmin, v8qi, v4hi, v2si, v2sf),
+VAR2 (BINOP, vrecps, v2sf, v4sf),
+VAR2 (BINOP, vrsqrts, v2sf, v4sf),
+VAR8 (SHIFTINSERT, vsri_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR8 (SHIFTINSERT, vsli_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di),
+VAR8 (UNOP, vabs, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR6 (UNOP, vqabs, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR8 (UNOP, vneg, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR6 (UNOP, vqneg, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR6 (UNOP, vcls, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR6 (UNOP, vclz, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+VAR2 (UNOP, vcnt, v8qi, v16qi),
+VAR4 (UNOP, vrecpe, v2si, v2sf, v4si, v4sf),
+VAR4 (UNOP, vrsqrte, v2si, v2sf, v4si, v4sf),
+VAR6 (UNOP, vmvn, v8qi, v4hi, v2si, v16qi, v8hi, v4si),
+  /* FIXME: vget_lane supports more variants than this!  */
+VAR10 (GETLANE, vget_lane,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (SETLANE, vset_lane,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR5 (CREATE, vcreate, v8qi, v4hi, v2si, v2sf, di),
+VAR10 (DUP, vdup_n,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (DUPLANE, vdup_lane,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR5 (COMBINE, vcombine, v8qi, v4hi, v2si, v2sf, di),
+VAR5 (SPLIT, vget_high, v16qi, v8hi, v4si, v4sf, v2di),
+VAR5 (SPLIT, vget_low, v16qi, v8hi, v4si, v4sf, v2di),
+VAR3 (UNOP, vmovn, v8hi, v4si, v2di),
+VAR3 (UNOP, vqmovn, v8hi, v4si, v2di),
+VAR3 (UNOP, vqmovun, v8hi, v4si, v2di),
+VAR3 (UNOP, vmovl, v8qi, v4hi, v2si),
+VAR6 (LANEMUL, vmul_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR6 (LANEMAC, vmla_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR2 (LANEMAC, vmlal_lane, v4hi, v2si),
+VAR2 (LANEMAC, vqdmlal_lane, v4hi, v2si),
+VAR6 (LANEMAC, vmls_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR2 (LANEMAC, vmlsl_lane, v4hi, v2si),
+VAR2 (LANEMAC, vqdmlsl_lane, v4hi, v2si),
+VAR6 (SCALARMUL, vmul_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR6 (SCALARMAC, vmla_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR2 (SCALARMAC, vmlal_n, v4hi, v2si),
+VAR2 (SCALARMAC, vqdmlal_n, v4hi, v2si),
+VAR6 (SCALARMAC, vmls_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR2 (SCALARMAC, vmlsl_n, v4hi, v2si),
+VAR2 (SCALARMAC, vqdmlsl_n, v4hi, v2si),
+VAR10 (BINOP, vext,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR8 (UNOP, vrev64, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR4 (UNOP, vrev32, v8qi, v4hi, v16qi, v8hi),
+VAR2 (UNOP, vrev16, v8qi, v16qi),
+VAR4 (CONVERT, vcvt, v2si, v2sf, v4si, v4sf),
+VAR4 (FIXCONV, vcvt_n, v2si, v2sf, v4si, v4sf),
+VAR1 (FLOAT_WIDEN, vcvtv4sf, v4hf),
+VAR1 (FLOAT_NARROW, vcvtv4hf, v4sf),
+VAR10 (SELECT, vbsl,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR2 (RINT, vrintn, v2sf, v4sf),
+VAR2 (RINT, vrinta, v2sf, v4sf),
+VAR2 (RINT, vrintp, v2sf, v4sf),
+VAR2 (RINT, vrintm, v2sf, v4sf),
+VAR2 (RINT, vrintz, v2sf, v4sf),
+VAR2 (RINT, vrintx, v2sf, v4sf),
+VAR1 (VTBL, vtbl1, v8qi),
+VAR1 (VTBL, vtbl2, v8qi),
+VAR1 (VTBL, vtbl3, v8qi),
+VAR1 (VTBL, vtbl4, v8qi),
+VAR1 (VTBX, vtbx1, v8qi),
+VAR1 (VTBX, vtbx2, v8qi),
+VAR1 (VTBX, vtbx3, v8qi),
+VAR1 (VTBX, vtbx4, v8qi),
+VAR8 (RESULTPAIR, vtrn, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR8 (RESULTPAIR, vzip, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR8 (RESULTPAIR, vuzp, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf),
+VAR5 (REINTERP, vreinterpretv8qi, v8qi, v4hi, v2si, v2sf, di),
+VAR5 (REINTERP, vreinterpretv4hi, v8qi, v4hi, v2si, v2sf, di),
+VAR5 (REINTERP, vreinterpretv2si, v8qi, v4hi, v2si, v2sf, di),
+VAR5 (REINTERP, vreinterpretv2sf, v8qi, v4hi, v2si, v2sf, di),
+VAR5 (REINTERP, vreinterpretdi, v8qi, v4hi, v2si, v2sf, di),
+VAR6 (REINTERP, vreinterpretv16qi, v16qi, v8hi, v4si, v4sf, v2di, ti),
+VAR6 (REINTERP, vreinterpretv8hi, v16qi, v8hi, v4si, v4sf, v2di, ti),
+VAR6 (REINTERP, vreinterpretv4si, v16qi, v8hi, v4si, v4sf, v2di, ti),
+VAR6 (REINTERP, vreinterpretv4sf, v16qi, v8hi, v4si, v4sf, v2di, ti),
+VAR6 (REINTERP, vreinterpretv2di, v16qi, v8hi, v4si, v4sf, v2di, ti),
+VAR6 (REINTERP, vreinterpretti, v16qi, v8hi, v4si, v4sf, v2di, ti),
+VAR10 (LOAD1, vld1,
+         v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (LOAD1LANE, vld1_lane,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (LOAD1, vld1_dup,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (STORE1, vst1,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (STORE1LANE, vst1_lane,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR9 (LOADSTRUCT,
+	vld2, v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
+VAR7 (LOADSTRUCTLANE, vld2_lane,
+	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR5 (LOADSTRUCT, vld2_dup, v8qi, v4hi, v2si, v2sf, di),
+VAR9 (STORESTRUCT, vst2,
+	v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
+VAR7 (STORESTRUCTLANE, vst2_lane,
+	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR9 (LOADSTRUCT,
+	vld3, v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
+VAR7 (LOADSTRUCTLANE, vld3_lane,
+	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR5 (LOADSTRUCT, vld3_dup, v8qi, v4hi, v2si, v2sf, di),
+VAR9 (STORESTRUCT, vst3,
+	v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
+VAR7 (STORESTRUCTLANE, vst3_lane,
+	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR9 (LOADSTRUCT, vld4,
+	v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
+VAR7 (LOADSTRUCTLANE, vld4_lane,
+	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR5 (LOADSTRUCT, vld4_dup, v8qi, v4hi, v2si, v2sf, di),
+VAR9 (STORESTRUCT, vst4,
+	v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf),
+VAR7 (STORESTRUCTLANE, vst4_lane,
+	v8qi, v4hi, v2si, v2sf, v8hi, v4si, v4sf),
+VAR10 (LOGICBINOP, vand,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (LOGICBINOP, vorr,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (BINOP, veor,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (LOGICBINOP, vbic,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),
+VAR10 (LOGICBINOP, vorn,
+	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di)
Index: b/src/gcc/config/arm/neon.ml
===================================================================
--- a/src/gcc/config/arm/neon.ml
+++ b/src/gcc/config/arm/neon.ml
@@ -21,8 +21,8 @@
    <http://www.gnu.org/licenses/>.  *)
 
 (* Shorthand types for vector elements.  *)
-type elts = S8 | S16 | S32 | S64 | F32 | U8 | U16 | U32 | U64 | P8 | P16
-          | I8 | I16 | I32 | I64 | B8 | B16 | B32 | B64 | Conv of elts * elts
+type elts = S8 | S16 | S32 | S64 | F16 | F32 | U8 | U16 | U32 | U64 | P8 | P16
+          | P64 | P128 | I8 | I16 | I32 | I64 | B8 | B16 | B32 | B64 | Conv of elts * elts
           | Cast of elts * elts | NoElts
 
 type eltclass = Signed | Unsigned | Float | Poly | Int | Bits
@@ -37,6 +37,7 @@ type vectype = T_int8x8    | T_int8x16
 	     | T_uint16x4  | T_uint16x8
 	     | T_uint32x2  | T_uint32x4
 	     | T_uint64x1  | T_uint64x2
+	     | T_float16x4
 	     | T_float32x2 | T_float32x4
 	     | T_poly8x8   | T_poly8x16
 	     | T_poly16x4  | T_poly16x8
@@ -46,11 +47,15 @@ type vectype = T_int8x8    | T_int8x16
              | T_uint8     | T_uint16
              | T_uint32    | T_uint64
              | T_poly8     | T_poly16
-             | T_float32   | T_arrayof of int * vectype
+             | T_poly64    | T_poly64x1
+             | T_poly64x2  | T_poly128
+             | T_float16   | T_float32
+             | T_arrayof of int * vectype
              | T_ptrto of vectype | T_const of vectype
              | T_void      | T_intQI
              | T_intHI     | T_intSI
-             | T_intDI     | T_floatSF
+             | T_intDI     | T_intTI
+             | T_floatHF   | T_floatSF
 
 (* The meanings of the following are:
      TImode : "Tetra", two registers (four words).
@@ -92,8 +97,8 @@ type arity = Arity0 of vectype
 	   | Arity3 of vectype * vectype * vectype * vectype
            | Arity4 of vectype * vectype * vectype * vectype * vectype
 
-type vecmode = V8QI | V4HI | V2SI | V2SF | DI
-             | V16QI | V8HI | V4SI | V4SF | V2DI
+type vecmode = V8QI | V4HI | V4HF |V2SI | V2SF | DI
+             | V16QI | V8HI | V4SI | V4SF | V2DI | TI
              | QI | HI | SI | SF
 
 type opcode =
@@ -284,26 +289,31 @@ type features =
   | Fixed_core_reg
     (* Mark that the intrinsic requires __ARM_FEATURE_string to be defined.  *)
   | Requires_feature of string
+    (* Mark that the intrinsic requires a particular architecture version.  *)
   | Requires_arch of int
+    (* Mark that the intrinsic requires a particular bit in __ARM_FP to
+    be set.   *)
+  | Requires_FP_bit of int
 
 exception MixedMode of elts * elts
 
 let rec elt_width = function
     S8 | U8 | P8 | I8 | B8 -> 8
-  | S16 | U16 | P16 | I16 | B16 -> 16
+  | S16 | U16 | P16 | I16 | B16 | F16 -> 16
   | S32 | F32 | U32 | I32 | B32 -> 32
-  | S64 | U64 | I64 | B64 -> 64
+  | S64 | U64 | P64 | I64 | B64 -> 64
+  | P128 -> 128
   | Conv (a, b) ->
       let wa = elt_width a and wb = elt_width b in
-      if wa = wb then wa else failwith "element width?"
+      if wa = wb then wa else raise (MixedMode (a, b))
   | Cast (a, b) -> raise (MixedMode (a, b))
   | NoElts -> failwith "No elts"
 
 let rec elt_class = function
     S8 | S16 | S32 | S64 -> Signed
   | U8 | U16 | U32 | U64 -> Unsigned
-  | P8 | P16 -> Poly
-  | F32 -> Float
+  | P8 | P16 | P64 | P128 -> Poly
+  | F16 | F32 -> Float
   | I8 | I16 | I32 | I64 -> Int
   | B8 | B16 | B32 | B64 -> Bits
   | Conv (a, b) | Cast (a, b) -> ConvClass (elt_class a, elt_class b)
@@ -315,6 +325,7 @@ let elt_of_class_width c w =
   | Signed, 16 -> S16
   | Signed, 32 -> S32
   | Signed, 64 -> S64
+  | Float, 16 -> F16
   | Float, 32 -> F32
   | Unsigned, 8 -> U8
   | Unsigned, 16 -> U16
@@ -322,6 +333,8 @@ let elt_of_class_width c w =
   | Unsigned, 64 -> U64
   | Poly, 8 -> P8
   | Poly, 16 -> P16
+  | Poly, 64 -> P64
+  | Poly, 128 -> P128
   | Int, 8 -> I8
   | Int, 16 -> I16
   | Int, 32 -> I32
@@ -384,27 +397,39 @@ let find_key_operand operands =
   in
     scan ((Array.length operands) - 1)
 
-let rec mode_of_elt elt shape =
+(* Find a vecmode from a shape_elt ELT for an instruction with shape_form
+   SHAPE.  For a Use_operands shape, if ARGPOS is passed then return the mode
+   for the given argument position, else determine which argument to return a
+   mode for automatically.  *)
+
+let rec mode_of_elt ?argpos elt shape =
   let flt = match elt_class elt with
     Float | ConvClass(_, Float) -> true | _ -> false in
   let idx =
     match elt_width elt with
-      8 -> 0 | 16 -> 1 | 32 -> 2 | 64 -> 3
+      8 -> 0 | 16 -> 1 | 32 -> 2 | 64 -> 3 | 128 -> 4
     | _ -> failwith "Bad element width"
   in match shape with
     All (_, Dreg) | By_scalar Dreg | Pair_result Dreg | Unary_scalar Dreg
   | Binary_imm Dreg | Long_noreg Dreg | Wide_noreg Dreg ->
-      [| V8QI; V4HI; if flt then V2SF else V2SI; DI |].(idx)
+      if flt then
+        [| V8QI; V4HF; V2SF; DI |].(idx)
+      else
+        [| V8QI; V4HI; V2SI; DI |].(idx)
   | All (_, Qreg) | By_scalar Qreg | Pair_result Qreg | Unary_scalar Qreg
   | Binary_imm Qreg | Long_noreg Qreg | Wide_noreg Qreg ->
-      [| V16QI; V8HI; if flt then V4SF else V4SI; V2DI |].(idx)
+      [| V16QI; V8HI; if flt then V4SF else V4SI; V2DI; TI|].(idx)
   | All (_, (Corereg | PtrTo _ | CstPtrTo _)) ->
       [| QI; HI; if flt then SF else SI; DI |].(idx)
   | Long | Wide | Wide_lane | Wide_scalar
   | Long_imm ->
       [| V8QI; V4HI; V2SI; DI |].(idx)
   | Narrow | Narrow_imm -> [| V16QI; V8HI; V4SI; V2DI |].(idx)
-  | Use_operands ops -> mode_of_elt elt (All (0, (find_key_operand ops)))
+  | Use_operands ops ->
+      begin match argpos with
+        None -> mode_of_elt ?argpos elt (All (0, (find_key_operand ops)))
+      | Some pos -> mode_of_elt ?argpos elt (All (0, ops.(pos)))
+      end
   | _ -> failwith "invalid shape"
 
 (* Modify an element type dependent on the shape of the instruction and the
@@ -454,10 +479,13 @@ let type_for_elt shape elt no =
         | U16 -> T_uint16x4
         | U32 -> T_uint32x2
         | U64 -> T_uint64x1
+        | P64 -> T_poly64x1
+        | P128 -> T_poly128
+        | F16 -> T_float16x4
         | F32 -> T_float32x2
         | P8 -> T_poly8x8
         | P16 -> T_poly16x4
-        | _ -> failwith "Bad elt type"
+        | _ -> failwith "Bad elt type for Dreg"
         end
     | Qreg ->
         begin match elt with
@@ -472,7 +500,9 @@ let type_for_elt shape elt no =
         | F32 -> T_float32x4
         | P8 -> T_poly8x16
         | P16 -> T_poly16x8
-        | _ -> failwith "Bad elt type"
+        | P64 -> T_poly64x2
+        | P128 -> T_poly128
+        | _ -> failwith "Bad elt type for Qreg"
         end
     | Corereg ->
         begin match elt with
@@ -486,8 +516,10 @@ let type_for_elt shape elt no =
         | U64 -> T_uint64
         | P8 -> T_poly8
         | P16 -> T_poly16
+        | P64 -> T_poly64
+        | P128 -> T_poly128
         | F32 -> T_float32
-        | _ -> failwith "Bad elt type"
+        | _ -> failwith "Bad elt type for Corereg"
         end
     | Immed ->
         T_immediate (0, 0)
@@ -506,10 +538,10 @@ let type_for_elt shape elt no =
 let vectype_size = function
     T_int8x8 | T_int16x4 | T_int32x2 | T_int64x1
   | T_uint8x8 | T_uint16x4 | T_uint32x2 | T_uint64x1
-  | T_float32x2 | T_poly8x8 | T_poly16x4 -> 64
+  | T_float32x2 | T_poly8x8 | T_poly64x1 | T_poly16x4 | T_float16x4 -> 64
   | T_int8x16 | T_int16x8 | T_int32x4 | T_int64x2
   | T_uint8x16 | T_uint16x8  | T_uint32x4  | T_uint64x2
-  | T_float32x4 | T_poly8x16 | T_poly16x8 -> 128
+  | T_float32x4 | T_poly8x16 | T_poly64x2 | T_poly16x8 -> 128
   | _ -> raise Not_found
 
 let inttype_for_array num elttype =
@@ -1020,14 +1052,22 @@ let ops =
       "vRsraQ_n", shift_right_acc, su_8_64;
 
     (* Vector shift right and insert.  *)
+    Vsri, [Requires_feature "CRYPTO"], Use_operands [| Dreg; Dreg; Immed |], "vsri_n", shift_insert,
+      [P64];
     Vsri, [], Use_operands [| Dreg; Dreg; Immed |], "vsri_n", shift_insert,
       P8 :: P16 :: su_8_64;
+    Vsri, [Requires_feature "CRYPTO"], Use_operands [| Qreg; Qreg; Immed |], "vsriQ_n", shift_insert,
+      [P64];
     Vsri, [], Use_operands [| Qreg; Qreg; Immed |], "vsriQ_n", shift_insert,
       P8 :: P16 :: su_8_64;
 
     (* Vector shift left and insert.  *)
+    Vsli, [Requires_feature "CRYPTO"], Use_operands [| Dreg; Dreg; Immed |], "vsli_n", shift_insert,
+      [P64];
     Vsli, [], Use_operands [| Dreg; Dreg; Immed |], "vsli_n", shift_insert,
       P8 :: P16 :: su_8_64;
+    Vsli, [Requires_feature "CRYPTO"], Use_operands [| Qreg; Qreg; Immed |], "vsliQ_n", shift_insert,
+      [P64];
     Vsli, [], Use_operands [| Qreg; Qreg; Immed |], "vsliQ_n", shift_insert,
       P8 :: P16 :: su_8_64;
 
@@ -1114,6 +1154,11 @@ let ops =
 
     (* Create vector from literal bit pattern.  *)
     Vcreate,
+      [Requires_feature "CRYPTO"; No_op], (* Not really, but it can yield various things that are too
+                                   hard for the test generator at this time.  *)
+      Use_operands [| Dreg; Corereg |], "vcreate", create_vector,
+      [P64];
+    Vcreate,
       [No_op], (* Not really, but it can yield various things that are too
                   hard for the test generator at this time.  *)
       Use_operands [| Dreg; Corereg |], "vcreate", create_vector,
@@ -1127,12 +1172,25 @@ let ops =
       Use_operands [| Dreg; Corereg |], "vdup_n", bits_1,
       pf_su_8_32;
     Vdup_n,
+      [No_op; Requires_feature "CRYPTO";
+       Instruction_name ["vmov"];
+       Disassembles_as [Use_operands [| Dreg; Corereg; Corereg |]]],
+      Use_operands [| Dreg; Corereg |], "vdup_n", notype_1,
+      [P64];
+    Vdup_n,
       [No_op;
        Instruction_name ["vmov"];
        Disassembles_as [Use_operands [| Dreg; Corereg; Corereg |]]],
       Use_operands [| Dreg; Corereg |], "vdup_n", notype_1,
       [S64; U64];
     Vdup_n,
+      [No_op; Requires_feature "CRYPTO";
+       Disassembles_as [Use_operands [| Qreg;
+                                        Alternatives [ Corereg;
+                                                       Element_of_dreg ] |]]],
+      Use_operands [| Qreg; Corereg |], "vdupQ_n", bits_1,
+      [P64];
+    Vdup_n,
       [Disassembles_as [Use_operands [| Qreg;
                                         Alternatives [ Corereg;
                                                        Element_of_dreg ] |]]],
@@ -1185,21 +1243,33 @@ let ops =
       [Disassembles_as [Use_operands [| Dreg; Element_of_dreg |]]],
       Unary_scalar Dreg, "vdup_lane", bits_2, pf_su_8_32;
     Vdup_lane,
+      [No_op; Requires_feature "CRYPTO"; Const_valuator (fun _ -> 0)],
+      Unary_scalar Dreg, "vdup_lane", bits_2, [P64];
+    Vdup_lane,
       [No_op; Const_valuator (fun _ -> 0)],
       Unary_scalar Dreg, "vdup_lane", bits_2, [S64; U64];
     Vdup_lane,
       [Disassembles_as [Use_operands [| Qreg; Element_of_dreg |]]],
       Unary_scalar Qreg, "vdupQ_lane", bits_2, pf_su_8_32;
     Vdup_lane,
+      [No_op; Requires_feature "CRYPTO"; Const_valuator (fun _ -> 0)],
+      Unary_scalar Qreg, "vdupQ_lane", bits_2, [P64];
+    Vdup_lane,
       [No_op; Const_valuator (fun _ -> 0)],
       Unary_scalar Qreg, "vdupQ_lane", bits_2, [S64; U64];
 
     (* Combining vectors.  *)
+    Vcombine, [Requires_feature "CRYPTO"; No_op],
+      Use_operands [| Qreg; Dreg; Dreg |], "vcombine", notype_2,
+      [P64];
     Vcombine, [No_op],
       Use_operands [| Qreg; Dreg; Dreg |], "vcombine", notype_2,
       pf_su_8_64;
 
     (* Splitting vectors.  *)
+    Vget_high, [Requires_feature "CRYPTO"; No_op],
+      Use_operands [| Dreg; Qreg |], "vget_high",
+      notype_1, [P64];
     Vget_high, [No_op],
       Use_operands [| Dreg; Qreg |], "vget_high",
       notype_1, pf_su_8_64;
@@ -1208,7 +1278,10 @@ let ops =
 	       Fixed_vector_reg],
       Use_operands [| Dreg; Qreg |], "vget_low",
       notype_1, pf_su_8_32;
-     Vget_low, [No_op],
+    Vget_low, [Requires_feature "CRYPTO"; No_op],
+      Use_operands [| Dreg; Qreg |], "vget_low",
+      notype_1, [P64];
+    Vget_low, [No_op],
       Use_operands [| Dreg; Qreg |], "vget_low",
       notype_1, [S64; U64];
 
@@ -1217,6 +1290,10 @@ let ops =
       [Conv (S32, F32); Conv (U32, F32); Conv (F32, S32); Conv (F32, U32)];
     Vcvt, [InfoWord], All (2, Qreg), "vcvtQ", conv_1,
       [Conv (S32, F32); Conv (U32, F32); Conv (F32, S32); Conv (F32, U32)];
+    Vcvt, [Builtin_name "vcvt" ; Requires_FP_bit 1],
+          Use_operands [| Dreg; Qreg; |], "vcvt", conv_1, [Conv (F16, F32)];
+    Vcvt, [Builtin_name "vcvt" ; Requires_FP_bit 1],
+          Use_operands [| Qreg; Dreg; |], "vcvt", conv_1, [Conv (F32, F16)];
     Vcvt_n, [InfoWord], Use_operands [| Dreg; Dreg; Immed |], "vcvt_n", conv_2,
       [Conv (S32, F32); Conv (U32, F32); Conv (F32, S32); Conv (F32, U32)];
     Vcvt_n, [InfoWord], Use_operands [| Qreg; Qreg; Immed |], "vcvtQ_n", conv_2,
@@ -1387,9 +1464,15 @@ let ops =
       [S16; S32];
 
     (* Vector extract.  *)
+    Vext, [Requires_feature "CRYPTO"; Const_valuator (fun _ -> 0)],
+      Use_operands [| Dreg; Dreg; Dreg; Immed |], "vext", extend,
+      [P64];
     Vext, [Const_valuator (fun _ -> 0)],
       Use_operands [| Dreg; Dreg; Dreg; Immed |], "vext", extend,
       pf_su_8_64;
+    Vext, [Requires_feature "CRYPTO"; Const_valuator (fun _ -> 0)],
+      Use_operands [| Qreg; Qreg; Qreg; Immed |], "vextQ", extend,
+      [P64];
     Vext, [Const_valuator (fun _ -> 0)],
       Use_operands [| Qreg; Qreg; Qreg; Immed |], "vextQ", extend,
       pf_su_8_64;
@@ -1410,11 +1493,21 @@ let ops =
 
     (* Bit selection.  *)
     Vbsl,
+      [Requires_feature "CRYPTO"; Instruction_name ["vbsl"; "vbit"; "vbif"];
+       Disassembles_as [Use_operands [| Dreg; Dreg; Dreg |]]],
+      Use_operands [| Dreg; Dreg; Dreg; Dreg |], "vbsl", bit_select,
+      [P64];
+    Vbsl,
       [Instruction_name ["vbsl"; "vbit"; "vbif"];
        Disassembles_as [Use_operands [| Dreg; Dreg; Dreg |]]],
       Use_operands [| Dreg; Dreg; Dreg; Dreg |], "vbsl", bit_select,
       pf_su_8_64;
     Vbsl,
+      [Requires_feature "CRYPTO"; Instruction_name ["vbsl"; "vbit"; "vbif"];
+       Disassembles_as [Use_operands [| Qreg; Qreg; Qreg |]]],
+      Use_operands [| Qreg; Qreg; Qreg; Qreg |], "vbslQ", bit_select,
+      [P64];
+    Vbsl,
       [Instruction_name ["vbsl"; "vbit"; "vbif"];
        Disassembles_as [Use_operands [| Qreg; Qreg; Qreg |]]],
       Use_operands [| Qreg; Qreg; Qreg; Qreg |], "vbslQ", bit_select,
@@ -1436,10 +1529,21 @@ let ops =
 
     (* Element/structure loads.  VLD1 variants.  *)
     Vldx 1,
+      [Requires_feature "CRYPTO";
+       Disassembles_as [Use_operands [| VecArray (1, Dreg);
+                                        CstPtrTo Corereg |]]],
+      Use_operands [| Dreg; CstPtrTo Corereg |], "vld1", bits_1,
+      [P64];
+    Vldx 1,
       [Disassembles_as [Use_operands [| VecArray (1, Dreg);
                                         CstPtrTo Corereg |]]],
       Use_operands [| Dreg; CstPtrTo Corereg |], "vld1", bits_1,
       pf_su_8_64;
+    Vldx 1, [Requires_feature "CRYPTO";
+             Disassembles_as [Use_operands [| VecArray (2, Dreg);
+					      CstPtrTo Corereg |]]],
+      Use_operands [| Qreg; CstPtrTo Corereg |], "vld1Q", bits_1,
+      [P64];
     Vldx 1, [Disassembles_as [Use_operands [| VecArray (2, Dreg);
 					      CstPtrTo Corereg |]]],
       Use_operands [| Qreg; CstPtrTo Corereg |], "vld1Q", bits_1,
@@ -1451,6 +1555,13 @@ let ops =
       Use_operands [| Dreg; CstPtrTo Corereg; Dreg; Immed |],
       "vld1_lane", bits_3, pf_su_8_32;
     Vldx_lane 1,
+      [Requires_feature "CRYPTO";
+       Disassembles_as [Use_operands [| VecArray (1, Dreg);
+                                        CstPtrTo Corereg |]];
+       Const_valuator (fun _ -> 0)],
+      Use_operands [| Dreg; CstPtrTo Corereg; Dreg; Immed |],
+      "vld1_lane", bits_3, [P64];
+    Vldx_lane 1,
       [Disassembles_as [Use_operands [| VecArray (1, Dreg);
                                         CstPtrTo Corereg |]];
        Const_valuator (fun _ -> 0)],
@@ -1462,6 +1573,12 @@ let ops =
       Use_operands [| Qreg; CstPtrTo Corereg; Qreg; Immed |],
       "vld1Q_lane", bits_3, pf_su_8_32;
     Vldx_lane 1,
+      [Requires_feature "CRYPTO";
+       Disassembles_as [Use_operands [| VecArray (1, Dreg);
+                                        CstPtrTo Corereg |]]],
+      Use_operands [| Qreg; CstPtrTo Corereg; Qreg; Immed |],
+      "vld1Q_lane", bits_3, [P64];
+    Vldx_lane 1,
       [Disassembles_as [Use_operands [| VecArray (1, Dreg);
                                         CstPtrTo Corereg |]]],
       Use_operands [| Qreg; CstPtrTo Corereg; Qreg; Immed |],
@@ -1473,6 +1590,12 @@ let ops =
       Use_operands [| Dreg; CstPtrTo Corereg |], "vld1_dup",
       bits_1, pf_su_8_32;
     Vldx_dup 1,
+      [Requires_feature "CRYPTO";
+       Disassembles_as [Use_operands [| VecArray (1, Dreg);
+                                        CstPtrTo Corereg |]]],
+      Use_operands [| Dreg; CstPtrTo Corereg |], "vld1_dup",
+      bits_1, [P64];
+    Vldx_dup 1,
       [Disassembles_as [Use_operands [| VecArray (1, Dreg);
                                         CstPtrTo Corereg |]]],
       Use_operands [| Dreg; CstPtrTo Corereg |], "vld1_dup",
@@ -1485,16 +1608,32 @@ let ops =
     (* Treated identically to vld1_dup above as we now
        do a single load followed by a duplicate.  *)
     Vldx_dup 1,
+      [Requires_feature "CRYPTO";
+       Disassembles_as [Use_operands [| VecArray (1, Dreg);
+                                        CstPtrTo Corereg |]]],
+      Use_operands [| Qreg; CstPtrTo Corereg |], "vld1Q_dup",
+      bits_1, [P64];
+    Vldx_dup 1,
       [Disassembles_as [Use_operands [| VecArray (1, Dreg);
                                         CstPtrTo Corereg |]]],
       Use_operands [| Qreg; CstPtrTo Corereg |], "vld1Q_dup",
       bits_1, [S64; U64];
 
     (* VST1 variants.  *)
+    Vstx 1, [Requires_feature "CRYPTO";
+             Disassembles_as [Use_operands [| VecArray (1, Dreg);
+                                              PtrTo Corereg |]]],
+      Use_operands [| PtrTo Corereg; Dreg |], "vst1",
+      store_1, [P64];
     Vstx 1, [Disassembles_as [Use_operands [| VecArray (1, Dreg);
                                               PtrTo Corereg |]]],
       Use_operands [| PtrTo Corereg; Dreg |], "vst1",
       store_1, pf_su_8_64;
+    Vstx 1, [Requires_feature "CRYPTO";
+             Disassembles_as [Use_operands [| VecArray (2, Dreg);
+					      PtrTo Corereg |]]],
+      Use_operands [| PtrTo Corereg; Qreg |], "vst1Q",
+      store_1, [P64];
     Vstx 1, [Disassembles_as [Use_operands [| VecArray (2, Dreg);
 					      PtrTo Corereg |]]],
       Use_operands [| PtrTo Corereg; Qreg |], "vst1Q",
@@ -1506,6 +1645,13 @@ let ops =
       Use_operands [| PtrTo Corereg; Dreg; Immed |],
       "vst1_lane", store_3, pf_su_8_32;
     Vstx_lane 1,
+      [Requires_feature "CRYPTO";
+       Disassembles_as [Use_operands [| VecArray (1, Dreg);
+                                        CstPtrTo Corereg |]];
+       Const_valuator (fun _ -> 0)],
+      Use_operands [| PtrTo Corereg; Dreg; Immed |],
+      "vst1_lane", store_3, [P64];
+    Vstx_lane 1,
       [Disassembles_as [Use_operands [| VecArray (1, Dreg);
                                         CstPtrTo Corereg |]];
        Const_valuator (fun _ -> 0)],
@@ -1517,6 +1663,12 @@ let ops =
       Use_operands [| PtrTo Corereg; Qreg; Immed |],
       "vst1Q_lane", store_3, pf_su_8_32;
     Vstx_lane 1,
+      [Requires_feature "CRYPTO";
+       Disassembles_as [Use_operands [| VecArray (1, Dreg);
+                                        CstPtrTo Corereg |]]],
+      Use_operands [| PtrTo Corereg; Qreg; Immed |],
+      "vst1Q_lane", store_3, [P64];
+    Vstx_lane 1,
       [Disassembles_as [Use_operands [| VecArray (1, Dreg);
                                         CstPtrTo Corereg |]]],
       Use_operands [| PtrTo Corereg; Qreg; Immed |],
@@ -1525,6 +1677,9 @@ let ops =
     (* VLD2 variants.  *)
     Vldx 2, [], Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],
       "vld2", bits_1, pf_su_8_32;
+    Vldx 2, [Requires_feature "CRYPTO"; Instruction_name ["vld1"]],
+       Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],
+      "vld2", bits_1, [P64];
     Vldx 2, [Instruction_name ["vld1"]],
        Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],
       "vld2", bits_1, [S64; U64];
@@ -1556,6 +1711,12 @@ let ops =
       Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],
       "vld2_dup", bits_1, pf_su_8_32;
     Vldx_dup 2,
+      [Requires_feature "CRYPTO";
+       Instruction_name ["vld1"]; Disassembles_as [Use_operands
+        [| VecArray (2, Dreg); CstPtrTo Corereg |]]],
+      Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],
+      "vld2_dup", bits_1, [P64];
+    Vldx_dup 2,
       [Instruction_name ["vld1"]; Disassembles_as [Use_operands
         [| VecArray (2, Dreg); CstPtrTo Corereg |]]],
       Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],
@@ -1566,6 +1727,12 @@ let ops =
                                               PtrTo Corereg |]]],
       Use_operands [| PtrTo Corereg; VecArray (2, Dreg) |], "vst2",
       store_1, pf_su_8_32;
+    Vstx 2, [Requires_feature "CRYPTO";
+             Disassembles_as [Use_operands [| VecArray (2, Dreg);
+                                              PtrTo Corereg |]];
+             Instruction_name ["vst1"]],
+      Use_operands [| PtrTo Corereg; VecArray (2, Dreg) |], "vst2",
+      store_1, [P64];
     Vstx 2, [Disassembles_as [Use_operands [| VecArray (2, Dreg);
                                               PtrTo Corereg |]];
              Instruction_name ["vst1"]],
@@ -1594,6 +1761,9 @@ let ops =
     (* VLD3 variants.  *)
     Vldx 3, [], Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],
       "vld3", bits_1, pf_su_8_32;
+    Vldx 3, [Requires_feature "CRYPTO"; Instruction_name ["vld1"]],
+      Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],
+      "vld3", bits_1, [P64];
     Vldx 3, [Instruction_name ["vld1"]],
       Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],
       "vld3", bits_1, [S64; U64];
@@ -1625,6 +1795,12 @@ let ops =
       Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],
       "vld3_dup", bits_1, pf_su_8_32;
     Vldx_dup 3,
+      [Requires_feature "CRYPTO";
+       Instruction_name ["vld1"]; Disassembles_as [Use_operands
+        [| VecArray (3, Dreg); CstPtrTo Corereg |]]],
+      Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],
+      "vld3_dup", bits_1, [P64];
+    Vldx_dup 3,
       [Instruction_name ["vld1"]; Disassembles_as [Use_operands
         [| VecArray (3, Dreg); CstPtrTo Corereg |]]],
       Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],
@@ -1635,6 +1811,12 @@ let ops =
                                               PtrTo Corereg |]]],
       Use_operands [| PtrTo Corereg; VecArray (3, Dreg) |], "vst3",
       store_1, pf_su_8_32;
+    Vstx 3, [Requires_feature "CRYPTO";
+             Disassembles_as [Use_operands [| VecArray (4, Dreg);
+                                              PtrTo Corereg |]];
+             Instruction_name ["vst1"]],
+      Use_operands [| PtrTo Corereg; VecArray (3, Dreg) |], "vst3",
+      store_1, [P64];
     Vstx 3, [Disassembles_as [Use_operands [| VecArray (4, Dreg);
                                               PtrTo Corereg |]];
              Instruction_name ["vst1"]],
@@ -1663,6 +1845,9 @@ let ops =
     (* VLD4/VST4 variants.  *)
     Vldx 4, [], Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],
       "vld4", bits_1, pf_su_8_32;
+    Vldx 4, [Requires_feature "CRYPTO"; Instruction_name ["vld1"]],
+      Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],
+      "vld4", bits_1, [P64];
     Vldx 4, [Instruction_name ["vld1"]],
       Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],
       "vld4", bits_1, [S64; U64];
@@ -1694,6 +1879,12 @@ let ops =
       Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],
       "vld4_dup", bits_1, pf_su_8_32;
     Vldx_dup 4,
+      [Requires_feature "CRYPTO";
+       Instruction_name ["vld1"]; Disassembles_as [Use_operands
+        [| VecArray (4, Dreg); CstPtrTo Corereg |]]],
+      Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],
+      "vld4_dup", bits_1, [P64];
+    Vldx_dup 4,
       [Instruction_name ["vld1"]; Disassembles_as [Use_operands
         [| VecArray (4, Dreg); CstPtrTo Corereg |]]],
       Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],
@@ -1703,6 +1894,12 @@ let ops =
                                               PtrTo Corereg |]]],
       Use_operands [| PtrTo Corereg; VecArray (4, Dreg) |], "vst4",
       store_1, pf_su_8_32;
+    Vstx 4, [Requires_feature "CRYPTO";
+             Disassembles_as [Use_operands [| VecArray (4, Dreg);
+                                              PtrTo Corereg |]];
+             Instruction_name ["vst1"]],
+      Use_operands [| PtrTo Corereg; VecArray (4, Dreg) |], "vst4",
+      store_1, [P64];
     Vstx 4, [Disassembles_as [Use_operands [| VecArray (4, Dreg);
                                               PtrTo Corereg |]];
              Instruction_name ["vst1"]],
@@ -1754,26 +1951,32 @@ let ops =
     Vorn, [], All (3, Qreg), "vornQ", notype_2, su_8_64;
   ]
 
+let type_in_crypto_only t
+  = (t == P64) or (t == P128)
+
+let cross_product s1 s2
+  = List.filter (fun (e, e') -> e <> e')
+                (List.concat (List.map (fun e1 -> List.map (fun e2 -> (e1,e2)) s1) s2))
+
 let reinterp =
-  let elems = P8 :: P16 :: F32 :: su_8_64 in
-  List.fold_right
-    (fun convto acc ->
-      let types = List.fold_right
-        (fun convfrom acc ->
-          if convfrom <> convto then
-            Cast (convto, convfrom) :: acc
-          else
-            acc)
-        elems
-        []
-      in
-        let dconv = Vreinterp, [No_op], Use_operands [| Dreg; Dreg |],
-                      "vreinterpret", conv_1, types
-        and qconv = Vreinterp, [No_op], Use_operands [| Qreg; Qreg |],
-		      "vreinterpretQ", conv_1, types in
-        dconv :: qconv :: acc)
-    elems
-    []
+  let elems = P8 :: P16 :: F32 :: P64 :: su_8_64 in
+  let casts = cross_product elems elems in
+  List.map
+    (fun (convto, convfrom) ->
+       Vreinterp, (if (type_in_crypto_only convto) or (type_in_crypto_only convfrom)
+                   then [Requires_feature "CRYPTO"] else []) @ [No_op], Use_operands [| Dreg; Dreg |],
+                   "vreinterpret", conv_1, [Cast (convto, convfrom)])
+    casts
+
+let reinterpq =
+  let elems = P8 :: P16 :: F32 :: P64 :: P128 :: su_8_64 in
+  let casts = cross_product elems elems in
+  List.map
+    (fun (convto, convfrom) ->
+       Vreinterp, (if (type_in_crypto_only convto) or (type_in_crypto_only convfrom)
+                   then [Requires_feature "CRYPTO"] else []) @ [No_op], Use_operands [| Qreg; Qreg |],
+                   "vreinterpretQ", conv_1, [Cast (convto, convfrom)])
+    casts
 
 (* Output routines.  *)
 
@@ -1782,7 +1985,8 @@ let rec string_of_elt = function
   | U8 -> "u8" | U16 -> "u16" | U32 -> "u32" | U64 -> "u64"
   | I8 -> "i8" | I16 -> "i16" | I32 -> "i32" | I64 -> "i64"
   | B8 -> "8" | B16 -> "16" | B32 -> "32" | B64 -> "64"
-  | F32 -> "f32" | P8 -> "p8" | P16 -> "p16"
+  | F16 -> "f16" | F32 -> "f32" | P8 -> "p8" | P16 -> "p16"
+  | P64 -> "p64" | P128 -> "p128"
   | Conv (a, b) | Cast (a, b) -> string_of_elt a ^ "_" ^ string_of_elt b
   | NoElts -> failwith "No elts"
 
@@ -1809,6 +2013,7 @@ let string_of_vectype vt =
   | T_uint32x4 -> affix "uint32x4"
   | T_uint64x1 -> affix "uint64x1"
   | T_uint64x2 -> affix "uint64x2"
+  | T_float16x4 -> affix "float16x4"
   | T_float32x2 -> affix "float32x2"
   | T_float32x4 -> affix "float32x4"
   | T_poly8x8 -> affix "poly8x8"
@@ -1825,6 +2030,11 @@ let string_of_vectype vt =
   | T_uint64 -> affix "uint64"
   | T_poly8 -> affix "poly8"
   | T_poly16 -> affix "poly16"
+  | T_poly64 -> affix "poly64"
+  | T_poly64x1 -> affix "poly64x1"
+  | T_poly64x2 -> affix "poly64x2"
+  | T_poly128 -> affix "poly128"
+  | T_float16 -> affix "float16"
   | T_float32 -> affix "float32"
   | T_immediate _ -> "const int"
   | T_void -> "void"
@@ -1832,6 +2042,8 @@ let string_of_vectype vt =
   | T_intHI -> "__builtin_neon_hi"
   | T_intSI -> "__builtin_neon_si"
   | T_intDI -> "__builtin_neon_di"
+  | T_intTI -> "__builtin_neon_ti"
+  | T_floatHF -> "__builtin_neon_hf"
   | T_floatSF -> "__builtin_neon_sf"
   | T_arrayof (num, base) ->
       let basename = name (fun x -> x) base in
@@ -1853,10 +2065,10 @@ let string_of_inttype = function
   | B_XImode -> "__builtin_neon_xi"
 
 let string_of_mode = function
-    V8QI -> "v8qi" | V4HI  -> "v4hi"  | V2SI -> "v2si" | V2SF -> "v2sf"
-  | DI   -> "di"   | V16QI -> "v16qi" | V8HI -> "v8hi" | V4SI -> "v4si"
-  | V4SF -> "v4sf" | V2DI  -> "v2di"  | QI -> "qi" | HI -> "hi" | SI -> "si"
-  | SF -> "sf"
+    V8QI -> "v8qi" | V4HI -> "v4hi" | V4HF  -> "v4hf"  | V2SI -> "v2si"
+  | V2SF -> "v2sf" | DI   -> "di"   | V16QI -> "v16qi" | V8HI -> "v8hi"
+  | V4SI -> "v4si" | V4SF -> "v4sf" | V2DI  -> "v2di"  | QI   -> "qi"
+  | HI -> "hi" | SI -> "si" | SF -> "sf" | TI -> "ti"
 
 (* Use uppercase chars for letters which form part of the intrinsic name, but
    should be omitted from the builtin name (the info is passed in an extra
@@ -1963,3 +2175,181 @@ let analyze_all_shapes features shape f
     | _ -> assert false
   with Not_found -> [f shape]
 
+(* The crypto intrinsics have unconventional shapes and are not that
+   numerous to be worth the trouble of encoding here.  We implement them
+   explicitly here.  *)
+let crypto_intrinsics =
+"
+#ifdef __ARM_FEATURE_CRYPTO
+
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vldrq_p128 (poly128_t const * __ptr)
+{
+#ifdef __ARM_BIG_ENDIAN
+  poly64_t* __ptmp = (poly64_t*) __ptr;
+  poly64_t __d0 = vld1_p64 (__ptmp);
+  poly64_t __d1 = vld1_p64 (__ptmp + 1);
+  return vreinterpretq_p128_p64 (vcombine_p64 (__d1, __d0));
+#else
+  return vreinterpretq_p128_p64 (vld1q_p64 ((poly64_t*) __ptr));
+#endif
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vstrq_p128 (poly128_t * __ptr, poly128_t __val)
+{
+#ifdef __ARM_BIG_ENDIAN
+  poly64x2_t __tmp = vreinterpretq_p64_p128 (__val);
+  poly64_t __d0 = vget_high_p64 (__tmp);
+  poly64_t __d1 = vget_low_p64 (__tmp);
+  vst1q_p64 ((poly64_t*) __ptr, vcombine_p64 (__d0, __d1));
+#else
+  vst1q_p64 ((poly64_t*) __ptr, vreinterpretq_p64_p128 (__val));
+#endif
+}
+
+/* The vceq_p64 intrinsic does not map to a single instruction.
+   Instead we emulate it by performing a 32-bit variant of the vceq
+   and applying a pairwise min reduction to the result.
+   vceq_u32 will produce two 32-bit halves, each of which will contain either
+   all ones or all zeros depending on whether the corresponding 32-bit
+   halves of the poly64_t were equal.  The whole poly64_t values are equal
+   if and only if both halves are equal, i.e. vceq_u32 returns all ones.
+   If the result is all zeroes for any half then the whole result is zeroes.
+   This is what the pairwise min reduction achieves.  */
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceq_p64 (poly64x1_t __a, poly64x1_t __b)
+{
+  uint32x2_t __t_a = vreinterpret_u32_p64 (__a);
+  uint32x2_t __t_b = vreinterpret_u32_p64 (__b);
+  uint32x2_t __c = vceq_u32 (__t_a, __t_b);
+  uint32x2_t __m = vpmin_u32 (__c, __c);
+  return vreinterpret_u64_u32 (__m);
+}
+
+/* The vtst_p64 intrinsic does not map to a single instruction.
+   We emulate it in way similar to vceq_p64 above but here we do
+   a reduction with max since if any two corresponding bits
+   in the two poly64_t's match, then the whole result must be all ones.  */
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vtst_p64 (poly64x1_t __a, poly64x1_t __b)
+{
+  uint32x2_t __t_a = vreinterpret_u32_p64 (__a);
+  uint32x2_t __t_b = vreinterpret_u32_p64 (__b);
+  uint32x2_t __c = vtst_u32 (__t_a, __t_b);
+  uint32x2_t __m = vpmax_u32 (__c, __c);
+  return vreinterpret_u64_u32 (__m);
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vaeseq_u8 (uint8x16_t __data, uint8x16_t __key)
+{
+  return __builtin_arm_crypto_aese (__data, __key);
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vaesdq_u8 (uint8x16_t __data, uint8x16_t __key)
+{
+  return __builtin_arm_crypto_aesd (__data, __key);
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vaesmcq_u8 (uint8x16_t __data)
+{
+  return __builtin_arm_crypto_aesmc (__data);
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vaesimcq_u8 (uint8x16_t __data)
+{
+  return __builtin_arm_crypto_aesimc (__data);
+}
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vsha1h_u32 (uint32_t __hash_e)
+{
+  uint32x4_t __t = vdupq_n_u32 (0);
+  __t = vsetq_lane_u32 (__hash_e, __t, 0);
+  __t = __builtin_arm_crypto_sha1h (__t);
+  return vgetq_lane_u32 (__t, 0);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsha1cq_u32 (uint32x4_t __hash_abcd, uint32_t __hash_e, uint32x4_t __wk)
+{
+  uint32x4_t __t = vdupq_n_u32 (0);
+  __t = vsetq_lane_u32 (__hash_e, __t, 0);
+  return __builtin_arm_crypto_sha1c (__hash_abcd, __t, __wk);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsha1pq_u32 (uint32x4_t __hash_abcd, uint32_t __hash_e, uint32x4_t __wk)
+{
+  uint32x4_t __t = vdupq_n_u32 (0);
+  __t = vsetq_lane_u32 (__hash_e, __t, 0);
+  return __builtin_arm_crypto_sha1p (__hash_abcd, __t, __wk);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsha1mq_u32 (uint32x4_t __hash_abcd, uint32_t __hash_e, uint32x4_t __wk)
+{
+  uint32x4_t __t = vdupq_n_u32 (0);
+  __t = vsetq_lane_u32 (__hash_e, __t, 0);
+  return __builtin_arm_crypto_sha1m (__hash_abcd, __t, __wk);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsha1su0q_u32 (uint32x4_t __w0_3, uint32x4_t __w4_7, uint32x4_t __w8_11)
+{
+  return __builtin_arm_crypto_sha1su0 (__w0_3, __w4_7, __w8_11);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsha1su1q_u32 (uint32x4_t __tw0_3, uint32x4_t __w12_15)
+{
+  return __builtin_arm_crypto_sha1su1 (__tw0_3, __w12_15);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsha256hq_u32 (uint32x4_t __hash_abcd, uint32x4_t __hash_efgh, uint32x4_t __wk)
+{
+  return __builtin_arm_crypto_sha256h (__hash_abcd, __hash_efgh, __wk);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsha256h2q_u32 (uint32x4_t __hash_abcd, uint32x4_t __hash_efgh, uint32x4_t __wk)
+{
+  return __builtin_arm_crypto_sha256h2 (__hash_abcd, __hash_efgh, __wk);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsha256su0q_u32 (uint32x4_t __w0_3, uint32x4_t __w4_7)
+{
+  return __builtin_arm_crypto_sha256su0 (__w0_3, __w4_7);
+}
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vsha256su1q_u32 (uint32x4_t __tw0_3, uint32x4_t __w8_11, uint32x4_t __w12_15)
+{
+  return __builtin_arm_crypto_sha256su1 (__tw0_3, __w8_11, __w12_15);
+}
+
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vmull_p64 (poly64_t __a, poly64_t __b)
+{
+  return (poly128_t) __builtin_arm_crypto_vmullp64 ((uint64_t) __a, (uint64_t) __b);
+}
+
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vmull_high_p64 (poly64x2_t __a, poly64x2_t __b)
+{
+  poly64_t __t1 = vget_high_p64 (__a);
+  poly64_t __t2 = vget_high_p64 (__b);
+
+  return (poly128_t) __builtin_arm_crypto_vmullp64 ((uint64_t) __t1, (uint64_t) __t2);
+}
+
+#endif
+"
Index: b/src/gcc/config/arm/constraints.md
===================================================================
--- a/src/gcc/config/arm/constraints.md
+++ b/src/gcc/config/arm/constraints.md
@@ -21,7 +21,7 @@
 ;; The following register constraints have been used:
 ;; - in ARM/Thumb-2 state: t, w, x, y, z
 ;; - in Thumb state: h, b
-;; - in both states: l, c, k
+;; - in both states: l, c, k, q, US
 ;; In ARM state, 'l' is an alias for 'r'
 ;; 'f' and 'v' were previously used for FPA and MAVERICK registers.
 
@@ -86,6 +86,9 @@
 (define_register_constraint "k" "STACK_REG"
  "@internal The stack register.")
 
+(define_register_constraint "q" "(TARGET_ARM && TARGET_LDRD) ? CORE_REGS : GENERAL_REGS"
+  "@internal In ARM state with LDRD support, core registers, otherwise general registers.")
+
 (define_register_constraint "b" "TARGET_THUMB ? BASE_REGS : NO_REGS"
  "@internal
   Thumb only.  The union of the low registers and the stack register.")
@@ -93,6 +96,9 @@
 (define_register_constraint "c" "CC_REG"
  "@internal The condition code register.")
 
+(define_register_constraint "Cs" "CALLER_SAVE_REGS"
+ "@internal The caller save registers.  Useful for sibcalls.")
+
 (define_constraint "I"
  "In ARM/Thumb-2 state a constant that can be used as an immediate value in a
   Data Processing instruction.  In Thumb-1 state a constant in the range
@@ -164,9 +170,9 @@
   		    && ival > 1020 && ival <= 1275")))
 
 (define_constraint "Pd"
-  "@internal In Thumb-1 state a constant in the range 0 to 7"
+  "@internal In Thumb state a constant in the range 0 to 7"
   (and (match_code "const_int")
-       (match_test "TARGET_THUMB1 && ival >= 0 && ival <= 7")))
+       (match_test "TARGET_THUMB && ival >= 0 && ival <= 7")))
 
 (define_constraint "Pe"
   "@internal In Thumb-1 state a constant in the range 256 to +510"
@@ -208,6 +214,11 @@
   (and (match_code "const_int")
        (match_test "TARGET_THUMB2 && ival >= 0 && ival <= 255")))
 
+(define_constraint "Pz"
+  "@internal In Thumb-2 state the constant 0"
+  (and (match_code "const_int")
+       (match_test "TARGET_THUMB2 && (ival == 0)")))
+
 (define_constraint "G"
  "In ARM/Thumb-2 state the floating-point constant 0."
  (and (match_code "const_double")
@@ -248,6 +259,24 @@
  (and (match_code "const_int")
       (match_test "TARGET_32BIT && const_ok_for_dimode_op (ival, PLUS)")))
 
+(define_constraint "De"
+ "@internal
+  In ARM/Thumb-2 state a const_int that can be used by insn anddi."
+ (and (match_code "const_int")
+      (match_test "TARGET_32BIT && const_ok_for_dimode_op (ival, AND)")))
+
+(define_constraint "Df"
+ "@internal
+  In ARM/Thumb-2 state a const_int that can be used by insn iordi."
+ (and (match_code "const_int")
+      (match_test "TARGET_32BIT && const_ok_for_dimode_op (ival, IOR)")))
+
+(define_constraint "Dg"
+ "@internal
+  In ARM/Thumb-2 state a const_int that can be used by insn xordi."
+ (and (match_code "const_int")
+      (match_test "TARGET_32BIT && const_ok_for_dimode_op (ival, XOR)")))
+
 (define_constraint "Di"
  "@internal
   In ARM/Thumb-2 state a const_int or const_double where both the high
@@ -305,6 +334,9 @@
   (and (match_code "const_double")
        (match_test "TARGET_32BIT && TARGET_VFP && vfp3_const_double_for_fract_bits (op)")))
 
+(define_register_constraint "Ts" "(arm_restrict_it) ? LO_REGS : GENERAL_REGS"
+ "For arm_restrict_it the core registers @code{r0}-@code{r7}.  GENERAL_REGS otherwise.")
+
 (define_memory_constraint "Ua"
  "@internal
   An address valid for loading/storing register exclusive"
@@ -392,9 +424,16 @@
 						   0)
 		   && GET_CODE (XEXP (op, 0)) != POST_INC")))
 
+(define_constraint "US"
+ "@internal
+  US is a symbol reference."
+ (match_code "symbol_ref")
+)
+
 ;; We used to have constraint letters for S and R in ARM state, but
 ;; all uses of these now appear to have been removed.
 
 ;; Additionally, we used to have a Q constraint in Thumb state, but
 ;; this wasn't really a valid memory constraint.  Again, all uses of
 ;; this now seem to have been removed.
+
Index: b/src/gcc/config/arm/cortex-a7.md
===================================================================
--- a/src/gcc/config/arm/cortex-a7.md
+++ b/src/gcc/config/arm/cortex-a7.md
@@ -20,6 +20,45 @@
 ;; along with GCC; see the file COPYING3.  If not see
 ;; <http://www.gnu.org/licenses/>.
 
+(define_attr "cortex_a7_neon_type"
+  "neon_mul, neon_mla, neon_other"
+  (cond [
+          (eq_attr "type" "neon_mul_b, neon_mul_b_q,\
+	                   neon_mul_h, neon_mul_h_q,\
+			   neon_mul_s, neon_mul_s_q,\
+			   neon_mul_b_long, neon_mul_h_long,\
+			   neon_mul_s_long, neon_mul_h_scalar,\
+			   neon_mul_h_scalar_q, neon_mul_s_scalar,\
+			   neon_mul_s_scalar_q, neon_mul_h_scalar_long,\
+			   neon_mul_s_scalar_long,\
+			   neon_sat_mul_b, neon_sat_mul_b_q,\
+			   neon_sat_mul_h, neon_sat_mul_h_q,\
+			   neon_sat_mul_s, neon_sat_mul_s_q,\
+			   neon_sat_mul_b_long, neon_sat_mul_h_long,\
+			   neon_sat_mul_s_long,\
+			   neon_sat_mul_h_scalar, neon_sat_mul_h_scalar_q,\
+			   neon_sat_mul_s_scalar, neon_sat_mul_s_scalar_q,\
+			   neon_sat_mul_h_scalar_long,\
+			   neon_sat_mul_s_scalar_long,\
+			   neon_fp_mul_s, neon_fp_mul_s_q,\
+			   neon_fp_mul_s_scalar, neon_fp_mul_s_scalar_q")
+             (const_string "neon_mul")
+          (eq_attr "type" "neon_mla_b, neon_mla_b_q, neon_mla_h,\
+	                   neon_mla_h_q, neon_mla_s, neon_mla_s_q,\
+			   neon_mla_b_long, neon_mla_h_long,\
+                           neon_mla_s_long,\
+			   neon_mla_h_scalar, neon_mla_h_scalar_q,\
+			   neon_mla_s_scalar, neon_mla_s_scalar_q,\
+			   neon_mla_h_scalar_long, neon_mla_s_scalar_long,\
+			   neon_sat_mla_b_long, neon_sat_mla_h_long,\
+			   neon_sat_mla_s_long,\
+			   neon_sat_mla_h_scalar_long,\
+                           neon_sat_mla_s_scalar_long,\
+			   neon_fp_mla_s, neon_fp_mla_s_q,\
+			   neon_fp_mla_s_scalar, neon_fp_mla_s_scalar_q")
+             (const_string "neon_mla")]
+           (const_string "neon_other")))
+
 (define_automaton "cortex_a7")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -67,8 +106,7 @@
 
 (define_insn_reservation "cortex_a7_branch" 0
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "branch")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "branch"))
   "(cortex_a7_ex2|cortex_a7_ex1)+cortex_a7_branch")
 
 ;; Call cannot dual-issue as an older instruction. It can dual-issue
@@ -77,8 +115,7 @@
 ;; cycle.
 (define_insn_reservation "cortex_a7_call" 1
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "call")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "call"))
   "(cortex_a7_ex2|cortex_a7_both)+cortex_a7_branch")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -88,25 +125,31 @@
 ;; ALU instruction with an immediate operand can dual-issue.
 (define_insn_reservation "cortex_a7_alu_imm" 2
   (and (eq_attr "tune" "cortexa7")
-       (and (ior (eq_attr "type" "simple_alu_imm")
-                 (ior (eq_attr "type" "simple_alu_shift")
-                      (and (eq_attr "insn" "mov")
-                           (not (eq_attr "length" "8")))))
-            (eq_attr "neon_type" "none")))
+       (ior (eq_attr "type" "adr,alu_imm,alus_imm,logic_imm,logics_imm,\
+                             mov_imm,mvn_imm,extend")
+            (and (eq_attr "type" "mov_reg,mov_shift,mov_shift_reg")
+                 (not (eq_attr "length" "8")))))
   "cortex_a7_ex2|cortex_a7_ex1")
 
 ;; ALU instruction with register operands can dual-issue
 ;; with a younger immediate-based instruction.
 (define_insn_reservation "cortex_a7_alu_reg" 2
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "alu_reg")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "alu_reg,alus_reg,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        bfm,rev,\
+                        shift_imm,shift_reg,mov_reg,mvn_reg"))
   "cortex_a7_ex1")
 
 (define_insn_reservation "cortex_a7_alu_shift" 2
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "alu_shift,alu_shift_reg")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "alu_shift_imm,alus_shift_imm,\
+                        logic_shift_imm,logics_shift_imm,\
+                        alu_shift_reg,alus_shift_reg,\
+                        logic_shift_reg,logics_shift_reg,\
+                        mov_shift,mov_shift_reg,\
+                        mvn_shift,mvn_shift_reg,\
+                        mrs,multiple,no_insn"))
   "cortex_a7_ex1")
 
 ;; Forwarding path for unshifted operands.
@@ -127,8 +170,8 @@
 
 (define_insn_reservation "cortex_a7_mul" 2
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "mult")
-            (eq_attr "neon_type" "none")))
+       (ior (eq_attr "mul32" "yes")
+            (eq_attr "mul64" "yes")))
   "cortex_a7_both")
 
 ;; Forward the result of a multiply operation to the accumulator 
@@ -140,7 +183,7 @@
 ;; The latency depends on the operands, so we use an estimate here.
 (define_insn_reservation "cortex_a7_idiv" 5
   (and (eq_attr "tune" "cortexa7")
-       (eq_attr "insn" "udiv,sdiv"))
+       (eq_attr "type" "udiv,sdiv"))
   "cortex_a7_both*5")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -153,50 +196,42 @@
 
 (define_insn_reservation "cortex_a7_load1" 2
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "load_byte,load1")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "load_byte,load1"))
   "cortex_a7_ex1")
 
 (define_insn_reservation "cortex_a7_store1" 0
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "store1")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "store1"))
   "cortex_a7_ex1")
 
 (define_insn_reservation "cortex_a7_load2" 2
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "load2")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "load2"))
   "cortex_a7_both")
 
 (define_insn_reservation "cortex_a7_store2" 0
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "store2")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "store2"))
   "cortex_a7_both")
 
 (define_insn_reservation "cortex_a7_load3" 3
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "load3")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "load3"))
   "cortex_a7_both, cortex_a7_ex1")
 
 (define_insn_reservation "cortex_a7_store3" 0
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "store4")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "store4"))
   "cortex_a7_both, cortex_a7_ex1")
 
 (define_insn_reservation "cortex_a7_load4" 3
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "load4")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "load4"))
   "cortex_a7_both, cortex_a7_both")
 
 (define_insn_reservation "cortex_a7_store4" 0
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "store3")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "store3"))
   "cortex_a7_both, cortex_a7_both")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -208,9 +243,8 @@
 
 (define_insn_reservation "cortex_a7_fpalu" 4
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "ffariths, fadds, ffarithd, faddd, fcpys,\
-                             f_cvt, fcmps, fcmpd")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "ffariths, fadds, ffarithd, faddd, fmov,\
+                        f_cvt, f_cvtf2i, f_cvti2f, fcmps, fcmpd"))
   "cortex_a7_ex1+cortex_a7_fpadd_pipe")
 
 ;; For fconsts and fconstd, 8-bit immediate data is passed directly from
@@ -218,8 +252,7 @@
 
 (define_insn_reservation "cortex_a7_fconst" 3
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "fconsts,fconstd")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "fconsts,fconstd"))
   "cortex_a7_ex1+cortex_a7_fpadd_pipe")
 
 ;; We should try not to attempt to issue a single-precision multiplication in
@@ -228,40 +261,22 @@
 
 (define_insn_reservation "cortex_a7_fpmuls" 4
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "fmuls")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "fmuls"))
   "cortex_a7_ex1+cortex_a7_fpmul_pipe")
 
 (define_insn_reservation "cortex_a7_neon_mul" 4
   (and (eq_attr "tune" "cortexa7")
-       (eq_attr "neon_type"
-                "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-                 neon_mul_qqq_8_16_32_ddd_32,\
-                 neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar,\
-                 neon_mul_ddd_16_scalar_32_16_long_scalar,\
-                 neon_mul_qqd_32_scalar,\
-                 neon_fp_vmul_ddd,\
-                 neon_fp_vmul_qqd"))
+       (eq_attr "cortex_a7_neon_type" "neon_mul"))
   "(cortex_a7_both+cortex_a7_fpmul_pipe)*2")
 
 (define_insn_reservation "cortex_a7_fpmacs" 8
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "fmacs,ffmas")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "fmacs,ffmas"))
   "cortex_a7_ex1+cortex_a7_fpmul_pipe")
 
 (define_insn_reservation "cortex_a7_neon_mla" 8
   (and (eq_attr "tune" "cortexa7")
-       (eq_attr "neon_type"
-                "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-                 neon_mla_qqq_8_16,\
-                 neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long,\
-                 neon_mla_qqq_32_qqd_32_scalar,\
-                 neon_mla_ddd_16_scalar_qdd_32_16_long_scalar,\
-                 neon_fp_vmla_ddd,\
-                 neon_fp_vmla_qqq,\
-                 neon_fp_vmla_ddd_scalar,\
-                 neon_fp_vmla_qqq_scalar"))
+       (eq_attr "cortex_a7_neon_type" "neon_mla"))
   "cortex_a7_both+cortex_a7_fpmul_pipe")
 
 (define_bypass 4 "cortex_a7_fpmacs,cortex_a7_neon_mla"
@@ -273,20 +288,17 @@
 
 (define_insn_reservation "cortex_a7_fpmuld" 7
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "fmuld")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "fmuld"))
   "cortex_a7_ex1+cortex_a7_fpmul_pipe, cortex_a7_fpmul_pipe*3")
 
 (define_insn_reservation "cortex_a7_fpmacd" 11
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "fmacd")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "fmacd"))
   "cortex_a7_ex1+cortex_a7_fpmul_pipe, cortex_a7_fpmul_pipe*3")
 
 (define_insn_reservation "cortex_a7_fpfmad" 8
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "ffmad")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "ffmad"))
   "cortex_a7_ex1+cortex_a7_fpmul_pipe, cortex_a7_fpmul_pipe*4")
 
 (define_bypass 7 "cortex_a7_fpmacd"
@@ -299,14 +311,12 @@
 
 (define_insn_reservation "cortex_a7_fdivs" 16
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "fdivs")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "fdivs, fsqrts"))
   "cortex_a7_ex1+cortex_a7_fp_div_sqrt, cortex_a7_fp_div_sqrt * 13")
 
 (define_insn_reservation "cortex_a7_fdivd" 31
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "fdivd")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "fdivd, fsqrtd"))
   "cortex_a7_ex1+cortex_a7_fp_div_sqrt, cortex_a7_fp_div_sqrt * 28")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -317,14 +327,12 @@
 
 (define_insn_reservation "cortex_a7_r2f" 4
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "r_2_f")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "f_mcr,f_mcrr"))
   "cortex_a7_both")
 
 (define_insn_reservation "cortex_a7_f2r" 2
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "f_2_r")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "f_mrc,f_mrrc"))
   "cortex_a7_ex1")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -336,8 +344,7 @@
 
 (define_insn_reservation "cortex_a7_f_flags" 4
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "f_flag")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "f_flag"))
   "cortex_a7_ex1")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -346,26 +353,22 @@
 
 (define_insn_reservation "cortex_a7_f_loads" 4
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "f_loads")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "f_loads"))
   "cortex_a7_ex1")
 
 (define_insn_reservation "cortex_a7_f_loadd" 4
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "f_loadd")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "f_loadd"))
   "cortex_a7_both")
 
 (define_insn_reservation "cortex_a7_f_stores" 0
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "f_stores")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "f_stores"))
   "cortex_a7_ex1")
 
 (define_insn_reservation "cortex_a7_f_stored" 0
   (and (eq_attr "tune" "cortexa7")
-       (and (eq_attr "type" "f_stored")
-            (eq_attr "neon_type" "none")))
+       (eq_attr "type" "f_stored"))
   "cortex_a7_both")
 
 ;; Load-to-use for floating-point values has a penalty of one cycle,
@@ -386,22 +389,6 @@
 
 (define_insn_reservation "cortex_a7_neon" 4
   (and (eq_attr "tune" "cortexa7")
-       (eq_attr "neon_type"
-                "!none,\
-                  neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-                  neon_mul_qqq_8_16_32_ddd_32,\
-                  neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar,\
-                  neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-                  neon_mla_qqq_8_16,\
-                  neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long,\
-                  neon_mla_qqq_32_qqd_32_scalar,\
-                  neon_mul_ddd_16_scalar_32_16_long_scalar,\
-                  neon_mul_qqd_32_scalar,\
-                  neon_mla_ddd_16_scalar_qdd_32_16_long_scalar,\
-                  neon_fp_vmul_ddd,\
-                  neon_fp_vmul_qqd,\
-                  neon_fp_vmla_ddd,\
-                  neon_fp_vmla_qqq,\
-                  neon_fp_vmla_ddd_scalar,\
-                  neon_fp_vmla_qqq_scalar"))
+       (and (eq_attr "is_neon_type" "yes")
+            (eq_attr "cortex_a7_neon_type" "neon_other")))
   "cortex_a7_both*2")
Index: b/src/gcc/config/arm/aarch-common-protos.h
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/aarch-common-protos.h
@@ -0,0 +1,36 @@
+/* Function prototypes for instruction scheduling dependeoncy routines,
+   defined in aarch-common.c
+
+   Copyright (C) 1991-2013 Free Software Foundation, Inc.
+   Contributed by ARM Ltd.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+
+#ifndef GCC_AARCH_COMMON_PROTOS_H
+#define GCC_AARCH_COMMON_PROTOS_H
+
+extern int arm_early_load_addr_dep (rtx, rtx);
+extern int arm_early_store_addr_dep (rtx, rtx);
+extern int arm_mac_accumulator_is_mul_result (rtx, rtx);
+extern int arm_mac_accumulator_is_result (rtx, rtx);
+extern int arm_no_early_alu_shift_dep (rtx, rtx);
+extern int arm_no_early_alu_shift_value_dep (rtx, rtx);
+extern int arm_no_early_mul_dep (rtx, rtx);
+extern int arm_no_early_store_addr_dep (rtx, rtx);
+
+#endif /* GCC_AARCH_COMMON_PROTOS_H */
Index: b/src/gcc/config/arm/arm-arches.def
===================================================================
--- a/src/gcc/config/arm/arm-arches.def
+++ b/src/gcc/config/arm/arm-arches.def
@@ -53,6 +53,7 @@ ARM_ARCH("armv7-a", cortexa8,	7A,  FL_CO
 ARM_ARCH("armv7-r", cortexr4,	7R,  FL_CO_PROC |	      FL_FOR_ARCH7R)
 ARM_ARCH("armv7-m", cortexm3,	7M,  FL_CO_PROC |	      FL_FOR_ARCH7M)
 ARM_ARCH("armv7e-m", cortexm4,  7EM, FL_CO_PROC |	      FL_FOR_ARCH7EM)
-ARM_ARCH("armv8-a", cortexa15,  8A,  FL_CO_PROC |             FL_FOR_ARCH8A)
+ARM_ARCH("armv8-a", cortexa53,  8A,  FL_CO_PROC |             FL_FOR_ARCH8A)
+ARM_ARCH("armv8-a+crc",cortexa53, 8A,FL_CO_PROC | FL_CRC32  | FL_FOR_ARCH8A)
 ARM_ARCH("iwmmxt",  iwmmxt,     5TE, FL_LDSCHED | FL_STRONG | FL_FOR_ARCH5TE | FL_XSCALE | FL_IWMMXT)
 ARM_ARCH("iwmmxt2", iwmmxt2,    5TE, FL_LDSCHED | FL_STRONG | FL_FOR_ARCH5TE | FL_XSCALE | FL_IWMMXT | FL_IWMMXT2)
Index: b/src/gcc/config/arm/t-arm
===================================================================
--- a/src/gcc/config/arm/t-arm
+++ b/src/gcc/config/arm/t-arm
@@ -39,6 +39,7 @@ MD_INCLUDES=	$(srcdir)/config/arm/arm102
 		$(srcdir)/config/arm/cortex-a8-neon.md \
 		$(srcdir)/config/arm/cortex-a9.md \
 		$(srcdir)/config/arm/cortex-a9-neon.md \
+		$(srcdir)/config/arm/cortex-a53.md \
 		$(srcdir)/config/arm/cortex-m4-fpu.md \
 		$(srcdir)/config/arm/cortex-m4.md \
 		$(srcdir)/config/arm/cortex-r4f.md \
@@ -52,6 +53,7 @@ MD_INCLUDES=	$(srcdir)/config/arm/arm102
 		$(srcdir)/config/arm/iwmmxt.md \
 		$(srcdir)/config/arm/iwmmxt2.md \
 		$(srcdir)/config/arm/ldmstm.md \
+		$(srcdir)/config/arm/ldrdstrd.md \
 		$(srcdir)/config/arm/marvell-f-iwmmxt.md \
 		$(srcdir)/config/arm/neon.md \
 		$(srcdir)/config/arm/predicates.md \
@@ -76,6 +78,11 @@ $(srcdir)/config/arm/arm-tables.opt: $(s
 	$(SHELL) $(srcdir)/config/arm/genopt.sh $(srcdir)/config/arm > \
 		$(srcdir)/config/arm/arm-tables.opt
 
+aarch-common.o: $(srcdir)/config/arm/aarch-common.c $(CONFIG_H) $(SYSTEM_H) \
+    coretypes.h $(TM_H) $(TM_P_H) $(RTL_H) $(TREE_H) output.h $(C_COMMON_H)
+	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
+		$(srcdir)/config/arm/aarch-common.c
+
 arm.o: $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
   $(RTL_H) $(TREE_H) $(OBSTACK_H) $(REGS_H) hard-reg-set.h \
   insn-config.h conditions.h output.h dumpfile.h \
@@ -84,7 +91,8 @@ arm.o: $(CONFIG_H) $(SYSTEM_H) coretypes
   $(GGC_H) except.h $(C_PRAGMA_H) $(TM_P_H) \
   $(TARGET_H) $(TARGET_DEF_H) debug.h langhooks.h $(DF_H) \
   intl.h libfuncs.h $(PARAMS_H) $(OPTS_H) $(srcdir)/config/arm/arm-cores.def \
-  $(srcdir)/config/arm/arm-arches.def $(srcdir)/config/arm/arm-fpus.def
+  $(srcdir)/config/arm/arm-arches.def $(srcdir)/config/arm/arm-fpus.def \
+  $(srcdir)/config/arm/arm_neon_builtins.def
 
 arm-c.o: $(srcdir)/config/arm/arm-c.c $(CONFIG_H) $(SYSTEM_H) \
     coretypes.h $(TM_H) $(TREE_H) output.h $(C_COMMON_H)
Index: b/src/gcc/config/arm/arm.opt
===================================================================
--- a/src/gcc/config/arm/arm.opt
+++ b/src/gcc/config/arm/arm.opt
@@ -239,6 +239,10 @@ mword-relocations
 Target Report Var(target_word_relocations) Init(TARGET_DEFAULT_WORD_RELOCATIONS)
 Only generate absolute relocations on word sized values.
 
+mrestrict-it
+Target Report Var(arm_restrict_it) Init(2)
+Generate IT blocks appropriate for ARMv8.
+
 mfix-cortex-m3-ldrd
 Target Report Var(fix_cm3_ldrd) Init(2)
 Avoid overlapping destination and address registers on LDRD instructions
@@ -247,3 +251,7 @@ that may trigger Cortex-M3 errata.
 munaligned-access
 Target Report Var(unaligned_access) Init(2)
 Enable unaligned word and halfword accesses to packed data.
+
+mneon-for-64bits
+Target Report RejectNegative Var(use_neon_for_64bits) Init(0)
+Use Neon to perform 64-bits operations rather than core registers.
Index: b/src/gcc/config/arm/arm926ejs.md
===================================================================
--- a/src/gcc/config/arm/arm926ejs.md
+++ b/src/gcc/config/arm/arm926ejs.md
@@ -58,7 +58,16 @@
 ;; ALU operations with no shifted operand
 (define_insn_reservation "9_alu_op" 1 
  (and (eq_attr "tune" "arm926ejs")
-      (eq_attr "type" "alu_reg,simple_alu_imm,simple_alu_shift,alu_shift"))
+      (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                       adr,bfm,rev,\
+                       alu_shift_imm,alus_shift_imm,\
+                       logic_shift_imm,logics_shift_imm,\
+                       shift_imm,shift_reg,extend,\
+                       mov_imm,mov_reg,mov_shift,\
+                       mvn_imm,mvn_reg,mvn_shift,\
+                       multiple,no_insn"))
  "e,m,w")
 
 ;; ALU operations with a shift-by-register operand
@@ -67,7 +76,9 @@
 ;; the execute stage.
 (define_insn_reservation "9_alu_shift_reg_op" 2 
  (and (eq_attr "tune" "arm926ejs")
-      (eq_attr "type" "alu_shift_reg"))
+      (eq_attr "type" "alu_shift_reg,alus_shift_reg,\
+                       logic_shift_reg,logics_shift_reg,\
+                       mov_shift_reg,mvn_shift_reg"))
  "e*2,m,w")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -81,32 +92,32 @@
 
 (define_insn_reservation "9_mult1" 3
  (and (eq_attr "tune" "arm926ejs")
-      (eq_attr "insn" "smlalxy,mul,mla"))
+      (eq_attr "type" "smlalxy,mul,mla"))
  "e*2,m,w")
 
 (define_insn_reservation "9_mult2" 4
  (and (eq_attr "tune" "arm926ejs")
-      (eq_attr "insn" "muls,mlas"))
+      (eq_attr "type" "muls,mlas"))
  "e*3,m,w")
 
 (define_insn_reservation "9_mult3" 4
  (and (eq_attr "tune" "arm926ejs")
-      (eq_attr "insn" "umull,umlal,smull,smlal"))
+      (eq_attr "type" "umull,umlal,smull,smlal"))
  "e*3,m,w")
 
 (define_insn_reservation "9_mult4" 5
  (and (eq_attr "tune" "arm926ejs")
-      (eq_attr "insn" "umulls,umlals,smulls,smlals"))
+      (eq_attr "type" "umulls,umlals,smulls,smlals"))
  "e*4,m,w")
 
 (define_insn_reservation "9_mult5" 2
  (and (eq_attr "tune" "arm926ejs")
-      (eq_attr "insn" "smulxy,smlaxy,smlawx"))
+      (eq_attr "type" "smulxy,smlaxy,smlawx"))
  "e,m,w")
 
 (define_insn_reservation "9_mult6" 3
  (and (eq_attr "tune" "arm926ejs")
-      (eq_attr "insn" "smlalxy"))
+      (eq_attr "type" "smlalxy"))
  "e*2,m,w")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Index: b/src/gcc/config/arm/ldrdstrd.md
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/ldrdstrd.md
@@ -0,0 +1,260 @@
+;; ARM ldrd/strd peephole optimizations.
+;;
+;; Copyright (C) 2013 Free Software Foundation, Inc.
+;;
+;; Written by Greta Yorsh <greta.yorsh@arm.com>
+
+;; This file is part of GCC.
+;;
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published by
+;; the Free Software Foundation; either version 3, or (at your option)
+;; any later version.
+;;
+;; GCC is distributed in the hope that it will be useful, but
+;; WITHOUT ANY WARRANTY; without even the implied warranty of
+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+;; General Public License for more details.
+;;
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+;; The following peephole optimizations identify consecutive memory
+;; accesses, and try to rearrange the operands to enable generation of
+;; ldrd/strd.
+
+(define_peephole2 ; ldrd
+  [(set (match_operand:SI 0 "arm_general_register_operand" "")
+        (match_operand:SI 2 "memory_operand" ""))
+   (set (match_operand:SI 1 "arm_general_register_operand" "")
+        (match_operand:SI 3 "memory_operand" ""))]
+  "TARGET_LDRD
+     && current_tune->prefer_ldrd_strd
+     && !optimize_function_for_size_p (cfun)"
+  [(const_int 0)]
+{
+  if (!gen_operands_ldrd_strd (operands, true, false, false))
+    FAIL;
+  else if (TARGET_ARM)
+  {
+    /* In ARM state, the destination registers of LDRD/STRD must be
+       consecutive. We emit DImode access.  */
+    operands[0] = gen_rtx_REG (DImode, REGNO (operands[0]));
+    operands[2] = adjust_address (operands[2], DImode, 0);
+    /* Emit [(set (match_dup 0) (match_dup 2))] */
+    emit_insn (gen_rtx_SET (VOIDmode, operands[0], operands[2]));
+    DONE;
+  }
+  else if (TARGET_THUMB2)
+  {
+    /* Emit the pattern:
+       [(parallel [(set (match_dup 0) (match_dup 2))
+                   (set (match_dup 1) (match_dup 3))])] */
+    rtx t1 = gen_rtx_SET (VOIDmode, operands[0], operands[2]);
+    rtx t2 = gen_rtx_SET (VOIDmode, operands[1], operands[3]);
+    emit_insn (gen_rtx_PARALLEL (VOIDmode, gen_rtvec (2, t1, t2)));
+    DONE;
+  }
+})
+
+(define_peephole2 ; strd
+  [(set (match_operand:SI 2 "memory_operand" "")
+	(match_operand:SI 0 "arm_general_register_operand" ""))
+   (set (match_operand:SI 3 "memory_operand" "")
+	(match_operand:SI 1 "arm_general_register_operand" ""))]
+  "TARGET_LDRD
+     && current_tune->prefer_ldrd_strd
+     && !optimize_function_for_size_p (cfun)"
+  [(const_int 0)]
+{
+  if (!gen_operands_ldrd_strd (operands, false, false, false))
+    FAIL;
+  else if (TARGET_ARM)
+  {
+    /* In ARM state, the destination registers of LDRD/STRD must be
+       consecutive. We emit DImode access.  */
+    operands[0] = gen_rtx_REG (DImode, REGNO (operands[0]));
+    operands[2] = adjust_address (operands[2], DImode, 0);
+    /* Emit [(set (match_dup 2) (match_dup 0))]  */
+    emit_insn (gen_rtx_SET (VOIDmode, operands[2], operands[0]));
+    DONE;
+  }
+  else if (TARGET_THUMB2)
+  {
+    /* Emit the pattern:
+       [(parallel [(set (match_dup 2) (match_dup 0))
+                   (set (match_dup 3) (match_dup 1))])]  */
+    rtx t1 = gen_rtx_SET (VOIDmode, operands[2], operands[0]);
+    rtx t2 = gen_rtx_SET (VOIDmode, operands[3], operands[1]);
+    emit_insn (gen_rtx_PARALLEL (VOIDmode, gen_rtvec (2, t1, t2)));
+    DONE;
+  }
+})
+
+;; The following peepholes reorder registers to enable LDRD/STRD.
+(define_peephole2 ; strd of constants
+  [(set (match_operand:SI 0 "arm_general_register_operand" "")
+        (match_operand:SI 4 "const_int_operand" ""))
+   (set (match_operand:SI 2 "memory_operand" "")
+        (match_dup 0))
+   (set (match_operand:SI 1 "arm_general_register_operand" "")
+        (match_operand:SI 5 "const_int_operand" ""))
+   (set (match_operand:SI 3 "memory_operand" "")
+        (match_dup 1))]
+ "TARGET_LDRD
+  && current_tune->prefer_ldrd_strd
+  && !optimize_function_for_size_p (cfun)"
+  [(const_int 0)]
+{
+  if (!gen_operands_ldrd_strd (operands, false, true, false))
+    FAIL;
+  else if (TARGET_ARM)
+  {
+   rtx tmp = gen_rtx_REG (DImode, REGNO (operands[0]));
+   operands[2] = adjust_address (operands[2], DImode, 0);
+   /* Emit the pattern:
+      [(set (match_dup 0) (match_dup 4))
+      (set (match_dup 1) (match_dup 5))
+      (set (match_dup 2) tmp)]  */
+   emit_insn (gen_rtx_SET (VOIDmode, operands[0], operands[4]));
+   emit_insn (gen_rtx_SET (VOIDmode, operands[1], operands[5]));
+   emit_insn (gen_rtx_SET (VOIDmode, operands[2], tmp));
+   DONE;
+  }
+  else if (TARGET_THUMB2)
+  {
+    /* Emit the pattern:
+       [(set (match_dup 0) (match_dup 4))
+        (set (match_dup 1) (match_dup 5))
+        (parallel [(set (match_dup 2) (match_dup 0))
+                   (set (match_dup 3) (match_dup 1))])]  */
+    emit_insn (gen_rtx_SET (VOIDmode, operands[0], operands[4]));
+    emit_insn (gen_rtx_SET (VOIDmode, operands[1], operands[5]));
+    rtx t1 = gen_rtx_SET (VOIDmode, operands[2], operands[0]);
+    rtx t2 = gen_rtx_SET (VOIDmode, operands[3], operands[1]);
+    emit_insn (gen_rtx_PARALLEL (VOIDmode, gen_rtvec (2, t1, t2)));
+    DONE;
+  }
+})
+
+(define_peephole2 ; strd of constants
+  [(set (match_operand:SI 0 "arm_general_register_operand" "")
+        (match_operand:SI 4 "const_int_operand" ""))
+   (set (match_operand:SI 1 "arm_general_register_operand" "")
+        (match_operand:SI 5 "const_int_operand" ""))
+   (set (match_operand:SI 2 "memory_operand" "")
+        (match_dup 0))
+   (set (match_operand:SI 3 "memory_operand" "")
+        (match_dup 1))]
+ "TARGET_LDRD
+  && current_tune->prefer_ldrd_strd
+  && !optimize_function_for_size_p (cfun)"
+   [(const_int 0)]
+{
+  if (!gen_operands_ldrd_strd (operands, false, true, false))
+     FAIL;
+  else if (TARGET_ARM)
+  {
+   rtx tmp = gen_rtx_REG (DImode, REGNO (operands[0]));
+   operands[2] = adjust_address (operands[2], DImode, 0);
+   /* Emit the pattern
+      [(set (match_dup 0) (match_dup 4))
+       (set (match_dup 1) (match_dup 5))
+       (set (match_dup 2) tmp)]  */
+   emit_insn (gen_rtx_SET (VOIDmode, operands[0], operands[4]));
+   emit_insn (gen_rtx_SET (VOIDmode, operands[1], operands[5]));
+   emit_insn (gen_rtx_SET (VOIDmode, operands[2], tmp));
+   DONE;
+  }
+  else if (TARGET_THUMB2)
+  {
+    /*  Emit the pattern:
+        [(set (match_dup 0) (match_dup 4))
+         (set (match_dup 1) (match_dup 5))
+         (parallel [(set (match_dup 2) (match_dup 0))
+                    (set (match_dup 3) (match_dup 1))])]  */
+    emit_insn (gen_rtx_SET (VOIDmode, operands[0], operands[4]));
+    emit_insn (gen_rtx_SET (VOIDmode, operands[1], operands[5]));
+    rtx t1 = gen_rtx_SET (VOIDmode, operands[2], operands[0]);
+    rtx t2 = gen_rtx_SET (VOIDmode, operands[3], operands[1]);
+    emit_insn (gen_rtx_PARALLEL (VOIDmode, gen_rtvec (2, t1, t2)));
+    DONE;
+  }
+})
+
+;; The following two peephole optimizations are only relevant for ARM
+;; mode where LDRD/STRD require consecutive registers.
+
+(define_peephole2 ; swap the destination registers of two loads
+		  ; before a commutative operation.
+  [(set (match_operand:SI 0 "arm_general_register_operand" "")
+        (match_operand:SI 2 "memory_operand" ""))
+   (set (match_operand:SI 1 "arm_general_register_operand" "")
+        (match_operand:SI 3 "memory_operand" ""))
+   (set (match_operand:SI 4 "arm_general_register_operand" "")
+        (match_operator:SI 5 "commutative_binary_operator"
+			   [(match_operand 6 "arm_general_register_operand" "")
+			    (match_operand 7 "arm_general_register_operand" "") ]))]
+  "TARGET_LDRD && TARGET_ARM
+   && current_tune->prefer_ldrd_strd
+   && !optimize_function_for_size_p (cfun)
+   && (  ((rtx_equal_p(operands[0], operands[6])) && (rtx_equal_p(operands[1], operands[7])))
+        ||((rtx_equal_p(operands[0], operands[7])) && (rtx_equal_p(operands[1], operands[6]))))
+   && (peep2_reg_dead_p (3, operands[0]) || rtx_equal_p (operands[0], operands[4]))
+   && (peep2_reg_dead_p (3, operands[1]) || rtx_equal_p (operands[1], operands[4]))"
+  [(set (match_dup 0) (match_dup 2))
+   (set (match_dup 4) (match_op_dup 5 [(match_dup 6) (match_dup 7)]))]
+  {
+    if (!gen_operands_ldrd_strd (operands, true, false, true))
+     {
+        FAIL;
+     }
+    else
+     {
+        operands[0] = gen_rtx_REG (DImode, REGNO (operands[0]));
+        operands[2] = adjust_address (operands[2], DImode, 0);
+     }
+   }
+)
+
+(define_peephole2 ; swap the destination registers of two loads
+		  ; before a commutative operation that sets the flags.
+  [(set (match_operand:SI 0 "arm_general_register_operand" "")
+        (match_operand:SI 2 "memory_operand" ""))
+   (set (match_operand:SI 1 "arm_general_register_operand" "")
+        (match_operand:SI 3 "memory_operand" ""))
+   (parallel
+      [(set (match_operand:SI 4 "arm_general_register_operand" "")
+	    (match_operator:SI 5 "commutative_binary_operator"
+			       [(match_operand 6 "arm_general_register_operand" "")
+				(match_operand 7 "arm_general_register_operand" "") ]))
+       (clobber (reg:CC CC_REGNUM))])]
+  "TARGET_LDRD && TARGET_ARM
+   && current_tune->prefer_ldrd_strd
+   && !optimize_function_for_size_p (cfun)
+   && (  ((rtx_equal_p(operands[0], operands[6])) && (rtx_equal_p(operands[1], operands[7])))
+       ||((rtx_equal_p(operands[0], operands[7])) && (rtx_equal_p(operands[1], operands[6]))))
+   && (peep2_reg_dead_p (3, operands[0]) || rtx_equal_p (operands[0], operands[4]))
+   && (peep2_reg_dead_p (3, operands[1]) || rtx_equal_p (operands[1], operands[4]))"
+  [(set (match_dup 0) (match_dup 2))
+   (parallel
+      [(set (match_dup 4)
+	    (match_op_dup 5 [(match_dup 6) (match_dup 7)]))
+       (clobber (reg:CC CC_REGNUM))])]
+  {
+    if (!gen_operands_ldrd_strd (operands, true, false, true))
+     {
+        FAIL;
+     }
+    else
+     {
+        operands[0] = gen_rtx_REG (DImode, REGNO (operands[0]));
+        operands[2] = adjust_address (operands[2], DImode, 0);
+     }
+   }
+)
+
+;; TODO: Handle LDRD/STRD with writeback:
+;; (a) memory operands can be POST_INC, POST_DEC, PRE_MODIFY, POST_MODIFY
+;; (b) Patterns may be followed by an update of the base address.
Index: b/src/gcc/config/arm/predicates.md
===================================================================
--- a/src/gcc/config/arm/predicates.md
+++ b/src/gcc/config/arm/predicates.md
@@ -31,6 +31,28 @@
 	      || REGNO_REG_CLASS (REGNO (op)) != NO_REGS));
 })
 
+(define_predicate "imm_for_neon_inv_logic_operand"
+  (match_code "const_vector")
+{
+  return (TARGET_NEON
+          && neon_immediate_valid_for_logic (op, mode, 1, NULL, NULL));
+})
+
+(define_predicate "neon_inv_logic_op2"
+  (ior (match_operand 0 "imm_for_neon_inv_logic_operand")
+       (match_operand 0 "s_register_operand")))
+
+(define_predicate "imm_for_neon_logic_operand"
+  (match_code "const_vector")
+{
+  return (TARGET_NEON
+          && neon_immediate_valid_for_logic (op, mode, 0, NULL, NULL));
+})
+
+(define_predicate "neon_logic_op2"
+  (ior (match_operand 0 "imm_for_neon_logic_operand")
+       (match_operand 0 "s_register_operand")))
+
 ;; Any general register.
 (define_predicate "arm_hard_general_register_operand"
   (match_code "reg")
@@ -151,6 +173,23 @@
   (ior (match_operand 0 "arm_rhs_operand")
        (match_operand 0 "arm_neg_immediate_operand")))
 
+(define_predicate "arm_anddi_operand_neon"
+  (ior (match_operand 0 "s_register_operand")
+       (and (match_code "const_int")
+	    (match_test "const_ok_for_dimode_op (INTVAL (op), AND)"))
+       (match_operand 0 "neon_inv_logic_op2")))
+
+(define_predicate "arm_iordi_operand_neon"
+  (ior (match_operand 0 "s_register_operand")
+       (and (match_code "const_int")
+	    (match_test "const_ok_for_dimode_op (INTVAL (op), IOR)"))
+       (match_operand 0 "neon_logic_op2")))
+
+(define_predicate "arm_xordi_operand"
+  (ior (match_operand 0 "s_register_operand")
+       (and (match_code "const_int")
+	    (match_test "const_ok_for_dimode_op (INTVAL (op), XOR)"))))
+
 (define_predicate "arm_adddi_operand"
   (ior (match_operand 0 "s_register_operand")
        (and (match_code "const_int")
@@ -213,6 +252,10 @@
   (and (match_code "plus,minus,ior,xor,and")
        (match_test "mode == GET_MODE (op)")))
 
+(define_special_predicate "shiftable_operator_strict_it"
+  (and (match_code "plus,and")
+       (match_test "mode == GET_MODE (op)")))
+
 ;; True for logical binary operators.
 (define_special_predicate "logical_binary_operator"
   (and (match_code "ior,xor,and")
@@ -276,6 +319,24 @@
 (define_special_predicate "lt_ge_comparison_operator"
   (match_code "lt,ge"))
 
+;; The vsel instruction only accepts the ARM condition codes listed below.
+(define_special_predicate "arm_vsel_comparison_operator"
+  (and (match_operand 0 "expandable_comparison_operator")
+       (match_test "maybe_get_arm_condition_code (op) == ARM_GE
+                    || maybe_get_arm_condition_code (op) == ARM_GT
+                    || maybe_get_arm_condition_code (op) == ARM_EQ
+                    || maybe_get_arm_condition_code (op) == ARM_VS
+                    || maybe_get_arm_condition_code (op) == ARM_LT
+                    || maybe_get_arm_condition_code (op) == ARM_LE
+                    || maybe_get_arm_condition_code (op) == ARM_NE
+                    || maybe_get_arm_condition_code (op) == ARM_VC")))
+
+(define_special_predicate "arm_cond_move_operator"
+  (if_then_else (match_test "arm_restrict_it")
+                (and (match_test "TARGET_FPU_ARMV8")
+                     (match_operand 0 "arm_vsel_comparison_operator"))
+                (match_operand 0 "expandable_comparison_operator")))
+
 (define_special_predicate "noov_comparison_operator"
   (match_code "lt,ge,eq,ne"))
 
@@ -512,28 +573,6 @@
   (ior (match_operand 0 "s_register_operand")
        (match_operand 0 "imm_for_neon_rshift_operand")))
 
-(define_predicate "imm_for_neon_logic_operand"
-  (match_code "const_vector")
-{
-  return (TARGET_NEON
-          && neon_immediate_valid_for_logic (op, mode, 0, NULL, NULL));
-})
-
-(define_predicate "imm_for_neon_inv_logic_operand"
-  (match_code "const_vector")
-{
-  return (TARGET_NEON
-          && neon_immediate_valid_for_logic (op, mode, 1, NULL, NULL));
-})
-
-(define_predicate "neon_logic_op2"
-  (ior (match_operand 0 "imm_for_neon_logic_operand")
-       (match_operand 0 "s_register_operand")))
-
-(define_predicate "neon_inv_logic_op2"
-  (ior (match_operand 0 "imm_for_neon_inv_logic_operand")
-       (match_operand 0 "s_register_operand")))
-
 ;; Predicates for named expanders that overlap multiple ISAs.
 
 (define_predicate "cmpdi_operand"
@@ -623,3 +662,7 @@
 (define_predicate "mem_noofs_operand"
   (and (match_code "mem")
        (match_code "reg" "0")))
+
+(define_predicate "call_insn_operand"
+  (ior (match_code "symbol_ref")
+       (match_operand 0 "s_register_operand")))
Index: b/src/gcc/config/arm/cortex-a15-neon.md
===================================================================
--- a/src/gcc/config/arm/cortex-a15-neon.md
+++ b/src/gcc/config/arm/cortex-a15-neon.md
@@ -17,6 +17,199 @@
 ;; along with GCC; see the file COPYING3.  If not see
 ;; <http://www.gnu.org/licenses/>.
 
+(define_attr "cortex_a15_neon_type"
+  "neon_abd, neon_abd_q, neon_arith_acc, neon_arith_acc_q,
+   neon_arith_basic, neon_arith_complex,
+   neon_reduc_add_acc, neon_multiply, neon_multiply_q,
+   neon_multiply_long, neon_mla, neon_mla_q, neon_mla_long,
+   neon_sat_mla_long, neon_shift_acc, neon_shift_imm_basic,\
+   neon_shift_imm_complex,
+   neon_shift_reg_basic, neon_shift_reg_basic_q, neon_shift_reg_complex,
+   neon_shift_reg_complex_q, neon_fp_negabs, neon_fp_arith,
+   neon_fp_arith_q, neon_fp_cvt_int,
+   neon_fp_cvt_int_q, neon_fp_cvt16, neon_fp_minmax, neon_fp_mul,
+   neon_fp_mul_q, neon_fp_mla, neon_fp_mla_q, neon_fp_recpe_rsqrte,
+   neon_fp_recpe_rsqrte_q, neon_bitops, neon_bitops_q, neon_from_gp,
+   neon_from_gp_q, neon_move, neon_tbl3_tbl4, neon_zip_q, neon_to_gp,
+   neon_load_a, neon_load_b, neon_load_c, neon_load_d, neon_load_e,
+   neon_load_f, neon_store_a, neon_store_b, neon_store_c, neon_store_d,
+   neon_store_e, neon_store_f, neon_store_g, neon_store_h,
+   unknown"
+  (cond [
+          (eq_attr "type" "neon_abd, neon_abd_long")
+            (const_string "neon_abd")
+          (eq_attr "type" "neon_abd_q")
+            (const_string "neon_abd_q")
+          (eq_attr "type" "neon_arith_acc, neon_reduc_add_acc,\
+                           neon_reduc_add_acc_q")
+            (const_string "neon_arith_acc")
+          (eq_attr "type" "neon_arith_acc_q")
+            (const_string "neon_arith_acc_q")
+          (eq_attr "type" "neon_add, neon_add_q, neon_add_long,\
+                           neon_add_widen, neon_neg, neon_neg_q,\
+                           neon_reduc_add, neon_reduc_add_q,\
+                           neon_reduc_add_long, neon_sub, neon_sub_q,\
+                           neon_sub_long, neon_sub_widen, neon_logic,\
+                           neon_logic_q, neon_tst, neon_tst_q")
+            (const_string "neon_arith_basic")
+          (eq_attr "type" "neon_abs, neon_abs_q, neon_add_halve_narrow_q,\
+                           neon_add_halve, neon_add_halve_q,\
+                           neon_sub_halve, neon_sub_halve_q, neon_qabs,\
+                           neon_qabs_q, neon_qadd, neon_qadd_q, neon_qneg,\
+                           neon_qneg_q, neon_qsub, neon_qsub_q,\
+                           neon_sub_halve_narrow_q,\
+                           neon_compare, neon_compare_q,\
+                           neon_compare_zero, neon_compare_zero_q,\
+                           neon_minmax, neon_minmax_q, neon_reduc_minmax,\
+                           neon_reduc_minmax_q")
+            (const_string "neon_arith_complex")
+
+          (eq_attr "type" "neon_mul_b, neon_mul_h, neon_mul_s,\
+                           neon_mul_h_scalar, neon_mul_s_scalar,\
+                           neon_sat_mul_b, neon_sat_mul_h,\
+                           neon_sat_mul_s, neon_sat_mul_h_scalar,\
+                           neon_sat_mul_s_scalar,\
+                           neon_mul_b_long, neon_mul_h_long,\
+                           neon_mul_s_long,\
+                           neon_mul_h_scalar_long, neon_mul_s_scalar_long,\
+                           neon_sat_mul_b_long, neon_sat_mul_h_long,\
+                           neon_sat_mul_s_long, neon_sat_mul_h_scalar_long,\
+                           neon_sat_mul_s_scalar_long")
+            (const_string "neon_multiply")
+          (eq_attr "type" "neon_mul_b_q, neon_mul_h_q, neon_mul_s_q,\
+                           neon_mul_h_scalar_q, neon_mul_s_scalar_q,\
+                           neon_sat_mul_b_q, neon_sat_mul_h_q,\
+                           neon_sat_mul_s_q, neon_sat_mul_h_scalar_q,\
+                           neon_sat_mul_s_scalar_q")
+            (const_string "neon_multiply_q")
+          (eq_attr "type" "neon_mla_b, neon_mla_h, neon_mla_s,\
+                           neon_mla_h_scalar, neon_mla_s_scalar,\
+                           neon_mla_b_long, neon_mla_h_long,\
+                           neon_mla_s_long,\
+                           neon_mla_h_scalar_long, neon_mla_s_scalar_long")
+            (const_string "neon_mla")
+          (eq_attr "type" "neon_mla_b_q, neon_mla_h_q, neon_mla_s_q,\
+                           neon_mla_h_scalar_q, neon_mla_s_scalar_q")
+            (const_string "neon_mla_q")
+          (eq_attr "type" "neon_sat_mla_b_long, neon_sat_mla_h_long,\
+                           neon_sat_mla_s_long, neon_sat_mla_h_scalar_long,\
+                           neon_sat_mla_s_scalar_long")
+            (const_string "neon_sat_mla_long")
+
+          (eq_attr "type" "neon_shift_acc, neon_shift_acc_q")
+            (const_string "neon_shift_acc")
+          (eq_attr "type" "neon_shift_imm, neon_shift_imm_q,\
+                           neon_shift_imm_narrow_q, neon_shift_imm_long")
+            (const_string "neon_shift_imm_basic")
+          (eq_attr "type" "neon_sat_shift_imm, neon_sat_shift_imm_q,\
+                           neon_sat_shift_imm_narrow_q")
+            (const_string "neon_shift_imm_complex")
+          (eq_attr "type" "neon_shift_reg")
+            (const_string "neon_shift_reg_basic")
+          (eq_attr "type" "neon_shift_reg_q")
+            (const_string "neon_shift_reg_basic_q")
+          (eq_attr "type" "neon_sat_shift_reg")
+            (const_string "neon_shift_reg_complex")
+          (eq_attr "type" "neon_sat_shift_reg_q")
+            (const_string "neon_shift_reg_complex_q")
+
+          (eq_attr "type" "neon_fp_neg_s, neon_fp_neg_s_q,\
+                           neon_fp_abs_s, neon_fp_abs_s_q")
+            (const_string "neon_fp_negabs")
+          (eq_attr "type" "neon_fp_addsub_s, neon_fp_abd_s,\
+                           neon_fp_reduc_add_s, neon_fp_compare_s,\
+                           neon_fp_minmax_s, neon_fp_minmax_s_q,\
+                           neon_fp_reduc_minmax_s, neon_fp_reduc_minmax_s_q")
+            (const_string "neon_fp_arith")
+          (eq_attr "type" "neon_fp_addsub_s_q, neon_fp_abd_s_q,\
+                           neon_fp_reduc_add_s_q, neon_fp_compare_s_q")
+            (const_string "neon_fp_arith_q")
+          (eq_attr "type" "neon_fp_to_int_s, neon_int_to_fp_s")
+            (const_string "neon_fp_cvt_int")
+          (eq_attr "type" "neon_fp_to_int_s_q, neon_int_to_fp_s_q")
+            (const_string "neon_fp_cvt_int_q")
+          (eq_attr "type" "neon_fp_cvt_narrow_s_q, neon_fp_cvt_widen_h")
+            (const_string "neon_fp_cvt16")
+          (eq_attr "type" "neon_fp_mul_s, neon_fp_mul_s_scalar")
+            (const_string "neon_fp_mul")
+          (eq_attr "type" "neon_fp_mul_s_q, neon_fp_mul_s_scalar_q")
+            (const_string "neon_fp_mul_q")
+          (eq_attr "type" "neon_fp_mla_s, neon_fp_mla_s_scalar")
+            (const_string "neon_fp_mla")
+          (eq_attr "type" "neon_fp_mla_s_q, neon_fp_mla_s_scalar_q")
+            (const_string "neon_fp_mla_q")
+          (eq_attr "type" "neon_fp_recpe_s, neon_fp_rsqrte_s")
+            (const_string "neon_fp_recpe_rsqrte")
+          (eq_attr "type" "neon_fp_recpe_s_q, neon_fp_rsqrte_s_q")
+            (const_string "neon_fp_recpe_rsqrte_q")
+
+          (eq_attr "type" "neon_bsl, neon_cls, neon_cnt,\
+                           neon_rev, neon_permute,\
+                           neon_tbl1, neon_tbl2, neon_zip,\
+                           neon_dup, neon_dup_q, neon_ext, neon_ext_q,\
+                           neon_move, neon_move_q, neon_move_narrow_q")
+            (const_string "neon_bitops")
+          (eq_attr "type" "neon_bsl_q, neon_cls_q, neon_cnt_q,\
+                           neon_rev_q, neon_permute_q")
+            (const_string "neon_bitops_q")
+          (eq_attr "type" "neon_from_gp")
+            (const_string "neon_from_gp")
+          (eq_attr "type" "neon_from_gp_q")
+            (const_string "neon_from_gp_q")
+          (eq_attr "type" "neon_tbl3, neon_tbl4")
+            (const_string "neon_tbl3_tbl4")
+          (eq_attr "type" "neon_zip_q")
+            (const_string "neon_zip_q")
+          (eq_attr "type" "neon_to_gp, neon_to_gp_q")
+            (const_string "neon_to_gp")
+
+          (eq_attr "type" "f_loads, f_loadd,\
+                           neon_load1_1reg, neon_load1_1reg_q,\
+                           neon_load1_2reg, neon_load1_2reg_q")
+            (const_string "neon_load_a")
+          (eq_attr "type" "neon_load1_3reg, neon_load1_3reg_q,\
+                           neon_load1_4reg, neon_load1_4reg_q")
+            (const_string "neon_load_b")
+          (eq_attr "type" "neon_load1_one_lane, neon_load1_one_lane_q,\
+                           neon_load1_all_lanes, neon_load1_all_lanes_q,\
+                           neon_load2_2reg, neon_load2_2reg_q,\
+                           neon_load2_all_lanes, neon_load2_all_lanes_q")
+            (const_string "neon_load_c")
+          (eq_attr "type" "neon_load2_4reg, neon_load2_4reg_q,\
+                           neon_load3_3reg, neon_load3_3reg_q,\
+                           neon_load3_one_lane, neon_load3_one_lane_q,\
+                           neon_load4_4reg, neon_load4_4reg_q")
+            (const_string "neon_load_d")
+          (eq_attr "type" "neon_load2_one_lane, neon_load2_one_lane_q,\
+                           neon_load3_all_lanes, neon_load3_all_lanes_q,\
+                           neon_load4_all_lanes, neon_load4_all_lanes_q")
+            (const_string "neon_load_e")
+          (eq_attr "type" "neon_load4_one_lane, neon_load4_one_lane_q")
+            (const_string "neon_load_f")
+
+          (eq_attr "type" "f_stores, f_stored,\
+                           neon_store1_1reg, neon_store1_1reg_q")
+            (const_string "neon_store_a")
+          (eq_attr "type" "neon_store1_2reg, neon_store1_2reg_q")
+            (const_string "neon_store_b")
+          (eq_attr "type" "neon_store1_3reg, neon_store1_3reg_q")
+            (const_string "neon_store_c")
+          (eq_attr "type" "neon_store1_4reg, neon_store1_4reg_q")
+            (const_string "neon_store_d")
+          (eq_attr "type" "neon_store1_one_lane, neon_store1_one_lane_q,\
+                           neon_store2_one_lane, neon_store2_one_lane_q")
+            (const_string "neon_store_e")
+          (eq_attr "type" "neon_store2_2reg, neon_store2_2reg_q,\
+                           neon_store3_one_lane, neon_store3_one_lane_q,\
+                           neon_store4_one_lane, neon_store4_one_lane_q")
+            (const_string "neon_store_f")
+          (eq_attr "type" "neon_store2_4reg, neon_store2_4reg_q,\
+                           neon_store4_4reg, neon_store4_4reg_q")
+            (const_string "neon_store_g")
+          (eq_attr "type" "neon_store3_3reg, neon_store3_3reg_q")
+            (const_string "neon_store_h")]
+          (const_string "unknown")))
+
 (define_automaton "cortex_a15_neon")
 
 ;; Dispatch unit.
@@ -91,392 +284,316 @@
 (define_reservation "ca15_cx_perm" "ca15_cx_ij|ca15_cx_ik")
 (define_reservation "ca15_cx_perm_2" "ca15_cx_ij+ca15_cx_ik")
 
-(define_insn_reservation  "cortex_a15_neon_int_1" 5
-  (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_int_1"))
-  "ca15_issue1,ca15_cx_ialu")
+;; Integer Arithmetic Instructions.
 
-(define_insn_reservation  "cortex_a15_neon_int_2" 5
+(define_insn_reservation  "cortex_a15_neon_abd" 5
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_int_2"))
+       (eq_attr "cortex_a15_neon_type" "neon_abd"))
   "ca15_issue1,ca15_cx_ialu")
 
-(define_insn_reservation  "cortex_a15_neon_int_3" 5
+(define_insn_reservation  "cortex_a15_neon_abd_q" 5
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_int_3"))
-  "ca15_issue1,ca15_cx_ialu")
+       (eq_attr "cortex_a15_neon_type" "neon_abd_q"))
+  "ca15_issue2,ca15_cx_ialu*2")
 
-(define_insn_reservation  "cortex_a15_neon_int_4" 5
+(define_insn_reservation  "cortex_a15_neon_aba" 7
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_int_4"))
-  "ca15_issue1,ca15_cx_ialu")
+       (eq_attr "cortex_a15_neon_type" "neon_arith_acc"))
+  "ca15_issue1,ca15_cx_ialu_with_acc")
 
-(define_insn_reservation  "cortex_a15_neon_int_5" 5
+(define_insn_reservation  "cortex_a15_neon_aba_q" 8
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_int_5"))
-  "ca15_issue1,ca15_cx_ialu")
+       (eq_attr "cortex_a15_neon_type" "neon_arith_acc_q"))
+  "ca15_issue2,ca15_cx_ialu_with_acc*2")
 
-(define_insn_reservation  "cortex_a15_neon_vqneg_vqabs" 5
+(define_insn_reservation  "cortex_a15_neon_arith_basic" 4
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_vqneg_vqabs"))
+       (eq_attr "cortex_a15_neon_type" "neon_arith_basic"))
   "ca15_issue1,ca15_cx_ialu")
 
-(define_insn_reservation  "cortex_a15_neon_vmov" 5
+(define_insn_reservation  "cortex_a15_neon_arith_complex" 5
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_vmov"))
+       (eq_attr "cortex_a15_neon_type" "neon_arith_complex"))
   "ca15_issue1,ca15_cx_ialu")
 
-(define_insn_reservation  "cortex_a15_neon_vaba" 7
-  (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_vaba"))
-  "ca15_issue1,ca15_cx_ialu_with_acc")
-
-(define_insn_reservation  "cortex_a15_neon_vaba_qqq" 8
-  (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_vaba_qqq"))
-  "ca15_issue2,ca15_cx_ialu_with_acc*2")
+;; Integer Multiply Instructions.
 
-(define_insn_reservation
-  "cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long" 6
+(define_insn_reservation "cortex_a15_neon_multiply" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_mul_ddd_8_16_qdd_16_8_long_32_16_long"))
+       (eq_attr "cortex_a15_neon_type" "neon_multiply"))
   "ca15_issue1,ca15_cx_imac")
 
-(define_insn_reservation "cortex_a15_neon_mul_qqq_8_16_32_ddd_32" 7
+(define_insn_reservation "cortex_a15_neon_multiply_q" 7
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type" "neon_mul_qqq_8_16_32_ddd_32"))
-  "ca15_issue1,ca15_cx_imac*2")
+       (eq_attr "cortex_a15_neon_type" "neon_multiply_q"))
+  "ca15_issue2,ca15_cx_imac*2")
 
-(define_insn_reservation
-  "cortex_a15_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar" 7
+(define_insn_reservation "cortex_a15_neon_mla" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar"))
-  "ca15_issue1,ca15_cx_imac*2")
-
-(define_insn_reservation
-  "cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long" 6
-  (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mla_ddd_8_16_qdd_16_8_long_32_16_long"))
+       (eq_attr "cortex_a15_neon_type" "neon_mla"))
   "ca15_issue1,ca15_cx_imac")
 
-(define_insn_reservation
-  "cortex_a15_neon_mla_qqq_8_16" 7
+(define_insn_reservation "cortex_a15_neon_mla_q" 7
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mla_qqq_8_16"))
+       (eq_attr "cortex_a15_neon_type" "neon_mla_q"))
   "ca15_issue1,ca15_cx_imac*2")
 
-(define_insn_reservation
-  "cortex_a15_neon_mla_ddd_32_qqd_16_ddd_32_scalar_\
-     qdd_64_32_long_scalar_qdd_64_32_long" 7
+(define_insn_reservation "cortex_a15_neon_sat_mla_long" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-  "neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long"))
+       (eq_attr "cortex_a15_neon_type" "neon_sat_mla_long"))
   "ca15_issue1,ca15_cx_imac")
 
-(define_insn_reservation
-  "cortex_a15_neon_mla_qqq_32_qqd_32_scalar" 7
-  (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mla_qqq_32_qqd_32_scalar"))
-  "ca15_issue1,ca15_cx_imac*2")
+;; Integer Shift Instructions.
 
 (define_insn_reservation
-  "cortex_a15_neon_mul_ddd_16_scalar_32_16_long_scalar" 6
+  "cortex_a15_neon_shift_acc" 7
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mul_ddd_16_scalar_32_16_long_scalar"))
-  "ca15_issue1,ca15_cx_imac")
-
-(define_insn_reservation
-  "cortex_a15_neon_mul_qqd_32_scalar" 7
-  (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mul_qqd_32_scalar"))
-  "ca15_issue1,ca15_cx_imac*2")
+       (eq_attr "cortex_a15_neon_type" "neon_shift_acc"))
+  "ca15_issue1,ca15_cx_ishf_with_acc")
 
 (define_insn_reservation
-  "cortex_a15_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar" 6
+  "cortex_a15_neon_shift_imm_basic" 4
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mla_ddd_16_scalar_qdd_32_16_long_scalar"))
-  "ca15_issue1,ca15_cx_imac")
+       (eq_attr "cortex_a15_neon_type" "neon_shift_imm_basic"))
+  "ca15_issue1,ca15_cx_ik+ca15_cx_ishf")
 
 (define_insn_reservation
-  "cortex_a15_neon_shift_1" 5
+  "cortex_a15_neon_shift_imm_complex" 5
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_shift_1"))
+       (eq_attr "cortex_a15_neon_type" "neon_shift_imm_complex"))
   "ca15_issue1,ca15_cx_ik+ca15_cx_ishf")
 
 (define_insn_reservation
-  "cortex_a15_neon_shift_2" 5
+  "cortex_a15_neon_shift_reg_basic" 4
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_shift_2"))
+       (eq_attr "cortex_a15_neon_type" "neon_shift_reg_basic"))
   "ca15_issue1,ca15_cx_ik+ca15_cx_ishf")
 
 (define_insn_reservation
-  "cortex_a15_neon_shift_3" 6
+  "cortex_a15_neon_shift_reg_basic_q" 5
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_shift_3"))
-  "ca15_issue2,(ca15_cx_ik+ca15_cx_ishf)*2")
+       (eq_attr "cortex_a15_neon_type" "neon_shift_reg_basic_q"))
+  "ca15_issue2,(ca15_cx_ik+ca15_cx_ishf*2)")
 
 (define_insn_reservation
-  "cortex_a15_neon_vshl_ddd" 5
+  "cortex_a15_neon_shift_reg_complex" 5
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vshl_ddd"))
-  "ca15_issue1,ca15_cx_ik+ca15_cx_ishf")
+       (eq_attr "cortex_a15_neon_type" "neon_shift_reg_complex"))
+  "ca15_issue2,ca15_cx_ik+ca15_cx_ishf")
 
 (define_insn_reservation
-  "cortex_a15_neon_vqshl_vrshl_vqrshl_qqq" 6
+  "cortex_a15_neon_shift_reg_complex_q" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vqshl_vrshl_vqrshl_qqq"))
+       (eq_attr "cortex_a15_neon_type" "neon_shift_reg_complex_q"))
   "ca15_issue2,(ca15_cx_ik+ca15_cx_ishf)*2")
 
+;; Floating Point Instructions.
+
 (define_insn_reservation
-  "cortex_a15_neon_vsra_vrsra" 7
+  "cortex_a15_neon_fp_negabs" 4
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vsra_vrsra"))
-  "ca15_issue1,ca15_cx_ishf_with_acc")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_negabs"))
+  "ca15_issue1,ca15_cx_falu")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vadd_ddd_vabs_dd" 6
+  "cortex_a15_neon_fp_arith" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vadd_ddd_vabs_dd"))
+       (eq_attr "cortex_a15_neon_type" "neon_fp_arith"))
   "ca15_issue1,ca15_cx_falu")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vadd_qqq_vabs_qq" 7
+  "cortex_a15_neon_fp_arith_q" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vadd_qqq_vabs_qq"))
+       (eq_attr "cortex_a15_neon_type" "neon_fp_arith_q"))
   "ca15_issue2,ca15_cx_falu_2")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vmul_ddd" 5
+  "cortex_a15_neon_fp_cvt_int" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vmul_ddd"))
-  "ca15_issue1,ca15_cx_fmul")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_cvt_int"))
+  "ca15_issue1,ca15_cx_falu+ca15_cx_ishf")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vmul_qqd" 6
+  "cortex_a15_neon_fp_cvt_int_q" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vmul_qqd"))
-  "ca15_issue2,ca15_cx_fmul_2")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_cvt_int_q"))
+  "ca15_issue2,(ca15_cx_falu+ca15_cx_ishf)*2")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vmla_ddd" 9
+  "cortex_a15_neon_fp_cvt16" 10
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vmla_ddd"))
-  "ca15_issue1,ca15_cx_fmac")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_cvt16"))
+  "ca15_issue3,(ca15_cx_falu+ca15_cx_ishf)*2+ca15_cx_falu")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vmla_qqq" 11
+  "cortex_a15_neon_fp_mul" 5
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vmla_qqq"))
-  "ca15_issue2,ca15_cx_fmac_2")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_mul"))
+  "ca15_issue1,ca15_cx_fmul")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vmla_ddd_scalar" 9
+  "cortex_a15_neon_fp_mul_q" 5
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vmla_ddd_scalar"))
-  "ca15_issue1,ca15_cx_fmac")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_mul_q"))
+  "ca15_issue2,ca15_cx_fmul_2")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vmla_qqq_scalar" 11
+  "cortex_a15_neon_fp_mla" 9
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vmla_qqq_scalar"))
-  "ca15_issue2,ca15_cx_fmac_2")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_mla"))
+  "ca15_issue1,ca15_cx_fmul")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vrecps_vrsqrts_ddd" 9
+  "cortex_a15_neon_fp_mla_q" 9
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vrecps_vrsqrts_ddd"))
-  "ca15_issue1,ca15_cx_fmac")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_mla_q"))
+  "ca15_issue2,ca15_cx_fmul_2")
 
 (define_insn_reservation
-  "cortex_a15_neon_fp_vrecps_vrsqrts_qqq" 11
+  "cortex_a15_neon_fp_recps_rsqrte" 9
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_fp_vrecps_vrsqrts_qqq"))
-  "ca15_issue2,ca15_cx_fmac_2")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_recpe_rsqrte"))
+  "ca15_issue1,ca15_cx_fmac")
 
 (define_insn_reservation
-  "cortex_a15_neon_bp_simple" 4
+  "cortex_a15_neon_fp_recps_rsqrte_q" 9
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_bp_simple"))
-  "ca15_issue3,ca15_ls+ca15_cx_perm_2,ca15_cx_perm")
+       (eq_attr "cortex_a15_neon_type" "neon_fp_recpe_rsqrte_q"))
+  "ca15_issue2,ca15_cx_fmac_2")
+
+;; Miscelaaneous Instructions.
 
 (define_insn_reservation
-  "cortex_a15_neon_bp_2cycle" 4
+  "cortex_a15_neon_bitops" 4
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_bp_2cycle"))
+       (eq_attr "cortex_a15_neon_type" "neon_bitops"))
   "ca15_issue1,ca15_cx_perm")
 
 (define_insn_reservation
-  "cortex_a15_neon_bp_3cycle" 7
+  "cortex_a15_neon_bitops_q" 4
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_bp_3cycle"))
-  "ca15_issue3,ca15_cx_ialu+ca15_cx_perm_2,ca15_cx_perm")
+       (eq_attr "cortex_a15_neon_type" "neon_bitops_q"))
+  "ca15_issue2,ca15_cx_perm_2")
 
 (define_insn_reservation
-  "cortex_a15_neon_vld1_1_2_regs" 7
+  "cortex_a15_neon_from_gp" 9
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vld1_1_2_regs"))
-  "ca15_issue2,ca15_ls,ca15_ldr")
+       (eq_attr "cortex_a15_neon_type" "neon_from_gp"))
+  "ca15_issue2,ca15_ls1+ca15_ls2+ca15_cx_perm")
 
 (define_insn_reservation
-  "cortex_a15_neon_vld1_3_4_regs" 8
+  "cortex_a15_neon_from_gp_q" 9
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vld1_3_4_regs"))
-  "ca15_issue3,ca15_ls1+ca15_ls2,ca15_ldr,ca15_ldr")
+       (eq_attr "cortex_a15_neon_type" "neon_from_gp_q"))
+  "ca15_issue2,ca15_ls1+ca15_ls2+ca15_cx_perm_2")
 
 (define_insn_reservation
-  "cortex_a15_neon_vld2_2_regs_vld1_vld2_all_lanes" 9
+  "cortex_a15_neon_tbl3_tbl4" 7
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vld2_2_regs_vld1_vld2_all_lanes"))
-  "ca15_issue3,ca15_ls,ca15_ldr")
+       (eq_attr "cortex_a15_neon_type" "neon_tbl3_tbl4"))
+  "ca15_issue2,ca15_cx_perm_2")
 
 (define_insn_reservation
-  "cortex_a15_neon_vld2_4_regs" 12
+  "cortex_a15_neon_zip_q" 7
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vld2_4_regs"))
-  "ca15_issue3,ca15_issue3+ca15_ls1+ca15_ls2,ca15_ldr*2")
+       (eq_attr "cortex_a15_neon_type" "neon_zip_q"))
+  "ca15_issue3,ca15_cx_perm*3")
 
 (define_insn_reservation
-  "cortex_a15_neon_vld3_vld4" 12
+  "cortex_a15_neon_to_gp" 7
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vld3_vld4"))
-  "ca15_issue3,ca15_issue3+ca15_ls1+ca15_ls2,ca15_ldr*2")
+       (eq_attr "cortex_a15_neon_type" "neon_to_gp"))
+  "ca15_issue2,ca15_ls1+ca15_ls2")
 
-(define_insn_reservation
-  "cortex_a15_neon_vst1_1_2_regs_vst2_2_regs" 0
-  (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vst1_1_2_regs_vst2_2_regs"))
-  "ca15_issue3,ca15_issue3+ca15_cx_perm+ca15_ls1+ca15_ls2,ca15_str*2")
+;; Load Instructions.
 
 (define_insn_reservation
-  "cortex_a15_neon_vst1_3_4_regs" 0
+  "cortex_a15_neon_load_a" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vst1_3_4_regs"))
-  "ca15_issue3,ca15_issue3+ca15_ls1+ca15_ls2,ca15_str*3")
+       (eq_attr "cortex_a15_neon_type" "neon_load_a"))
+  "ca15_issue1,ca15_ls,ca15_ldr")
 
 (define_insn_reservation
-  "cortex_a15_neon_vst2_4_regs_vst3_vst4" 0
+  "cortex_a15_neon_load_b" 7
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vst2_4_regs_vst3_vst4"))
-  "ca15_issue3,ca15_issue3+ca15_cx_perm_2+ca15_ls1+ca15_ls2,\
-   ca15_issue3+ca15_str,ca15_str*3")
+       (eq_attr "cortex_a15_neon_type" "neon_load_b"))
+  "ca15_issue2,ca15_ls1+ca15_ls2,ca15_ldr,ca15_ldr")
 
 (define_insn_reservation
-  "cortex_a15_neon_vst3_vst4" 0
+  "cortex_a15_neon_load_c" 9
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vst3_vst4"))
-  "ca15_issue3,ca15_issue3+ca15_cx_perm_2+ca15_ls1+ca15_ls2,ca15_str*4")
+       (eq_attr "cortex_a15_neon_type" "neon_load_c"))
+  "ca15_issue2,ca15_ls1+ca15_ls2,ca15_ldr,ca15_ldr")
 
 (define_insn_reservation
-  "cortex_a15_neon_vld1_vld2_lane" 9
+  "cortex_a15_neon_load_d" 11
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vld1_vld2_lane"))
-  "ca15_issue3,ca15_ls,ca15_ldr")
+       (eq_attr "cortex_a15_neon_type" "neon_load_d"))
+  "ca15_issue1,ca15_issue3+ca15_ls1+ca15_ls2,ca15_ldr*2")
 
 (define_insn_reservation
-  "cortex_a15_neon_vld3_vld4_lane" 10
+  "cortex_a15_neon_load_e" 9
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vld3_vld4_lane"))
-  "ca15_issue3,ca15_issue3+ca15_ls,ca15_issue3+ca15_ldr")
+       (eq_attr "cortex_a15_neon_type" "neon_load_e"))
+  "ca15_issue3+ca15_ls1+ca15_ls2,ca15_ldr*2")
 
 (define_insn_reservation
-  "cortex_a15_neon_vst1_vst2_lane" 0
+  "cortex_a15_neon_load_f" 11
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vst1_vst2_lane"))
-  "ca15_issue3,ca15_cx_perm+ca15_ls,ca15_str")
+       (eq_attr "cortex_a15_neon_type" "neon_load_f"))
+  "ca15_issue3,ca15_issue3+ca15_ls1+ca15_ls2,ca15_ldr*2")
+
+;; Store Instructions.
 
 (define_insn_reservation
-  "cortex_a15_neon_vst3_vst4_lane" 0
+  "cortex_a15_neon_store_a" 0
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vst3_vst4_lane"))
-  "ca15_issue3,ca15_issue3+ca15_cx_perm+ca15_ls1+ca15_ls2,ca15_str*2")
+       (eq_attr "cortex_a15_neon_type" "neon_store_a"))
+  "ca15_issue1,ca15_ls1+ca15_ls2,ca15_str")
 
 (define_insn_reservation
-  "cortex_a15_neon_vld3_vld4_all_lanes" 11
+  "cortex_a15_neon_store_b" 0
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_vld3_vld4_all_lanes"))
-  "ca15_issue3,ca15_issue3+ca15_ls,ca15_ldr")
+       (eq_attr "cortex_a15_neon_type" "neon_store_b"))
+  "ca15_issue2,ca15_ls1+ca15_ls2,ca15_str*2")
 
 (define_insn_reservation
-  "cortex_a15_neon_ldm_2" 20
+  "cortex_a15_neon_store_c" 0
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_ldm_2"))
-  "ca15_issue3*6")
+       (eq_attr "cortex_a15_neon_type" "neon_store_c"))
+  "ca15_issue3,ca15_ls1+ca15_ls2,ca15_str*3")
 
 (define_insn_reservation
-  "cortex_a15_neon_stm_2" 0
+  "cortex_a15_neon_store_d" 0
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_stm_2"))
-  "ca15_issue3*6")
+       (eq_attr "cortex_a15_neon_type" "neon_store_d"))
+  "ca15_issue3,ca15_issue1,ca15_ls1+ca15_ls2,ca15_str*4")
 
 (define_insn_reservation
-  "cortex_a15_neon_mcr" 6
+  "cortex_a15_neon_store_e" 0
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mcr"))
-  "ca15_issue2,ca15_ls,ca15_cx_perm")
+       (eq_attr "cortex_a15_neon_type" "neon_store_e"))
+  "ca15_issue2,ca15_ls1+ca15_ls2,ca15_str+ca15_cx_perm")
 
 (define_insn_reservation
-  "cortex_a15_neon_mcr_2_mcrr" 6
+  "cortex_a15_neon_store_f" 0
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mcr_2_mcrr"))
-  "ca15_issue2,ca15_ls1+ca15_ls2")
+       (eq_attr "cortex_a15_neon_type" "neon_store_f"))
+  "ca15_issue3,ca15_ls1+ca15_ls2,ca15_str*2+ca15_cx_perm")
 
 (define_insn_reservation
-  "cortex_a15_neon_mrc" 5
+  "cortex_a15_neon_store_g" 0
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mrc"))
-  "ca15_issue1,ca15_ls")
+       (eq_attr "cortex_a15_neon_type" "neon_store_g"))
+  "ca15_issue3,ca15_issue3+ca15_cx_perm+ca15_ls1+ca15_ls2,ca15_str*2")
 
 (define_insn_reservation
-  "cortex_a15_neon_mrrc" 6
+  "cortex_a15_neon_store_h" 0
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "neon_type"
-              "neon_mrrc"))
-  "ca15_issue2,ca15_ls1+ca15_ls2")
+       (eq_attr "cortex_a15_neon_type" "neon_store_h"))
+  "ca15_issue3,ca15_issue2+ca15_cx_perm+ca15_ls1+ca15_ls2,ca15_str*2")
+
+;; VFP Operations.
 
 (define_insn_reservation "cortex_a15_vfp_const" 4
   (and (eq_attr "tune" "cortexa15")
@@ -515,7 +632,7 @@
 
 (define_insn_reservation "cortex_a15_vfp_cvt" 6
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "type" "f_cvt"))
+       (eq_attr "type" "f_cvt,f_cvtf2i,f_cvti2f"))
   "ca15_issue1,ca15_cx_vfp")
 
 (define_insn_reservation "cortex_a15_vfp_cmpd" 8
@@ -535,9 +652,14 @@
 
 (define_insn_reservation "cortex_a15_vfp_cpys" 4
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "type" "fcpys"))
+       (eq_attr "type" "fmov"))
   "ca15_issue1,ca15_cx_perm")
 
+(define_insn_reservation "cortex_a15_vfp_to_from_gp" 5
+  (and (eq_attr "tune" "cortexa15")
+       (eq_attr "type" "f_mcr, f_mcrr, f_mrc, f_mrrc"))
+  "ca15_issue1,ca15_ls1+ca15_ls2")
+
 (define_insn_reservation "cortex_a15_vfp_ariths" 7
   (and (eq_attr "tune" "cortexa15")
        (eq_attr "type" "ffariths"))
@@ -545,671 +667,11 @@
 
 (define_insn_reservation "cortex_a15_vfp_divs" 10
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "type" "fdivs"))
+       (eq_attr "type" "fdivs, fsqrts"))
   "ca15_issue1,ca15_cx_ik")
 
 (define_insn_reservation "cortex_a15_vfp_divd" 18
   (and (eq_attr "tune" "cortexa15")
-       (eq_attr "type" "fdivd"))
+       (eq_attr "type" "fdivd, fsqrtd"))
   "ca15_issue1,ca15_cx_ik")
 
-;; Define bypasses.
-(define_bypass 5 "cortex_a15_neon_mcr_2_mcrr"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a15_neon_mcr"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 10 "cortex_a15_neon_vld3_vld4_all_lanes"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 9 "cortex_a15_neon_vld3_vld4_lane"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 8 "cortex_a15_neon_vld1_vld2_lane"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 11 "cortex_a15_neon_vld3_vld4"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 11 "cortex_a15_neon_vld2_4_regs"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 8 "cortex_a15_neon_vld2_2_regs_vld1_vld2_all_lanes"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 7 "cortex_a15_neon_vld1_3_4_regs"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6 "cortex_a15_neon_vld1_1_2_regs"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6 "cortex_a15_neon_bp_3cycle"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 3 "cortex_a15_neon_bp_2cycle"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 3 "cortex_a15_neon_bp_simple"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 10 "cortex_a15_neon_fp_vrecps_vrsqrts_qqq"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 8 "cortex_a15_neon_fp_vrecps_vrsqrts_ddd"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 10 "cortex_a15_neon_fp_vmla_qqq_scalar"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 8 "cortex_a15_neon_fp_vmla_ddd_scalar"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 10 "cortex_a15_neon_fp_vmla_qqq"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 8 "cortex_a15_neon_fp_vmla_ddd"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a15_neon_fp_vmul_qqd"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_fp_vmul_ddd"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6 "cortex_a15_neon_fp_vadd_qqq_vabs_qq"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a15_neon_fp_vadd_ddd_vabs_dd"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6 "cortex_a15_neon_vsra_vrsra"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a15_neon_vqshl_vrshl_vqrshl_qqq"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_vshl_ddd"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a15_neon_shift_3"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_shift_2"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_shift_1"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a15_neon_mla_ddd_16_scalar_qdd_32_16_long_scalar"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6 "cortex_a15_neon_mul_qqd_32_scalar"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a15_neon_mul_ddd_16_scalar_32_16_long_scalar"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6 "cortex_a15_neon_mla_qqq_32_qqd_32_scalar"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6 "cortex_a15_neon_mla_qqq_8_16"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6
-     "cortex_a15_neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6 "cortex_a15_neon_mul_qqq_8_16_32_ddd_32"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 5 "cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 7 "cortex_a15_neon_vaba_qqq"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 6 "cortex_a15_neon_vaba"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_vmov"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_vqneg_vqabs"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_int_5"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_int_4"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_int_3"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_int_2"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
-(define_bypass 4 "cortex_a15_neon_int_1"
-               "cortex_a15_neon_int_1,\
-               cortex_a15_neon_int_4,\
-               cortex_a15_neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mul_qqq_8_16_32_ddd_32,\
-               cortex_a15_neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-               cortex_a15_neon_mla_qqq_8_16,\
-               cortex_a15_neon_fp_vadd_ddd_vabs_dd,\
-               cortex_a15_neon_fp_vadd_qqq_vabs_qq,\
-               cortex_a15_neon_fp_vmla_ddd,\
-               cortex_a15_neon_fp_vmla_qqq,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_ddd,\
-               cortex_a15_neon_fp_vrecps_vrsqrts_qqq")
-
Index: b/src/gcc/config/arm/arm_neon.h
===================================================================
--- a/src/gcc/config/arm/arm_neon.h
+++ b/src/gcc/config/arm/arm_neon.h
@@ -42,9 +42,13 @@ typedef __builtin_neon_qi int8x8_t	__att
 typedef __builtin_neon_hi int16x4_t	__attribute__ ((__vector_size__ (8)));
 typedef __builtin_neon_si int32x2_t	__attribute__ ((__vector_size__ (8)));
 typedef __builtin_neon_di int64x1_t;
+typedef __builtin_neon_hf float16x4_t	__attribute__ ((__vector_size__ (8)));
 typedef __builtin_neon_sf float32x2_t	__attribute__ ((__vector_size__ (8)));
 typedef __builtin_neon_poly8 poly8x8_t	__attribute__ ((__vector_size__ (8)));
 typedef __builtin_neon_poly16 poly16x4_t	__attribute__ ((__vector_size__ (8)));
+#ifdef __ARM_FEATURE_CRYPTO
+typedef __builtin_neon_poly64 poly64x1_t;
+#endif
 typedef __builtin_neon_uqi uint8x8_t	__attribute__ ((__vector_size__ (8)));
 typedef __builtin_neon_uhi uint16x4_t	__attribute__ ((__vector_size__ (8)));
 typedef __builtin_neon_usi uint32x2_t	__attribute__ ((__vector_size__ (8)));
@@ -56,6 +60,9 @@ typedef __builtin_neon_di int64x2_t	__at
 typedef __builtin_neon_sf float32x4_t	__attribute__ ((__vector_size__ (16)));
 typedef __builtin_neon_poly8 poly8x16_t	__attribute__ ((__vector_size__ (16)));
 typedef __builtin_neon_poly16 poly16x8_t	__attribute__ ((__vector_size__ (16)));
+#ifdef __ARM_FEATURE_CRYPTO
+typedef __builtin_neon_poly64 poly64x2_t	__attribute__ ((__vector_size__ (16)));
+#endif
 typedef __builtin_neon_uqi uint8x16_t	__attribute__ ((__vector_size__ (16)));
 typedef __builtin_neon_uhi uint16x8_t	__attribute__ ((__vector_size__ (16)));
 typedef __builtin_neon_usi uint32x4_t	__attribute__ ((__vector_size__ (16)));
@@ -64,6 +71,10 @@ typedef __builtin_neon_udi uint64x2_t	__
 typedef float float32_t;
 typedef __builtin_neon_poly8 poly8_t;
 typedef __builtin_neon_poly16 poly16_t;
+#ifdef __ARM_FEATURE_CRYPTO
+typedef __builtin_neon_poly64 poly64_t;
+typedef __builtin_neon_poly128 poly128_t;
+#endif
 
 typedef struct int8x8x2_t
 {
@@ -175,6 +186,22 @@ typedef struct poly16x8x2_t
   poly16x8_t val[2];
 } poly16x8x2_t;
 
+#ifdef __ARM_FEATURE_CRYPTO
+typedef struct poly64x1x2_t
+{
+  poly64x1_t val[2];
+} poly64x1x2_t;
+#endif
+
+
+#ifdef __ARM_FEATURE_CRYPTO
+typedef struct poly64x2x2_t
+{
+  poly64x2_t val[2];
+} poly64x2x2_t;
+#endif
+
+
 typedef struct int8x8x3_t
 {
   int8x8_t val[3];
@@ -285,6 +312,22 @@ typedef struct poly16x8x3_t
   poly16x8_t val[3];
 } poly16x8x3_t;
 
+#ifdef __ARM_FEATURE_CRYPTO
+typedef struct poly64x1x3_t
+{
+  poly64x1_t val[3];
+} poly64x1x3_t;
+#endif
+
+
+#ifdef __ARM_FEATURE_CRYPTO
+typedef struct poly64x2x3_t
+{
+  poly64x2_t val[3];
+} poly64x2x3_t;
+#endif
+
+
 typedef struct int8x8x4_t
 {
   int8x8_t val[4];
@@ -395,6 +438,22 @@ typedef struct poly16x8x4_t
   poly16x8_t val[4];
 } poly16x8x4_t;
 
+#ifdef __ARM_FEATURE_CRYPTO
+typedef struct poly64x1x4_t
+{
+  poly64x1_t val[4];
+} poly64x1x4_t;
+#endif
+
+
+#ifdef __ARM_FEATURE_CRYPTO
+typedef struct poly64x2x4_t
+{
+  poly64x2_t val[4];
+} poly64x2x4_t;
+#endif
+
+
 
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vadd_s8 (int8x8_t __a, int8x8_t __b)
@@ -4360,6 +4419,14 @@ vrsraq_n_u64 (uint64x2_t __a, uint64x2_t
   return (uint64x2_t)__builtin_neon_vsra_nv2di ((int64x2_t) __a, (int64x2_t) __b, __c, 4);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vsri_n_p64 (poly64x1_t __a, poly64x1_t __b, const int __c)
+{
+  return (poly64x1_t)__builtin_neon_vsri_ndi (__a, __b, __c);
+}
+
+#endif
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vsri_n_s8 (int8x8_t __a, int8x8_t __b, const int __c)
 {
@@ -4420,6 +4487,14 @@ vsri_n_p16 (poly16x4_t __a, poly16x4_t _
   return (poly16x4_t)__builtin_neon_vsri_nv4hi ((int16x4_t) __a, (int16x4_t) __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vsriq_n_p64 (poly64x2_t __a, poly64x2_t __b, const int __c)
+{
+  return (poly64x2_t)__builtin_neon_vsri_nv2di ((int64x2_t) __a, (int64x2_t) __b, __c);
+}
+
+#endif
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vsriq_n_s8 (int8x16_t __a, int8x16_t __b, const int __c)
 {
@@ -4480,6 +4555,14 @@ vsriq_n_p16 (poly16x8_t __a, poly16x8_t
   return (poly16x8_t)__builtin_neon_vsri_nv8hi ((int16x8_t) __a, (int16x8_t) __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vsli_n_p64 (poly64x1_t __a, poly64x1_t __b, const int __c)
+{
+  return (poly64x1_t)__builtin_neon_vsli_ndi (__a, __b, __c);
+}
+
+#endif
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vsli_n_s8 (int8x8_t __a, int8x8_t __b, const int __c)
 {
@@ -4540,6 +4623,14 @@ vsli_n_p16 (poly16x4_t __a, poly16x4_t _
   return (poly16x4_t)__builtin_neon_vsli_nv4hi ((int16x4_t) __a, (int16x4_t) __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vsliq_n_p64 (poly64x2_t __a, poly64x2_t __b, const int __c)
+{
+  return (poly64x2_t)__builtin_neon_vsli_nv2di ((int64x2_t) __a, (int64x2_t) __b, __c);
+}
+
+#endif
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vsliq_n_s8 (int8x16_t __a, int8x16_t __b, const int __c)
 {
@@ -5308,6 +5399,14 @@ vsetq_lane_u64 (uint64_t __a, uint64x2_t
   return (uint64x2_t)__builtin_neon_vset_lanev2di ((__builtin_neon_di) __a, (int64x2_t) __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vcreate_p64 (uint64_t __a)
+{
+  return (poly64x1_t)__builtin_neon_vcreatedi ((__builtin_neon_di) __a);
+}
+
+#endif
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vcreate_s8 (uint64_t __a)
 {
@@ -5428,6 +5527,14 @@ vdup_n_p16 (poly16_t __a)
   return (poly16x4_t)__builtin_neon_vdup_nv4hi ((__builtin_neon_hi) __a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vdup_n_p64 (poly64_t __a)
+{
+  return (poly64x1_t)__builtin_neon_vdup_ndi ((__builtin_neon_di) __a);
+}
+
+#endif
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vdup_n_s64 (int64_t __a)
 {
@@ -5440,6 +5547,14 @@ vdup_n_u64 (uint64_t __a)
   return (uint64x1_t)__builtin_neon_vdup_ndi ((__builtin_neon_di) __a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vdupq_n_p64 (poly64_t __a)
+{
+  return (poly64x2_t)__builtin_neon_vdup_nv2di ((__builtin_neon_di) __a);
+}
+
+#endif
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vdupq_n_s8 (int8_t __a)
 {
@@ -5692,6 +5807,14 @@ vdup_lane_p16 (poly16x4_t __a, const int
   return (poly16x4_t)__builtin_neon_vdup_lanev4hi ((int16x4_t) __a, __b);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vdup_lane_p64 (poly64x1_t __a, const int __b)
+{
+  return (poly64x1_t)__builtin_neon_vdup_lanedi (__a, __b);
+}
+
+#endif
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vdup_lane_s64 (int64x1_t __a, const int __b)
 {
@@ -5758,6 +5881,14 @@ vdupq_lane_p16 (poly16x4_t __a, const in
   return (poly16x8_t)__builtin_neon_vdup_lanev8hi ((int16x4_t) __a, __b);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vdupq_lane_p64 (poly64x1_t __a, const int __b)
+{
+  return (poly64x2_t)__builtin_neon_vdup_lanev2di (__a, __b);
+}
+
+#endif
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
 vdupq_lane_s64 (int64x1_t __a, const int __b)
 {
@@ -5770,6 +5901,14 @@ vdupq_lane_u64 (uint64x1_t __a, const in
   return (uint64x2_t)__builtin_neon_vdup_lanev2di ((int64x1_t) __a, __b);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vcombine_p64 (poly64x1_t __a, poly64x1_t __b)
+{
+  return (poly64x2_t)__builtin_neon_vcombinedi (__a, __b);
+}
+
+#endif
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vcombine_s8 (int8x8_t __a, int8x8_t __b)
 {
@@ -5836,6 +5975,14 @@ vcombine_p16 (poly16x4_t __a, poly16x4_t
   return (poly16x8_t)__builtin_neon_vcombinev4hi ((int16x4_t) __a, (int16x4_t) __b);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vget_high_p64 (poly64x2_t __a)
+{
+  return (poly64x1_t)__builtin_neon_vget_highv2di ((int64x2_t) __a);
+}
+
+#endif
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vget_high_s8 (int8x16_t __a)
 {
@@ -5956,6 +6103,14 @@ vget_low_p16 (poly16x8_t __a)
   return (poly16x4_t)__builtin_neon_vget_lowv8hi ((int16x8_t) __a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vget_low_p64 (poly64x2_t __a)
+{
+  return (poly64x1_t)__builtin_neon_vget_lowv2di ((int64x2_t) __a);
+}
+
+#endif
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vget_low_s64 (int64x2_t __a)
 {
@@ -6016,6 +6171,22 @@ vcvtq_u32_f32 (float32x4_t __a)
   return (uint32x4_t)__builtin_neon_vcvtv4sf (__a, 0);
 }
 
+#if ((__ARM_FP & 0x2) != 0)
+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))
+vcvt_f16_f32 (float32x4_t __a)
+{
+  return (float16x4_t)__builtin_neon_vcvtv4hfv4sf (__a);
+}
+
+#endif
+#if ((__ARM_FP & 0x2) != 0)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vcvt_f32_f16 (float16x4_t __a)
+{
+  return (float32x4_t)__builtin_neon_vcvtv4sfv4hf (__a);
+}
+
+#endif
 __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
 vcvt_n_s32_f32 (float32x2_t __a, const int __b)
 {
@@ -7024,6 +7195,14 @@ vqdmlsl_n_s32 (int64x2_t __a, int32x2_t
   return (int64x2_t)__builtin_neon_vqdmlsl_nv2si (__a, __b, (__builtin_neon_si) __c, 1);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vext_p64 (poly64x1_t __a, poly64x1_t __b, const int __c)
+{
+  return (poly64x1_t)__builtin_neon_vextdi (__a, __b, __c);
+}
+
+#endif
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vext_s8 (int8x8_t __a, int8x8_t __b, const int __c)
 {
@@ -7090,6 +7269,14 @@ vext_p16 (poly16x4_t __a, poly16x4_t __b
   return (poly16x4_t)__builtin_neon_vextv4hi ((int16x4_t) __a, (int16x4_t) __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vextq_p64 (poly64x2_t __a, poly64x2_t __b, const int __c)
+{
+  return (poly64x2_t)__builtin_neon_vextv2di ((int64x2_t) __a, (int64x2_t) __b, __c);
+}
+
+#endif
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vextq_s8 (int8x16_t __a, int8x16_t __b, const int __c)
 {
@@ -7372,6 +7559,14 @@ vrev16q_p8 (poly8x16_t __a)
   return (poly8x16_t) __builtin_shuffle (__a, (uint8x16_t) { 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14 });
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vbsl_p64 (uint64x1_t __a, poly64x1_t __b, poly64x1_t __c)
+{
+  return (poly64x1_t)__builtin_neon_vbsldi ((int64x1_t) __a, __b, __c);
+}
+
+#endif
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vbsl_s8 (uint8x8_t __a, int8x8_t __b, int8x8_t __c)
 {
@@ -7438,6 +7633,14 @@ vbsl_p16 (uint16x4_t __a, poly16x4_t __b
   return (poly16x4_t)__builtin_neon_vbslv4hi ((int16x4_t) __a, (int16x4_t) __b, (int16x4_t) __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vbslq_p64 (uint64x2_t __a, poly64x2_t __b, poly64x2_t __c)
+{
+  return (poly64x2_t)__builtin_neon_vbslv2di ((int64x2_t) __a, (int64x2_t) __b, (int64x2_t) __c);
+}
+
+#endif
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vbslq_s8 (uint8x16_t __a, int8x16_t __b, int8x16_t __c)
 {
@@ -7990,6 +8193,14 @@ vuzpq_p16 (poly16x8_t __a, poly16x8_t __
   return __rv;
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vld1_p64 (const poly64_t * __a)
+{
+  return (poly64x1_t)__builtin_neon_vld1di ((const __builtin_neon_di *) __a);
+}
+
+#endif
 __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
 vld1_s8 (const int8_t * __a)
 {
@@ -8056,6 +8267,14 @@ vld1_p16 (const poly16_t * __a)
   return (poly16x4_t)__builtin_neon_vld1v4hi ((const __builtin_neon_hi *) __a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vld1q_p64 (const poly64_t * __a)
+{
+  return (poly64x2_t)__builtin_neon_vld1v2di ((const __builtin_neon_di *) __a);
+}
+
+#endif
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
 vld1q_s8 (const int8_t * __a)
 {
@@ -8176,6 +8395,14 @@ vld1_lane_p16 (const poly16_t * __a, pol
   return (poly16x4_t)__builtin_neon_vld1_lanev4hi ((const __builtin_neon_hi *) __a, (int16x4_t) __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vld1_lane_p64 (const poly64_t * __a, poly64x1_t __b, const int __c)
+{
+  return (poly64x1_t)__builtin_neon_vld1_lanedi ((const __builtin_neon_di *) __a, __b, __c);
+}
+
+#endif
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vld1_lane_s64 (const int64_t * __a, int64x1_t __b, const int __c)
 {
@@ -8242,6 +8469,14 @@ vld1q_lane_p16 (const poly16_t * __a, po
   return (poly16x8_t)__builtin_neon_vld1_lanev8hi ((const __builtin_neon_hi *) __a, (int16x8_t) __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vld1q_lane_p64 (const poly64_t * __a, poly64x2_t __b, const int __c)
+{
+  return (poly64x2_t)__builtin_neon_vld1_lanev2di ((const __builtin_neon_di *) __a, (int64x2_t) __b, __c);
+}
+
+#endif
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
 vld1q_lane_s64 (const int64_t * __a, int64x2_t __b, const int __c)
 {
@@ -8308,6 +8543,14 @@ vld1_dup_p16 (const poly16_t * __a)
   return (poly16x4_t)__builtin_neon_vld1_dupv4hi ((const __builtin_neon_hi *) __a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vld1_dup_p64 (const poly64_t * __a)
+{
+  return (poly64x1_t)__builtin_neon_vld1_dupdi ((const __builtin_neon_di *) __a);
+}
+
+#endif
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vld1_dup_s64 (const int64_t * __a)
 {
@@ -8374,6 +8617,14 @@ vld1q_dup_p16 (const poly16_t * __a)
   return (poly16x8_t)__builtin_neon_vld1_dupv8hi ((const __builtin_neon_hi *) __a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vld1q_dup_p64 (const poly64_t * __a)
+{
+  return (poly64x2_t)__builtin_neon_vld1_dupv2di ((const __builtin_neon_di *) __a);
+}
+
+#endif
 __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
 vld1q_dup_s64 (const int64_t * __a)
 {
@@ -8386,6 +8637,14 @@ vld1q_dup_u64 (const uint64_t * __a)
   return (uint64x2_t)__builtin_neon_vld1_dupv2di ((const __builtin_neon_di *) __a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_p64 (poly64_t * __a, poly64x1_t __b)
+{
+  __builtin_neon_vst1di ((__builtin_neon_di *) __a, __b);
+}
+
+#endif
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst1_s8 (int8_t * __a, int8x8_t __b)
 {
@@ -8452,6 +8711,14 @@ vst1_p16 (poly16_t * __a, poly16x4_t __b
   __builtin_neon_vst1v4hi ((__builtin_neon_hi *) __a, (int16x4_t) __b);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_p64 (poly64_t * __a, poly64x2_t __b)
+{
+  __builtin_neon_vst1v2di ((__builtin_neon_di *) __a, (int64x2_t) __b);
+}
+
+#endif
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst1q_s8 (int8_t * __a, int8x16_t __b)
 {
@@ -8572,6 +8839,14 @@ vst1_lane_p16 (poly16_t * __a, poly16x4_
   __builtin_neon_vst1_lanev4hi ((__builtin_neon_hi *) __a, (int16x4_t) __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_p64 (poly64_t * __a, poly64x1_t __b, const int __c)
+{
+  __builtin_neon_vst1_lanedi ((__builtin_neon_di *) __a, __b, __c);
+}
+
+#endif
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst1_lane_s64 (int64_t * __a, int64x1_t __b, const int __c)
 {
@@ -8638,6 +8913,14 @@ vst1q_lane_p16 (poly16_t * __a, poly16x8
   __builtin_neon_vst1_lanev8hi ((__builtin_neon_hi *) __a, (int16x8_t) __b, __c);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_p64 (poly64_t * __a, poly64x2_t __b, const int __c)
+{
+  __builtin_neon_vst1_lanev2di ((__builtin_neon_di *) __a, (int64x2_t) __b, __c);
+}
+
+#endif
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst1q_lane_s64 (int64_t * __a, int64x2_t __b, const int __c)
 {
@@ -8722,6 +9005,16 @@ vld2_p16 (const poly16_t * __a)
   return __rv.__i;
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1x2_t __attribute__ ((__always_inline__))
+vld2_p64 (const poly64_t * __a)
+{
+  union { poly64x1x2_t __i; __builtin_neon_ti __o; } __rv;
+  __rv.__o = __builtin_neon_vld2di ((const __builtin_neon_di *) __a);
+  return __rv.__i;
+}
+
+#endif
 __extension__ static __inline int64x1x2_t __attribute__ ((__always_inline__))
 vld2_s64 (const int64_t * __a)
 {
@@ -9017,6 +9310,16 @@ vld2_dup_p16 (const poly16_t * __a)
   return __rv.__i;
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1x2_t __attribute__ ((__always_inline__))
+vld2_dup_p64 (const poly64_t * __a)
+{
+  union { poly64x1x2_t __i; __builtin_neon_ti __o; } __rv;
+  __rv.__o = __builtin_neon_vld2_dupdi ((const __builtin_neon_di *) __a);
+  return __rv.__i;
+}
+
+#endif
 __extension__ static __inline int64x1x2_t __attribute__ ((__always_inline__))
 vld2_dup_s64 (const int64_t * __a)
 {
@@ -9096,6 +9399,15 @@ vst2_p16 (poly16_t * __a, poly16x4x2_t _
   __builtin_neon_vst2v4hi ((__builtin_neon_hi *) __a, __bu.__o);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst2_p64 (poly64_t * __a, poly64x1x2_t __b)
+{
+  union { poly64x1x2_t __i; __builtin_neon_ti __o; } __bu = { __b };
+  __builtin_neon_vst2di ((__builtin_neon_di *) __a, __bu.__o);
+}
+
+#endif
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst2_s64 (int64_t * __a, int64x1x2_t __b)
 {
@@ -9350,6 +9662,16 @@ vld3_p16 (const poly16_t * __a)
   return __rv.__i;
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1x3_t __attribute__ ((__always_inline__))
+vld3_p64 (const poly64_t * __a)
+{
+  union { poly64x1x3_t __i; __builtin_neon_ei __o; } __rv;
+  __rv.__o = __builtin_neon_vld3di ((const __builtin_neon_di *) __a);
+  return __rv.__i;
+}
+
+#endif
 __extension__ static __inline int64x1x3_t __attribute__ ((__always_inline__))
 vld3_s64 (const int64_t * __a)
 {
@@ -9645,6 +9967,16 @@ vld3_dup_p16 (const poly16_t * __a)
   return __rv.__i;
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1x3_t __attribute__ ((__always_inline__))
+vld3_dup_p64 (const poly64_t * __a)
+{
+  union { poly64x1x3_t __i; __builtin_neon_ei __o; } __rv;
+  __rv.__o = __builtin_neon_vld3_dupdi ((const __builtin_neon_di *) __a);
+  return __rv.__i;
+}
+
+#endif
 __extension__ static __inline int64x1x3_t __attribute__ ((__always_inline__))
 vld3_dup_s64 (const int64_t * __a)
 {
@@ -9724,6 +10056,15 @@ vst3_p16 (poly16_t * __a, poly16x4x3_t _
   __builtin_neon_vst3v4hi ((__builtin_neon_hi *) __a, __bu.__o);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst3_p64 (poly64_t * __a, poly64x1x3_t __b)
+{
+  union { poly64x1x3_t __i; __builtin_neon_ei __o; } __bu = { __b };
+  __builtin_neon_vst3di ((__builtin_neon_di *) __a, __bu.__o);
+}
+
+#endif
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst3_s64 (int64_t * __a, int64x1x3_t __b)
 {
@@ -9978,6 +10319,16 @@ vld4_p16 (const poly16_t * __a)
   return __rv.__i;
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1x4_t __attribute__ ((__always_inline__))
+vld4_p64 (const poly64_t * __a)
+{
+  union { poly64x1x4_t __i; __builtin_neon_oi __o; } __rv;
+  __rv.__o = __builtin_neon_vld4di ((const __builtin_neon_di *) __a);
+  return __rv.__i;
+}
+
+#endif
 __extension__ static __inline int64x1x4_t __attribute__ ((__always_inline__))
 vld4_s64 (const int64_t * __a)
 {
@@ -10273,6 +10624,16 @@ vld4_dup_p16 (const poly16_t * __a)
   return __rv.__i;
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1x4_t __attribute__ ((__always_inline__))
+vld4_dup_p64 (const poly64_t * __a)
+{
+  union { poly64x1x4_t __i; __builtin_neon_oi __o; } __rv;
+  __rv.__o = __builtin_neon_vld4_dupdi ((const __builtin_neon_di *) __a);
+  return __rv.__i;
+}
+
+#endif
 __extension__ static __inline int64x1x4_t __attribute__ ((__always_inline__))
 vld4_dup_s64 (const int64_t * __a)
 {
@@ -10352,6 +10713,15 @@ vst4_p16 (poly16_t * __a, poly16x4x4_t _
   __builtin_neon_vst4v4hi ((__builtin_neon_hi *) __a, __bu.__o);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst4_p64 (poly64_t * __a, poly64x1x4_t __b)
+{
+  union { poly64x1x4_t __i; __builtin_neon_oi __o; } __bu = { __b };
+  __builtin_neon_vst4di ((__builtin_neon_di *) __a, __bu.__o);
+}
+
+#endif
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst4_s64 (int64_t * __a, int64x1x4_t __b)
 {
@@ -11016,23 +11386,25 @@ vornq_u64 (uint64x2_t __a, uint64x2_t __
 
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vreinterpret_p8_s8 (int8x8_t __a)
+vreinterpret_p8_p16 (poly16x4_t __a)
 {
-  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv8qi (__a);
+  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
 }
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vreinterpret_p8_s16 (int16x4_t __a)
+vreinterpret_p8_f32 (float32x2_t __a)
 {
-  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv4hi (__a);
+  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv2sf (__a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vreinterpret_p8_s32 (int32x2_t __a)
+vreinterpret_p8_p64 (poly64x1_t __a)
 {
-  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv2si (__a);
+  return (poly8x8_t)__builtin_neon_vreinterpretv8qidi (__a);
 }
 
+#endif
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
 vreinterpret_p8_s64 (int64x1_t __a)
 {
@@ -11040,99 +11412,77 @@ vreinterpret_p8_s64 (int64x1_t __a)
 }
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vreinterpret_p8_f32 (float32x2_t __a)
+vreinterpret_p8_u64 (uint64x1_t __a)
 {
-  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv2sf (__a);
+  return (poly8x8_t)__builtin_neon_vreinterpretv8qidi ((int64x1_t) __a);
 }
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vreinterpret_p8_u8 (uint8x8_t __a)
+vreinterpret_p8_s8 (int8x8_t __a)
 {
-  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv8qi ((int8x8_t) __a);
+  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv8qi (__a);
 }
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vreinterpret_p8_u16 (uint16x4_t __a)
+vreinterpret_p8_s16 (int16x4_t __a)
 {
-  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
+  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv4hi (__a);
 }
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vreinterpret_p8_u32 (uint32x2_t __a)
+vreinterpret_p8_s32 (int32x2_t __a)
 {
-  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv2si ((int32x2_t) __a);
+  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv2si (__a);
 }
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vreinterpret_p8_u64 (uint64x1_t __a)
+vreinterpret_p8_u8 (uint8x8_t __a)
 {
-  return (poly8x8_t)__builtin_neon_vreinterpretv8qidi ((int64x1_t) __a);
+  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv8qi ((int8x8_t) __a);
 }
 
 __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
-vreinterpret_p8_p16 (poly16x4_t __a)
+vreinterpret_p8_u16 (uint16x4_t __a)
 {
   return (poly8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_s8 (int8x16_t __a)
+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))
+vreinterpret_p8_u32 (uint32x2_t __a)
 {
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv16qi (__a);
+  return (poly8x8_t)__builtin_neon_vreinterpretv8qiv2si ((int32x2_t) __a);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_s16 (int16x8_t __a)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vreinterpret_p16_p8 (poly8x8_t __a)
 {
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv8hi (__a);
+  return (poly16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_s32 (int32x4_t __a)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vreinterpret_p16_f32 (float32x2_t __a)
 {
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv4si (__a);
+  return (poly16x4_t)__builtin_neon_vreinterpretv4hiv2sf (__a);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_s64 (int64x2_t __a)
-{
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv2di (__a);
-}
-
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_f32 (float32x4_t __a)
-{
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv4sf (__a);
-}
-
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_u8 (uint8x16_t __a)
-{
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv16qi ((int8x16_t) __a);
-}
-
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_u16 (uint16x8_t __a)
-{
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
-}
-
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_u32 (uint32x4_t __a)
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vreinterpret_p16_p64 (poly64x1_t __a)
 {
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv4si ((int32x4_t) __a);
+  return (poly16x4_t)__builtin_neon_vreinterpretv4hidi (__a);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_u64 (uint64x2_t __a)
+#endif
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vreinterpret_p16_s64 (int64x1_t __a)
 {
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv2di ((int64x2_t) __a);
+  return (poly16x4_t)__builtin_neon_vreinterpretv4hidi (__a);
 }
 
-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_p8_p16 (poly16x8_t __a)
+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
+vreinterpret_p16_u64 (uint64x1_t __a)
 {
-  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
+  return (poly16x4_t)__builtin_neon_vreinterpretv4hidi ((int64x1_t) __a);
 }
 
 __extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
@@ -11154,18 +11504,6 @@ vreinterpret_p16_s32 (int32x2_t __a)
 }
 
 __extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vreinterpret_p16_s64 (int64x1_t __a)
-{
-  return (poly16x4_t)__builtin_neon_vreinterpretv4hidi (__a);
-}
-
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vreinterpret_p16_f32 (float32x2_t __a)
-{
-  return (poly16x4_t)__builtin_neon_vreinterpretv4hiv2sf (__a);
-}
-
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
 vreinterpret_p16_u8 (uint8x8_t __a)
 {
   return (poly16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
@@ -11183,76 +11521,36 @@ vreinterpret_p16_u32 (uint32x2_t __a)
   return (poly16x4_t)__builtin_neon_vreinterpretv4hiv2si ((int32x2_t) __a);
 }
 
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vreinterpret_p16_u64 (uint64x1_t __a)
-{
-  return (poly16x4_t)__builtin_neon_vreinterpretv4hidi ((int64x1_t) __a);
-}
-
-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))
-vreinterpret_p16_p8 (poly8x8_t __a)
-{
-  return (poly16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
-}
-
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_s8 (int8x16_t __a)
-{
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv16qi (__a);
-}
-
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_s16 (int16x8_t __a)
-{
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv8hi (__a);
-}
-
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_s32 (int32x4_t __a)
-{
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv4si (__a);
-}
-
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_s64 (int64x2_t __a)
-{
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv2di (__a);
-}
-
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_f32 (float32x4_t __a)
-{
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv4sf (__a);
-}
-
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_u8 (uint8x16_t __a)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vreinterpret_f32_p8 (poly8x8_t __a)
 {
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
+  return (float32x2_t)__builtin_neon_vreinterpretv2sfv8qi ((int8x8_t) __a);
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_u16 (uint16x8_t __a)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vreinterpret_f32_p16 (poly16x4_t __a)
 {
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv8hi ((int16x8_t) __a);
+  return (float32x2_t)__builtin_neon_vreinterpretv2sfv4hi ((int16x4_t) __a);
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_u32 (uint32x4_t __a)
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vreinterpret_f32_p64 (poly64x1_t __a)
 {
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv4si ((int32x4_t) __a);
+  return (float32x2_t)__builtin_neon_vreinterpretv2sfdi (__a);
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_u64 (uint64x2_t __a)
+#endif
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vreinterpret_f32_s64 (int64x1_t __a)
 {
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv2di ((int64x2_t) __a);
+  return (float32x2_t)__builtin_neon_vreinterpretv2sfdi (__a);
 }
 
-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_p16_p8 (poly8x16_t __a)
+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
+vreinterpret_f32_u64 (uint64x1_t __a)
 {
-  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
+  return (float32x2_t)__builtin_neon_vreinterpretv2sfdi ((int64x1_t) __a);
 }
 
 __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
@@ -11274,12 +11572,6 @@ vreinterpret_f32_s32 (int32x2_t __a)
 }
 
 __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vreinterpret_f32_s64 (int64x1_t __a)
-{
-  return (float32x2_t)__builtin_neon_vreinterpretv2sfdi (__a);
-}
-
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
 vreinterpret_f32_u8 (uint8x8_t __a)
 {
   return (float32x2_t)__builtin_neon_vreinterpretv2sfv8qi ((int8x8_t) __a);
@@ -11297,82 +11589,124 @@ vreinterpret_f32_u32 (uint32x2_t __a)
   return (float32x2_t)__builtin_neon_vreinterpretv2sfv2si ((int32x2_t) __a);
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vreinterpret_f32_u64 (uint64x1_t __a)
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_p8 (poly8x8_t __a)
 {
-  return (float32x2_t)__builtin_neon_vreinterpretv2sfdi ((int64x1_t) __a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdiv8qi ((int8x8_t) __a);
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vreinterpret_f32_p8 (poly8x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_p16 (poly16x4_t __a)
 {
-  return (float32x2_t)__builtin_neon_vreinterpretv2sfv8qi ((int8x8_t) __a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdiv4hi ((int16x4_t) __a);
 }
 
-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
-vreinterpret_f32_p16 (poly16x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_f32 (float32x2_t __a)
 {
-  return (float32x2_t)__builtin_neon_vreinterpretv2sfv4hi ((int16x4_t) __a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdiv2sf (__a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_s8 (int8x16_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_s64 (int64x1_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv16qi (__a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdidi (__a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_s16 (int16x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_u64 (uint64x1_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv8hi (__a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdidi ((int64x1_t) __a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_s32 (int32x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_s8 (int8x8_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv4si (__a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdiv8qi (__a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_s64 (int64x2_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_s16 (int16x4_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv2di (__a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdiv4hi (__a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_u8 (uint8x16_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_s32 (int32x2_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv16qi ((int8x16_t) __a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdiv2si (__a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_u16 (uint16x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_u8 (uint8x8_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv8hi ((int16x8_t) __a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdiv8qi ((int8x8_t) __a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_u32 (uint32x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_u16 (uint16x4_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv4si ((int32x4_t) __a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdiv4hi ((int16x4_t) __a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_u64 (uint64x2_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x1_t __attribute__ ((__always_inline__))
+vreinterpret_p64_u32 (uint32x2_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv2di ((int64x2_t) __a);
+  return (poly64x1_t)__builtin_neon_vreinterpretdiv2si ((int32x2_t) __a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_p8 (poly8x16_t __a)
+#endif
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vreinterpret_s64_p8 (poly8x8_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv16qi ((int8x16_t) __a);
+  return (int64x1_t)__builtin_neon_vreinterpretdiv8qi ((int8x8_t) __a);
 }
 
-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_f32_p16 (poly16x8_t __a)
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vreinterpret_s64_p16 (poly16x4_t __a)
 {
-  return (float32x4_t)__builtin_neon_vreinterpretv4sfv8hi ((int16x8_t) __a);
+  return (int64x1_t)__builtin_neon_vreinterpretdiv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vreinterpret_s64_f32 (float32x2_t __a)
+{
+  return (int64x1_t)__builtin_neon_vreinterpretdiv2sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vreinterpret_s64_p64 (poly64x1_t __a)
+{
+  return (int64x1_t)__builtin_neon_vreinterpretdidi (__a);
+}
+
+#endif
+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
+vreinterpret_s64_u64 (uint64x1_t __a)
+{
+  return (int64x1_t)__builtin_neon_vreinterpretdidi ((int64x1_t) __a);
 }
 
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
@@ -11394,12 +11728,6 @@ vreinterpret_s64_s32 (int32x2_t __a)
 }
 
 __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vreinterpret_s64_f32 (float32x2_t __a)
-{
-  return (int64x1_t)__builtin_neon_vreinterpretdiv2sf (__a);
-}
-
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
 vreinterpret_s64_u8 (uint8x8_t __a)
 {
   return (int64x1_t)__builtin_neon_vreinterpretdiv8qi ((int8x8_t) __a);
@@ -11417,310 +11745,1052 @@ vreinterpret_s64_u32 (uint32x2_t __a)
   return (int64x1_t)__builtin_neon_vreinterpretdiv2si ((int32x2_t) __a);
 }
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vreinterpret_s64_u64 (uint64x1_t __a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_p8 (poly8x8_t __a)
 {
-  return (int64x1_t)__builtin_neon_vreinterpretdidi ((int64x1_t) __a);
+  return (uint64x1_t)__builtin_neon_vreinterpretdiv8qi ((int8x8_t) __a);
 }
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vreinterpret_s64_p8 (poly8x8_t __a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_p16 (poly16x4_t __a)
 {
-  return (int64x1_t)__builtin_neon_vreinterpretdiv8qi ((int8x8_t) __a);
+  return (uint64x1_t)__builtin_neon_vreinterpretdiv4hi ((int16x4_t) __a);
 }
 
-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))
-vreinterpret_s64_p16 (poly16x4_t __a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_f32 (float32x2_t __a)
 {
-  return (int64x1_t)__builtin_neon_vreinterpretdiv4hi ((int16x4_t) __a);
+  return (uint64x1_t)__builtin_neon_vreinterpretdiv2sf (__a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_s8 (int8x16_t __a)
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_p64 (poly64x1_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div16qi (__a);
+  return (uint64x1_t)__builtin_neon_vreinterpretdidi (__a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_s16 (int16x8_t __a)
+#endif
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_s64 (int64x1_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div8hi (__a);
+  return (uint64x1_t)__builtin_neon_vreinterpretdidi (__a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_s32 (int32x4_t __a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_s8 (int8x8_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div4si (__a);
+  return (uint64x1_t)__builtin_neon_vreinterpretdiv8qi (__a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_f32 (float32x4_t __a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_s16 (int16x4_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div4sf (__a);
+  return (uint64x1_t)__builtin_neon_vreinterpretdiv4hi (__a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_u8 (uint8x16_t __a)
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_s32 (int32x2_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
+  return (uint64x1_t)__builtin_neon_vreinterpretdiv2si (__a);
+}
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_u8 (uint8x8_t __a)
+{
+  return (uint64x1_t)__builtin_neon_vreinterpretdiv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_u16 (uint16x4_t __a)
+{
+  return (uint64x1_t)__builtin_neon_vreinterpretdiv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vreinterpret_u64_u32 (uint32x2_t __a)
+{
+  return (uint64x1_t)__builtin_neon_vreinterpretdiv2si ((int32x2_t) __a);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_p8 (poly8x8_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qiv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_p16 (poly16x4_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_f32 (float32x2_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qiv2sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_p64 (poly64x1_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qidi (__a);
+}
+
+#endif
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_s64 (int64x1_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qidi (__a);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_u64 (uint64x1_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qidi ((int64x1_t) __a);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_s16 (int16x4_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qiv4hi (__a);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_s32 (int32x2_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qiv2si (__a);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_u8 (uint8x8_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qiv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_u16 (uint16x4_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
+vreinterpret_s8_u32 (uint32x2_t __a)
+{
+  return (int8x8_t)__builtin_neon_vreinterpretv8qiv2si ((int32x2_t) __a);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_p8 (poly8x8_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_p16 (poly16x4_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hiv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_f32 (float32x2_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hiv2sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_p64 (poly64x1_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hidi (__a);
+}
+
+#endif
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_s64 (int64x1_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hidi (__a);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_u64 (uint64x1_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hidi ((int64x1_t) __a);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_s8 (int8x8_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hiv8qi (__a);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_s32 (int32x2_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hiv2si (__a);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_u8 (uint8x8_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_u16 (uint16x4_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hiv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
+vreinterpret_s16_u32 (uint32x2_t __a)
+{
+  return (int16x4_t)__builtin_neon_vreinterpretv4hiv2si ((int32x2_t) __a);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_p8 (poly8x8_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2siv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_p16 (poly16x4_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2siv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_f32 (float32x2_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2siv2sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_p64 (poly64x1_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2sidi (__a);
+}
+
+#endif
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_s64 (int64x1_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2sidi (__a);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_u64 (uint64x1_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2sidi ((int64x1_t) __a);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_s8 (int8x8_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2siv8qi (__a);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_s16 (int16x4_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2siv4hi (__a);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_u8 (uint8x8_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2siv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_u16 (uint16x4_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2siv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
+vreinterpret_s32_u32 (uint32x2_t __a)
+{
+  return (int32x2_t)__builtin_neon_vreinterpretv2siv2si ((int32x2_t) __a);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_p8 (poly8x8_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_p16 (poly16x4_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_f32 (float32x2_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv2sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_p64 (poly64x1_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qidi (__a);
+}
+
+#endif
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_s64 (int64x1_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qidi (__a);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_u64 (uint64x1_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qidi ((int64x1_t) __a);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_s8 (int8x8_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv8qi (__a);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_s16 (int16x4_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv4hi (__a);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_s32 (int32x2_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv2si (__a);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_u16 (uint16x4_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
+vreinterpret_u8_u32 (uint32x2_t __a)
+{
+  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv2si ((int32x2_t) __a);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_p8 (poly8x8_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_p16 (poly16x4_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_f32 (float32x2_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv2sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_p64 (poly64x1_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hidi (__a);
+}
+
+#endif
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_s64 (int64x1_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hidi (__a);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_u64 (uint64x1_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hidi ((int64x1_t) __a);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_s8 (int8x8_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv8qi (__a);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_s16 (int16x4_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv4hi (__a);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_s32 (int32x2_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv2si (__a);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_u8 (uint8x8_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
+vreinterpret_u16_u32 (uint32x2_t __a)
+{
+  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv2si ((int32x2_t) __a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_p8 (poly8x8_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2siv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_p16 (poly16x4_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2siv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_f32 (float32x2_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2siv2sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_p64 (poly64x1_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2sidi (__a);
+}
+
+#endif
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_s64 (int64x1_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2sidi (__a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_u64 (uint64x1_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2sidi ((int64x1_t) __a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_s8 (int8x8_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2siv8qi (__a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_s16 (int16x4_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2siv4hi (__a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_s32 (int32x2_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2siv2si (__a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_u8 (uint8x8_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2siv8qi ((int8x8_t) __a);
+}
+
+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
+vreinterpret_u32_u16 (uint16x4_t __a)
+{
+  return (uint32x2_t)__builtin_neon_vreinterpretv2siv4hi ((int16x4_t) __a);
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_p16 (poly16x8_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_f32 (float32x4_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv4sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_p64 (poly64x2_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv2di ((int64x2_t) __a);
+}
+
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_p128 (poly128_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiti ((__builtin_neon_ti) __a);
+}
+
+#endif
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_s64 (int64x2_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv2di (__a);
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_u64 (uint64x2_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv2di ((int64x2_t) __a);
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_s8 (int8x16_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv16qi (__a);
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_s16 (int16x8_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv8hi (__a);
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_s32 (int32x4_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv4si (__a);
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_u8 (uint8x16_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv16qi ((int8x16_t) __a);
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_u16 (uint16x8_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
+}
+
+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_p8_u32 (uint32x4_t __a)
+{
+  return (poly8x16_t)__builtin_neon_vreinterpretv16qiv4si ((int32x4_t) __a);
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_p8 (poly8x16_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_f32 (float32x4_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv4sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_p64 (poly64x2_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv2di ((int64x2_t) __a);
+}
+
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_p128 (poly128_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiti ((__builtin_neon_ti) __a);
+}
+
+#endif
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_s64 (int64x2_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv2di (__a);
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_u64 (uint64x2_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv2di ((int64x2_t) __a);
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_s8 (int8x16_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv16qi (__a);
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_s16 (int16x8_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv8hi (__a);
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_s32 (int32x4_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv4si (__a);
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_u8 (uint8x16_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_u16 (uint16x8_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv8hi ((int16x8_t) __a);
+}
+
+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_p16_u32 (uint32x4_t __a)
+{
+  return (poly16x8_t)__builtin_neon_vreinterpretv8hiv4si ((int32x4_t) __a);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_p8 (poly8x16_t __a)
+{
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv16qi ((int8x16_t) __a);
+}
+
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_p16 (poly16x8_t __a)
+{
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv8hi ((int16x8_t) __a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_p64 (poly64x2_t __a)
+{
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv2di ((int64x2_t) __a);
+}
+
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_p128 (poly128_t __a)
+{
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfti ((__builtin_neon_ti) __a);
+}
+
+#endif
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_s64 (int64x2_t __a)
+{
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv2di (__a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_u16 (uint16x8_t __a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_u64 (uint64x2_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_u32 (uint32x4_t __a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_s8 (int8x16_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div4si ((int32x4_t) __a);
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv16qi (__a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_u64 (uint64x2_t __a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_s16 (int16x8_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div2di ((int64x2_t) __a);
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv8hi (__a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_p8 (poly8x16_t __a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_s32 (int32x4_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv4si (__a);
 }
 
-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_s64_p16 (poly16x8_t __a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_u8 (uint8x16_t __a)
 {
-  return (int64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_s8 (int8x8_t __a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_u16 (uint16x8_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdiv8qi (__a);
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_s16 (int16x4_t __a)
+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_f32_u32 (uint32x4_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdiv4hi (__a);
+  return (float32x4_t)__builtin_neon_vreinterpretv4sfv4si ((int32x4_t) __a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_s32 (int32x2_t __a)
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_p8 (poly8x16_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdiv2si (__a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_s64 (int64x1_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_p16 (poly16x8_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdidi (__a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_f32 (float32x2_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_f32 (float32x4_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdiv2sf (__a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div4sf (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_u8 (uint8x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_p128 (poly128_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdiv8qi ((int8x8_t) __a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2diti ((__builtin_neon_ti) __a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_u16 (uint16x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_s64 (int64x2_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdiv4hi ((int16x4_t) __a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div2di (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_u32 (uint32x2_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_u64 (uint64x2_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdiv2si ((int32x2_t) __a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_p8 (poly8x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_s8 (int8x16_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdiv8qi ((int8x8_t) __a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div16qi (__a);
 }
 
-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
-vreinterpret_u64_p16 (poly16x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_s16 (int16x8_t __a)
 {
-  return (uint64x1_t)__builtin_neon_vreinterpretdiv4hi ((int16x4_t) __a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div8hi (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_s8 (int8x16_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_s32 (int32x4_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div16qi (__a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div4si (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_s16 (int16x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_u8 (uint8x16_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div8hi (__a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_s32 (int32x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_u16 (uint16x8_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div4si (__a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_s64 (int64x2_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_p64_u32 (uint32x4_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div2di (__a);
+  return (poly64x2_t)__builtin_neon_vreinterpretv2div4si ((int32x4_t) __a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_f32 (float32x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_p8 (poly8x16_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div4sf (__a);
+  return (poly128_t)__builtin_neon_vreinterprettiv16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_u8 (uint8x16_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_p16 (poly16x8_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
+  return (poly128_t)__builtin_neon_vreinterprettiv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_u16 (uint16x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_f32 (float32x4_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
+  return (poly128_t)__builtin_neon_vreinterprettiv4sf (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_u32 (uint32x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_p64 (poly64x2_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div4si ((int32x4_t) __a);
+  return (poly128_t)__builtin_neon_vreinterprettiv2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_p8 (poly8x16_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_s64 (int64x2_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
+  return (poly128_t)__builtin_neon_vreinterprettiv2di (__a);
 }
 
-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
-vreinterpretq_u64_p16 (poly16x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_u64 (uint64x2_t __a)
 {
-  return (uint64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
+  return (poly128_t)__builtin_neon_vreinterprettiv2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_s16 (int16x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_s8 (int8x16_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qiv4hi (__a);
+  return (poly128_t)__builtin_neon_vreinterprettiv16qi (__a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_s32 (int32x2_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_s16 (int16x8_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qiv2si (__a);
+  return (poly128_t)__builtin_neon_vreinterprettiv8hi (__a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_s64 (int64x1_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_s32 (int32x4_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qidi (__a);
+  return (poly128_t)__builtin_neon_vreinterprettiv4si (__a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_f32 (float32x2_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_u8 (uint8x16_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qiv2sf (__a);
+  return (poly128_t)__builtin_neon_vreinterprettiv16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_u8 (uint8x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_u16 (uint16x8_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qiv8qi ((int8x8_t) __a);
+  return (poly128_t)__builtin_neon_vreinterprettiv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_u16 (uint16x4_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vreinterpretq_p128_u32 (uint32x4_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
+  return (poly128_t)__builtin_neon_vreinterprettiv4si ((int32x4_t) __a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_u32 (uint32x2_t __a)
+#endif
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_p8 (poly8x16_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qiv2si ((int32x2_t) __a);
+  return (int64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_u64 (uint64x1_t __a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_p16 (poly16x8_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qidi ((int64x1_t) __a);
+  return (int64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_p8 (poly8x8_t __a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_f32 (float32x4_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qiv8qi ((int8x8_t) __a);
+  return (int64x2_t)__builtin_neon_vreinterpretv2div4sf (__a);
 }
 
-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
-vreinterpret_s8_p16 (poly16x4_t __a)
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_p64 (poly64x2_t __a)
 {
-  return (int8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
+  return (int64x2_t)__builtin_neon_vreinterpretv2div2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_s8_s16 (int16x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_p128 (poly128_t __a)
 {
-  return (int8x16_t)__builtin_neon_vreinterpretv16qiv8hi (__a);
+  return (int64x2_t)__builtin_neon_vreinterpretv2diti ((__builtin_neon_ti) __a);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_s8_s32 (int32x4_t __a)
+#endif
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_u64 (uint64x2_t __a)
 {
-  return (int8x16_t)__builtin_neon_vreinterpretv16qiv4si (__a);
+  return (int64x2_t)__builtin_neon_vreinterpretv2div2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_s8_s64 (int64x2_t __a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_s8 (int8x16_t __a)
 {
-  return (int8x16_t)__builtin_neon_vreinterpretv16qiv2di (__a);
+  return (int64x2_t)__builtin_neon_vreinterpretv2div16qi (__a);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_s8_f32 (float32x4_t __a)
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_s16 (int16x8_t __a)
 {
-  return (int8x16_t)__builtin_neon_vreinterpretv16qiv4sf (__a);
+  return (int64x2_t)__builtin_neon_vreinterpretv2div8hi (__a);
+}
+
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_s32 (int32x4_t __a)
+{
+  return (int64x2_t)__builtin_neon_vreinterpretv2div4si (__a);
+}
+
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_u8 (uint8x16_t __a)
+{
+  return (int64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
+}
+
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_u16 (uint16x8_t __a)
+{
+  return (int64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
+}
+
+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_s64_u32 (uint32x4_t __a)
+{
+  return (int64x2_t)__builtin_neon_vreinterpretv2div4si ((int32x4_t) __a);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_p8 (poly8x16_t __a)
+{
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_p16 (poly16x8_t __a)
+{
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_f32 (float32x4_t __a)
+{
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div4sf (__a);
+}
+
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_p64 (poly64x2_t __a)
+{
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div2di ((int64x2_t) __a);
+}
+
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_p128 (poly128_t __a)
+{
+  return (uint64x2_t)__builtin_neon_vreinterpretv2diti ((__builtin_neon_ti) __a);
+}
+
+#endif
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_s64 (int64x2_t __a)
+{
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div2di (__a);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_s8 (int8x16_t __a)
+{
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div16qi (__a);
+}
+
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_s16 (int16x8_t __a)
+{
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div8hi (__a);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_s8_u8 (uint8x16_t __a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_s32 (int32x4_t __a)
 {
-  return (int8x16_t)__builtin_neon_vreinterpretv16qiv16qi ((int8x16_t) __a);
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div4si (__a);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_s8_u16 (uint16x8_t __a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_u8 (uint8x16_t __a)
 {
-  return (int8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_s8_u32 (uint32x4_t __a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_u16 (uint16x8_t __a)
 {
-  return (int8x16_t)__builtin_neon_vreinterpretv16qiv4si ((int32x4_t) __a);
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_s8_u64 (uint64x2_t __a)
+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
+vreinterpretq_u64_u32 (uint32x4_t __a)
 {
-  return (int8x16_t)__builtin_neon_vreinterpretv16qiv2di ((int64x2_t) __a);
+  return (uint64x2_t)__builtin_neon_vreinterpretv2div4si ((int32x4_t) __a);
 }
 
 __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
@@ -11735,82 +12805,80 @@ vreinterpretq_s8_p16 (poly16x8_t __a)
   return (int8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_s8 (int8x8_t __a)
-{
-  return (int16x4_t)__builtin_neon_vreinterpretv4hiv8qi (__a);
-}
-
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_s32 (int32x2_t __a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_f32 (float32x4_t __a)
 {
-  return (int16x4_t)__builtin_neon_vreinterpretv4hiv2si (__a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiv4sf (__a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_s64 (int64x1_t __a)
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_p64 (poly64x2_t __a)
 {
-  return (int16x4_t)__builtin_neon_vreinterpretv4hidi (__a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiv2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_f32 (float32x2_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_p128 (poly128_t __a)
 {
-  return (int16x4_t)__builtin_neon_vreinterpretv4hiv2sf (__a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiti ((__builtin_neon_ti) __a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_u8 (uint8x8_t __a)
+#endif
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_s64 (int64x2_t __a)
 {
-  return (int16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiv2di (__a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_u16 (uint16x4_t __a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_u64 (uint64x2_t __a)
 {
-  return (int16x4_t)__builtin_neon_vreinterpretv4hiv4hi ((int16x4_t) __a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiv2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_u32 (uint32x2_t __a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_s16 (int16x8_t __a)
 {
-  return (int16x4_t)__builtin_neon_vreinterpretv4hiv2si ((int32x2_t) __a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiv8hi (__a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_u64 (uint64x1_t __a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_s32 (int32x4_t __a)
 {
-  return (int16x4_t)__builtin_neon_vreinterpretv4hidi ((int64x1_t) __a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiv4si (__a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_p8 (poly8x8_t __a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_u8 (uint8x16_t __a)
 {
-  return (int16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiv16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))
-vreinterpret_s16_p16 (poly16x4_t __a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_u16 (uint16x8_t __a)
 {
-  return (int16x4_t)__builtin_neon_vreinterpretv4hiv4hi ((int16x4_t) __a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_s16_s8 (int8x16_t __a)
+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_s8_u32 (uint32x4_t __a)
 {
-  return (int16x8_t)__builtin_neon_vreinterpretv8hiv16qi (__a);
+  return (int8x16_t)__builtin_neon_vreinterpretv16qiv4si ((int32x4_t) __a);
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_s16_s32 (int32x4_t __a)
+vreinterpretq_s16_p8 (poly8x16_t __a)
 {
-  return (int16x8_t)__builtin_neon_vreinterpretv8hiv4si (__a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_s16_s64 (int64x2_t __a)
+vreinterpretq_s16_p16 (poly16x8_t __a)
 {
-  return (int16x8_t)__builtin_neon_vreinterpretv8hiv2di (__a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiv8hi ((int16x8_t) __a);
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
@@ -11819,22 +12887,26 @@ vreinterpretq_s16_f32 (float32x4_t __a)
   return (int16x8_t)__builtin_neon_vreinterpretv8hiv4sf (__a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_s16_u8 (uint8x16_t __a)
+vreinterpretq_s16_p64 (poly64x2_t __a)
 {
-  return (int16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiv2di ((int64x2_t) __a);
 }
 
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_s16_u16 (uint16x8_t __a)
+vreinterpretq_s16_p128 (poly128_t __a)
 {
-  return (int16x8_t)__builtin_neon_vreinterpretv8hiv8hi ((int16x8_t) __a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiti ((__builtin_neon_ti) __a);
 }
 
+#endif
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_s16_u32 (uint32x4_t __a)
+vreinterpretq_s16_s64 (int64x2_t __a)
 {
-  return (int16x8_t)__builtin_neon_vreinterpretv8hiv4si ((int32x4_t) __a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiv2di (__a);
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
@@ -11844,93 +12916,45 @@ vreinterpretq_s16_u64 (uint64x2_t __a)
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_s16_p8 (poly8x16_t __a)
+vreinterpretq_s16_s8 (int8x16_t __a)
 {
-  return (int16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiv16qi (__a);
 }
 
 __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_s16_p16 (poly16x8_t __a)
-{
-  return (int16x8_t)__builtin_neon_vreinterpretv8hiv8hi ((int16x8_t) __a);
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_s8 (int8x8_t __a)
-{
-  return (int32x2_t)__builtin_neon_vreinterpretv2siv8qi (__a);
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_s16 (int16x4_t __a)
-{
-  return (int32x2_t)__builtin_neon_vreinterpretv2siv4hi (__a);
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_s64 (int64x1_t __a)
-{
-  return (int32x2_t)__builtin_neon_vreinterpretv2sidi (__a);
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_f32 (float32x2_t __a)
-{
-  return (int32x2_t)__builtin_neon_vreinterpretv2siv2sf (__a);
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_u8 (uint8x8_t __a)
-{
-  return (int32x2_t)__builtin_neon_vreinterpretv2siv8qi ((int8x8_t) __a);
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_u16 (uint16x4_t __a)
-{
-  return (int32x2_t)__builtin_neon_vreinterpretv2siv4hi ((int16x4_t) __a);
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_u32 (uint32x2_t __a)
-{
-  return (int32x2_t)__builtin_neon_vreinterpretv2siv2si ((int32x2_t) __a);
-}
-
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_u64 (uint64x1_t __a)
+vreinterpretq_s16_s32 (int32x4_t __a)
 {
-  return (int32x2_t)__builtin_neon_vreinterpretv2sidi ((int64x1_t) __a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiv4si (__a);
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_p8 (poly8x8_t __a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_s16_u8 (uint8x16_t __a)
 {
-  return (int32x2_t)__builtin_neon_vreinterpretv2siv8qi ((int8x8_t) __a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
-vreinterpret_s32_p16 (poly16x4_t __a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_s16_u16 (uint16x8_t __a)
 {
-  return (int32x2_t)__builtin_neon_vreinterpretv2siv4hi ((int16x4_t) __a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_s32_s8 (int8x16_t __a)
+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_s16_u32 (uint32x4_t __a)
 {
-  return (int32x4_t)__builtin_neon_vreinterpretv4siv16qi (__a);
+  return (int16x8_t)__builtin_neon_vreinterpretv8hiv4si ((int32x4_t) __a);
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_s32_s16 (int16x8_t __a)
+vreinterpretq_s32_p8 (poly8x16_t __a)
 {
-  return (int32x4_t)__builtin_neon_vreinterpretv4siv8hi (__a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siv16qi ((int8x16_t) __a);
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_s32_s64 (int64x2_t __a)
+vreinterpretq_s32_p16 (poly16x8_t __a)
 {
-  return (int32x4_t)__builtin_neon_vreinterpretv4siv2di (__a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siv8hi ((int16x8_t) __a);
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
@@ -11939,22 +12963,26 @@ vreinterpretq_s32_f32 (float32x4_t __a)
   return (int32x4_t)__builtin_neon_vreinterpretv4siv4sf (__a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_s32_u8 (uint8x16_t __a)
+vreinterpretq_s32_p64 (poly64x2_t __a)
 {
-  return (int32x4_t)__builtin_neon_vreinterpretv4siv16qi ((int8x16_t) __a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siv2di ((int64x2_t) __a);
 }
 
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_s32_u16 (uint16x8_t __a)
+vreinterpretq_s32_p128 (poly128_t __a)
 {
-  return (int32x4_t)__builtin_neon_vreinterpretv4siv8hi ((int16x8_t) __a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siti ((__builtin_neon_ti) __a);
 }
 
+#endif
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_s32_u32 (uint32x4_t __a)
+vreinterpretq_s32_s64 (int64x2_t __a)
 {
-  return (int32x4_t)__builtin_neon_vreinterpretv4siv4si ((int32x4_t) __a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siv2di (__a);
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
@@ -11964,117 +12992,73 @@ vreinterpretq_s32_u64 (uint64x2_t __a)
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_s32_p8 (poly8x16_t __a)
+vreinterpretq_s32_s8 (int8x16_t __a)
 {
-  return (int32x4_t)__builtin_neon_vreinterpretv4siv16qi ((int8x16_t) __a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siv16qi (__a);
 }
 
 __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_s32_p16 (poly16x8_t __a)
-{
-  return (int32x4_t)__builtin_neon_vreinterpretv4siv8hi ((int16x8_t) __a);
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_s8 (int8x8_t __a)
-{
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv8qi (__a);
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_s16 (int16x4_t __a)
-{
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv4hi (__a);
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_s32 (int32x2_t __a)
-{
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv2si (__a);
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_s64 (int64x1_t __a)
-{
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qidi (__a);
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_f32 (float32x2_t __a)
-{
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv2sf (__a);
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_u16 (uint16x4_t __a)
-{
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_u32 (uint32x2_t __a)
-{
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv2si ((int32x2_t) __a);
-}
-
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_u64 (uint64x1_t __a)
+vreinterpretq_s32_s16 (int16x8_t __a)
 {
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qidi ((int64x1_t) __a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siv8hi (__a);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_p8 (poly8x8_t __a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_s32_u8 (uint8x16_t __a)
 {
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv8qi ((int8x8_t) __a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siv16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
-vreinterpret_u8_p16 (poly16x4_t __a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_s32_u16 (uint16x8_t __a)
 {
-  return (uint8x8_t)__builtin_neon_vreinterpretv8qiv4hi ((int16x4_t) __a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_u8_s8 (int8x16_t __a)
+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_s32_u32 (uint32x4_t __a)
 {
-  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv16qi (__a);
+  return (int32x4_t)__builtin_neon_vreinterpretv4siv4si ((int32x4_t) __a);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_u8_s16 (int16x8_t __a)
+vreinterpretq_u8_p8 (poly8x16_t __a)
 {
-  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv8hi (__a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv16qi ((int8x16_t) __a);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_u8_s32 (int32x4_t __a)
+vreinterpretq_u8_p16 (poly16x8_t __a)
 {
-  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv4si (__a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_u8_s64 (int64x2_t __a)
+vreinterpretq_u8_f32 (float32x4_t __a)
 {
-  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv2di (__a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv4sf (__a);
 }
 
+#ifdef __ARM_FEATURE_CRYPTO
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_u8_f32 (float32x4_t __a)
+vreinterpretq_u8_p64 (poly64x2_t __a)
 {
-  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv4sf (__a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv2di ((int64x2_t) __a);
 }
 
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_u8_u16 (uint16x8_t __a)
+vreinterpretq_u8_p128 (poly128_t __a)
 {
-  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiti ((__builtin_neon_ti) __a);
 }
 
+#endif
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_u8_u32 (uint32x4_t __a)
+vreinterpretq_u8_s64 (int64x2_t __a)
 {
-  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv4si ((int32x4_t) __a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv2di (__a);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
@@ -12084,75 +13068,79 @@ vreinterpretq_u8_u64 (uint64x2_t __a)
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_u8_p8 (poly8x16_t __a)
+vreinterpretq_u8_s8 (int8x16_t __a)
 {
-  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv16qi ((int8x16_t) __a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv16qi (__a);
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
-vreinterpretq_u8_p16 (poly16x8_t __a)
+vreinterpretq_u8_s16 (int16x8_t __a)
 {
-  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv8hi (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_s8 (int8x8_t __a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_u8_s32 (int32x4_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv8qi (__a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv4si (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_s16 (int16x4_t __a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_u8_u16 (uint16x8_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv4hi (__a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_s32 (int32x2_t __a)
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vreinterpretq_u8_u32 (uint32x4_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv2si (__a);
+  return (uint8x16_t)__builtin_neon_vreinterpretv16qiv4si ((int32x4_t) __a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_s64 (int64x1_t __a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_u16_p8 (poly8x16_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hidi (__a);
+  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_f32 (float32x2_t __a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_u16_p16 (poly16x8_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv2sf (__a);
+  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_u8 (uint8x8_t __a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_u16_f32 (float32x4_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
+  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv4sf (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_u32 (uint32x2_t __a)
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_u16_p64 (poly64x2_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv2si ((int32x2_t) __a);
+  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_u64 (uint64x1_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_u16_p128 (poly128_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hidi ((int64x1_t) __a);
+  return (uint16x8_t)__builtin_neon_vreinterpretv8hiti ((__builtin_neon_ti) __a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_p8 (poly8x8_t __a)
+#endif
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_u16_s64 (int64x2_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv8qi ((int8x8_t) __a);
+  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv2di (__a);
 }
 
-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))
-vreinterpret_u16_p16 (poly16x4_t __a)
+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
+vreinterpretq_u16_u64 (uint64x2_t __a)
 {
-  return (uint16x4_t)__builtin_neon_vreinterpretv4hiv4hi ((int16x4_t) __a);
+  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv2di ((int64x2_t) __a);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
@@ -12174,167 +13162,266 @@ vreinterpretq_u16_s32 (int32x4_t __a)
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_u16_s64 (int64x2_t __a)
+vreinterpretq_u16_u8 (uint8x16_t __a)
 {
-  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv2di (__a);
+  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
 }
 
 __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_u16_f32 (float32x4_t __a)
+vreinterpretq_u16_u32 (uint32x4_t __a)
 {
-  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv4sf (__a);
+  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv4si ((int32x4_t) __a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_u16_u8 (uint8x16_t __a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_p8 (poly8x16_t __a)
 {
-  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_u16_u32 (uint32x4_t __a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_p16 (poly16x8_t __a)
 {
-  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv4si ((int32x4_t) __a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_u16_u64 (uint64x2_t __a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_f32 (float32x4_t __a)
 {
-  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv2di ((int64x2_t) __a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv4sf (__a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_u16_p8 (poly8x16_t __a)
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_p64 (poly64x2_t __a)
 {
-  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv16qi ((int8x16_t) __a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
-vreinterpretq_u16_p16 (poly16x8_t __a)
+#endif
+#ifdef __ARM_FEATURE_CRYPTO
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_p128 (poly128_t __a)
 {
-  return (uint16x8_t)__builtin_neon_vreinterpretv8hiv8hi ((int16x8_t) __a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siti ((__builtin_neon_ti) __a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_s8 (int8x8_t __a)
+#endif
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_s64 (int64x2_t __a)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2siv8qi (__a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv2di (__a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_s16 (int16x4_t __a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_u64 (uint64x2_t __a)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2siv4hi (__a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv2di ((int64x2_t) __a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_s32 (int32x2_t __a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_s8 (int8x16_t __a)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2siv2si (__a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv16qi (__a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_s64 (int64x1_t __a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_s16 (int16x8_t __a)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2sidi (__a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv8hi (__a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_f32 (float32x2_t __a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_s32 (int32x4_t __a)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2siv2sf (__a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv4si (__a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_u8 (uint8x8_t __a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_u8 (uint8x16_t __a)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2siv8qi ((int8x8_t) __a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv16qi ((int8x16_t) __a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_u16 (uint16x4_t __a)
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
+vreinterpretq_u32_u16 (uint16x8_t __a)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2siv4hi ((int16x4_t) __a);
+  return (uint32x4_t)__builtin_neon_vreinterpretv4siv8hi ((int16x8_t) __a);
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_u64 (uint64x1_t __a)
+
+#ifdef __ARM_FEATURE_CRYPTO
+
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vldrq_p128 (poly128_t const * __ptr)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2sidi ((int64x1_t) __a);
+#ifdef __ARM_BIG_ENDIAN
+  poly64_t* __ptmp = (poly64_t*) __ptr;
+  poly64_t __d0 = vld1_p64 (__ptmp);
+  poly64_t __d1 = vld1_p64 (__ptmp + 1);
+  return vreinterpretq_p128_p64 (vcombine_p64 (__d1, __d0));
+#else
+  return vreinterpretq_p128_p64 (vld1q_p64 ((poly64_t*) __ptr));
+#endif
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_p8 (poly8x8_t __a)
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vstrq_p128 (poly128_t * __ptr, poly128_t __val)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2siv8qi ((int8x8_t) __a);
+#ifdef __ARM_BIG_ENDIAN
+  poly64x2_t __tmp = vreinterpretq_p64_p128 (__val);
+  poly64_t __d0 = vget_high_p64 (__tmp);
+  poly64_t __d1 = vget_low_p64 (__tmp);
+  vst1q_p64 ((poly64_t*) __ptr, vcombine_p64 (__d0, __d1));
+#else
+  vst1q_p64 ((poly64_t*) __ptr, vreinterpretq_p64_p128 (__val));
+#endif
 }
 
-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
-vreinterpret_u32_p16 (poly16x4_t __a)
+/* The vceq_p64 intrinsic does not map to a single instruction.
+   Instead we emulate it by performing a 32-bit variant of the vceq
+   and applying a pairwise min reduction to the result.
+   vceq_u32 will produce two 32-bit halves, each of which will contain either
+   all ones or all zeros depending on whether the corresponding 32-bit
+   halves of the poly64_t were equal.  The whole poly64_t values are equal
+   if and only if both halves are equal, i.e. vceq_u32 returns all ones.
+   If the result is all zeroes for any half then the whole result is zeroes.
+   This is what the pairwise min reduction achieves.  */
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vceq_p64 (poly64x1_t __a, poly64x1_t __b)
 {
-  return (uint32x2_t)__builtin_neon_vreinterpretv2siv4hi ((int16x4_t) __a);
+  uint32x2_t __t_a = vreinterpret_u32_p64 (__a);
+  uint32x2_t __t_b = vreinterpret_u32_p64 (__b);
+  uint32x2_t __c = vceq_u32 (__t_a, __t_b);
+  uint32x2_t __m = vpmin_u32 (__c, __c);
+  return vreinterpret_u64_u32 (__m);
 }
 
-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_s8 (int8x16_t __a)
+/* The vtst_p64 intrinsic does not map to a single instruction.
+   We emulate it in way similar to vceq_p64 above but here we do
+   a reduction with max since if any two corresponding bits
+   in the two poly64_t's match, then the whole result must be all ones.  */
+
+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
+vtst_p64 (poly64x1_t __a, poly64x1_t __b)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv16qi (__a);
+  uint32x2_t __t_a = vreinterpret_u32_p64 (__a);
+  uint32x2_t __t_b = vreinterpret_u32_p64 (__b);
+  uint32x2_t __c = vtst_u32 (__t_a, __t_b);
+  uint32x2_t __m = vpmax_u32 (__c, __c);
+  return vreinterpret_u64_u32 (__m);
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vaeseq_u8 (uint8x16_t __data, uint8x16_t __key)
+{
+  return __builtin_arm_crypto_aese (__data, __key);
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vaesdq_u8 (uint8x16_t __data, uint8x16_t __key)
+{
+  return __builtin_arm_crypto_aesd (__data, __key);
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vaesmcq_u8 (uint8x16_t __data)
+{
+  return __builtin_arm_crypto_aesmc (__data);
+}
+
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
+vaesimcq_u8 (uint8x16_t __data)
+{
+  return __builtin_arm_crypto_aesimc (__data);
+}
+
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
+vsha1h_u32 (uint32_t __hash_e)
+{
+  uint32x4_t __t = vdupq_n_u32 (0);
+  __t = vsetq_lane_u32 (__hash_e, __t, 0);
+  __t = __builtin_arm_crypto_sha1h (__t);
+  return vgetq_lane_u32 (__t, 0);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_s16 (int16x8_t __a)
+vsha1cq_u32 (uint32x4_t __hash_abcd, uint32_t __hash_e, uint32x4_t __wk)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv8hi (__a);
+  uint32x4_t __t = vdupq_n_u32 (0);
+  __t = vsetq_lane_u32 (__hash_e, __t, 0);
+  return __builtin_arm_crypto_sha1c (__hash_abcd, __t, __wk);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_s32 (int32x4_t __a)
+vsha1pq_u32 (uint32x4_t __hash_abcd, uint32_t __hash_e, uint32x4_t __wk)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv4si (__a);
+  uint32x4_t __t = vdupq_n_u32 (0);
+  __t = vsetq_lane_u32 (__hash_e, __t, 0);
+  return __builtin_arm_crypto_sha1p (__hash_abcd, __t, __wk);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_s64 (int64x2_t __a)
+vsha1mq_u32 (uint32x4_t __hash_abcd, uint32_t __hash_e, uint32x4_t __wk)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv2di (__a);
+  uint32x4_t __t = vdupq_n_u32 (0);
+  __t = vsetq_lane_u32 (__hash_e, __t, 0);
+  return __builtin_arm_crypto_sha1m (__hash_abcd, __t, __wk);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_f32 (float32x4_t __a)
+vsha1su0q_u32 (uint32x4_t __w0_3, uint32x4_t __w4_7, uint32x4_t __w8_11)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv4sf (__a);
+  return __builtin_arm_crypto_sha1su0 (__w0_3, __w4_7, __w8_11);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_u8 (uint8x16_t __a)
+vsha1su1q_u32 (uint32x4_t __tw0_3, uint32x4_t __w12_15)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv16qi ((int8x16_t) __a);
+  return __builtin_arm_crypto_sha1su1 (__tw0_3, __w12_15);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_u16 (uint16x8_t __a)
+vsha256hq_u32 (uint32x4_t __hash_abcd, uint32x4_t __hash_efgh, uint32x4_t __wk)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv8hi ((int16x8_t) __a);
+  return __builtin_arm_crypto_sha256h (__hash_abcd, __hash_efgh, __wk);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_u64 (uint64x2_t __a)
+vsha256h2q_u32 (uint32x4_t __hash_abcd, uint32x4_t __hash_efgh, uint32x4_t __wk)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv2di ((int64x2_t) __a);
+  return __builtin_arm_crypto_sha256h2 (__hash_abcd, __hash_efgh, __wk);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_p8 (poly8x16_t __a)
+vsha256su0q_u32 (uint32x4_t __w0_3, uint32x4_t __w4_7)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv16qi ((int8x16_t) __a);
+  return __builtin_arm_crypto_sha256su0 (__w0_3, __w4_7);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
-vreinterpretq_u32_p16 (poly16x8_t __a)
+vsha256su1q_u32 (uint32x4_t __tw0_3, uint32x4_t __w8_11, uint32x4_t __w12_15)
 {
-  return (uint32x4_t)__builtin_neon_vreinterpretv4siv8hi ((int16x8_t) __a);
+  return __builtin_arm_crypto_sha256su1 (__tw0_3, __w8_11, __w12_15);
 }
 
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vmull_p64 (poly64_t __a, poly64_t __b)
+{
+  return (poly128_t) __builtin_arm_crypto_vmullp64 ((uint64_t) __a, (uint64_t) __b);
+}
+
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
+vmull_high_p64 (poly64x2_t __a, poly64x2_t __b)
+{
+  poly64_t __t1 = vget_high_p64 (__a);
+  poly64_t __t2 = vget_high_p64 (__b);
+
+  return (poly128_t) __builtin_arm_crypto_vmullp64 ((uint64_t) __t1, (uint64_t) __t2);
+}
+
+#endif
 #ifdef __cplusplus
 }
 #endif
Index: b/src/gcc/config/arm/arm-ldmstm.ml
===================================================================
--- a/src/gcc/config/arm/arm-ldmstm.ml
+++ b/src/gcc/config/arm/arm-ldmstm.ml
@@ -149,12 +149,15 @@ let can_thumb addrmode update is_store =
   | IA, true, true -> true
   | _ -> false
 
+exception InvalidAddrMode of string;;
+
 let target addrmode thumb =
   match addrmode, thumb with
     IA, true -> "TARGET_THUMB1"
   | IA, false -> "TARGET_32BIT"
   | DB, false -> "TARGET_32BIT"
   | _, false -> "TARGET_ARM"
+  | _, _ -> raise (InvalidAddrMode "ERROR: Invalid Addressing mode for Thumb1.")
 
 let write_pattern_1 name ls addrmode nregs write_set_fn update thumb =
   let astr = string_of_addrmode addrmode in
@@ -184,8 +187,10 @@ let write_pattern_1 name ls addrmode nre
   done;
   Printf.printf "}\"\n";
   Printf.printf "  [(set_attr \"type\" \"%s%d\")" ls nregs;
-  begin if not thumb then
+  if not thumb then begin
     Printf.printf "\n   (set_attr \"predicable\" \"yes\")";
+    if addrmode == IA || addrmode == DB then
+      Printf.printf "\n   (set_attr \"predicable_short_it\" \"no\")";
   end;
   Printf.printf "])\n\n"
 
Index: b/src/gcc/config/arm/iwmmxt.md
===================================================================
--- a/src/gcc/config/arm/iwmmxt.md
+++ b/src/gcc/config/arm/iwmmxt.md
@@ -33,7 +33,7 @@
   "TARGET_REALLY_IWMMXT"
   "tbcstb%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tbcst")]
+   (set_attr "type" "wmmx_tbcst")]
 )
 
 (define_insn "tbcstv4hi"
@@ -42,7 +42,7 @@
   "TARGET_REALLY_IWMMXT"
   "tbcsth%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tbcst")]
+   (set_attr "type" "wmmx_tbcst")]
 )
 
 (define_insn "tbcstv2si"
@@ -51,7 +51,7 @@
   "TARGET_REALLY_IWMMXT"
   "tbcstw%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tbcst")]
+   (set_attr "type" "wmmx_tbcst")]
 )
 
 (define_insn "iwmmxt_iordi3"
@@ -65,7 +65,7 @@
    #"
   [(set_attr "predicable" "yes")
    (set_attr "length" "4,8,8")
-   (set_attr "wtype" "wor,none,none")]
+   (set_attr "type" "wmmx_wor,*,*")]
 )
 
 (define_insn "iwmmxt_xordi3"
@@ -79,7 +79,7 @@
    #"
   [(set_attr "predicable" "yes")
    (set_attr "length" "4,8,8")
-   (set_attr "wtype" "wxor,none,none")]
+   (set_attr "type" "wmmx_wxor,*,*")]
 )
 
 (define_insn "iwmmxt_anddi3"
@@ -93,7 +93,7 @@
    #"
   [(set_attr "predicable" "yes")
    (set_attr "length" "4,8,8")
-   (set_attr "wtype" "wand,none,none")]
+   (set_attr "type" "wmmx_wand,*,*")]
 )
 
 (define_insn "iwmmxt_nanddi3"
@@ -103,7 +103,7 @@
   "TARGET_REALLY_IWMMXT"
   "wandn%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wandn")]
+   (set_attr "type" "wmmx_wandn")]
 )
 
 (define_insn "*iwmmxt_arm_movdi"
@@ -155,10 +155,10 @@
                                  (const_int 8)
                                  (const_int 4))]
                               (const_int 4)))
-   (set_attr "type" "*,*,*,load2,store2,*,*,*,*,*,r_2_f,f_2_r,ffarithd,f_loadd,f_stored")
+   (set_attr "type" "*,*,*,load2,store2,*,*,*,*,*,f_mcrr,f_mrrc,\
+                     ffarithd,f_loadd,f_stored")
    (set_attr "arm_pool_range" "*,*,*,1020,*,*,*,*,*,*,*,*,*,1020,*")
-   (set_attr "arm_neg_pool_range" "*,*,*,1008,*,*,*,*,*,*,*,*,*,1008,*")
-   (set_attr "wtype" "*,*,*,*,*,wmov,tmcrr,tmrrc,wldr,wstr,*,*,*,*,*")]
+   (set_attr "arm_neg_pool_range" "*,*,*,1008,*,*,*,*,*,*,*,*,*,1008,*")]
 )
 
 (define_insn "*iwmmxt_movsi_insn"
@@ -188,7 +188,8 @@
      default:
        gcc_unreachable ();
      }"
-  [(set_attr "type"           "*,*,*,*,load1,store1,*,*,*,*,r_2_f,f_2_r,fcpys,f_loads,f_stores")
+  [(set_attr "type"           "*,*,*,*,load1,store1,*,*,*,*,f_mcr,f_mrc,\
+                               fmov,f_loads,f_stores")
    (set_attr "length"         "*,*,*,*,*,        *,*,*,  16,     *,*,*,*,*,*")
    (set_attr "pool_range"     "*,*,*,*,4096,     *,*,*,1024,     *,*,*,*,1020,*")
    (set_attr "neg_pool_range" "*,*,*,*,4084,     *,*,*,   *,  1012,*,*,*,1008,*")
@@ -200,8 +201,7 @@
    ;; Also - we have to pretend that these insns clobber the condition code
    ;; bits as otherwise arm_final_prescan_insn() will try to conditionalize
    ;; them.
-   (set_attr "conds" "clob")
-   (set_attr "wtype" "*,*,*,*,*,*,tmcr,tmrc,wldr,wstr,*,*,*,*,*")]
+   (set_attr "conds" "clob")]
 )
 
 ;; Because iwmmxt_movsi_insn is not predicable, we provide the
@@ -249,10 +249,9 @@
    }"
   [(set_attr "predicable" "yes")
    (set_attr "length"         "4,     4,   4,4,4,8,   8,8")
-   (set_attr "type"           "*,*,*,*,*,*,load1,store1")
+   (set_attr "type"           "wmmx_wmov,wmmx_wstr,wmmx_wldr,wmmx_tmrrc,wmmx_tmcrr,*,load1,store1")
    (set_attr "pool_range"     "*,     *, 256,*,*,*, 256,*")
-   (set_attr "neg_pool_range" "*,     *, 244,*,*,*, 244,*")
-   (set_attr "wtype"          "wmov,wstr,wldr,tmrrc,tmcrr,*,*,*")]
+   (set_attr "neg_pool_range" "*,     *, 244,*,*,*, 244,*")]
 )
 
 (define_expand "iwmmxt_setwcgr0"
@@ -318,7 +317,7 @@
   "TARGET_REALLY_IWMMXT"
   "wand\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wand")]
+   (set_attr "type" "wmmx_wand")]
 )
 
 (define_insn "*ior<mode>3_iwmmxt"
@@ -328,7 +327,7 @@
   "TARGET_REALLY_IWMMXT"
   "wor\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wor")]
+   (set_attr "type" "wmmx_wor")]
 )
 
 (define_insn "*xor<mode>3_iwmmxt"
@@ -338,7 +337,7 @@
   "TARGET_REALLY_IWMMXT"
   "wxor\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wxor")]
+   (set_attr "type" "wmmx_wxor")]
 )
 
 
@@ -351,7 +350,7 @@
   "TARGET_REALLY_IWMMXT"
   "wadd<MMX_char>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wadd")]
+   (set_attr "type" "wmmx_wadd")]
 )
 
 (define_insn "ssaddv8qi3"
@@ -361,7 +360,7 @@
   "TARGET_REALLY_IWMMXT"
   "waddbss%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wadd")]
+   (set_attr "type" "wmmx_wadd")]
 )
 
 (define_insn "ssaddv4hi3"
@@ -371,7 +370,7 @@
   "TARGET_REALLY_IWMMXT"
   "waddhss%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wadd")]
+   (set_attr "type" "wmmx_wadd")]
 )
 
 (define_insn "ssaddv2si3"
@@ -381,7 +380,7 @@
   "TARGET_REALLY_IWMMXT"
   "waddwss%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wadd")]
+   (set_attr "type" "wmmx_wadd")]
 )
 
 (define_insn "usaddv8qi3"
@@ -391,7 +390,7 @@
   "TARGET_REALLY_IWMMXT"
   "waddbus%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wadd")]
+   (set_attr "type" "wmmx_wadd")]
 )
 
 (define_insn "usaddv4hi3"
@@ -401,7 +400,7 @@
   "TARGET_REALLY_IWMMXT"
   "waddhus%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wadd")]
+   (set_attr "type" "wmmx_wadd")]
 )
 
 (define_insn "usaddv2si3"
@@ -411,7 +410,7 @@
   "TARGET_REALLY_IWMMXT"
   "waddwus%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wadd")]
+   (set_attr "type" "wmmx_wadd")]
 )
 
 (define_insn "*sub<mode>3_iwmmxt"
@@ -421,7 +420,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsub<MMX_char>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsub")]
+   (set_attr "type" "wmmx_wsub")]
 )
 
 (define_insn "sssubv8qi3"
@@ -431,7 +430,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsubbss%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsub")]
+   (set_attr "type" "wmmx_wsub")]
 )
 
 (define_insn "sssubv4hi3"
@@ -441,7 +440,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsubhss%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsub")]
+   (set_attr "type" "wmmx_wsub")]
 )
 
 (define_insn "sssubv2si3"
@@ -451,7 +450,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsubwss%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsub")]
+   (set_attr "type" "wmmx_wsub")]
 )
 
 (define_insn "ussubv8qi3"
@@ -461,7 +460,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsubbus%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsub")]
+   (set_attr "type" "wmmx_wsub")]
 )
 
 (define_insn "ussubv4hi3"
@@ -471,7 +470,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsubhus%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsub")]
+   (set_attr "type" "wmmx_wsub")]
 )
 
 (define_insn "ussubv2si3"
@@ -481,7 +480,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsubwus%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsub")]
+   (set_attr "type" "wmmx_wsub")]
 )
 
 (define_insn "*mulv4hi3_iwmmxt"
@@ -491,7 +490,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulul%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmul")]
+   (set_attr "type" "wmmx_wmul")]
 )
 
 (define_insn "smulv4hi3_highpart"
@@ -504,7 +503,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulsm%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmul")]
+   (set_attr "type" "wmmx_wmul")]
 )
 
 (define_insn "umulv4hi3_highpart"
@@ -517,7 +516,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulum%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmul")]
+   (set_attr "type" "wmmx_wmul")]
 )
 
 (define_insn "iwmmxt_wmacs"
@@ -528,7 +527,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmacs%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmac")]
+   (set_attr "type" "wmmx_wmac")]
 )
 
 (define_insn "iwmmxt_wmacsz"
@@ -538,7 +537,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmacsz%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmac")]
+   (set_attr "type" "wmmx_wmac")]
 )
 
 (define_insn "iwmmxt_wmacu"
@@ -549,7 +548,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmacu%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmac")]
+   (set_attr "type" "wmmx_wmac")]
 )
 
 (define_insn "iwmmxt_wmacuz"
@@ -559,7 +558,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmacuz%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmac")]
+   (set_attr "type" "wmmx_wmac")]
 )
 
 ;; Same as xordi3, but don't show input operands so that we don't think
@@ -570,7 +569,7 @@
   "TARGET_REALLY_IWMMXT"
   "wxor%?\\t%0, %0, %0"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wxor")]
+   (set_attr "type" "wmmx_wxor")]
 )
 
 ;; Seems like cse likes to generate these, so we have to support them.
@@ -584,7 +583,7 @@
   "TARGET_REALLY_IWMMXT"
   "wxor%?\\t%0, %0, %0"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wxor")]
+   (set_attr "type" "wmmx_wxor")]
 )
 
 (define_insn "iwmmxt_clrv4hi"
@@ -594,7 +593,7 @@
   "TARGET_REALLY_IWMMXT"
   "wxor%?\\t%0, %0, %0"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wxor")]
+   (set_attr "type" "wmmx_wxor")]
 )
 
 (define_insn "iwmmxt_clrv2si"
@@ -603,7 +602,7 @@
   "TARGET_REALLY_IWMMXT"
   "wxor%?\\t%0, %0, %0"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wxor")]
+   (set_attr "type" "wmmx_wxor")]
 )
 
 ;; Unsigned averages/sum of absolute differences
@@ -627,7 +626,7 @@
   "TARGET_REALLY_IWMMXT"
   "wavg2br%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wavg2")]
+   (set_attr "type" "wmmx_wavg2")]
 )
 
 (define_insn "iwmmxt_uavgrndv4hi3"
@@ -645,7 +644,7 @@
   "TARGET_REALLY_IWMMXT"
   "wavg2hr%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wavg2")]
+   (set_attr "type" "wmmx_wavg2")]
 )
 
 (define_insn "iwmmxt_uavgv8qi3"
@@ -658,7 +657,7 @@
   "TARGET_REALLY_IWMMXT"
   "wavg2b%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wavg2")]
+   (set_attr "type" "wmmx_wavg2")]
 )
 
 (define_insn "iwmmxt_uavgv4hi3"
@@ -671,7 +670,7 @@
   "TARGET_REALLY_IWMMXT"
   "wavg2h%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wavg2")]
+   (set_attr "type" "wmmx_wavg2")]
 )
 
 ;; Insert/extract/shuffle
@@ -690,7 +689,7 @@
    }
    "
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tinsr")]
+   (set_attr "type" "wmmx_tinsr")]
 )
 
 (define_insn "iwmmxt_tinsrh"
@@ -707,7 +706,7 @@
    }
    "
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tinsr")]
+   (set_attr "type" "wmmx_tinsr")]
 )
 
 (define_insn "iwmmxt_tinsrw"
@@ -724,7 +723,7 @@
    }
    "
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tinsr")]
+   (set_attr "type" "wmmx_tinsr")]
 )
 
 (define_insn "iwmmxt_textrmub"
@@ -735,7 +734,7 @@
   "TARGET_REALLY_IWMMXT"
   "textrmub%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "textrm")]
+   (set_attr "type" "wmmx_textrm")]
 )
 
 (define_insn "iwmmxt_textrmsb"
@@ -746,7 +745,7 @@
   "TARGET_REALLY_IWMMXT"
   "textrmsb%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "textrm")]
+   (set_attr "type" "wmmx_textrm")]
 )
 
 (define_insn "iwmmxt_textrmuh"
@@ -757,7 +756,7 @@
   "TARGET_REALLY_IWMMXT"
   "textrmuh%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "textrm")]
+   (set_attr "type" "wmmx_textrm")]
 )
 
 (define_insn "iwmmxt_textrmsh"
@@ -768,7 +767,7 @@
   "TARGET_REALLY_IWMMXT"
   "textrmsh%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "textrm")]
+   (set_attr "type" "wmmx_textrm")]
 )
 
 ;; There are signed/unsigned variants of this instruction, but they are
@@ -780,7 +779,7 @@
   "TARGET_REALLY_IWMMXT"
   "textrmsw%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "textrm")]
+   (set_attr "type" "wmmx_textrm")]
 )
 
 (define_insn "iwmmxt_wshufh"
@@ -790,7 +789,7 @@
   "TARGET_REALLY_IWMMXT"
   "wshufh%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wshufh")]
+   (set_attr "type" "wmmx_wshufh")]
 )
 
 ;; Mask-generating comparisons
@@ -812,7 +811,7 @@
   "TARGET_REALLY_IWMMXT"
   "wcmpeqb%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wcmpeq")]
+   (set_attr "type" "wmmx_wcmpeq")]
 )
 
 (define_insn "eqv4hi3"
@@ -823,7 +822,7 @@
   "TARGET_REALLY_IWMMXT"
   "wcmpeqh%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wcmpeq")]
+   (set_attr "type" "wmmx_wcmpeq")]
 )
 
 (define_insn "eqv2si3"
@@ -835,7 +834,7 @@
   "TARGET_REALLY_IWMMXT"
   "wcmpeqw%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wcmpeq")]
+   (set_attr "type" "wmmx_wcmpeq")]
 )
 
 (define_insn "gtuv8qi3"
@@ -846,7 +845,7 @@
   "TARGET_REALLY_IWMMXT"
   "wcmpgtub%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wcmpgt")]
+   (set_attr "type" "wmmx_wcmpgt")]
 )
 
 (define_insn "gtuv4hi3"
@@ -857,7 +856,7 @@
   "TARGET_REALLY_IWMMXT"
   "wcmpgtuh%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wcmpgt")]
+   (set_attr "type" "wmmx_wcmpgt")]
 )
 
 (define_insn "gtuv2si3"
@@ -868,7 +867,7 @@
   "TARGET_REALLY_IWMMXT"
   "wcmpgtuw%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wcmpgt")]
+   (set_attr "type" "wmmx_wcmpgt")]
 )
 
 (define_insn "gtv8qi3"
@@ -879,7 +878,7 @@
   "TARGET_REALLY_IWMMXT"
   "wcmpgtsb%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wcmpgt")]
+   (set_attr "type" "wmmx_wcmpgt")]
 )
 
 (define_insn "gtv4hi3"
@@ -890,7 +889,7 @@
   "TARGET_REALLY_IWMMXT"
   "wcmpgtsh%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wcmpgt")]
+   (set_attr "type" "wmmx_wcmpgt")]
 )
 
 (define_insn "gtv2si3"
@@ -901,7 +900,7 @@
   "TARGET_REALLY_IWMMXT"
   "wcmpgtsw%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wcmpgt")]
+   (set_attr "type" "wmmx_wcmpgt")]
 )
 
 ;; Max/min insns
@@ -913,7 +912,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmaxs<MMX_char>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmax")]
+   (set_attr "type" "wmmx_wmax")]
 )
 
 (define_insn "*umax<mode>3_iwmmxt"
@@ -923,7 +922,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmaxu<MMX_char>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmax")]
+   (set_attr "type" "wmmx_wmax")]
 )
 
 (define_insn "*smin<mode>3_iwmmxt"
@@ -933,7 +932,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmins<MMX_char>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmin")]
+   (set_attr "type" "wmmx_wmin")]
 )
 
 (define_insn "*umin<mode>3_iwmmxt"
@@ -943,7 +942,7 @@
   "TARGET_REALLY_IWMMXT"
   "wminu<MMX_char>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmin")]
+   (set_attr "type" "wmmx_wmin")]
 )
 
 ;; Pack/unpack insns.
@@ -956,7 +955,7 @@
   "TARGET_REALLY_IWMMXT"
   "wpackhss%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wpack")]
+   (set_attr "type" "wmmx_wpack")]
 )
 
 (define_insn "iwmmxt_wpackwss"
@@ -967,7 +966,7 @@
   "TARGET_REALLY_IWMMXT"
   "wpackwss%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wpack")]
+   (set_attr "type" "wmmx_wpack")]
 )
 
 (define_insn "iwmmxt_wpackdss"
@@ -978,7 +977,7 @@
   "TARGET_REALLY_IWMMXT"
   "wpackdss%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wpack")]
+   (set_attr "type" "wmmx_wpack")]
 )
 
 (define_insn "iwmmxt_wpackhus"
@@ -989,7 +988,7 @@
   "TARGET_REALLY_IWMMXT"
   "wpackhus%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wpack")]
+   (set_attr "type" "wmmx_wpack")]
 )
 
 (define_insn "iwmmxt_wpackwus"
@@ -1000,7 +999,7 @@
   "TARGET_REALLY_IWMMXT"
   "wpackwus%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wpack")]
+   (set_attr "type" "wmmx_wpack")]
 )
 
 (define_insn "iwmmxt_wpackdus"
@@ -1011,7 +1010,7 @@
   "TARGET_REALLY_IWMMXT"
   "wpackdus%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wpack")]
+   (set_attr "type" "wmmx_wpack")]
 )
 
 (define_insn "iwmmxt_wunpckihb"
@@ -1039,7 +1038,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckihb%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckih")]
+   (set_attr "type" "wmmx_wunpckih")]
 )
 
 (define_insn "iwmmxt_wunpckihh"
@@ -1059,7 +1058,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckihh%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckih")]
+   (set_attr "type" "wmmx_wunpckih")]
 )
 
 (define_insn "iwmmxt_wunpckihw"
@@ -1075,7 +1074,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckihw%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckih")]
+   (set_attr "type" "wmmx_wunpckih")]
 )
 
 (define_insn "iwmmxt_wunpckilb"
@@ -1103,7 +1102,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckilb%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckil")]
+   (set_attr "type" "wmmx_wunpckil")]
 )
 
 (define_insn "iwmmxt_wunpckilh"
@@ -1123,7 +1122,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckilh%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckil")]
+   (set_attr "type" "wmmx_wunpckil")]
 )
 
 (define_insn "iwmmxt_wunpckilw"
@@ -1139,7 +1138,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckilw%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckil")]
+   (set_attr "type" "wmmx_wunpckil")]
 )
 
 (define_insn "iwmmxt_wunpckehub"
@@ -1151,7 +1150,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckehub%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckeh")]
+   (set_attr "type" "wmmx_wunpckeh")]
 )
 
 (define_insn "iwmmxt_wunpckehuh"
@@ -1162,7 +1161,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckehuh%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckeh")]
+   (set_attr "type" "wmmx_wunpckeh")]
 )
 
 (define_insn "iwmmxt_wunpckehuw"
@@ -1173,7 +1172,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckehuw%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckeh")]
+   (set_attr "type" "wmmx_wunpckeh")]
 )
 
 (define_insn "iwmmxt_wunpckehsb"
@@ -1185,7 +1184,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckehsb%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckeh")]
+   (set_attr "type" "wmmx_wunpckeh")]
 )
 
 (define_insn "iwmmxt_wunpckehsh"
@@ -1196,7 +1195,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckehsh%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckeh")]
+   (set_attr "type" "wmmx_wunpckeh")]
 )
 
 (define_insn "iwmmxt_wunpckehsw"
@@ -1207,7 +1206,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckehsw%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckeh")]
+   (set_attr "type" "wmmx_wunpckeh")]
 )
 
 (define_insn "iwmmxt_wunpckelub"
@@ -1219,7 +1218,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckelub%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckel")]
+   (set_attr "type" "wmmx_wunpckel")]
 )
 
 (define_insn "iwmmxt_wunpckeluh"
@@ -1230,7 +1229,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckeluh%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckel")]
+   (set_attr "type" "wmmx_wunpckel")]
 )
 
 (define_insn "iwmmxt_wunpckeluw"
@@ -1241,7 +1240,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckeluw%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckel")]
+   (set_attr "type" "wmmx_wunpckel")]
 )
 
 (define_insn "iwmmxt_wunpckelsb"
@@ -1253,7 +1252,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckelsb%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckel")]
+   (set_attr "type" "wmmx_wunpckel")]
 )
 
 (define_insn "iwmmxt_wunpckelsh"
@@ -1264,7 +1263,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckelsh%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckel")]
+   (set_attr "type" "wmmx_wunpckel")]
 )
 
 (define_insn "iwmmxt_wunpckelsw"
@@ -1275,7 +1274,7 @@
   "TARGET_REALLY_IWMMXT"
   "wunpckelsw%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wunpckel")]
+   (set_attr "type" "wmmx_wunpckel")]
 )
 
 ;; Shifts
@@ -1298,7 +1297,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "arch" "*, iwmmxt2")
-   (set_attr "wtype" "wror, wror")]
+   (set_attr "type" "wmmx_wror, wmmx_wror")]
 )
 
 (define_insn "ashr<mode>3_iwmmxt"
@@ -1319,7 +1318,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "arch" "*, iwmmxt2")
-   (set_attr "wtype" "wsra, wsra")]
+   (set_attr "type" "wmmx_wsra, wmmx_wsra")]
 )
 
 (define_insn "lshr<mode>3_iwmmxt"
@@ -1340,7 +1339,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "arch" "*, iwmmxt2")
-   (set_attr "wtype" "wsrl, wsrl")]
+   (set_attr "type" "wmmx_wsrl, wmmx_wsrl")]
 )
 
 (define_insn "ashl<mode>3_iwmmxt"
@@ -1361,7 +1360,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "arch" "*, iwmmxt2")
-   (set_attr "wtype" "wsll, wsll")]
+   (set_attr "type" "wmmx_wsll, wmmx_wsll")]
 )
 
 (define_insn "ror<mode>3_di"
@@ -1382,7 +1381,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "arch" "*, iwmmxt2")
-   (set_attr "wtype" "wror, wror")]
+   (set_attr "type" "wmmx_wror, wmmx_wror")]
 )
 
 (define_insn "ashr<mode>3_di"
@@ -1403,7 +1402,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "arch" "*, iwmmxt2")
-   (set_attr "wtype" "wsra, wsra")]
+   (set_attr "type" "wmmx_wsra, wmmx_wsra")]
 )
 
 (define_insn "lshr<mode>3_di"
@@ -1424,7 +1423,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "arch" "*, iwmmxt2")
-   (set_attr "wtype" "wsrl, wsrl")]
+   (set_attr "type" "wmmx_wsrl, wmmx_wsrl")]
 )
 
 (define_insn "ashl<mode>3_di"
@@ -1445,7 +1444,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "arch" "*, iwmmxt2")
-   (set_attr "wtype" "wsll, wsll")]
+   (set_attr "type" "wmmx_wsll, wmmx_wsll")]
 )
 
 (define_insn "iwmmxt_wmadds"
@@ -1464,7 +1463,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmadds%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmadd")]
+   (set_attr "type" "wmmx_wmadd")]
 )
 
 (define_insn "iwmmxt_wmaddu"
@@ -1483,7 +1482,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmaddu%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmadd")]
+   (set_attr "type" "wmmx_wmadd")]
 )
 
 (define_insn "iwmmxt_tmia"
@@ -1496,7 +1495,7 @@
   "TARGET_REALLY_IWMMXT"
   "tmia%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tmia")]
+   (set_attr "type" "wmmx_tmia")]
 )
 
 (define_insn "iwmmxt_tmiaph"
@@ -1514,7 +1513,7 @@
   "TARGET_REALLY_IWMMXT"
   "tmiaph%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tmiaph")]
+   (set_attr "type" "wmmx_tmiaph")]
 )
 
 (define_insn "iwmmxt_tmiabb"
@@ -1527,7 +1526,7 @@
   "TARGET_REALLY_IWMMXT"
   "tmiabb%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tmiaxy")]
+   (set_attr "type" "wmmx_tmiaxy")]
 )
 
 (define_insn "iwmmxt_tmiatb"
@@ -1544,7 +1543,7 @@
   "TARGET_REALLY_IWMMXT"
   "tmiatb%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tmiaxy")]
+   (set_attr "type" "wmmx_tmiaxy")]
 )
 
 (define_insn "iwmmxt_tmiabt"
@@ -1561,7 +1560,7 @@
   "TARGET_REALLY_IWMMXT"
   "tmiabt%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tmiaxy")]
+   (set_attr "type" "wmmx_tmiaxy")]
 )
 
 (define_insn "iwmmxt_tmiatt"
@@ -1580,7 +1579,7 @@
   "TARGET_REALLY_IWMMXT"
   "tmiatt%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tmiaxy")]
+   (set_attr "type" "wmmx_tmiaxy")]
 )
 
 (define_insn "iwmmxt_tmovmskb"
@@ -1589,7 +1588,7 @@
   "TARGET_REALLY_IWMMXT"
   "tmovmskb%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tmovmsk")]
+   (set_attr "type" "wmmx_tmovmsk")]
 )
 
 (define_insn "iwmmxt_tmovmskh"
@@ -1598,7 +1597,7 @@
   "TARGET_REALLY_IWMMXT"
   "tmovmskh%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tmovmsk")]
+   (set_attr "type" "wmmx_tmovmsk")]
 )
 
 (define_insn "iwmmxt_tmovmskw"
@@ -1607,7 +1606,7 @@
   "TARGET_REALLY_IWMMXT"
   "tmovmskw%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tmovmsk")]
+   (set_attr "type" "wmmx_tmovmsk")]
 )
 
 (define_insn "iwmmxt_waccb"
@@ -1616,7 +1615,7 @@
   "TARGET_REALLY_IWMMXT"
   "waccb%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wacc")]
+   (set_attr "type" "wmmx_wacc")]
 )
 
 (define_insn "iwmmxt_wacch"
@@ -1625,7 +1624,7 @@
   "TARGET_REALLY_IWMMXT"
   "wacch%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wacc")]
+   (set_attr "type" "wmmx_wacc")]
 )
 
 (define_insn "iwmmxt_waccw"
@@ -1634,7 +1633,7 @@
   "TARGET_REALLY_IWMMXT"
   "waccw%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wacc")]
+   (set_attr "type" "wmmx_wacc")]
 )
 
 ;; use unspec here to prevent 8 * imm to be optimized by cse
@@ -1651,7 +1650,7 @@
   "TARGET_REALLY_IWMMXT"
   "waligni%?\\t%0, %1, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "waligni")]
+   (set_attr "type" "wmmx_waligni")]
 )
 
 (define_insn "iwmmxt_walignr"
@@ -1666,7 +1665,7 @@
   "TARGET_REALLY_IWMMXT"
   "walignr%U3%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "walignr")]
+   (set_attr "type" "wmmx_walignr")]
 )
 
 (define_insn "iwmmxt_walignr0"
@@ -1681,7 +1680,7 @@
   "TARGET_REALLY_IWMMXT"
   "walignr0%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "walignr")]
+   (set_attr "type" "wmmx_walignr")]
 )
 
 (define_insn "iwmmxt_walignr1"
@@ -1696,7 +1695,7 @@
   "TARGET_REALLY_IWMMXT"
   "walignr1%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "walignr")]
+   (set_attr "type" "wmmx_walignr")]
 )
 
 (define_insn "iwmmxt_walignr2"
@@ -1711,7 +1710,7 @@
   "TARGET_REALLY_IWMMXT"
   "walignr2%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "walignr")]
+   (set_attr "type" "wmmx_walignr")]
 )
 
 (define_insn "iwmmxt_walignr3"
@@ -1726,7 +1725,7 @@
   "TARGET_REALLY_IWMMXT"
   "walignr3%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "walignr")]
+   (set_attr "type" "wmmx_walignr")]
 )
 
 (define_insn "iwmmxt_wsadb"
@@ -1738,7 +1737,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsadb%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsad")]
+   (set_attr "type" "wmmx_wsad")]
 )
 
 (define_insn "iwmmxt_wsadh"
@@ -1750,7 +1749,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsadh%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsad")]
+   (set_attr "type" "wmmx_wsad")]
 )
 
 (define_insn "iwmmxt_wsadbz"
@@ -1760,7 +1759,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsadbz%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsad")]
+   (set_attr "type" "wmmx_wsad")]
 )
 
 (define_insn "iwmmxt_wsadhz"
@@ -1770,7 +1769,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsadhz%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsad")]
+   (set_attr "type" "wmmx_wsad")]
 )
 
 (include "iwmmxt2.md")
Index: b/src/gcc/config/arm/aarch-common.c
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/aarch-common.c
@@ -0,0 +1,356 @@
+/* Dependency checks for instruction scheduling, shared between ARM and
+   AARCH64.
+
+   Copyright (C) 1991-2013 Free Software Foundation, Inc.
+   Contributed by ARM Ltd.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+
+/* Return nonzero if the CONSUMER instruction (a load) does need
+   PRODUCER's value to calculate the address.  */
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "tm.h"
+#include "tm_p.h"
+#include "rtl.h"
+#include "tree.h"
+#include "c-family/c-common.h"
+#include "rtl.h"
+
+typedef struct
+{
+  rtx_code search_code;
+  rtx search_result;
+  bool find_any_shift;
+} search_term;
+
+/* Return TRUE if X is either an arithmetic shift left, or
+   is a multiplication by a power of two.  */
+static bool
+arm_rtx_shift_left_p (rtx x)
+{
+  enum rtx_code code = GET_CODE (x);
+
+  if (code == MULT && CONST_INT_P (XEXP (x, 1))
+      && exact_log2 (INTVAL (XEXP (x, 1))) > 0)
+    return true;
+
+  if (code == ASHIFT)
+    return true;
+
+  return false;
+}
+
+static rtx_code shift_rtx_codes[] =
+  { ASHIFT, ROTATE, ASHIFTRT, LSHIFTRT,
+    ROTATERT, ZERO_EXTEND, SIGN_EXTEND };
+
+/* Callback function for arm_find_sub_rtx_with_code.
+   DATA is safe to treat as a SEARCH_TERM, ST.  This will
+   hold a SEARCH_CODE.  PATTERN is checked to see if it is an
+   RTX with that code.  If it is, write SEARCH_RESULT in ST
+   and return 1.  Otherwise, or if we have been passed a NULL_RTX
+   return 0.  If ST.FIND_ANY_SHIFT then we are interested in
+   anything which can reasonably be described as a SHIFT RTX.  */
+static int
+arm_find_sub_rtx_with_search_term (rtx *pattern, void *data)
+{
+  search_term *st = (search_term *) data;
+  rtx_code pattern_code;
+  int found = 0;
+
+  gcc_assert (pattern);
+  gcc_assert (st);
+
+  /* Poorly formed patterns can really ruin our day.  */
+  if (*pattern == NULL_RTX)
+    return 0;
+
+  pattern_code = GET_CODE (*pattern);
+
+  if (st->find_any_shift)
+    {
+      unsigned i = 0;
+
+      /* Left shifts might have been canonicalized to a MULT of some
+	 power of two.  Make sure we catch them.  */
+      if (arm_rtx_shift_left_p (*pattern))
+	found = 1;
+      else
+	for (i = 0; i < ARRAY_SIZE (shift_rtx_codes); i++)
+	  if (pattern_code == shift_rtx_codes[i])
+	    found = 1;
+    }
+
+  if (pattern_code == st->search_code)
+    found = 1;
+
+  if (found)
+    st->search_result = *pattern;
+
+  return found;
+}
+
+/* Traverse PATTERN looking for a sub-rtx with RTX_CODE CODE.  */
+static rtx
+arm_find_sub_rtx_with_code (rtx pattern, rtx_code code, bool find_any_shift)
+{
+  search_term st;
+  int result = 0;
+
+  gcc_assert (pattern != NULL_RTX);
+  st.search_code = code;
+  st.search_result = NULL_RTX;
+  st.find_any_shift = find_any_shift;
+  result = for_each_rtx (&pattern, arm_find_sub_rtx_with_search_term, &st);
+  if (result)
+    return st.search_result;
+  else
+    return NULL_RTX;
+}
+
+/* Traverse PATTERN looking for any sub-rtx which looks like a shift.  */
+static rtx
+arm_find_shift_sub_rtx (rtx pattern)
+{
+  return arm_find_sub_rtx_with_code (pattern, ASHIFT, true);
+}
+
+/* PRODUCER and CONSUMER are two potentially dependant RTX.  PRODUCER
+   (possibly) contains a SET which will provide a result we can access
+   using the SET_DEST macro.  We will place the RTX which would be
+   written by PRODUCER in SET_SOURCE.
+   Similarly, CONSUMER (possibly) contains a SET which has an operand
+   we can access using SET_SRC.  We place this operand in
+   SET_DESTINATION.
+
+   Return nonzero if we found the SET RTX we expected.  */
+static int
+arm_get_set_operands (rtx producer, rtx consumer,
+		      rtx *set_source, rtx *set_destination)
+{
+  rtx set_producer = arm_find_sub_rtx_with_code (producer, SET, false);
+  rtx set_consumer = arm_find_sub_rtx_with_code (consumer, SET, false);
+
+  if (set_producer && set_consumer)
+    {
+      *set_source = SET_DEST (set_producer);
+      *set_destination = SET_SRC (set_consumer);
+      return 1;
+    }
+  return 0;
+}
+
+/* Return nonzero if the CONSUMER instruction (a load) does need
+   PRODUCER's value to calculate the address.  */
+int
+arm_early_load_addr_dep (rtx producer, rtx consumer)
+{
+  rtx value, addr;
+
+  if (!arm_get_set_operands (producer, consumer, &value, &addr))
+    return 0;
+
+  return reg_overlap_mentioned_p (value, addr);
+}
+
+/* Return nonzero if the CONSUMER instruction (an ALU op) does not
+   have an early register shift value or amount dependency on the
+   result of PRODUCER.  */
+int
+arm_no_early_alu_shift_dep (rtx producer, rtx consumer)
+{
+  rtx value, op;
+  rtx early_op;
+
+  if (!arm_get_set_operands (producer, consumer, &value, &op))
+    return 0;
+
+  if ((early_op = arm_find_shift_sub_rtx (op)))
+    {
+      if (REG_P (early_op))
+	early_op = op;
+
+      return !reg_overlap_mentioned_p (value, early_op);
+    }
+
+  return 0;
+}
+
+/* Return nonzero if the CONSUMER instruction (an ALU op) does not
+   have an early register shift value dependency on the result of
+   PRODUCER.  */
+int
+arm_no_early_alu_shift_value_dep (rtx producer, rtx consumer)
+{
+  rtx value, op;
+  rtx early_op;
+
+  if (!arm_get_set_operands (producer, consumer, &value, &op))
+    return 0;
+
+  if ((early_op = arm_find_shift_sub_rtx (op)))
+    /* We want to check the value being shifted.  */
+    if (!reg_overlap_mentioned_p (value, XEXP (early_op, 0)))
+      return 1;
+
+  return 0;
+}
+
+/* Return nonzero if the CONSUMER (a mul or mac op) does not
+   have an early register mult dependency on the result of
+   PRODUCER.  */
+int
+arm_no_early_mul_dep (rtx producer, rtx consumer)
+{
+  rtx value, op;
+
+  if (!arm_get_set_operands (producer, consumer, &value, &op))
+    return 0;
+
+  if (GET_CODE (op) == PLUS || GET_CODE (op) == MINUS)
+    {
+      if (GET_CODE (XEXP (op, 0)) == MULT)
+	return !reg_overlap_mentioned_p (value, XEXP (op, 0));
+      else
+	return !reg_overlap_mentioned_p (value, XEXP (op, 1));
+    }
+
+  return 0;
+}
+
+/* Return nonzero if the CONSUMER instruction (a store) does not need
+   PRODUCER's value to calculate the address.  */
+
+int
+arm_no_early_store_addr_dep (rtx producer, rtx consumer)
+{
+  rtx value = arm_find_sub_rtx_with_code (producer, SET, false);
+  rtx addr = arm_find_sub_rtx_with_code (consumer, SET, false);
+
+  if (value)
+    value = SET_DEST (value);
+
+  if (addr)
+    addr = SET_DEST (addr);
+
+  if (!value || !addr)
+    return 0;
+
+  return !reg_overlap_mentioned_p (value, addr);
+}
+
+/* Return nonzero if the CONSUMER instruction (a store) does need
+   PRODUCER's value to calculate the address.  */
+
+int
+arm_early_store_addr_dep (rtx producer, rtx consumer)
+{
+  return !arm_no_early_store_addr_dep (producer, consumer);
+}
+
+/* Return non-zero iff the consumer (a multiply-accumulate or a
+   multiple-subtract instruction) has an accumulator dependency on the
+   result of the producer and no other dependency on that result.  It
+   does not check if the producer is multiply-accumulate instruction.  */
+int
+arm_mac_accumulator_is_result (rtx producer, rtx consumer)
+{
+  rtx result;
+  rtx op0, op1, acc;
+
+  producer = PATTERN (producer);
+  consumer = PATTERN (consumer);
+
+  if (GET_CODE (producer) == COND_EXEC)
+    producer = COND_EXEC_CODE (producer);
+  if (GET_CODE (consumer) == COND_EXEC)
+    consumer = COND_EXEC_CODE (consumer);
+
+  if (GET_CODE (producer) != SET)
+    return 0;
+
+  result = XEXP (producer, 0);
+
+  if (GET_CODE (consumer) != SET)
+    return 0;
+
+  /* Check that the consumer is of the form
+     (set (...) (plus (mult ...) (...)))
+     or
+     (set (...) (minus (...) (mult ...))).  */
+  if (GET_CODE (XEXP (consumer, 1)) == PLUS)
+    {
+      if (GET_CODE (XEXP (XEXP (consumer, 1), 0)) != MULT)
+        return 0;
+
+      op0 = XEXP (XEXP (XEXP (consumer, 1), 0), 0);
+      op1 = XEXP (XEXP (XEXP (consumer, 1), 0), 1);
+      acc = XEXP (XEXP (consumer, 1), 1);
+    }
+  else if (GET_CODE (XEXP (consumer, 1)) == MINUS)
+    {
+      if (GET_CODE (XEXP (XEXP (consumer, 1), 1)) != MULT)
+        return 0;
+
+      op0 = XEXP (XEXP (XEXP (consumer, 1), 1), 0);
+      op1 = XEXP (XEXP (XEXP (consumer, 1), 1), 1);
+      acc = XEXP (XEXP (consumer, 1), 0);
+    }
+  else
+    return 0;
+
+  return (reg_overlap_mentioned_p (result, acc)
+          && !reg_overlap_mentioned_p (result, op0)
+          && !reg_overlap_mentioned_p (result, op1));
+}
+
+/* Return non-zero if the consumer (a multiply-accumulate instruction)
+   has an accumulator dependency on the result of the producer (a
+   multiplication instruction) and no other dependency on that result.  */
+int
+arm_mac_accumulator_is_mul_result (rtx producer, rtx consumer)
+{
+  rtx mul = PATTERN (producer);
+  rtx mac = PATTERN (consumer);
+  rtx mul_result;
+  rtx mac_op0, mac_op1, mac_acc;
+
+  if (GET_CODE (mul) == COND_EXEC)
+    mul = COND_EXEC_CODE (mul);
+  if (GET_CODE (mac) == COND_EXEC)
+    mac = COND_EXEC_CODE (mac);
+
+  /* Check that mul is of the form (set (...) (mult ...))
+     and mla is of the form (set (...) (plus (mult ...) (...))).  */
+  if ((GET_CODE (mul) != SET || GET_CODE (XEXP (mul, 1)) != MULT)
+      || (GET_CODE (mac) != SET || GET_CODE (XEXP (mac, 1)) != PLUS
+          || GET_CODE (XEXP (XEXP (mac, 1), 0)) != MULT))
+    return 0;
+
+  mul_result = XEXP (mul, 0);
+  mac_op0 = XEXP (XEXP (XEXP (mac, 1), 0), 0);
+  mac_op1 = XEXP (XEXP (XEXP (mac, 1), 0), 1);
+  mac_acc = XEXP (XEXP (mac, 1), 1);
+
+  return (reg_overlap_mentioned_p (mul_result, mac_acc)
+          && !reg_overlap_mentioned_p (mul_result, mac_op0)
+          && !reg_overlap_mentioned_p (mul_result, mac_op1));
+}
Index: b/src/gcc/config/arm/cortex-a53.md
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/cortex-a53.md
@@ -0,0 +1,309 @@
+;; ARM Cortex-A53 pipeline description
+;; Copyright (C) 2013 Free Software Foundation, Inc.
+;;
+;; Contributed by ARM Ltd.
+;;
+;; This file is part of GCC.
+;;
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published by
+;; the Free Software Foundation; either version 3, or (at your option)
+;; any later version.
+;;
+;; GCC is distributed in the hope that it will be useful, but
+;; WITHOUT ANY WARRANTY; without even the implied warranty of
+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+;; General Public License for more details.
+;;
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+(define_automaton "cortex_a53")
+
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; Functional units.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+;; There are two main integer execution pipelines, described as
+;; slot 0 and issue slot 1.
+
+(define_cpu_unit "cortex_a53_slot0" "cortex_a53")
+(define_cpu_unit "cortex_a53_slot1" "cortex_a53")
+
+(define_reservation "cortex_a53_slot_any" "cortex_a53_slot0|cortex_a53_slot1")
+(define_reservation "cortex_a53_single_issue" "cortex_a53_slot0+cortex_a53_slot1")
+
+;; The load/store pipeline.  Load/store instructions can dual-issue from
+;; either pipeline, but two load/stores cannot simultaneously issue.
+
+(define_cpu_unit "cortex_a53_ls" "cortex_a53")
+
+;; The store pipeline.  Shared between both execution pipelines.
+
+(define_cpu_unit "cortex_a53_store" "cortex_a53")
+
+;; The branch pipeline.  Branches can dual-issue with other instructions
+;; (except when those instructions take multiple cycles to issue).
+
+(define_cpu_unit "cortex_a53_branch" "cortex_a53")
+
+;; The integer divider.
+
+(define_cpu_unit "cortex_a53_idiv" "cortex_a53")
+
+;; The floating-point add pipeline used to model the usage
+;; of the add pipeline by fmac instructions.
+
+(define_cpu_unit "cortex_a53_fpadd_pipe" "cortex_a53")
+
+;; Floating-point div/sqrt (long latency, out-of-order completion).
+
+(define_cpu_unit "cortex_a53_fp_div_sqrt" "cortex_a53")
+
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; ALU instructions.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+(define_insn_reservation "cortex_a53_alu" 2
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        adr,bfm,csel,rev,\
+                        shift_imm,shift_reg,\
+                        mov_imm,mov_reg,mvn_imm,mvn_reg,\
+                        mrs,multiple,no_insn"))
+  "cortex_a53_slot_any")
+
+(define_insn_reservation "cortex_a53_alu_shift" 2
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "alu_shift_imm,alus_shift_imm,\
+                        logic_shift_imm,logics_shift_imm,\
+                        alu_shift_reg,alus_shift_reg,\
+                        logic_shift_reg,logics_shift_reg,\
+                        extend,mov_shift,mov_shift_reg,\
+                        mvn_shift,mvn_shift_reg"))
+  "cortex_a53_slot_any")
+
+;; Forwarding path for unshifted operands.
+
+(define_bypass 1 "cortex_a53_alu,cortex_a53_alu_shift"
+  "cortex_a53_alu")
+
+(define_bypass 1 "cortex_a53_alu,cortex_a53_alu_shift"
+  "cortex_a53_alu_shift"
+  "arm_no_early_alu_shift_dep")
+
+;; The multiplier pipeline can forward results so there's no need to specify
+;; bypasses. Multiplies can only single-issue currently.
+
+(define_insn_reservation "cortex_a53_mul" 3
+  (and (eq_attr "tune" "cortexa53")
+       (ior (eq_attr "mul32" "yes")
+            (eq_attr "mul64" "yes")))
+  "cortex_a53_single_issue")
+
+;; A multiply with a single-register result or an MLA, followed by an
+;; MLA with an accumulator dependency, has its result forwarded so two
+;; such instructions can issue back-to-back.
+
+(define_bypass 1 "cortex_a53_mul"
+               "cortex_a53_mul"
+               "arm_mac_accumulator_is_mul_result")
+
+;; Punt with a high enough latency for divides.
+(define_insn_reservation "cortex_a53_udiv" 8
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "udiv"))
+  "(cortex_a53_slot0+cortex_a53_idiv),cortex_a53_idiv*7")
+
+(define_insn_reservation "cortex_a53_sdiv" 9
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "sdiv"))
+  "(cortex_a53_slot0+cortex_a53_idiv),cortex_a53_idiv*8")
+
+
+(define_bypass 2 "cortex_a53_mul,cortex_a53_udiv,cortex_a53_sdiv"
+               "cortex_a53_alu")
+(define_bypass 2 "cortex_a53_mul,cortex_a53_udiv,cortex_a53_sdiv"
+               "cortex_a53_alu_shift"
+               "arm_no_early_alu_shift_dep")
+
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; Load/store instructions.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+;; Address-generation happens in the issue stage.
+
+(define_insn_reservation "cortex_a53_load1" 3
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "load_byte,load1,load_acq"))
+  "cortex_a53_slot_any+cortex_a53_ls")
+
+(define_insn_reservation "cortex_a53_store1" 2
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "store1,store_rel"))
+  "cortex_a53_slot_any+cortex_a53_ls+cortex_a53_store")
+
+(define_insn_reservation "cortex_a53_load2" 3
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "load2"))
+  "cortex_a53_single_issue+cortex_a53_ls")
+
+(define_insn_reservation "cortex_a53_store2" 2
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "store2"))
+  "cortex_a53_single_issue+cortex_a53_ls+cortex_a53_store")
+
+(define_insn_reservation "cortex_a53_load3plus" 4
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "load3,load4"))
+  "(cortex_a53_single_issue+cortex_a53_ls)*2")
+
+(define_insn_reservation "cortex_a53_store3plus" 3
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "store3,store4"))
+  "(cortex_a53_single_issue+cortex_a53_ls+cortex_a53_store)*2")
+
+;; Load/store addresses are required early in Issue.
+(define_bypass 3 "cortex_a53_load1,cortex_a53_load2,cortex_a53_load3plus,cortex_a53_alu,cortex_a53_alu_shift"
+                 "cortex_a53_load*"
+                 "arm_early_load_addr_dep")
+(define_bypass 3 "cortex_a53_load1,cortex_a53_load2,cortex_a53_load3plus,cortex_a53_alu,cortex_a53_alu_shift"
+                 "cortex_a53_store*"
+                 "arm_early_store_addr_dep")
+
+;; Load data can forward in the ALU pipeline
+(define_bypass 2 "cortex_a53_load1,cortex_a53_load2"
+               "cortex_a53_alu")
+(define_bypass 2 "cortex_a53_load1,cortex_a53_load2"
+               "cortex_a53_alu_shift"
+               "arm_no_early_alu_shift_dep")
+
+;; ALU ops can forward to stores.
+(define_bypass 0 "cortex_a53_alu,cortex_a53_alu_shift"
+                 "cortex_a53_store1,cortex_a53_store2,cortex_a53_store3plus"
+                 "arm_no_early_store_addr_dep")
+
+(define_bypass 1 "cortex_a53_mul,cortex_a53_udiv,cortex_a53_sdiv,cortex_a53_load1,cortex_a53_load2,cortex_a53_load3plus"
+                 "cortex_a53_store1,cortex_a53_store2,cortex_a53_store3plus"
+                 "arm_no_early_store_addr_dep")
+
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; Branches.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+;; Currently models all branches as dual-issuable from either execution
+;; slot, which isn't true for all cases. We still need to model indirect
+;; branches.
+
+(define_insn_reservation "cortex_a53_branch" 0
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "branch,call"))
+  "cortex_a53_slot_any+cortex_a53_branch")
+
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; Floating-point arithmetic.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+(define_insn_reservation "cortex_a53_fpalu" 4
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "ffariths, fadds, ffarithd, faddd, fmov, fmuls,\
+                        f_cvt,f_cvtf2i,f_cvti2f,\
+			fcmps, fcmpd, fcsel"))
+  "cortex_a53_slot0+cortex_a53_fpadd_pipe")
+
+(define_insn_reservation "cortex_a53_fconst" 2
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "fconsts,fconstd"))
+  "cortex_a53_slot0+cortex_a53_fpadd_pipe")
+
+(define_insn_reservation "cortex_a53_fpmul" 4
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "fmuls,fmuld"))
+  "cortex_a53_slot0")
+
+;; For single-precision multiply-accumulate, the add (accumulate) is issued after
+;; the multiply completes. Model that accordingly.
+
+(define_insn_reservation "cortex_a53_fpmac" 8
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "fmacs,fmacd,ffmas,ffmad"))
+  "cortex_a53_slot0, nothing*3, cortex_a53_fpadd_pipe")
+
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; Floating-point divide/square root instructions.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; fsqrt really takes one cycle less, but that is not modelled.
+
+(define_insn_reservation "cortex_a53_fdivs" 14
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "fdivs, fsqrts"))
+  "cortex_a53_slot0, cortex_a53_fp_div_sqrt * 13")
+
+(define_insn_reservation "cortex_a53_fdivd" 29
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "fdivd, fsqrtd"))
+  "cortex_a53_slot0, cortex_a53_fp_div_sqrt * 28")
+
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; VFP to/from core transfers.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+(define_insn_reservation "cortex_a53_r2f" 4
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "f_mcr,f_mcrr"))
+  "cortex_a53_slot0")
+
+(define_insn_reservation "cortex_a53_f2r" 2
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "f_mrc,f_mrrc"))
+  "cortex_a53_slot0")
+
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; VFP flag transfer.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+(define_insn_reservation "cortex_a53_f_flags" 4
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "f_flag"))
+  "cortex_a53_slot0")
+
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;; VFP load/store.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+(define_insn_reservation "cortex_a53_f_loads" 4
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "f_loads"))
+  "cortex_a53_slot0")
+
+(define_insn_reservation "cortex_a53_f_loadd" 5
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "f_loadd"))
+  "cortex_a53_slot0")
+
+(define_insn_reservation "cortex_a53_f_stores" 0
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "f_stores"))
+  "cortex_a53_slot0")
+
+(define_insn_reservation "cortex_a53_f_stored" 0
+  (and (eq_attr "tune" "cortexa53")
+       (eq_attr "type" "f_stored"))
+  "cortex_a53_slot0")
+
+;; Load-to-use for floating-point values has a penalty of one cycle,
+;; i.e. a latency of two.
+
+(define_bypass 2 "cortex_a53_f_loads"
+                 "cortex_a53_fpalu, cortex_a53_fpmac, cortex_a53_fpmul,\
+		  cortex_a53_fdivs, cortex_a53_fdivd,\
+		  cortex_a53_f2r")
+
+(define_bypass 2 "cortex_a53_f_loadd"
+                 "cortex_a53_fpalu, cortex_a53_fpmac, cortex_a53_fpmul,\
+		  cortex_a53_fdivs, cortex_a53_fdivd,\
+		  cortex_a53_f2r")
+
Index: b/src/gcc/config/arm/cortex-r4f.md
===================================================================
--- a/src/gcc/config/arm/cortex-r4f.md
+++ b/src/gcc/config/arm/cortex-r4f.md
@@ -48,7 +48,7 @@
 
 (define_insn_reservation "cortex_r4_fcpys" 2
  (and (eq_attr "tune_cortexr4" "yes")
-      (eq_attr "type" "fcpys"))
+      (eq_attr "type" "fmov"))
  "cortex_r4_issue_ab")
 
 (define_insn_reservation "cortex_r4_ffariths" 2
@@ -68,7 +68,7 @@
 
 (define_insn_reservation "cortex_r4_fdivs" 17
  (and (eq_attr "tune_cortexr4" "yes")
-      (eq_attr "type" "fdivs"))
+      (eq_attr "type" "fdivs, fsqrts"))
  "cortex_r4_issue_ab+cortex_r4_v1,cortex_r4_issue_a+cortex_r4_v1")
 
 (define_insn_reservation "cortex_r4_floads" 2
@@ -83,12 +83,12 @@
 
 (define_insn_reservation "cortex_r4_mcr" 2
  (and (eq_attr "tune_cortexr4" "yes")
-      (eq_attr "type" "r_2_f"))
+      (eq_attr "type" "f_mcr,f_mcrr"))
  "cortex_r4_issue_ab")
 
 (define_insn_reservation "cortex_r4_mrc" 3
  (and (eq_attr "tune_cortexr4" "yes")
-      (eq_attr "type" "f_2_r"))
+      (eq_attr "type" "f_mrc,f_mrrc"))
  "cortex_r4_issue_ab")
 
 ;; Bypasses for normal (not early) regs.
@@ -131,7 +131,7 @@
 ;; out of order.  Chances are this is not a pipelined operation.
 (define_insn_reservation "cortex_r4_fdivd" 97
  (and (eq_attr "tune_cortexr4" "yes")
-      (eq_attr "type" "fdivd"))
+      (eq_attr "type" "fdivd, fsqrtd"))
  "cortex_r4_single_issue*3")
 
 (define_insn_reservation "cortex_r4_ffarithd" 2
@@ -146,7 +146,7 @@
 
 (define_insn_reservation "cortex_r4_f_cvt" 8
  (and (eq_attr "tune_cortexr4" "yes")
-      (eq_attr "type" "f_cvt"))
+      (eq_attr "type" "f_cvt,f_cvtf2i,f_cvti2f"))
  "cortex_r4_single_issue*3")
 
 (define_insn_reservation "cortex_r4_f_memd" 8
Index: b/src/gcc/config/arm/bpabi.h
===================================================================
--- a/src/gcc/config/arm/bpabi.h
+++ b/src/gcc/config/arm/bpabi.h
@@ -60,6 +60,7 @@
    |mcpu=cortex-a7                                      \
    |mcpu=cortex-a8|mcpu=cortex-a9|mcpu=cortex-a15       \
    |mcpu=marvell-pj4					\
+   |mcpu=cortex-a53					\
    |mcpu=generic-armv7-a                                \
    |march=armv7-m|mcpu=cortex-m3                        \
    |march=armv7e-m|mcpu=cortex-m4                       \
@@ -71,6 +72,7 @@
   " %{mbig-endian:%{march=armv7-a|mcpu=cortex-a5        \
    |mcpu=cortex-a7                                      \
    |mcpu=cortex-a8|mcpu=cortex-a9|mcpu=cortex-a15       \
+   |mcpu=cortex-a53					\
    |mcpu=marvell-pj4					\
    |mcpu=generic-armv7-a                                \
    |march=armv7-m|mcpu=cortex-m3                        \
Index: b/src/gcc/config/arm/marvell-f-iwmmxt.md
===================================================================
--- a/src/gcc/config/arm/marvell-f-iwmmxt.md
+++ b/src/gcc/config/arm/marvell-f-iwmmxt.md
@@ -63,52 +63,62 @@
 ;; An attribute appended to instructions for classification
 
 (define_attr "wmmxt_shift" "yes,no"
-  (if_then_else (eq_attr "wtype" "wror, wsll, wsra, wsrl")
+  (if_then_else (eq_attr "type" "wmmx_wror, wmmx_wsll, wmmx_wsra, wmmx_wsrl")
 		(const_string "yes") (const_string "no"))
 )
 
 (define_attr "wmmxt_pack" "yes,no"
-  (if_then_else (eq_attr "wtype" "waligni, walignr, wmerge, wpack, wshufh, wunpckeh, wunpckih, wunpckel, wunpckil")
+  (if_then_else (eq_attr "type" "wmmx_waligni, wmmx_walignr, wmmx_wmerge,\
+                                 wmmx_wpack, wmmx_wshufh, wmmx_wunpckeh,\
+                                 wmmx_wunpckih, wmmx_wunpckel, wmmx_wunpckil")
 		(const_string "yes") (const_string "no"))
 )
 
 (define_attr "wmmxt_mult_c1" "yes,no"
-  (if_then_else (eq_attr "wtype" "wmac, wmadd, wmiaxy, wmiawxy, wmulw, wqmiaxy, wqmulwm")
+  (if_then_else (eq_attr "type" "wmmx_wmac, wmmx_wmadd, wmmx_wmiaxy,\
+                                 wmmx_wmiawxy, wmmx_wmulw, wmmx_wqmiaxy,\
+                                 wmmx_wqmulwm")
 		(const_string "yes") (const_string "no"))
 )
 
 (define_attr "wmmxt_mult_c2" "yes,no"
-  (if_then_else (eq_attr "wtype" "wmul, wqmulm")
+  (if_then_else (eq_attr "type" "wmmx_wmul, wmmx_wqmulm")
 		(const_string "yes") (const_string "no"))
 )
 
 (define_attr "wmmxt_alu_c1" "yes,no"
-  (if_then_else (eq_attr "wtype" "wabs, wabsdiff, wand, wandn, wmov, wor, wxor")
+  (if_then_else (eq_attr "type" "wmmx_wabs, wmmx_wabsdiff, wmmx_wand,\
+                                 wmmx_wandn, wmmx_wmov, wmmx_wor, wmmx_wxor")
 	        (const_string "yes") (const_string "no"))
 )
 
 (define_attr "wmmxt_alu_c2" "yes,no"
-  (if_then_else (eq_attr "wtype" "wacc, wadd, waddsubhx, wavg2, wavg4, wcmpeq, wcmpgt, wmax, wmin, wsub, waddbhus, wsubaddhx")
+  (if_then_else (eq_attr "type" "wmmx_wacc, wmmx_wadd, wmmx_waddsubhx,\
+                                 wmmx_wavg2, wmmx_wavg4, wmmx_wcmpeq,\
+                                 wmmx_wcmpgt, wmmx_wmax, wmmx_wmin,\
+                                 wmmx_wsub, wmmx_waddbhus, wmmx_wsubaddhx")
 		(const_string "yes") (const_string "no"))
 )
 
 (define_attr "wmmxt_alu_c3" "yes,no"
-  (if_then_else (eq_attr "wtype" "wsad")
+  (if_then_else (eq_attr "type" "wmmx_wsad")
 	        (const_string "yes") (const_string "no"))
 )
 
 (define_attr "wmmxt_transfer_c1" "yes,no"
-  (if_then_else (eq_attr "wtype" "tbcst, tinsr, tmcr, tmcrr")
+  (if_then_else (eq_attr "type" "wmmx_tbcst, wmmx_tinsr,\
+                                 wmmx_tmcr, wmmx_tmcrr")
                 (const_string "yes") (const_string "no"))
 )
 
 (define_attr "wmmxt_transfer_c2" "yes,no"
-  (if_then_else (eq_attr "wtype" "textrm, tmovmsk, tmrc, tmrrc")
+  (if_then_else (eq_attr "type" "wmmx_textrm, wmmx_tmovmsk,\
+                                 wmmx_tmrc, wmmx_tmrrc")
 	        (const_string "yes") (const_string "no"))
 )
 
 (define_attr "wmmxt_transfer_c3" "yes,no"
-  (if_then_else (eq_attr "wtype" "tmia, tmiaph, tmiaxy")
+  (if_then_else (eq_attr "type" "wmmx_tmia, wmmx_tmiaph, wmmx_tmiaxy")
 	        (const_string "yes") (const_string "no"))
 )
 
@@ -169,11 +179,11 @@
 
 (define_insn_reservation "marvell_f_iwmmxt_wstr" 0
   (and (eq_attr "marvell_f_iwmmxt" "yes")
-       (eq_attr "wtype" "wstr"))
+       (eq_attr "type" "wmmx_wstr"))
   "mf_iwmmxt_pipeline")
 
 ;There is a forwarding path from MW stage
 (define_insn_reservation "marvell_f_iwmmxt_wldr" 5
   (and (eq_attr "marvell_f_iwmmxt" "yes")
-       (eq_attr "wtype" "wldr"))
+       (eq_attr "type" "wmmx_wldr"))
   "mf_iwmmxt_pipeline")
Index: b/src/gcc/config/arm/t-mlibs
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/t-mlibs
@@ -0,0 +1,21 @@
+# A set of predefined MULTILIB for different ARM targets.
+# Through the configure option --with-multilib-list, user can customize the
+# final MULTILIB implementation.
+
+comma := ,
+space :=
+space +=
+
+MULTILIB_OPTIONS   = marm
+MULTILIB_DIRNAMES  = arm
+MULTILIB_OPTIONS  += march=armv4t
+MULTILIB_DIRNAMES += armv4t
+MULTILIB_OPTIONS  += mfloat-abi=soft
+MULTILIB_DIRNAMES += soft
+
+MULTILIB_EXCEPTIONS =
+
+MULTILIB_REQUIRED = marm/march=armv4t/mfloat-abi=soft
+
+MULTILIB_OSDIRNAMES = marm/march.armv4t/mfloat-abi.soft=!arm-linux-gnueabi
+
Index: b/src/gcc/config/arm/iterators.md
===================================================================
--- a/src/gcc/config/arm/iterators.md
+++ b/src/gcc/config/arm/iterators.md
@@ -201,6 +201,20 @@
 (define_int_iterator NEON_VRINT [UNSPEC_NVRINTP UNSPEC_NVRINTZ UNSPEC_NVRINTM
                               UNSPEC_NVRINTX UNSPEC_NVRINTA UNSPEC_NVRINTN])
 
+(define_int_iterator CRC [UNSPEC_CRC32B UNSPEC_CRC32H UNSPEC_CRC32W
+                          UNSPEC_CRC32CB UNSPEC_CRC32CH UNSPEC_CRC32CW])
+
+(define_int_iterator CRYPTO_UNARY [UNSPEC_AESMC UNSPEC_AESIMC])
+
+(define_int_iterator CRYPTO_BINARY [UNSPEC_AESD UNSPEC_AESE
+                                    UNSPEC_SHA1SU1 UNSPEC_SHA256SU0])
+
+(define_int_iterator CRYPTO_TERNARY [UNSPEC_SHA1SU0 UNSPEC_SHA256H
+                                     UNSPEC_SHA256H2 UNSPEC_SHA256SU1])
+
+(define_int_iterator CRYPTO_SELECTING [UNSPEC_SHA1C UNSPEC_SHA1M
+                                       UNSPEC_SHA1P])
+
 ;;----------------------------------------------------------------------------
 ;; Mode attributes
 ;;----------------------------------------------------------------------------
@@ -355,6 +369,12 @@
                              (DI   "64") (V2DI  "64")
                  (V2SF "32") (V4SF  "32")])
 
+(define_mode_attr V_elem_ch [(V8QI "b")  (V16QI "b")
+                             (V4HI "h") (V8HI  "h")
+                             (V2SI "s") (V4SI  "s")
+                             (DI   "d") (V2DI  "d")
+                             (V2SF "s") (V4SF  "s")])
+
 ;; Element sizes for duplicating ARM registers to all elements of a vector.
 (define_mode_attr VD_dup [(V8QI "8") (V4HI "16") (V2SI "32") (V2SF "32")])
 
@@ -391,7 +411,7 @@
 (define_mode_attr scalar_mul_constraint [(V4HI "x") (V2SI "t") (V2SF "t")
                                          (V8HI "x") (V4SI "t") (V4SF "t")])
 
-;; Predicates used for setting neon_type
+;; Predicates used for setting type for neon instructions
 
 (define_mode_attr Is_float_mode [(V8QI "false") (V16QI "false")
                  (V4HI "false") (V8HI "false")
@@ -452,6 +472,14 @@
 (define_mode_attr vfp_type [(SF "s") (DF "d")])
 (define_mode_attr vfp_double_cond [(SF "") (DF "&& TARGET_VFP_DOUBLE")])
 
+;; Mode attribute used to build the "type" attribute.
+(define_mode_attr q [(V8QI "") (V16QI "_q")
+                     (V4HI "") (V8HI "_q")
+                     (V2SI "") (V4SI "_q")
+                     (V2SF "") (V4SF "_q")
+                     (DI "")   (V2DI "_q")
+                     (DF "")   (V2DF "_q")])
+
 ;;----------------------------------------------------------------------------
 ;; Code attributes
 ;;----------------------------------------------------------------------------
@@ -460,6 +488,10 @@
 (define_code_attr VQH_mnem [(plus "vadd") (smin "vmin") (smax "vmax")
                 (umin "vmin") (umax "vmax")])
 
+;; Type attributes for vqh_ops and vqhs_ops iterators.
+(define_code_attr VQH_type [(plus "add") (smin "minmax") (smax "minmax")
+                (umin "minmax") (umax "minmax")])
+
 ;; Signs of above, where relevant.
 (define_code_attr VQH_sign [(plus "i") (smin "s") (smax "s") (umin "u")
                 (umax "u")])
@@ -500,3 +532,54 @@
 (define_int_attr nvrint_variant [(UNSPEC_NVRINTZ "z") (UNSPEC_NVRINTP "p")
                                 (UNSPEC_NVRINTA "a") (UNSPEC_NVRINTM "m")
                                 (UNSPEC_NVRINTX "x") (UNSPEC_NVRINTN "n")])
+
+(define_int_attr crc_variant [(UNSPEC_CRC32B "crc32b") (UNSPEC_CRC32H "crc32h")
+                        (UNSPEC_CRC32W "crc32w") (UNSPEC_CRC32CB "crc32cb")
+                        (UNSPEC_CRC32CH "crc32ch") (UNSPEC_CRC32CW "crc32cw")])
+
+(define_int_attr crc_mode [(UNSPEC_CRC32B "QI") (UNSPEC_CRC32H "HI")
+                        (UNSPEC_CRC32W "SI") (UNSPEC_CRC32CB "QI")
+                        (UNSPEC_CRC32CH "HI") (UNSPEC_CRC32CW "SI")])
+
+(define_int_attr crypto_pattern [(UNSPEC_SHA1H "sha1h") (UNSPEC_AESMC "aesmc")
+                          (UNSPEC_AESIMC "aesimc") (UNSPEC_AESD "aesd")
+                          (UNSPEC_AESE "aese") (UNSPEC_SHA1SU1 "sha1su1")
+                          (UNSPEC_SHA256SU0 "sha256su0") (UNSPEC_SHA1C "sha1c")
+                          (UNSPEC_SHA1M "sha1m") (UNSPEC_SHA1P "sha1p")
+                          (UNSPEC_SHA1SU0 "sha1su0") (UNSPEC_SHA256H "sha256h")
+                          (UNSPEC_SHA256H2 "sha256h2")
+                          (UNSPEC_SHA256SU1 "sha256su1")])
+
+(define_int_attr crypto_type
+ [(UNSPEC_AESE "crypto_aes") (UNSPEC_AESD "crypto_aes")
+ (UNSPEC_AESMC "crypto_aes") (UNSPEC_AESIMC "crypto_aes")
+ (UNSPEC_SHA1C "crypto_sha1_slow") (UNSPEC_SHA1P "crypto_sha1_slow")
+ (UNSPEC_SHA1M "crypto_sha1_slow") (UNSPEC_SHA1SU1 "crypto_sha1_fast")
+ (UNSPEC_SHA1SU0 "crypto_sha1_xor") (UNSPEC_SHA256H "crypto_sha256_slow")
+ (UNSPEC_SHA256H2 "crypto_sha256_slow") (UNSPEC_SHA256SU0 "crypto_sha256_fast")
+ (UNSPEC_SHA256SU1 "crypto_sha256_slow")])
+
+(define_int_attr crypto_size_sfx [(UNSPEC_SHA1H "32") (UNSPEC_AESMC "8")
+                          (UNSPEC_AESIMC "8") (UNSPEC_AESD "8")
+                          (UNSPEC_AESE "8") (UNSPEC_SHA1SU1 "32")
+                          (UNSPEC_SHA256SU0 "32") (UNSPEC_SHA1C "32")
+                          (UNSPEC_SHA1M "32") (UNSPEC_SHA1P "32")
+                          (UNSPEC_SHA1SU0 "32") (UNSPEC_SHA256H "32")
+                          (UNSPEC_SHA256H2 "32") (UNSPEC_SHA256SU1 "32")])
+
+(define_int_attr crypto_mode [(UNSPEC_SHA1H "V4SI") (UNSPEC_AESMC "V16QI")
+                          (UNSPEC_AESIMC "V16QI") (UNSPEC_AESD "V16QI")
+                          (UNSPEC_AESE "V16QI") (UNSPEC_SHA1SU1 "V4SI")
+                          (UNSPEC_SHA256SU0 "V4SI") (UNSPEC_SHA1C "V4SI")
+                          (UNSPEC_SHA1M "V4SI") (UNSPEC_SHA1P "V4SI")
+                          (UNSPEC_SHA1SU0 "V4SI") (UNSPEC_SHA256H "V4SI")
+                          (UNSPEC_SHA256H2 "V4SI") (UNSPEC_SHA256SU1 "V4SI")])
+
+;; Both kinds of return insn.
+(define_code_iterator returns [return simple_return])
+(define_code_attr return_str [(return "") (simple_return "simple_")])
+(define_code_attr return_simple_p [(return "false") (simple_return "true")])
+(define_code_attr return_cond_false [(return " && USE_RETURN_INSN (FALSE)")
+                               (simple_return " && use_simple_return_p ()")])
+(define_code_attr return_cond_true [(return " && USE_RETURN_INSN (TRUE)")
+                               (simple_return " && use_simple_return_p ()")])
Index: b/src/gcc/config/arm/sync.md
===================================================================
--- a/src/gcc/config/arm/sync.md
+++ b/src/gcc/config/arm/sync.md
@@ -65,6 +65,42 @@
    (set_attr "conds" "unconditional")
    (set_attr "predicable" "no")])
 
+(define_insn "atomic_load<mode>"
+  [(set (match_operand:QHSI 0 "register_operand" "=r")
+    (unspec_volatile:QHSI
+      [(match_operand:QHSI 1 "arm_sync_memory_operand" "Q")
+       (match_operand:SI 2 "const_int_operand")]		;; model
+      VUNSPEC_LDA))]
+  "TARGET_HAVE_LDACQ"
+  {
+    enum memmodel model = (enum memmodel) INTVAL (operands[2]);
+    if (model == MEMMODEL_RELAXED
+        || model == MEMMODEL_CONSUME
+        || model == MEMMODEL_RELEASE)
+      return \"ldr<sync_sfx>\\t%0, %1\";
+    else
+      return \"lda<sync_sfx>\\t%0, %1\";
+  }
+)
+
+(define_insn "atomic_store<mode>"
+  [(set (match_operand:QHSI 0 "memory_operand" "=Q")
+    (unspec_volatile:QHSI
+      [(match_operand:QHSI 1 "general_operand" "r")
+       (match_operand:SI 2 "const_int_operand")]		;; model
+      VUNSPEC_STL))]
+  "TARGET_HAVE_LDACQ"
+  {
+    enum memmodel model = (enum memmodel) INTVAL (operands[2]);
+    if (model == MEMMODEL_RELAXED
+        || model == MEMMODEL_CONSUME
+        || model == MEMMODEL_ACQUIRE)
+      return \"str<sync_sfx>\t%1, %0\";
+    else
+      return \"stl<sync_sfx>\t%1, %0\";
+  }
+)
+
 ;; Note that ldrd and vldr are *not* guaranteed to be single-copy atomic,
 ;; even for a 64-bit aligned address.  Instead we use a ldrexd unparied
 ;; with a store.
@@ -88,7 +124,8 @@
 		   UNSPEC_LL))]
   "TARGET_HAVE_LDREXD && ARM_DOUBLEWORD_ALIGN"
   "ldrexd%?\t%0, %H0, %C1"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_expand "atomic_compare_and_swap<mode>"
   [(match_operand:SI 0 "s_register_operand" "")		;; bool out
@@ -325,7 +362,19 @@
 	    VUNSPEC_LL)))]
   "TARGET_HAVE_LDREXBH"
   "ldrex<sync_sfx>%?\t%0, %C1"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
+
+(define_insn "arm_load_acquire_exclusive<mode>"
+  [(set (match_operand:SI 0 "s_register_operand" "=r")
+        (zero_extend:SI
+	  (unspec_volatile:NARROW
+	    [(match_operand:NARROW 1 "mem_noofs_operand" "Ua")]
+	    VUNSPEC_LAX)))]
+  "TARGET_HAVE_LDACQ"
+  "ldaex<sync_sfx>%?\\t%0, %C1"
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "arm_load_exclusivesi"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
@@ -334,7 +383,18 @@
 	  VUNSPEC_LL))]
   "TARGET_HAVE_LDREX"
   "ldrex%?\t%0, %C1"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
+
+(define_insn "arm_load_acquire_exclusivesi"
+  [(set (match_operand:SI 0 "s_register_operand" "=r")
+	(unspec_volatile:SI
+	  [(match_operand:SI 1 "mem_noofs_operand" "Ua")]
+	  VUNSPEC_LAX))]
+  "TARGET_HAVE_LDACQ"
+  "ldaex%?\t%0, %C1"
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "arm_load_exclusivedi"
   [(set (match_operand:DI 0 "s_register_operand" "=r")
@@ -343,7 +403,18 @@
 	  VUNSPEC_LL))]
   "TARGET_HAVE_LDREXD"
   "ldrexd%?\t%0, %H0, %C1"
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
+
+(define_insn "arm_load_acquire_exclusivedi"
+  [(set (match_operand:DI 0 "s_register_operand" "=r")
+	(unspec_volatile:DI
+	  [(match_operand:DI 1 "mem_noofs_operand" "Ua")]
+	  VUNSPEC_LAX))]
+  "TARGET_HAVE_LDACQ && ARM_DOUBLEWORD_ALIGN"
+  "ldaexd%?\t%0, %H0, %C1"
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "arm_store_exclusive<mode>"
   [(set (match_operand:SI 0 "s_register_operand" "=&r")
@@ -367,4 +438,35 @@
       }
     return "strex<sync_sfx>%?\t%0, %2, %C1";
   }
-  [(set_attr "predicable" "yes")])
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
+
+(define_insn "arm_store_release_exclusivedi"
+  [(set (match_operand:SI 0 "s_register_operand" "=&r")
+	(unspec_volatile:SI [(const_int 0)] VUNSPEC_SLX))
+   (set (match_operand:DI 1 "mem_noofs_operand" "=Ua")
+	(unspec_volatile:DI
+	  [(match_operand:DI 2 "s_register_operand" "r")]
+	  VUNSPEC_SLX))]
+  "TARGET_HAVE_LDACQ && ARM_DOUBLEWORD_ALIGN"
+  {
+    rtx value = operands[2];
+    /* See comment in arm_store_exclusive<mode> above.  */
+    gcc_assert ((REGNO (value) & 1) == 0 || TARGET_THUMB2);
+    operands[3] = gen_rtx_REG (SImode, REGNO (value) + 1);
+    return "stlexd%?\t%0, %2, %3, %C1";
+  }
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
+
+(define_insn "arm_store_release_exclusive<mode>"
+  [(set (match_operand:SI 0 "s_register_operand" "=&r")
+	(unspec_volatile:SI [(const_int 0)] VUNSPEC_SLX))
+   (set (match_operand:QHSI 1 "mem_noofs_operand" "=Ua")
+	(unspec_volatile:QHSI
+	  [(match_operand:QHSI 2 "s_register_operand" "r")]
+	  VUNSPEC_SLX))]
+  "TARGET_HAVE_LDACQ"
+  "stlex<sync_sfx>%?\t%0, %2, %C1"
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
Index: b/src/gcc/config/arm/fa726te.md
===================================================================
--- a/src/gcc/config/arm/fa726te.md
+++ b/src/gcc/config/arm/fa726te.md
@@ -78,15 +78,20 @@
 ;; Move instructions.
 (define_insn_reservation "726te_shift_op" 1
   (and (eq_attr "tune" "fa726te")
-       (eq_attr "insn" "mov,mvn"))
+       (eq_attr "type" "mov_imm,mov_reg,mov_shift,mov_shift_reg,\
+                        mvn_imm,mvn_reg,mvn_shift,mvn_shift_reg"))
   "fa726te_issue+(fa726te_alu0_pipe|fa726te_alu1_pipe)")
 
 ;; ALU operations with no shifted operand will finished in 1 cycle
 ;; Other ALU instructions 2 cycles.
 (define_insn_reservation "726te_alu_op" 1
  (and (eq_attr "tune" "fa726te")
-      (and (eq_attr "type" "alu_reg,simple_alu_imm")
-           (not (eq_attr "insn" "mov,mvn"))))
+      (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                       adr,bfm,rev,\
+                       shift_imm,shift_reg,\
+                       mrs,multiple,no_insn"))
   "fa726te_issue+(fa726te_alu0_pipe|fa726te_alu1_pipe)")
 
 ;; ALU operations with a shift-by-register operand.
@@ -95,14 +100,14 @@
 ;; it takes 3 cycles.
 (define_insn_reservation "726te_alu_shift_op" 3
  (and (eq_attr "tune" "fa726te")
-      (and (eq_attr "type" "simple_alu_shift,alu_shift")
-           (not (eq_attr "insn" "mov,mvn"))))
+      (eq_attr "type" "extend,alu_shift_imm,alus_shift_imm,\
+                       logic_shift_imm,logics_shift_imm"))
   "fa726te_issue+(fa726te_alu0_pipe|fa726te_alu1_pipe)")
 
 (define_insn_reservation "726te_alu_shift_reg_op" 3
  (and (eq_attr "tune" "fa726te")
-      (and (eq_attr "type" "alu_shift_reg")
-           (not (eq_attr "insn" "mov,mvn"))))
+      (eq_attr "type" "alu_shift_reg,alus_shift_reg,\
+                       logic_shift_reg,logics_shift_reg"))
   "fa726te_issue+(fa726te_alu0_pipe|fa726te_alu1_pipe)")
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
 ;; Multiplication Instructions
@@ -115,7 +120,7 @@
 
 (define_insn_reservation "726te_mult_op" 3
  (and (eq_attr "tune" "fa726te")
-      (eq_attr "insn" "smlalxy,mul,mla,muls,mlas,umull,umlal,smull,smlal,\
+      (eq_attr "type" "smlalxy,mul,mla,muls,mlas,umull,umlal,smull,smlal,\
                        umulls,umlals,smulls,smlals,smlawx,smulxy,smlaxy"))
  "fa726te_issue+fa726te_mac_pipe")
 
Index: b/src/gcc/config/arm/neon-testgen.ml
===================================================================
--- a/src/gcc/config/arm/neon-testgen.ml
+++ b/src/gcc/config/arm/neon-testgen.ml
@@ -163,10 +163,13 @@ let effective_target features =
     match List.find (fun feature ->
                        match feature with Requires_feature _ -> true
                                         | Requires_arch _ -> true
+                                        | Requires_FP_bit 1 -> true
                                         | _ -> false)
                      features with
       Requires_feature "FMA" -> "arm_neonv2"
+    | Requires_feature "CRYPTO" -> "arm_crypto"
     | Requires_arch 8 -> "arm_v8_neon"
+    | Requires_FP_bit 1 -> "arm_neon_fp16"
     | _ -> assert false
   with Not_found -> "arm_neon"
 
@@ -298,5 +301,5 @@ let test_intrinsic_group dir (opcode, fe
 (* Program entry point.  *)
 let _ =
   let directory = if Array.length Sys.argv <> 1 then Sys.argv.(1) else "." in
-    List.iter (test_intrinsic_group directory) (reinterp @ ops)
+    List.iter (test_intrinsic_group directory) (reinterp @ reinterpq @ ops)
 
Index: b/src/gcc/config/arm/arm.md
===================================================================
--- a/src/gcc/config/arm/arm.md
+++ b/src/gcc/config/arm/arm.md
@@ -74,6 +74,15 @@
 ; IS_THUMB1 is set to 'yes' iff we are generating Thumb-1 code.
 (define_attr "is_thumb1" "no,yes" (const (symbol_ref "thumb1_code")))
 
+; We use this attribute to disable alternatives that can produce 32-bit
+; instructions inside an IT-block in Thumb2 state.  ARMv8 deprecates IT blocks
+; that contain 32-bit instructions.
+(define_attr "enabled_for_depr_it" "no,yes" (const_string "yes"))
+
+; This attribute is used to disable a predicated alternative when we have
+; arm_restrict_it.
+(define_attr "predicable_short_it" "no,yes" (const_string "yes"))
+
 ;; Operand number of an input operand that is shifted.  Zero if the
 ;; given instruction does not shift one of its input operands.
 (define_attr "shift" "" (const_int 0))
@@ -84,6 +93,8 @@
 (define_attr "fpu" "none,vfp"
   (const (symbol_ref "arm_fpu_attr")))
 
+(define_attr "predicated" "yes,no" (const_string "no"))
+
 ; LENGTH of an instruction (in bytes)
 (define_attr "length" ""
   (const_int 4))
@@ -92,11 +103,11 @@
 ; This can be "a" for ARM, "t" for either of the Thumbs, "32" for
 ; TARGET_32BIT, "t1" or "t2" to specify a specific Thumb mode.  "v6"
 ; for ARM or Thumb-2 with arm_arch6, and nov6 for ARM without
+; arm_arch6.  This attribute is used to compute attribute "enabled",
 ; arm_arch6.  "v6t2" for Thumb-2 with arm_arch6.  This attribute is
 ; used to compute attribute "enabled", use type "any" to enable an
 ; alternative in all cases.
-
-(define_attr "arch" "any,a,t,32,t1,t2,v6,nov6,v6t2,onlya8,neon_onlya8,nota8,neon_nota8,iwmmxt,iwmmxt2"
+(define_attr "arch" "any,a,t,32,t1,t2,v6,nov6,v6t2,neon_for_64bits,avoid_neon_for_64bits,iwmmxt,iwmmxt2"
   (const_string "any"))
 
 (define_attr "arch_enabled" "no,yes"
@@ -131,26 +142,18 @@
 	      (match_test "TARGET_32BIT && !arm_arch6"))
 	 (const_string "yes")
 
-	 (and (eq_attr "arch" "v6t2")
-	      (match_test "TARGET_32BIT && arm_arch6 && arm_arch_thumb2"))
-	 (const_string "yes")
-
-	 (and (eq_attr "arch" "onlya8")
-	      (eq_attr "tune" "cortexa8"))
-	 (const_string "yes")
-
-	 (and (eq_attr "arch" "neon_onlya8")
-	      (eq_attr "tune" "cortexa8")
-	      (match_test "TARGET_NEON"))
+	 (and (eq_attr "arch" "avoid_neon_for_64bits")
+	      (match_test "TARGET_NEON")
+	      (not (match_test "TARGET_PREFER_NEON_64BITS")))
 	 (const_string "yes")
 
-	 (and (eq_attr "arch" "nota8")
-	      (not (eq_attr "tune" "cortexa8")))
+	 (and (eq_attr "arch" "v6t2")
+	      (match_test "TARGET_32BIT && arm_arch6 && arm_arch_thumb2"))
 	 (const_string "yes")
 
-	 (and (eq_attr "arch" "neon_nota8")
-	      (not (eq_attr "tune" "cortexa8"))
-	      (match_test "TARGET_NEON"))
+	 (and (eq_attr "arch" "neon_for_64bits")
+	      (match_test "TARGET_NEON")
+	      (match_test "TARGET_PREFER_NEON_64BITS"))
 	 (const_string "yes")
 
 	 (and (eq_attr "arch" "iwmmxt2")
@@ -185,6 +188,15 @@
    (cond [(eq_attr "insn_enabled" "no")
 	  (const_string "no")
 
+	  (and (eq_attr "predicable_short_it" "no")
+	       (and (eq_attr "predicated" "yes")
+	            (match_test "arm_restrict_it")))
+	  (const_string "no")
+
+	  (and (eq_attr "enabled_for_depr_it" "no")
+	       (match_test "arm_restrict_it"))
+	  (const_string "no")
+
 	  (eq_attr "arch_enabled" "no")
 	  (const_string "no")
 
@@ -220,196 +232,109 @@
   (set_attr "length" "4")
   (set_attr "pool_range" "250")])
 
-;; The instruction used to implement a particular pattern.  This
-;; information is used by pipeline descriptions to provide accurate
-;; scheduling information.
-
-(define_attr "insn"
-        "mov,mvn,smulxy,smlaxy,smlalxy,smulwy,smlawx,mul,muls,mla,mlas,umull,umulls,umlal,umlals,smull,smulls,smlal,smlals,smlawy,smuad,smuadx,smlad,smladx,smusd,smusdx,smlsd,smlsdx,smmul,smmulr,smmla,umaal,smlald,smlsld,clz,mrs,msr,xtab,sdiv,udiv,sat,other"
-        (const_string "other"))
-
-; TYPE attribute is used to detect floating point instructions which, if
-; running on a co-processor can run in parallel with other, basic instructions
-; If write-buffer scheduling is enabled then it can also be used in the
-; scheduling of writes.
-
-; Classification of each insn
-; Note: vfp.md has different meanings for some of these, and some further
-; types as well.  See that file for details.
-; simple_alu_imm  a simple alu instruction that doesn't hit memory or fp
-;               regs or have a shifted source operand and has an immediate
-;               operand. This currently only tracks very basic immediate
-;               alu operations.
-; alu_reg       any alu instruction that doesn't hit memory or fp
-;               regs or have a shifted source operand
-;               and does not have an immediate operand. This is
-;               also the default
-; simple_alu_shift covers UXTH, UXTB, SXTH, SXTB
-; alu_shift	any data instruction that doesn't hit memory or fp
-;		regs, but has a source operand shifted by a constant
-; alu_shift_reg	any data instruction that doesn't hit memory or fp
-;		regs, but has a source operand shifted by a register value
-; mult		a multiply instruction
-; block		blockage insn, this blocks all functional units
-; float		a floating point arithmetic operation (subject to expansion)
-; fdivd		DFmode floating point division
-; fdivs		SFmode floating point division
-; f_load[sd]	A single/double load from memory. Used for VFP unit.
-; f_store[sd]	A single/double store to memory. Used for VFP unit.
-; f_flag	a transfer of co-processor flags to the CPSR
-; f_2_r		transfer float to core (no memory needed)
-; r_2_f		transfer core to float
-; f_cvt		convert floating<->integral
-; branch	a branch
-; call		a subroutine call
-; load_byte	load byte(s) from memory to arm registers
-; load1		load 1 word from memory to arm registers
-; load2         load 2 words from memory to arm registers
-; load3         load 3 words from memory to arm registers
-; load4         load 4 words from memory to arm registers
-; store		store 1 word to memory from arm registers
-; store2	store 2 words
-; store3	store 3 words
-; store4	store 4 (or more) words
-;
-
-(define_attr "type"
- "simple_alu_imm,\
-  alu_reg,\
-  simple_alu_shift,\
-  alu_shift,\
-  alu_shift_reg,\
-  mult,\
-  block,\
-  float,\
-  fdivd,\
-  fdivs,\
-  fmuls,\
-  fmuld,\
-  fmacs,\
-  fmacd,\
-  ffmas,\
-  ffmad,\
-  f_rints,\
-  f_rintd,\
-  f_minmaxs,\
-  f_minmaxd,\
-  f_flag,\
-  f_loads,\
-  f_loadd,\
-  f_stores,\
-  f_stored,\
-  f_2_r,\
-  r_2_f,\
-  f_cvt,\
-  branch,\
-  call,\
-  load_byte,\
-  load1,\
-  load2,\
-  load3,\
-  load4,\
-  store1,\
-  store2,\
-  store3,\
-  store4,\
-  fconsts,\
-  fconstd,\
-  fadds,\
-  faddd,\
-  ffariths,\
-  ffarithd,\
-  fcmps,\
-  fcmpd,\
-  fcpys"
- (if_then_else 
-    (eq_attr "insn" "smulxy,smlaxy,smlalxy,smulwy,smlawx,mul,muls,mla,mlas,\
-	     	     umull,umulls,umlal,umlals,smull,smulls,smlal,smlals")
-    (const_string "mult")
-    (const_string "alu_reg")))
-
-; Is this an (integer side) multiply with a 64-bit result?
-(define_attr "mul64" "no,yes"
-  (if_then_else
-    (eq_attr "insn"
-     "smlalxy,umull,umulls,umlal,umlals,smull,smulls,smlal,smlals")
-    (const_string "yes")
-    (const_string "no")))
-
-; wtype for WMMX insn scheduling purposes.
-(define_attr "wtype"
-        "none,wor,wxor,wand,wandn,wmov,tmcrr,tmrrc,wldr,wstr,tmcr,tmrc,wadd,wsub,wmul,wmac,wavg2,tinsr,textrm,wshufh,wcmpeq,wcmpgt,wmax,wmin,wpack,wunpckih,wunpckil,wunpckeh,wunpckel,wror,wsra,wsrl,wsll,wmadd,tmia,tmiaph,tmiaxy,tbcst,tmovmsk,wacc,waligni,walignr,tandc,textrc,torc,torvsc,wsad,wabs,wabsdiff,waddsubhx,wsubaddhx,wavg4,wmulw,wqmulm,wqmulwm,waddbhus,wqmiaxy,wmiaxy,wmiawxy,wmerge" (const_string "none"))
+;; Instruction classification types
+(include "types.md")
 
 ; Load scheduling, set from the arm_ld_sched variable
 ; initialized by arm_option_override()
 (define_attr "ldsched" "no,yes" (const (symbol_ref "arm_ld_sched")))
 
-;; Classification of NEON instructions for scheduling purposes.
-(define_attr "neon_type"
-   "neon_int_1,\
-   neon_int_2,\
-   neon_int_3,\
-   neon_int_4,\
-   neon_int_5,\
-   neon_vqneg_vqabs,\
-   neon_vmov,\
-   neon_vaba,\
-   neon_vsma,\
-   neon_vaba_qqq,\
-   neon_mul_ddd_8_16_qdd_16_8_long_32_16_long,\
-   neon_mul_qqq_8_16_32_ddd_32,\
-   neon_mul_qdd_64_32_long_qqd_16_ddd_32_scalar_64_32_long_scalar,\
-   neon_mla_ddd_8_16_qdd_16_8_long_32_16_long,\
-   neon_mla_qqq_8_16,\
-   neon_mla_ddd_32_qqd_16_ddd_32_scalar_qdd_64_32_long_scalar_qdd_64_32_long,\
-   neon_mla_qqq_32_qqd_32_scalar,\
-   neon_mul_ddd_16_scalar_32_16_long_scalar,\
-   neon_mul_qqd_32_scalar,\
-   neon_mla_ddd_16_scalar_qdd_32_16_long_scalar,\
-   neon_shift_1,\
-   neon_shift_2,\
-   neon_shift_3,\
-   neon_vshl_ddd,\
-   neon_vqshl_vrshl_vqrshl_qqq,\
-   neon_vsra_vrsra,\
-   neon_fp_vadd_ddd_vabs_dd,\
-   neon_fp_vadd_qqq_vabs_qq,\
-   neon_fp_vsum,\
-   neon_fp_vmul_ddd,\
-   neon_fp_vmul_qqd,\
-   neon_fp_vmla_ddd,\
-   neon_fp_vmla_qqq,\
-   neon_fp_vmla_ddd_scalar,\
-   neon_fp_vmla_qqq_scalar,\
-   neon_fp_vrecps_vrsqrts_ddd,\
-   neon_fp_vrecps_vrsqrts_qqq,\
-   neon_bp_simple,\
-   neon_bp_2cycle,\
-   neon_bp_3cycle,\
-   neon_ldr,\
-   neon_str,\
-   neon_vld1_1_2_regs,\
-   neon_vld1_3_4_regs,\
-   neon_vld2_2_regs_vld1_vld2_all_lanes,\
-   neon_vld2_4_regs,\
-   neon_vld3_vld4,\
-   neon_vst1_1_2_regs_vst2_2_regs,\
-   neon_vst1_3_4_regs,\
-   neon_vst2_4_regs_vst3_vst4,\
-   neon_vst3_vst4,\
-   neon_vld1_vld2_lane,\
-   neon_vld3_vld4_lane,\
-   neon_vst1_vst2_lane,\
-   neon_vst3_vst4_lane,\
-   neon_vld3_vld4_all_lanes,\
-   neon_mcr,\
-   neon_mcr_2_mcrr,\
-   neon_mrc,\
-   neon_mrrc,\
-   neon_ldm_2,\
-   neon_stm_2,\
-   none"
- (const_string "none"))
+; YES if the "type" attribute assigned to the insn denotes an
+; Advanced SIMD instruction, NO otherwise.
+(define_attr "is_neon_type" "yes,no"
+	 (if_then_else (eq_attr "type"
+	 "neon_add, neon_add_q, neon_add_widen, neon_add_long,\
+          neon_qadd, neon_qadd_q, neon_add_halve, neon_add_halve_q,\
+          neon_add_halve_narrow_q,\
+          neon_sub, neon_sub_q, neon_sub_widen, neon_sub_long, neon_qsub,\
+          neon_qsub_q, neon_sub_halve, neon_sub_halve_q,\
+          neon_sub_halve_narrow_q,\
+          neon_abs, neon_abs_q, neon_neg, neon_neg_q, neon_qneg,\
+          neon_qneg_q, neon_qabs, neon_qabs_q, neon_abd, neon_abd_q,\
+          neon_abd_long, neon_minmax, neon_minmax_q, neon_compare,\
+          neon_compare_q, neon_compare_zero, neon_compare_zero_q,\
+          neon_arith_acc, neon_arith_acc_q, neon_reduc_add,\
+          neon_reduc_add_q, neon_reduc_add_long, neon_reduc_add_acc,\
+          neon_reduc_add_acc_q, neon_reduc_minmax, neon_reduc_minmax_q,\
+          neon_logic, neon_logic_q, neon_tst, neon_tst_q,\
+          neon_shift_imm, neon_shift_imm_q, neon_shift_imm_narrow_q,\
+          neon_shift_imm_long, neon_shift_reg, neon_shift_reg_q,\
+          neon_shift_acc, neon_shift_acc_q, neon_sat_shift_imm,\
+          neon_sat_shift_imm_q, neon_sat_shift_imm_narrow_q,\
+          neon_sat_shift_reg, neon_sat_shift_reg_q,\
+          neon_ins, neon_ins_q, neon_move, neon_move_q, neon_move_narrow_q,\
+          neon_permute, neon_permute_q, neon_zip, neon_zip_q, neon_tbl1,\
+          neon_tbl1_q, neon_tbl2, neon_tbl2_q, neon_tbl3, neon_tbl3_q,\
+          neon_tbl4, neon_tbl4_q, neon_bsl, neon_bsl_q, neon_cls,\
+          neon_cls_q, neon_cnt, neon_cnt_q, neon_dup, neon_dup_q,\
+          neon_ext, neon_ext_q, neon_rbit, neon_rbit_q,\
+          neon_rev, neon_rev_q, neon_mul_b, neon_mul_b_q, neon_mul_h,\
+          neon_mul_h_q, neon_mul_s, neon_mul_s_q, neon_mul_b_long,\
+          neon_mul_h_long, neon_mul_s_long, neon_mul_h_scalar,\
+          neon_mul_h_scalar_q, neon_mul_s_scalar, neon_mul_s_scalar_q,\
+          neon_mul_h_scalar_long, neon_mul_s_scalar_long, neon_sat_mul_b,\
+          neon_sat_mul_b_q, neon_sat_mul_h, neon_sat_mul_h_q,\
+          neon_sat_mul_s, neon_sat_mul_s_q, neon_sat_mul_b_long,\
+          neon_sat_mul_h_long, neon_sat_mul_s_long, neon_sat_mul_h_scalar,\
+          neon_sat_mul_h_scalar_q, neon_sat_mul_s_scalar,\
+          neon_sat_mul_s_scalar_q, neon_sat_mul_h_scalar_long,\
+          neon_sat_mul_s_scalar_long, neon_mla_b, neon_mla_b_q, neon_mla_h,\
+          neon_mla_h_q, neon_mla_s, neon_mla_s_q, neon_mla_b_long,\
+          neon_mla_h_long, neon_mla_s_long, neon_mla_h_scalar,\
+          neon_mla_h_scalar_q, neon_mla_s_scalar, neon_mla_s_scalar_q,\
+          neon_mla_h_scalar_long, neon_mla_s_scalar_long,\
+          neon_sat_mla_b_long, neon_sat_mla_h_long,\
+          neon_sat_mla_s_long, neon_sat_mla_h_scalar_long,\
+          neon_sat_mla_s_scalar_long,\
+          neon_to_gp, neon_to_gp_q, neon_from_gp, neon_from_gp_q,\
+          neon_ldr, neon_load1_1reg, neon_load1_1reg_q, neon_load1_2reg,\
+          neon_load1_2reg_q, neon_load1_3reg, neon_load1_3reg_q,\
+          neon_load1_4reg, neon_load1_4reg_q, neon_load1_all_lanes,\
+          neon_load1_all_lanes_q, neon_load1_one_lane, neon_load1_one_lane_q,\
+          neon_load2_2reg, neon_load2_2reg_q, neon_load2_4reg,\
+          neon_load2_4reg_q, neon_load2_all_lanes, neon_load2_all_lanes_q,\
+          neon_load2_one_lane, neon_load2_one_lane_q,\
+          neon_load3_3reg, neon_load3_3reg_q, neon_load3_all_lanes,\
+          neon_load3_all_lanes_q, neon_load3_one_lane, neon_load3_one_lane_q,\
+          neon_load4_4reg, neon_load4_4reg_q, neon_load4_all_lanes,\
+          neon_load4_all_lanes_q, neon_load4_one_lane, neon_load4_one_lane_q,\
+          neon_str, neon_store1_1reg, neon_store1_1reg_q, neon_store1_2reg,\
+          neon_store1_2reg_q, neon_store1_3reg, neon_store1_3reg_q,\
+          neon_store1_4reg, neon_store1_4reg_q, neon_store1_one_lane,\
+          neon_store1_one_lane_q, neon_store2_2reg, neon_store2_2reg_q,\
+          neon_store2_4reg, neon_store2_4reg_q, neon_store2_one_lane,\
+          neon_store2_one_lane_q, neon_store3_3reg, neon_store3_3reg_q,\
+          neon_store3_one_lane, neon_store3_one_lane_q, neon_store4_4reg,\
+          neon_store4_4reg_q, neon_store4_one_lane, neon_store4_one_lane_q,\
+          neon_fp_abd_s, neon_fp_abd_s_q, neon_fp_abd_d, neon_fp_abd_d_q,\
+          neon_fp_addsub_s, neon_fp_addsub_s_q, neon_fp_addsub_d,\
+          neon_fp_addsub_d_q, neon_fp_compare_s, neon_fp_compare_s_q,\
+          neon_fp_compare_d, neon_fp_compare_d_q, neon_fp_minmax_s,\
+          neon_fp_minmax_s_q, neon_fp_minmax_d, neon_fp_minmax_d_q,\
+          neon_fp_reduc_add_s, neon_fp_reduc_add_s_q, neon_fp_reduc_add_d,\
+          neon_fp_reduc_add_d_q, neon_fp_reduc_minmax_s,
+          neon_fp_reduc_minmax_s_q, neon_fp_reduc_minmax_d,\
+          neon_fp_reduc_minmax_d_q,\
+          neon_fp_cvt_narrow_s_q, neon_fp_cvt_narrow_d_q,\
+          neon_fp_cvt_widen_h, neon_fp_cvt_widen_s, neon_fp_to_int_s,\
+          neon_fp_to_int_s_q, neon_int_to_fp_s, neon_int_to_fp_s_q,\
+          neon_fp_round_s, neon_fp_round_s_q, neon_fp_recpe_s,\
+          neon_fp_recpe_s_q,\
+          neon_fp_recpe_d, neon_fp_recpe_d_q, neon_fp_recps_s,\
+          neon_fp_recps_s_q, neon_fp_recps_d, neon_fp_recps_d_q,\
+          neon_fp_recpx_s, neon_fp_recpx_s_q, neon_fp_recpx_d,\
+          neon_fp_recpx_d_q, neon_fp_rsqrte_s, neon_fp_rsqrte_s_q,\
+          neon_fp_rsqrte_d, neon_fp_rsqrte_d_q, neon_fp_rsqrts_s,\
+          neon_fp_rsqrts_s_q, neon_fp_rsqrts_d, neon_fp_rsqrts_d_q,\
+          neon_fp_mul_s, neon_fp_mul_s_q, neon_fp_mul_s_scalar,\
+          neon_fp_mul_s_scalar_q, neon_fp_mul_d, neon_fp_mul_d_q,\
+          neon_fp_mul_d_scalar_q, neon_fp_mla_s, neon_fp_mla_s_q,\
+          neon_fp_mla_s_scalar, neon_fp_mla_s_scalar_q, neon_fp_mla_d,\
+          neon_fp_mla_d_q, neon_fp_mla_d_scalar_q, neon_fp_sqrt_s,\
+          neon_fp_sqrt_s_q, neon_fp_sqrt_d, neon_fp_sqrt_d_q,\
+          neon_fp_div_s, neon_fp_div_s_q, neon_fp_div_d, neon_fp_div_d_q")
+        (const_string "yes")
+        (const_string "no")))
 
 ; condition codes: this one is used by final_prescan_insn to speed up
 ; conditionalizing instructions.  It saves having to scan the rtl to see if
@@ -436,9 +361,9 @@
 	 (ior (eq_attr "is_thumb1" "yes")
 	      (eq_attr "type" "call"))
 	 (const_string "clob")
-	 (if_then_else (eq_attr "neon_type" "none")
-	  (const_string "nocond")
-	  (const_string "unconditional"))))
+	 (if_then_else (eq_attr "is_neon_type" "no")
+	 (const_string "nocond")
+	 (const_string "unconditional"))))
 
 ; Predicable means that the insn can be conditionally executed based on
 ; an automatically added predicate (additional patterns are generated by 
@@ -464,9 +389,22 @@
 ; than one on the main cpu execution unit.
 (define_attr "core_cycles" "single,multi"
   (if_then_else (eq_attr "type"
-		 "simple_alu_imm,alu_reg,\
-                  simple_alu_shift,alu_shift,\
-                  float,fdivd,fdivs")
+    "adc_imm, adc_reg, adcs_imm, adcs_reg, adr, alu_ext, alu_imm, alu_reg,\
+    alu_shift_imm, alu_shift_reg, alus_ext, alus_imm, alus_reg,\
+    alus_shift_imm, alus_shift_reg, bfm, csel, rev, logic_imm, logic_reg,\
+    logic_shift_imm, logic_shift_reg, logics_imm, logics_reg,\
+    logics_shift_imm, logics_shift_reg, extend, shift_imm, float, fcsel,\
+    wmmx_wor, wmmx_wxor, wmmx_wand, wmmx_wandn, wmmx_wmov, wmmx_tmcrr,\
+    wmmx_tmrrc, wmmx_wldr, wmmx_wstr, wmmx_tmcr, wmmx_tmrc, wmmx_wadd,\
+    wmmx_wsub, wmmx_wmul, wmmx_wmac, wmmx_wavg2, wmmx_tinsr, wmmx_textrm,\
+    wmmx_wshufh, wmmx_wcmpeq, wmmx_wcmpgt, wmmx_wmax, wmmx_wmin, wmmx_wpack,\
+    wmmx_wunpckih, wmmx_wunpckil, wmmx_wunpckeh, wmmx_wunpckel, wmmx_wror,\
+    wmmx_wsra, wmmx_wsrl, wmmx_wsll, wmmx_wmadd, wmmx_tmia, wmmx_tmiaph,\
+    wmmx_tmiaxy, wmmx_tbcst, wmmx_tmovmsk, wmmx_wacc, wmmx_waligni,\
+    wmmx_walignr, wmmx_tandc, wmmx_textrc, wmmx_torc, wmmx_torvsc, wmmx_wsad,\
+    wmmx_wabs, wmmx_wabsdiff, wmmx_waddsubhx, wmmx_wsubaddhx, wmmx_wavg4,\
+    wmmx_wmulw, wmmx_wqmulm, wmmx_wqmulwm, wmmx_waddbhus, wmmx_wqmiaxy,\
+    wmmx_wmiaxy, wmmx_wmiawxy, wmmx_wmerge")
 		(const_string "single")
 	        (const_string "multi")))
 
@@ -508,7 +446,7 @@
 
 (define_attr "generic_sched" "yes,no"
   (const (if_then_else
-          (ior (eq_attr "tune" "fa526,fa626,fa606te,fa626te,fmp626,fa726te,arm926ejs,arm1020e,arm1026ejs,arm1136js,arm1136jfs,cortexa5,cortexa7,cortexa8,cortexa9,cortexa15,cortexm4,marvell_pj4")
+          (ior (eq_attr "tune" "fa526,fa626,fa606te,fa626te,fmp626,fa726te,arm926ejs,arm1020e,arm1026ejs,arm1136js,arm1136jfs,cortexa5,cortexa7,cortexa8,cortexa9,cortexa15,cortexa53,cortexm4,marvell_pj4")
 	       (eq_attr "tune_cortexr4" "yes"))
           (const_string "no")
           (const_string "yes"))))
@@ -516,7 +454,7 @@
 (define_attr "generic_vfp" "yes,no"
   (const (if_then_else
 	  (and (eq_attr "fpu" "vfp")
-	       (eq_attr "tune" "!arm1020e,arm1022e,cortexa5,cortexa7,cortexa8,cortexa9,cortexm4,marvell_pj4")
+	       (eq_attr "tune" "!arm1020e,arm1022e,cortexa5,cortexa7,cortexa8,cortexa9,cortexa53,cortexm4,marvell_pj4")
 	       (eq_attr "tune_cortexr4" "no"))
 	  (const_string "yes")
 	  (const_string "no"))))
@@ -537,6 +475,7 @@
 (include "cortex-a8.md")
 (include "cortex-a9.md")
 (include "cortex-a15.md")
+(include "cortex-a53.md")
 (include "cortex-r4.md")
 (include "cortex-r4f.md")
 (include "cortex-m4.md")
@@ -580,7 +519,8 @@
   ]
   "TARGET_THUMB1"
   "add\\t%Q0, %Q0, %Q2\;adc\\t%R0, %R0, %R2"
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "*arm_adddi3"
@@ -608,7 +548,8 @@
     operands[2] = gen_lowpart (SImode, operands[2]);
   }"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "*adddi_sesidi_di"
@@ -637,7 +578,8 @@
     operands[2] = gen_lowpart (SImode, operands[2]);
   }"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "*adddi_zesidi_di"
@@ -664,7 +606,8 @@
     operands[2] = gen_lowpart (SImode, operands[2]);
   }"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "addsi3"
@@ -703,14 +646,17 @@
 ;;  (plus (reg rN) (reg sp)) into (reg rN).  In this case reload will
 ;; put the duplicated register first, and not try the commutative version.
 (define_insn_and_split "*arm_addsi3"
-  [(set (match_operand:SI          0 "s_register_operand" "=rk, r,k, r,r, k, r, k,k,r, k, r")
-	(plus:SI (match_operand:SI 1 "s_register_operand" "%0, rk,k, r,rk,k, rk,k,r,rk,k, rk")
-		 (match_operand:SI 2 "reg_or_int_operand" "rk, rI,rI,k,Pj,Pj,L, L,L,PJ,PJ,?n")))]
+  [(set (match_operand:SI          0 "s_register_operand" "=rk,l,l ,l ,r ,k ,r,r ,k ,r ,k,k,r ,k ,r")
+        (plus:SI (match_operand:SI 1 "s_register_operand" "%0 ,l,0 ,l ,rk,k ,r,rk,k ,rk,k,r,rk,k ,rk")
+                 (match_operand:SI 2 "reg_or_int_operand" "rk ,l,Py,Pd,rI,rI,k,Pj,Pj,L ,L,L,PJ,PJ,?n")))]
   "TARGET_32BIT"
   "@
    add%?\\t%0, %0, %2
    add%?\\t%0, %1, %2
    add%?\\t%0, %1, %2
+   add%?\\t%0, %1, %2
+   add%?\\t%0, %1, %2
+   add%?\\t%0, %1, %2
    add%?\\t%0, %2, %1
    addw%?\\t%0, %1, %2
    addw%?\\t%0, %1, %2
@@ -731,11 +677,12 @@
 		      operands[1], 0);
   DONE;
   "
-  [(set_attr "length" "2,4,4,4,4,4,4,4,4,4,4,16")
+  [(set_attr "length" "2,4,4,4,4,4,4,4,4,4,4,4,4,4,16")
    (set_attr "predicable" "yes")
-   (set_attr "arch" "t2,*,*,*,t2,t2,*,*,a,t2,t2,*")
+   (set_attr "predicable_short_it" "yes,yes,yes,yes,no,no,no,no,no,no,no,no,no,no,no")
+   (set_attr "arch" "t2,t2,t2,t2,*,*,*,t2,t2,*,*,a,t2,t2,*")
    (set (attr "type") (if_then_else (match_operand 2 "const_int_operand" "")
-		      (const_string "simple_alu_imm")
+		      (const_string "alu_imm")
 		      (const_string "alu_reg")))
  ]
 )
@@ -786,7 +733,9 @@
     operands[3] = GEN_INT (offset);
     operands[2] = GEN_INT (INTVAL (operands[2]) - offset);
   }
-  [(set_attr "length" "2,2,2,2,2,2,2,4,4,4")]
+  [(set_attr "length" "2,2,2,2,2,2,2,4,4,4")
+   (set_attr "type" "alus_imm,alus_imm,alus_reg,alus_reg,alus_reg,
+		     alus_reg,alus_reg,multiple,multiple,multiple")]
 )
 
 ;; Reloading and elimination of the frame pointer can
@@ -817,7 +766,7 @@
    sub%.\\t%0, %1, #%n2
    add%.\\t%0, %1, %2"
   [(set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm, simple_alu_imm, *")]
+   (set_attr "type" "alus_imm,alus_imm,alus_reg")]
 )
 
 (define_insn "*addsi3_compare0_scratch"
@@ -833,24 +782,27 @@
    cmn%?\\t%0, %1"
   [(set_attr "conds" "set")
    (set_attr "predicable" "yes")
-   (set_attr "type" "simple_alu_imm, simple_alu_imm, *")
-   ]
+   (set_attr "type" "alus_imm,alus_imm,alus_reg")]
 )
 
 (define_insn "*compare_negsi_si"
   [(set (reg:CC_Z CC_REGNUM)
 	(compare:CC_Z
-	 (neg:SI (match_operand:SI 0 "s_register_operand" "r"))
-	 (match_operand:SI 1 "s_register_operand" "r")))]
+	 (neg:SI (match_operand:SI 0 "s_register_operand" "l,r"))
+	 (match_operand:SI 1 "s_register_operand" "l,r")))]
   "TARGET_32BIT"
   "cmn%?\\t%1, %0"
   [(set_attr "conds" "set")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "arch" "t2,*")
+   (set_attr "length" "2,4")
+   (set_attr "predicable_short_it" "yes,no")
+   (set_attr "type" "alus_reg")]
 )
 
 ;; This is the canonicalization of addsi3_compare0_for_combiner when the
 ;; addend is a constant.
-(define_insn "*cmpsi2_addneg"
+(define_insn "cmpsi2_addneg"
   [(set (reg:CC CC_REGNUM)
 	(compare:CC
 	 (match_operand:SI 1 "s_register_operand" "r,r")
@@ -862,7 +814,8 @@
   "@
    add%.\\t%0, %1, %3
    sub%.\\t%0, %1, #%n3"
-  [(set_attr "conds" "set")]
+  [(set_attr "conds" "set")
+   (set_attr "type" "alus_reg")]
 )
 
 ;; Convert the sequence
@@ -920,7 +873,7 @@
    sub%.\\t%0, %1, #%n2
    add%.\\t%0, %1, %2"
   [(set_attr "conds" "set")
-   (set_attr "type"  "simple_alu_imm,simple_alu_imm,*")]
+   (set_attr "type"  "alus_imm,alus_imm,alus_reg")]
 )
 
 (define_insn "*addsi3_compare_op2"
@@ -937,63 +890,85 @@
    add%.\\t%0, %1, %2
    sub%.\\t%0, %1, #%n2"
   [(set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm,simple_alu_imm,*")]
+   (set_attr "type" "alus_imm,alus_imm,alus_reg")]
 )
 
 (define_insn "*compare_addsi2_op0"
   [(set (reg:CC_C CC_REGNUM)
-	(compare:CC_C
-	 (plus:SI (match_operand:SI 0 "s_register_operand" "r,r,r")
-		  (match_operand:SI 1 "arm_add_operand" "I,L,r"))
-	 (match_dup 0)))]
+        (compare:CC_C
+          (plus:SI (match_operand:SI 0 "s_register_operand" "l,l,r,r,r")
+                   (match_operand:SI 1 "arm_add_operand" "Pv,l,I,L,r"))
+          (match_dup 0)))]
   "TARGET_32BIT"
   "@
+   cmp%?\\t%0, #%n1
+   cmn%?\\t%0, %1
    cmn%?\\t%0, %1
    cmp%?\\t%0, #%n1
    cmn%?\\t%0, %1"
   [(set_attr "conds" "set")
    (set_attr "predicable" "yes")
-   (set_attr "type" "simple_alu_imm,simple_alu_imm,*")]
+   (set_attr "arch" "t2,t2,*,*,*")
+   (set_attr "predicable_short_it" "yes,yes,no,no,no")
+   (set_attr "length" "2,2,4,4,4")
+   (set_attr "type" "alus_imm,alus_reg,alus_imm,alus_imm,alus_reg")]
 )
 
 (define_insn "*compare_addsi2_op1"
   [(set (reg:CC_C CC_REGNUM)
-	(compare:CC_C
-	 (plus:SI (match_operand:SI 0 "s_register_operand" "r,r,r")
-		  (match_operand:SI 1 "arm_add_operand" "I,L,r"))
-	 (match_dup 1)))]
+        (compare:CC_C
+          (plus:SI (match_operand:SI 0 "s_register_operand" "l,l,r,r,r")
+                   (match_operand:SI 1 "arm_add_operand" "Pv,l,I,L,r"))
+          (match_dup 1)))]
   "TARGET_32BIT"
   "@
+   cmp%?\\t%0, #%n1
+   cmn%?\\t%0, %1
    cmn%?\\t%0, %1
    cmp%?\\t%0, #%n1
    cmn%?\\t%0, %1"
   [(set_attr "conds" "set")
    (set_attr "predicable" "yes")
-   (set_attr "type" "simple_alu_imm,simple_alu_imm,*")]
-)
+   (set_attr "arch" "t2,t2,*,*,*")
+   (set_attr "predicable_short_it" "yes,yes,no,no,no")
+   (set_attr "length" "2,2,4,4,4")
+   (set_attr "type" "alus_imm,alus_reg,alus_imm,alus_imm,alus_reg")]
+ )
 
 (define_insn "*addsi3_carryin_<optab>"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r")
-	(plus:SI (plus:SI (match_operand:SI 1 "s_register_operand" "%r,r")
-			  (match_operand:SI 2 "arm_not_operand" "rI,K"))
-		 (LTUGEU:SI (reg:<cnb> CC_REGNUM) (const_int 0))))]
+  [(set (match_operand:SI 0 "s_register_operand" "=l,r,r")
+        (plus:SI (plus:SI (match_operand:SI 1 "s_register_operand" "%l,r,r")
+                          (match_operand:SI 2 "arm_not_operand" "0,rI,K"))
+                 (LTUGEU:SI (reg:<cnb> CC_REGNUM) (const_int 0))))]
   "TARGET_32BIT"
   "@
    adc%?\\t%0, %1, %2
+   adc%?\\t%0, %1, %2
    sbc%?\\t%0, %1, #%B2"
-  [(set_attr "conds" "use")]
+  [(set_attr "conds" "use")
+   (set_attr "predicable" "yes")
+   (set_attr "arch" "t2,*,*")
+   (set_attr "length" "4")
+   (set_attr "predicable_short_it" "yes,no,no")
+   (set_attr "type" "adc_reg,adc_reg,adc_imm")]
 )
 
 (define_insn "*addsi3_carryin_alt2_<optab>"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r")
-	(plus:SI (plus:SI (LTUGEU:SI (reg:<cnb> CC_REGNUM) (const_int 0))
-			  (match_operand:SI 1 "s_register_operand" "%r,r"))
-		 (match_operand:SI 2 "arm_rhs_operand" "rI,K")))]
+  [(set (match_operand:SI 0 "s_register_operand" "=l,r,r")
+        (plus:SI (plus:SI (LTUGEU:SI (reg:<cnb> CC_REGNUM) (const_int 0))
+                          (match_operand:SI 1 "s_register_operand" "%l,r,r"))
+                 (match_operand:SI 2 "arm_rhs_operand" "l,rI,K")))]
   "TARGET_32BIT"
   "@
    adc%?\\t%0, %1, %2
+   adc%?\\t%0, %1, %2
    sbc%?\\t%0, %1, #%B2"
-  [(set_attr "conds" "use")]
+  [(set_attr "conds" "use")
+   (set_attr "predicable" "yes")
+   (set_attr "arch" "t2,*,*")
+   (set_attr "length" "4")
+   (set_attr "predicable_short_it" "yes,no,no")
+   (set_attr "type" "adc_reg,adc_reg,adc_imm")]
 )
 
 (define_insn "*addsi3_carryin_shift_<optab>"
@@ -1007,8 +982,10 @@
   "TARGET_32BIT"
   "adc%?\\t%0, %1, %3%S2"
   [(set_attr "conds" "use")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set (attr "type") (if_then_else (match_operand 4 "const_int_operand" "")
-		      (const_string "alu_shift")
+		      (const_string "alu_shift_imm")
 		      (const_string "alu_shift_reg")))]
 )
 
@@ -1020,29 +997,97 @@
    (clobber (reg:CC CC_REGNUM))]
    "TARGET_32BIT"
    "adc%.\\t%0, %1, %2"
-   [(set_attr "conds" "set")]
+   [(set_attr "conds" "set")
+    (set_attr "type" "adcs_reg")]
 )
 
-(define_expand "incscc"
+(define_insn "*subsi3_carryin"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r")
-        (plus:SI (match_operator:SI 2 "arm_comparison_operator"
-                    [(match_operand:CC 3 "cc_register" "") (const_int 0)])
-                 (match_operand:SI 1 "s_register_operand" "0,?r")))]
+        (minus:SI (minus:SI (match_operand:SI 1 "reg_or_int_operand" "r,I")
+                            (match_operand:SI 2 "s_register_operand" "r,r"))
+                  (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
   "TARGET_32BIT"
-  ""
+  "@
+   sbc%?\\t%0, %1, %2
+   rsc%?\\t%0, %2, %1"
+  [(set_attr "conds" "use")
+   (set_attr "arch" "*,a")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "adc_reg,adc_imm")]
 )
 
-(define_insn "*arm_incscc"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r")
-        (plus:SI (match_operator:SI 2 "arm_comparison_operator"
-                    [(match_operand:CC 3 "cc_register" "") (const_int 0)])
-                 (match_operand:SI 1 "s_register_operand" "0,?r")))]
+(define_insn "*subsi3_carryin_const"
+  [(set (match_operand:SI 0 "s_register_operand" "=r")
+        (minus:SI (plus:SI (match_operand:SI 1 "reg_or_int_operand" "r")
+                           (match_operand:SI 2 "arm_not_operand" "K"))
+                  (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  "TARGET_32BIT"
+  "sbc\\t%0, %1, #%B2"
+  [(set_attr "conds" "use")
+   (set_attr "type" "adc_imm")]
+)
+
+(define_insn "*subsi3_carryin_compare"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_operand:SI 1 "s_register_operand" "r")
+                    (match_operand:SI 2 "s_register_operand" "r")))
+   (set (match_operand:SI 0 "s_register_operand" "=r")
+        (minus:SI (minus:SI (match_dup 1)
+                            (match_dup 2))
+                  (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  "TARGET_32BIT"
+  "sbcs\\t%0, %1, %2"
+  [(set_attr "conds" "set")
+   (set_attr "type" "adcs_reg")]
+)
+
+(define_insn "*subsi3_carryin_compare_const"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_operand:SI 1 "reg_or_int_operand" "r")
+                    (match_operand:SI 2 "arm_not_operand" "K")))
+   (set (match_operand:SI 0 "s_register_operand" "=r")
+        (minus:SI (plus:SI (match_dup 1)
+                           (match_dup 2))
+                  (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  "TARGET_32BIT"
+  "sbcs\\t%0, %1, #%B2"
+  [(set_attr "conds" "set")
+   (set_attr "type" "adcs_imm")]
+)
+
+(define_insn "*subsi3_carryin_shift"
+  [(set (match_operand:SI 0 "s_register_operand" "=r")
+	(minus:SI (minus:SI
+		  (match_operand:SI 1 "s_register_operand" "r")
+                  (match_operator:SI 2 "shift_operator"
+                   [(match_operand:SI 3 "s_register_operand" "r")
+                    (match_operand:SI 4 "reg_or_int_operand" "rM")]))
+                 (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  "TARGET_32BIT"
+  "sbc%?\\t%0, %1, %3%S2"
+  [(set_attr "conds" "use")
+   (set_attr "predicable" "yes")
+   (set (attr "type") (if_then_else (match_operand 4 "const_int_operand" "")
+		      (const_string "alu_shift_imm")
+                     (const_string "alu_shift_reg")))]
+)
+
+(define_insn "*rsbsi3_carryin_shift"
+  [(set (match_operand:SI 0 "s_register_operand" "=r")
+	(minus:SI (minus:SI
+                  (match_operator:SI 2 "shift_operator"
+                   [(match_operand:SI 3 "s_register_operand" "r")
+                    (match_operand:SI 4 "reg_or_int_operand" "rM")])
+		   (match_operand:SI 1 "s_register_operand" "r"))
+                 (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
   "TARGET_ARM"
-  "@
-  add%d2\\t%0, %1, #1
-  mov%D2\\t%0, %1\;add%d2\\t%0, %1, #1"
+  "rsc%?\\t%0, %1, %3%S2"
   [(set_attr "conds" "use")
-   (set_attr "length" "4,8")]
+   (set_attr "predicable" "yes")
+   (set (attr "type") (if_then_else (match_operand 4 "const_int_operand" "")
+		      (const_string "alu_shift_imm")
+		      (const_string "alu_shift_reg")))]
 )
 
 ; transform ((x << y) - 1) to ~(~(x-1) << y)  Where X is a constant.
@@ -1093,15 +1138,30 @@
   "
 )
 
-(define_insn "*arm_subdi3"
+(define_insn_and_split "*arm_subdi3"
   [(set (match_operand:DI           0 "s_register_operand" "=&r,&r,&r")
 	(minus:DI (match_operand:DI 1 "s_register_operand" "0,r,0")
 		  (match_operand:DI 2 "s_register_operand" "r,0,0")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_32BIT && !TARGET_NEON"
-  "subs\\t%Q0, %Q1, %Q2\;sbc\\t%R0, %R1, %R2"
+  "#"  ; "subs\\t%Q0, %Q1, %Q2\;sbc\\t%R0, %R1, %R2"
+  "&& reload_completed"
+  [(parallel [(set (reg:CC CC_REGNUM)
+		   (compare:CC (match_dup 1) (match_dup 2)))
+	      (set (match_dup 0) (minus:SI (match_dup 1) (match_dup 2)))])
+   (set (match_dup 3) (minus:SI (minus:SI (match_dup 4) (match_dup 5))
+			       (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  {
+    operands[3] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    operands[4] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+    operands[5] = gen_highpart (SImode, operands[2]);
+    operands[2] = gen_lowpart (SImode, operands[2]);
+   }
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*thumb_subdi3"
@@ -1111,58 +1171,121 @@
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB1"
   "sub\\t%Q0, %Q0, %Q2\;sbc\\t%R0, %R0, %R2"
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*subdi_di_zesidi"
+(define_insn_and_split "*subdi_di_zesidi"
   [(set (match_operand:DI           0 "s_register_operand" "=&r,&r")
 	(minus:DI (match_operand:DI 1 "s_register_operand"  "0,r")
 		  (zero_extend:DI
 		   (match_operand:SI 2 "s_register_operand"  "r,r"))))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_32BIT"
-  "subs\\t%Q0, %Q1, %2\;sbc\\t%R0, %R1, #0"
+  "#"   ; "subs\\t%Q0, %Q1, %2\;sbc\\t%R0, %R1, #0"
+  "&& reload_completed"
+  [(parallel [(set (reg:CC CC_REGNUM)
+		   (compare:CC (match_dup 1) (match_dup 2)))
+	      (set (match_dup 0) (minus:SI (match_dup 1) (match_dup 2)))])
+   (set (match_dup 3) (minus:SI (plus:SI (match_dup 4) (match_dup 5))
+                                (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  {
+    operands[3] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    operands[4] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+    operands[5] = GEN_INT (~0);
+   }
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*subdi_di_sesidi"
+(define_insn_and_split "*subdi_di_sesidi"
   [(set (match_operand:DI            0 "s_register_operand" "=&r,&r")
 	(minus:DI (match_operand:DI  1 "s_register_operand"  "0,r")
 		  (sign_extend:DI
 		   (match_operand:SI 2 "s_register_operand"  "r,r"))))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_32BIT"
-  "subs\\t%Q0, %Q1, %2\;sbc\\t%R0, %R1, %2, asr #31"
+  "#"   ; "subs\\t%Q0, %Q1, %2\;sbc\\t%R0, %R1, %2, asr #31"
+  "&& reload_completed"
+  [(parallel [(set (reg:CC CC_REGNUM)
+		   (compare:CC (match_dup 1) (match_dup 2)))
+	      (set (match_dup 0) (minus:SI (match_dup 1) (match_dup 2)))])
+   (set (match_dup 3) (minus:SI (minus:SI (match_dup 4)
+                                         (ashiftrt:SI (match_dup 2)
+                                                      (const_int 31)))
+                                (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  {
+    operands[3] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    operands[4] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+  }
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*subdi_zesidi_di"
+(define_insn_and_split "*subdi_zesidi_di"
   [(set (match_operand:DI            0 "s_register_operand" "=&r,&r")
 	(minus:DI (zero_extend:DI
 		   (match_operand:SI 2 "s_register_operand"  "r,r"))
 		  (match_operand:DI  1 "s_register_operand" "0,r")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "rsbs\\t%Q0, %Q1, %2\;rsc\\t%R0, %R1, #0"
+  "#"   ; "rsbs\\t%Q0, %Q1, %2\;rsc\\t%R0, %R1, #0"
+        ; is equivalent to:
+        ; "subs\\t%Q0, %2, %Q1\;rsc\\t%R0, %R1, #0"
+  "&& reload_completed"
+  [(parallel [(set (reg:CC CC_REGNUM)
+		   (compare:CC (match_dup 2) (match_dup 1)))
+	      (set (match_dup 0) (minus:SI (match_dup 2) (match_dup 1)))])
+   (set (match_dup 3) (minus:SI (minus:SI (const_int 0) (match_dup 4))
+			       (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  {
+    operands[3] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    operands[4] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+  }
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*subdi_sesidi_di"
+(define_insn_and_split "*subdi_sesidi_di"
   [(set (match_operand:DI            0 "s_register_operand" "=&r,&r")
 	(minus:DI (sign_extend:DI
 		   (match_operand:SI 2 "s_register_operand"   "r,r"))
 		  (match_operand:DI  1 "s_register_operand"  "0,r")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "rsbs\\t%Q0, %Q1, %2\;rsc\\t%R0, %R1, %2, asr #31"
+  "#"   ; "rsbs\\t%Q0, %Q1, %2\;rsc\\t%R0, %R1, %2, asr #31"
+        ; is equivalent to:
+        ; "subs\\t%Q0, %2, %Q1\;rsc\\t%R0, %R1, %2, asr #31"
+  "&& reload_completed"
+  [(parallel [(set (reg:CC CC_REGNUM)
+		   (compare:CC (match_dup 2) (match_dup 1)))
+	      (set (match_dup 0) (minus:SI (match_dup 2) (match_dup 1)))])
+   (set (match_dup 3) (minus:SI (minus:SI
+                                (ashiftrt:SI (match_dup 2)
+                                             (const_int 31))
+                                (match_dup 4))
+			       (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  {
+    operands[3] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    operands[4] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+  }
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*subdi_zesidi_zesidi"
+(define_insn_and_split "*subdi_zesidi_zesidi"
   [(set (match_operand:DI            0 "s_register_operand" "=r")
 	(minus:DI (zero_extend:DI
 		   (match_operand:SI 1 "s_register_operand"  "r"))
@@ -1170,9 +1293,20 @@
 		   (match_operand:SI 2 "s_register_operand"  "r"))))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_32BIT"
-  "subs\\t%Q0, %1, %2\;sbc\\t%R0, %1, %1"
+  "#"   ; "subs\\t%Q0, %1, %2\;sbc\\t%R0, %1, %1"
+  "&& reload_completed"
+  [(parallel [(set (reg:CC CC_REGNUM)
+		   (compare:CC (match_dup 1) (match_dup 2)))
+	      (set (match_dup 0) (minus:SI (match_dup 1) (match_dup 2)))])
+   (set (match_dup 3) (minus:SI (minus:SI (match_dup 1) (match_dup 1))
+			       (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  {
+       operands[3] = gen_highpart (SImode, operands[0]);
+       operands[0] = gen_lowpart (SImode, operands[0]);
+  }
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "subsi3"
@@ -1203,15 +1337,21 @@
   "TARGET_THUMB1"
   "sub\\t%0, %1, %2"
   [(set_attr "length" "2")
-   (set_attr "conds" "set")])
+   (set_attr "conds" "set")
+   (set_attr "type" "alus_reg")]
+)
 
 ; ??? Check Thumb-2 split length
 (define_insn_and_split "*arm_subsi3_insn"
-  [(set (match_operand:SI           0 "s_register_operand" "=r,r,r,rk,r")
-	(minus:SI (match_operand:SI 1 "reg_or_int_operand" "rI,r,r,k,?n")
-		  (match_operand:SI 2 "reg_or_int_operand" "r,I,r,r, r")))]
+  [(set (match_operand:SI           0 "s_register_operand" "=l,l ,l ,l ,r ,r,r,rk,r")
+	(minus:SI (match_operand:SI 1 "reg_or_int_operand" "l ,0 ,l ,Pz,rI,r,r,k ,?n")
+		  (match_operand:SI 2 "reg_or_int_operand" "l ,Py,Pd,l ,r ,I,r,r ,r")))]
   "TARGET_32BIT"
   "@
+   sub%?\\t%0, %1, %2
+   sub%?\\t%0, %2
+   sub%?\\t%0, %1, %2
+   rsb%?\\t%0, %2, %1
    rsb%?\\t%0, %2, %1
    sub%?\\t%0, %1, %2
    sub%?\\t%0, %1, %2
@@ -1225,9 +1365,11 @@
                       INTVAL (operands[1]), operands[0], operands[2], 0);
   DONE;
   "
-  [(set_attr "length" "4,4,4,4,16")
+  [(set_attr "length" "4,4,4,4,4,4,4,4,16")
+   (set_attr "arch" "t2,t2,t2,t2,*,*,*,*,*")
    (set_attr "predicable" "yes")
-   (set_attr "type"  "*,simple_alu_imm,*,*,*")]
+   (set_attr "predicable_short_it" "yes,yes,yes,yes,no,no,no,no,no")
+   (set_attr "type" "alu_reg,alu_reg,alu_reg,alu_reg,alu_imm,alu_imm,alu_reg,alu_reg,multiple")]
 )
 
 (define_peephole2
@@ -1257,10 +1399,10 @@
    sub%.\\t%0, %1, %2
    rsb%.\\t%0, %2, %1"
   [(set_attr "conds" "set")
-   (set_attr "type"  "simple_alu_imm,*,*")]
+   (set_attr "type"  "alus_imm,alus_reg,alus_reg")]
 )
 
-(define_insn "*subsi3_compare"
+(define_insn "subsi3_compare"
   [(set (reg:CC CC_REGNUM)
 	(compare:CC (match_operand:SI 1 "arm_rhs_operand" "r,r,I")
 		    (match_operand:SI 2 "arm_rhs_operand" "I,r,r")))
@@ -1272,30 +1414,7 @@
    sub%.\\t%0, %1, %2
    rsb%.\\t%0, %2, %1"
   [(set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm,*,*")]
-)
-
-(define_expand "decscc"
-  [(set (match_operand:SI            0 "s_register_operand" "=r,r")
-        (minus:SI (match_operand:SI  1 "s_register_operand" "0,?r")
-		  (match_operator:SI 2 "arm_comparison_operator"
-                   [(match_operand   3 "cc_register" "") (const_int 0)])))]
-  "TARGET_32BIT"
-  ""
-)
-
-(define_insn "*arm_decscc"
-  [(set (match_operand:SI            0 "s_register_operand" "=r,r")
-        (minus:SI (match_operand:SI  1 "s_register_operand" "0,?r")
-		  (match_operator:SI 2 "arm_comparison_operator"
-                   [(match_operand   3 "cc_register" "") (const_int 0)])))]
-  "TARGET_ARM"
-  "@
-   sub%d2\\t%0, %1, #1
-   mov%D2\\t%0, %1\;sub%d2\\t%0, %1, #1"
-  [(set_attr "conds" "use")
-   (set_attr "length" "*,8")
-   (set_attr "type" "simple_alu_imm,*")]
+   (set_attr "type" "alus_imm,alus_reg,alus_reg")]
 )
 
 (define_expand "subsf3"
@@ -1317,6 +1436,20 @@
 
 ;; Multiplication insns
 
+(define_expand "mulhi3"
+  [(set (match_operand:HI 0 "s_register_operand" "")
+	(mult:HI (match_operand:HI 1 "s_register_operand" "")
+		 (match_operand:HI 2 "s_register_operand" "")))]
+  "TARGET_DSP_MULTIPLY"
+  "
+  {
+    rtx result = gen_reg_rtx (SImode);
+    emit_insn (gen_mulhisi3 (result, operands[1], operands[2]));
+    emit_move_insn (operands[0], gen_lowpart (HImode, result));
+    DONE;
+  }"
+)
+
 (define_expand "mulsi3"
   [(set (match_operand:SI          0 "s_register_operand" "")
 	(mult:SI (match_operand:SI 2 "s_register_operand" "")
@@ -1332,18 +1465,21 @@
 		 (match_operand:SI 1 "s_register_operand" "%0,r")))]
   "TARGET_32BIT && !arm_arch6"
   "mul%?\\t%0, %2, %1"
-  [(set_attr "insn" "mul")
+  [(set_attr "type" "mul")
    (set_attr "predicable" "yes")]
 )
 
 (define_insn "*arm_mulsi3_v6"
-  [(set (match_operand:SI          0 "s_register_operand" "=r")
-	(mult:SI (match_operand:SI 1 "s_register_operand" "r")
-		 (match_operand:SI 2 "s_register_operand" "r")))]
+  [(set (match_operand:SI          0 "s_register_operand" "=l,l,r")
+	(mult:SI (match_operand:SI 1 "s_register_operand" "0,l,r")
+		 (match_operand:SI 2 "s_register_operand" "l,0,r")))]
   "TARGET_32BIT && arm_arch6"
   "mul%?\\t%0, %1, %2"
-  [(set_attr "insn" "mul")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "mul")
+   (set_attr "predicable" "yes")
+   (set_attr "arch" "t2,t2,*")
+   (set_attr "length" "4")
+   (set_attr "predicable_short_it" "yes,yes,no")]
 )
 
 ; Unfortunately with the Thumb the '&'/'0' trick can fails when operands 
@@ -1363,7 +1499,7 @@
     return \"mul\\t%0, %2\";
   "
   [(set_attr "length" "4,4,2")
-   (set_attr "insn" "mul")]
+   (set_attr "type" "muls")]
 )
 
 (define_insn "*thumb_mulsi3_v6"
@@ -1376,7 +1512,7 @@
    mul\\t%0, %1
    mul\\t%0, %1"
   [(set_attr "length" "2")
-   (set_attr "insn" "mul")]
+   (set_attr "type" "muls")]
 )
 
 (define_insn "*mulsi3_compare0"
@@ -1390,7 +1526,7 @@
   "TARGET_ARM && !arm_arch6"
   "mul%.\\t%0, %2, %1"
   [(set_attr "conds" "set")
-   (set_attr "insn" "muls")]
+   (set_attr "type" "muls")]
 )
 
 (define_insn "*mulsi3_compare0_v6"
@@ -1404,7 +1540,7 @@
   "TARGET_ARM && arm_arch6 && optimize_size"
   "mul%.\\t%0, %2, %1"
   [(set_attr "conds" "set")
-   (set_attr "insn" "muls")]
+   (set_attr "type" "muls")]
 )
 
 (define_insn "*mulsi_compare0_scratch"
@@ -1417,7 +1553,7 @@
   "TARGET_ARM && !arm_arch6"
   "mul%.\\t%0, %2, %1"
   [(set_attr "conds" "set")
-   (set_attr "insn" "muls")]
+   (set_attr "type" "muls")]
 )
 
 (define_insn "*mulsi_compare0_scratch_v6"
@@ -1430,7 +1566,7 @@
   "TARGET_ARM && arm_arch6 && optimize_size"
   "mul%.\\t%0, %2, %1"
   [(set_attr "conds" "set")
-   (set_attr "insn" "muls")]
+   (set_attr "type" "muls")]
 )
 
 ;; Unnamed templates to match MLA instruction.
@@ -1443,7 +1579,7 @@
 	  (match_operand:SI 3 "s_register_operand" "r,r,0,0")))]
   "TARGET_32BIT && !arm_arch6"
   "mla%?\\t%0, %2, %1, %3"
-  [(set_attr "insn" "mla")
+  [(set_attr "type" "mla")
    (set_attr "predicable" "yes")]
 )
 
@@ -1455,8 +1591,9 @@
 	  (match_operand:SI 3 "s_register_operand" "r")))]
   "TARGET_32BIT && arm_arch6"
   "mla%?\\t%0, %2, %1, %3"
-  [(set_attr "insn" "mla")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "mla")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_insn "*mulsi3addsi_compare0"
@@ -1473,7 +1610,7 @@
   "TARGET_ARM && arm_arch6"
   "mla%.\\t%0, %2, %1, %3"
   [(set_attr "conds" "set")
-   (set_attr "insn" "mlas")]
+   (set_attr "type" "mlas")]
 )
 
 (define_insn "*mulsi3addsi_compare0_v6"
@@ -1490,7 +1627,7 @@
   "TARGET_ARM && arm_arch6 && optimize_size"
   "mla%.\\t%0, %2, %1, %3"
   [(set_attr "conds" "set")
-   (set_attr "insn" "mlas")]
+   (set_attr "type" "mlas")]
 )
 
 (define_insn "*mulsi3addsi_compare0_scratch"
@@ -1505,7 +1642,7 @@
   "TARGET_ARM && !arm_arch6"
   "mla%.\\t%0, %2, %1, %3"
   [(set_attr "conds" "set")
-   (set_attr "insn" "mlas")]
+   (set_attr "type" "mlas")]
 )
 
 (define_insn "*mulsi3addsi_compare0_scratch_v6"
@@ -1520,7 +1657,7 @@
   "TARGET_ARM && arm_arch6 && optimize_size"
   "mla%.\\t%0, %2, %1, %3"
   [(set_attr "conds" "set")
-   (set_attr "insn" "mlas")]
+   (set_attr "type" "mlas")]
 )
 
 (define_insn "*mulsi3subsi"
@@ -1531,8 +1668,9 @@
 		   (match_operand:SI 1 "s_register_operand" "r"))))]
   "TARGET_32BIT && arm_arch_thumb2"
   "mls%?\\t%0, %2, %1, %3"
-  [(set_attr "insn" "mla")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "mla")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_expand "maddsidi4"
@@ -1554,7 +1692,7 @@
 	 (match_operand:DI 1 "s_register_operand" "0")))]
   "TARGET_32BIT && arm_arch3m && !arm_arch6"
   "smlal%?\\t%Q0, %R0, %3, %2"
-  [(set_attr "insn" "smlal")
+  [(set_attr "type" "smlal")
    (set_attr "predicable" "yes")]
 )
 
@@ -1567,8 +1705,9 @@
 	 (match_operand:DI 1 "s_register_operand" "0")))]
   "TARGET_32BIT && arm_arch6"
   "smlal%?\\t%Q0, %R0, %3, %2"
-  [(set_attr "insn" "smlal")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "smlal")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 ;; 32x32->64 widening multiply.
@@ -1593,7 +1732,7 @@
 	 (sign_extend:DI (match_operand:SI 2 "s_register_operand" "r"))))]
   "TARGET_32BIT && arm_arch3m && !arm_arch6"
   "smull%?\\t%Q0, %R0, %1, %2"
-  [(set_attr "insn" "smull")
+  [(set_attr "type" "smull")
    (set_attr "predicable" "yes")]
 )
 
@@ -1604,8 +1743,9 @@
 	 (sign_extend:DI (match_operand:SI 2 "s_register_operand" "r"))))]
   "TARGET_32BIT && arm_arch6"
   "smull%?\\t%Q0, %R0, %1, %2"
-  [(set_attr "insn" "smull")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "smull")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_expand "umulsidi3"
@@ -1624,7 +1764,7 @@
 	 (zero_extend:DI (match_operand:SI 2 "s_register_operand" "r"))))]
   "TARGET_32BIT && arm_arch3m && !arm_arch6"
   "umull%?\\t%Q0, %R0, %1, %2"
-  [(set_attr "insn" "umull")
+  [(set_attr "type" "umull")
    (set_attr "predicable" "yes")]
 )
 
@@ -1635,8 +1775,9 @@
 	 (zero_extend:DI (match_operand:SI 2 "s_register_operand" "r"))))]
   "TARGET_32BIT && arm_arch6"
   "umull%?\\t%Q0, %R0, %1, %2"
-  [(set_attr "insn" "umull")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "umull")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_expand "umaddsidi4"
@@ -1658,7 +1799,7 @@
 	 (match_operand:DI 1 "s_register_operand" "0")))]
   "TARGET_32BIT && arm_arch3m && !arm_arch6"
   "umlal%?\\t%Q0, %R0, %3, %2"
-  [(set_attr "insn" "umlal")
+  [(set_attr "type" "umlal")
    (set_attr "predicable" "yes")]
 )
 
@@ -1671,8 +1812,9 @@
 	 (match_operand:DI 1 "s_register_operand" "0")))]
   "TARGET_32BIT && arm_arch6"
   "umlal%?\\t%Q0, %R0, %3, %2"
-  [(set_attr "insn" "umlal")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "umlal")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_expand "smulsi3_highpart"
@@ -1700,7 +1842,7 @@
    (clobber (match_scratch:SI 3 "=&r,&r"))]
   "TARGET_32BIT && arm_arch3m && !arm_arch6"
   "smull%?\\t%3, %0, %2, %1"
-  [(set_attr "insn" "smull")
+  [(set_attr "type" "smull")
    (set_attr "predicable" "yes")]
 )
 
@@ -1715,8 +1857,9 @@
    (clobber (match_scratch:SI 3 "=r"))]
   "TARGET_32BIT && arm_arch6"
   "smull%?\\t%3, %0, %2, %1"
-  [(set_attr "insn" "smull")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "smull")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_expand "umulsi3_highpart"
@@ -1744,7 +1887,7 @@
    (clobber (match_scratch:SI 3 "=&r,&r"))]
   "TARGET_32BIT && arm_arch3m && !arm_arch6"
   "umull%?\\t%3, %0, %2, %1"
-  [(set_attr "insn" "umull")
+  [(set_attr "type" "umull")
    (set_attr "predicable" "yes")]
 )
 
@@ -1759,8 +1902,9 @@
    (clobber (match_scratch:SI 3 "=r"))]
   "TARGET_32BIT && arm_arch6"
   "umull%?\\t%3, %0, %2, %1"
-  [(set_attr "insn" "umull")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "umull")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_insn "mulhisi3"
@@ -1771,7 +1915,7 @@
 		  (match_operand:HI 2 "s_register_operand" "r"))))]
   "TARGET_DSP_MULTIPLY"
   "smulbb%?\\t%0, %1, %2"
-  [(set_attr "insn" "smulxy")
+  [(set_attr "type" "smulxy")
    (set_attr "predicable" "yes")]
 )
 
@@ -1784,8 +1928,9 @@
 		  (match_operand:HI 2 "s_register_operand" "r"))))]
   "TARGET_DSP_MULTIPLY"
   "smultb%?\\t%0, %1, %2"
-  [(set_attr "insn" "smulxy")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "smulxy")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_insn "*mulhisi3bt"
@@ -1797,8 +1942,9 @@
 		  (const_int 16))))]
   "TARGET_DSP_MULTIPLY"
   "smulbt%?\\t%0, %1, %2"
-  [(set_attr "insn" "smulxy")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "smulxy")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_insn "*mulhisi3tt"
@@ -1811,8 +1957,9 @@
 		  (const_int 16))))]
   "TARGET_DSP_MULTIPLY"
   "smultt%?\\t%0, %1, %2"
-  [(set_attr "insn" "smulxy")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "smulxy")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_insn "maddhisi4"
@@ -1824,8 +1971,9 @@
 		 (match_operand:SI 3 "s_register_operand" "r")))]
   "TARGET_DSP_MULTIPLY"
   "smlabb%?\\t%0, %1, %2, %3"
-  [(set_attr "insn" "smlaxy")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "smlaxy")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 ;; Note: there is no maddhisi4ibt because this one is canonical form
@@ -1839,8 +1987,9 @@
 		 (match_operand:SI 3 "s_register_operand" "r")))]
   "TARGET_DSP_MULTIPLY"
   "smlatb%?\\t%0, %1, %2, %3"
-  [(set_attr "insn" "smlaxy")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "smlaxy")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_insn "*maddhisi4tt"
@@ -1854,22 +2003,24 @@
 		 (match_operand:SI 3 "s_register_operand" "r")))]
   "TARGET_DSP_MULTIPLY"
   "smlatt%?\\t%0, %1, %2, %3"
-  [(set_attr "insn" "smlaxy")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "smlaxy")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_insn "maddhidi4"
   [(set (match_operand:DI 0 "s_register_operand" "=r")
 	(plus:DI
 	  (mult:DI (sign_extend:DI
-	 	    (match_operand:HI 1 "s_register_operand" "r"))
+		    (match_operand:HI 1 "s_register_operand" "r"))
 		   (sign_extend:DI
 		    (match_operand:HI 2 "s_register_operand" "r")))
 	  (match_operand:DI 3 "s_register_operand" "0")))]
   "TARGET_DSP_MULTIPLY"
   "smlalbb%?\\t%Q0, %R0, %1, %2"
-  [(set_attr "insn" "smlalxy")
-   (set_attr "predicable" "yes")])
+  [(set_attr "type" "smlalxy")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 ;; Note: there is no maddhidi4ibt because this one is canonical form
 (define_insn "*maddhidi4tb"
@@ -1884,8 +2035,9 @@
 	  (match_operand:DI 3 "s_register_operand" "0")))]
   "TARGET_DSP_MULTIPLY"
   "smlaltb%?\\t%Q0, %R0, %1, %2"
-  [(set_attr "insn" "smlalxy")
-   (set_attr "predicable" "yes")])
+  [(set_attr "type" "smlalxy")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*maddhidi4tt"
   [(set (match_operand:DI 0 "s_register_operand" "=r")
@@ -1901,8 +2053,9 @@
 	  (match_operand:DI 3 "s_register_operand" "0")))]
   "TARGET_DSP_MULTIPLY"
   "smlaltt%?\\t%Q0, %R0, %1, %2"
-  [(set_attr "insn" "smlalxy")
-   (set_attr "predicable" "yes")])
+  [(set_attr "type" "smlalxy")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_expand "mulsf3"
   [(set (match_operand:SF          0 "s_register_operand" "")
@@ -2030,13 +2183,50 @@
   ""
 )
 
-(define_insn "*anddi3_insn"
-  [(set (match_operand:DI         0 "s_register_operand" "=&r,&r")
-	(and:DI (match_operand:DI 1 "s_register_operand"  "%0,r")
-		(match_operand:DI 2 "s_register_operand"   "r,r")))]
-  "TARGET_32BIT && !TARGET_IWMMXT && !TARGET_NEON"
-  "#"
-  [(set_attr "length" "8")]
+(define_insn_and_split "*anddi3_insn"
+  [(set (match_operand:DI         0 "s_register_operand"     "=w,w ,&r,&r,&r,&r,?w,?w")
+        (and:DI (match_operand:DI 1 "s_register_operand"     "%w,0 ,0 ,r ,0 ,r ,w ,0")
+                (match_operand:DI 2 "arm_anddi_operand_neon" "w ,DL,r ,r ,De,De,w ,DL")))]
+  "TARGET_32BIT && !TARGET_IWMMXT"
+{
+  switch (which_alternative)
+    {
+    case 0: /* fall through */
+    case 6: return "vand\t%P0, %P1, %P2";
+    case 1: /* fall through */
+    case 7: return neon_output_logic_immediate ("vand", &operands[2],
+                    DImode, 1, VALID_NEON_QREG_MODE (DImode));
+    case 2:
+    case 3:
+    case 4:
+    case 5: /* fall through */
+      return "#";
+    default: gcc_unreachable ();
+    }
+}
+  "TARGET_32BIT && !TARGET_IWMMXT && reload_completed
+   && !(IS_VFP_REGNUM (REGNO (operands[0])))"
+  [(set (match_dup 3) (match_dup 4))
+   (set (match_dup 5) (match_dup 6))]
+  "
+  {
+    operands[3] = gen_lowpart (SImode, operands[0]);
+    operands[5] = gen_highpart (SImode, operands[0]);
+
+    operands[4] = simplify_gen_binary (AND, SImode,
+                                           gen_lowpart (SImode, operands[1]),
+                                           gen_lowpart (SImode, operands[2]));
+    operands[6] = simplify_gen_binary (AND, SImode,
+                                           gen_highpart (SImode, operands[1]),
+                                           gen_highpart_mode (SImode, DImode, operands[2]));
+
+  }"
+  [(set_attr "type" "neon_logic,neon_logic,multiple,multiple,\
+                     multiple,multiple,neon_logic,neon_logic")
+   (set_attr "arch" "neon_for_64bits,neon_for_64bits,*,*,*,*,
+                     avoid_neon_for_64bits,avoid_neon_for_64bits")
+   (set_attr "length" "*,*,8,8,8,8,*,*")
+  ]
 )
 
 (define_insn_and_split "*anddi_zesidi_di"
@@ -2057,7 +2247,8 @@
     operands[0] = gen_lowpart (SImode, operands[0]);
     operands[1] = gen_lowpart (SImode, operands[1]);
   }"
-  [(set_attr "length" "8")]
+  [(set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*anddi_sesdi_di"
@@ -2067,7 +2258,8 @@
 		(match_operand:DI  1 "s_register_operand" "0,r")))]
   "TARGET_32BIT"
   "#"
-  [(set_attr "length" "8")]
+  [(set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "andsi3"
@@ -2151,12 +2343,13 @@
 
 ; ??? Check split length for Thumb-2
 (define_insn_and_split "*arm_andsi3_insn"
-  [(set (match_operand:SI         0 "s_register_operand" "=r,r,r,r")
-	(and:SI (match_operand:SI 1 "s_register_operand" "r,r,r,r")
-		(match_operand:SI 2 "reg_or_int_operand" "I,K,r,?n")))]
+  [(set (match_operand:SI         0 "s_register_operand" "=r,l,r,r,r")
+	(and:SI (match_operand:SI 1 "s_register_operand" "%r,0,r,r,r")
+		(match_operand:SI 2 "reg_or_int_operand" "I,l,K,r,?n")))]
   "TARGET_32BIT"
   "@
    and%?\\t%0, %1, %2
+   and%?\\t%0, %1, %2
    bic%?\\t%0, %1, #%B2
    and%?\\t%0, %1, %2
    #"
@@ -2170,9 +2363,10 @@
 	               INTVAL (operands[2]), operands[0], operands[1], 0);
   DONE;
   "
-  [(set_attr "length" "4,4,4,16")
+  [(set_attr "length" "4,4,4,4,16")
    (set_attr "predicable" "yes")
-   (set_attr "type" 	"simple_alu_imm,simple_alu_imm,*,simple_alu_imm")]
+   (set_attr "predicable_short_it" "no,yes,no,no,no")
+   (set_attr "type" "logic_imm,logic_imm,logic_reg,logic_reg,logic_imm")]
 )
 
 (define_insn "*thumb1_andsi3_insn"
@@ -2182,7 +2376,7 @@
   "TARGET_THUMB1"
   "and\\t%0, %2"
   [(set_attr "length" "2")
-   (set_attr "type"  "simple_alu_imm")
+   (set_attr "type"  "logic_imm")
    (set_attr "conds" "set")])
 
 (define_insn "*andsi3_compare0"
@@ -2199,7 +2393,7 @@
    bic%.\\t%0, %1, #%B2
    and%.\\t%0, %1, %2"
   [(set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm,simple_alu_imm,*")]
+   (set_attr "type" "logics_imm,logics_imm,logics_reg")]
 )
 
 (define_insn "*andsi3_compare0_scratch"
@@ -2215,14 +2409,14 @@
    bic%.\\t%2, %0, #%B1
    tst%?\\t%0, %1"
   [(set_attr "conds" "set")
-   (set_attr "type"  "simple_alu_imm,simple_alu_imm,*")]
+   (set_attr "type"  "logics_imm,logics_imm,logics_reg")]
 )
 
 (define_insn "*zeroextractsi_compare0_scratch"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV (zero_extract:SI
 			  (match_operand:SI 0 "s_register_operand" "r")
-		 	  (match_operand 1 "const_int_operand" "n")
+			  (match_operand 1 "const_int_operand" "n")
 			  (match_operand 2 "const_int_operand" "n"))
 			 (const_int 0)))]
   "TARGET_32BIT
@@ -2238,7 +2432,8 @@
   "
   [(set_attr "conds" "set")
    (set_attr "predicable" "yes")
-   (set_attr "type" "simple_alu_imm")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "logics_imm")]
 )
 
 (define_insn_and_split "*ne_zeroextractsi"
@@ -2275,7 +2470,8 @@
    (set (attr "length")
 	(if_then_else (eq_attr "is_thumb" "yes")
 		      (const_int 12)
-		      (const_int 8)))]
+		      (const_int 8)))
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "*ne_zeroextractsi_shifted"
@@ -2300,7 +2496,8 @@
   operands[2] = GEN_INT (32 - INTVAL (operands[2]));
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "*ite_ne_zeroextractsi"
@@ -2338,7 +2535,8 @@
 			 << INTVAL (operands[3])); 
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "*ite_ne_zeroextractsi_shifted"
@@ -2365,7 +2563,8 @@
   operands[2] = GEN_INT (32 - INTVAL (operands[2]));
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_split
@@ -2665,7 +2864,9 @@
   "arm_arch_thumb2"
   "bfc%?\t%0, %2, %1"
   [(set_attr "length" "4")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "bfm")]
 )
 
 (define_insn "insv_t2"
@@ -2676,7 +2877,9 @@
   "arm_arch_thumb2"
   "bfi%?\t%0, %3, %2, %1"
   [(set_attr "length" "4")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "bfm")]
 )
 
 ; constants for op 2 will never be given to these patterns.
@@ -2701,9 +2904,10 @@
     operands[2] = gen_lowpart (SImode, operands[2]);
   }"
   [(set_attr "length" "8")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "type" "multiple")]
 )
-  
+
 (define_insn_and_split "*anddi_notzesidi_di"
   [(set (match_operand:DI 0 "s_register_operand" "=&r,&r")
 	(and:DI (not:DI (zero_extend:DI
@@ -2728,9 +2932,11 @@
     operands[1] = gen_lowpart (SImode, operands[1]);
   }"
   [(set_attr "length" "4,8")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "multiple")]
 )
-  
+
 (define_insn_and_split "*anddi_notsesidi_di"
   [(set (match_operand:DI 0 "s_register_operand" "=&r,&r")
 	(and:DI (not:DI (sign_extend:DI
@@ -2751,16 +2957,20 @@
     operands[1] = gen_lowpart (SImode, operands[1]);
   }"
   [(set_attr "length" "8")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "multiple")]
 )
-  
+
 (define_insn "andsi_notsi_si"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(and:SI (not:SI (match_operand:SI 2 "s_register_operand" "r"))
 		(match_operand:SI 1 "s_register_operand" "r")))]
   "TARGET_32BIT"
   "bic%?\\t%0, %1, %2"
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "logic_reg")]
 )
 
 (define_insn "thumb1_bicsi3"
@@ -2770,7 +2980,9 @@
   "TARGET_THUMB1"
   "bic\\t%0, %1"
   [(set_attr "length" "2")
-   (set_attr "conds" "set")])
+   (set_attr "conds" "set")
+   (set_attr "type" "logics_reg")]
+)
 
 (define_insn "andsi_not_shiftsi_si"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
@@ -2783,8 +2995,8 @@
   [(set_attr "predicable" "yes")
    (set_attr "shift" "2")
    (set (attr "type") (if_then_else (match_operand 3 "const_int_operand" "")
-		      (const_string "alu_shift")
-		      (const_string "alu_shift_reg")))]
+		      (const_string "logic_shift_imm")
+		      (const_string "logic_shift_reg")))]
 )
 
 (define_insn "*andsi_notsi_si_compare0"
@@ -2797,7 +3009,8 @@
 	(and:SI (not:SI (match_dup 2)) (match_dup 1)))]
   "TARGET_32BIT"
   "bic%.\\t%0, %1, %2"
-  [(set_attr "conds" "set")]
+  [(set_attr "conds" "set")
+   (set_attr "type" "logics_shift_reg")]
 )
 
 (define_insn "*andsi_notsi_si_compare0_scratch"
@@ -2809,7 +3022,8 @@
    (clobber (match_scratch:SI 0 "=r"))]
   "TARGET_32BIT"
   "bic%.\\t%0, %1, %2"
-  [(set_attr "conds" "set")]
+  [(set_attr "conds" "set")
+   (set_attr "type" "logics_shift_reg")]
 )
 
 (define_expand "iordi3"
@@ -2820,14 +3034,48 @@
   ""
 )
 
-(define_insn "*iordi3_insn"
-  [(set (match_operand:DI         0 "s_register_operand" "=&r,&r")
-	(ior:DI (match_operand:DI 1 "s_register_operand"  "%0,r")
-		(match_operand:DI 2 "s_register_operand"   "r,r")))]
-  "TARGET_32BIT && !TARGET_IWMMXT && !TARGET_NEON"
-  "#"
-  [(set_attr "length" "8")
-   (set_attr "predicable" "yes")]
+(define_insn_and_split "*iordi3_insn"
+  [(set (match_operand:DI         0 "s_register_operand"     "=w,w ,&r,&r,&r,&r,?w,?w")
+	(ior:DI (match_operand:DI 1 "s_register_operand"     "%w,0 ,0 ,r ,0 ,r ,w ,0")
+		(match_operand:DI 2 "arm_iordi_operand_neon" "w ,Dl,r ,r ,Df,Df,w ,Dl")))]
+  "TARGET_32BIT && !TARGET_IWMMXT"
+  {
+  switch (which_alternative)
+    {
+    case 0: /* fall through */
+    case 6: return "vorr\t%P0, %P1, %P2";
+    case 1: /* fall through */
+    case 7: return neon_output_logic_immediate ("vorr", &operands[2],
+		     DImode, 0, VALID_NEON_QREG_MODE (DImode));
+    case 2:
+    case 3:
+    case 4:
+    case 5:
+      return "#";
+    default: gcc_unreachable ();
+    }
+  }
+  "TARGET_32BIT && !TARGET_IWMMXT && reload_completed
+   && !(IS_VFP_REGNUM (REGNO (operands[0])))"
+  [(set (match_dup 3) (match_dup 4))
+   (set (match_dup 5) (match_dup 6))]
+  "
+  {
+    operands[3] = gen_lowpart (SImode, operands[0]);
+    operands[5] = gen_highpart (SImode, operands[0]);
+
+    operands[4] = simplify_gen_binary (IOR, SImode,
+                                           gen_lowpart (SImode, operands[1]),
+                                           gen_lowpart (SImode, operands[2]));
+    operands[6] = simplify_gen_binary (IOR, SImode,
+                                           gen_highpart (SImode, operands[1]),
+                                           gen_highpart_mode (SImode, DImode, operands[2]));
+
+  }"
+  [(set_attr "type" "neon_logic,neon_logic,multiple,multiple,multiple,\
+                     multiple,neon_logic,neon_logic")
+   (set_attr "length" "*,*,8,8,8,8,*,*")
+   (set_attr "arch" "neon_for_64bits,neon_for_64bits,*,*,*,*,avoid_neon_for_64bits,avoid_neon_for_64bits")]
 )
 
 (define_insn "*iordi_zesidi_di"
@@ -2840,7 +3088,9 @@
    orr%?\\t%Q0, %Q1, %2
    #"
   [(set_attr "length" "4,8")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "logic_reg,multiple")]
 )
 
 (define_insn "*iordi_sesidi_di"
@@ -2851,7 +3101,8 @@
   "TARGET_32BIT"
   "#"
   [(set_attr "length" "8")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "iorsi3"
@@ -2885,12 +3136,13 @@
 )
 
 (define_insn_and_split "*iorsi3_insn"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r,r,r")
-	(ior:SI (match_operand:SI 1 "s_register_operand" "%r,r,r,r")
-		(match_operand:SI 2 "reg_or_int_operand" "I,K,r,?n")))]
+  [(set (match_operand:SI 0 "s_register_operand" "=r,l,r,r,r")
+	(ior:SI (match_operand:SI 1 "s_register_operand" "%r,0,r,r,r")
+		(match_operand:SI 2 "reg_or_int_operand" "I,l,K,r,?n")))]
   "TARGET_32BIT"
   "@
    orr%?\\t%0, %1, %2
+   orr%?\\t%0, %1, %2
    orn%?\\t%0, %1, #%B2
    orr%?\\t%0, %1, %2
    #"
@@ -2900,14 +3152,15 @@
         || (TARGET_THUMB2 && const_ok_for_arm (~INTVAL (operands[2]))))"
   [(clobber (const_int 0))]
 {
-  arm_split_constant (IOR, SImode, curr_insn, 
+  arm_split_constant (IOR, SImode, curr_insn,
                       INTVAL (operands[2]), operands[0], operands[1], 0);
   DONE;
 }
-  [(set_attr "length" "4,4,4,16")
-   (set_attr "arch" "32,t2,32,32")
+  [(set_attr "length" "4,4,4,4,16")
+   (set_attr "arch" "32,t2,t2,32,32")
    (set_attr "predicable" "yes")
-   (set_attr "type" "simple_alu_imm,simple_alu_imm,*,*")]
+   (set_attr "predicable_short_it" "no,yes,no,no,no")
+   (set_attr "type" "logic_imm,logic_reg,logic_imm,logic_reg,logic_reg")]
 )
 
 (define_insn "*thumb1_iorsi3_insn"
@@ -2917,7 +3170,8 @@
   "TARGET_THUMB1"
   "orr\\t%0, %2"
   [(set_attr "length" "2")
-   (set_attr "conds" "set")])
+   (set_attr "conds" "set")
+   (set_attr "type" "logics_reg")])
 
 (define_peephole2
   [(match_scratch:SI 3 "r")
@@ -2942,7 +3196,7 @@
   "TARGET_32BIT"
   "orr%.\\t%0, %1, %2"
   [(set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm,*")]
+   (set_attr "type" "logics_imm,logics_reg")]
 )
 
 (define_insn "*iorsi3_compare0_scratch"
@@ -2954,25 +3208,55 @@
   "TARGET_32BIT"
   "orr%.\\t%0, %1, %2"
   [(set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm, *")]
+   (set_attr "type" "logics_imm,logics_reg")]
 )
 
 (define_expand "xordi3"
   [(set (match_operand:DI         0 "s_register_operand" "")
 	(xor:DI (match_operand:DI 1 "s_register_operand" "")
-		(match_operand:DI 2 "s_register_operand" "")))]
+		(match_operand:DI 2 "arm_xordi_operand" "")))]
   "TARGET_32BIT"
   ""
 )
 
-(define_insn "*xordi3_insn"
-  [(set (match_operand:DI         0 "s_register_operand" "=&r,&r")
-	(xor:DI (match_operand:DI 1 "s_register_operand"  "%0,r")
-		(match_operand:DI 2 "s_register_operand"   "r,r")))]
-  "TARGET_32BIT && !TARGET_IWMMXT && !TARGET_NEON"
-  "#"
-  [(set_attr "length" "8")
-   (set_attr "predicable" "yes")]
+(define_insn_and_split "*xordi3_insn"
+  [(set (match_operand:DI         0 "s_register_operand" "=w,&r,&r,&r,&r,?w")
+	(xor:DI (match_operand:DI 1 "s_register_operand" "w ,%0,r ,0 ,r ,w")
+		(match_operand:DI 2 "arm_xordi_operand"  "w ,r ,r ,Dg,Dg,w")))]
+  "TARGET_32BIT && !TARGET_IWMMXT"
+{
+  switch (which_alternative)
+    {
+    case 1:
+    case 2:
+    case 3:
+    case 4:  /* fall through */
+      return "#";
+    case 0: /* fall through */
+    case 5: return "veor\t%P0, %P1, %P2";
+    default: gcc_unreachable ();
+    }
+}
+  "TARGET_32BIT && !TARGET_IWMMXT && reload_completed
+   && !(IS_VFP_REGNUM (REGNO (operands[0])))"
+  [(set (match_dup 3) (match_dup 4))
+   (set (match_dup 5) (match_dup 6))]
+  "
+  {
+    operands[3] = gen_lowpart (SImode, operands[0]);
+    operands[5] = gen_highpart (SImode, operands[0]);
+
+    operands[4] = simplify_gen_binary (XOR, SImode,
+                                           gen_lowpart (SImode, operands[1]),
+                                           gen_lowpart (SImode, operands[2]));
+    operands[6] = simplify_gen_binary (XOR, SImode,
+                                           gen_highpart (SImode, operands[1]),
+                                           gen_highpart_mode (SImode, DImode, operands[2]));
+
+  }"
+  [(set_attr "length" "*,8,8,8,8,*")
+   (set_attr "type" "neon_logic,multiple,multiple,multiple,multiple,neon_logic")
+   (set_attr "arch" "neon_for_64bits,*,*,*,*,avoid_neon_for_64bits")]
 )
 
 (define_insn "*xordi_zesidi_di"
@@ -2985,7 +3269,9 @@
    eor%?\\t%Q0, %Q1, %2
    #"
   [(set_attr "length" "4,8")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "logic_reg")]
 )
 
 (define_insn "*xordi_sesidi_di"
@@ -2996,7 +3282,8 @@
   "TARGET_32BIT"
   "#"
   [(set_attr "length" "8")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "xorsi3"
@@ -3028,13 +3315,14 @@
 )
 
 (define_insn_and_split "*arm_xorsi3"
-  [(set (match_operand:SI         0 "s_register_operand" "=r,r,r")
-	(xor:SI (match_operand:SI 1 "s_register_operand" "%r,r,r")
-		(match_operand:SI 2 "reg_or_int_operand" "I,r,?n")))]
+  [(set (match_operand:SI         0 "s_register_operand" "=r,l,r,r")
+	(xor:SI (match_operand:SI 1 "s_register_operand" "%r,0,r,r")
+		(match_operand:SI 2 "reg_or_int_operand" "I,l,r,?n")))]
   "TARGET_32BIT"
   "@
    eor%?\\t%0, %1, %2
    eor%?\\t%0, %1, %2
+   eor%?\\t%0, %1, %2
    #"
   "TARGET_32BIT
    && CONST_INT_P (operands[2])
@@ -3045,9 +3333,10 @@
                       INTVAL (operands[2]), operands[0], operands[1], 0);
   DONE;
 }
-  [(set_attr "length" "4,4,16")
+  [(set_attr "length" "4,4,4,16")
    (set_attr "predicable" "yes")
-   (set_attr "type"  "simple_alu_imm,*,*")]
+   (set_attr "predicable_short_it" "no,yes,no,no")
+   (set_attr "type"  "logic_imm,logic_reg,logic_reg,multiple")]
 )
 
 (define_insn "*thumb1_xorsi3_insn"
@@ -3058,7 +3347,7 @@
   "eor\\t%0, %2"
   [(set_attr "length" "2")
    (set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm")]
+   (set_attr "type" "logics_reg")]
 )
 
 (define_insn "*xorsi3_compare0"
@@ -3071,7 +3360,7 @@
   "TARGET_32BIT"
   "eor%.\\t%0, %1, %2"
   [(set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm,*")]
+   (set_attr "type" "logics_imm,logics_reg")]
 )
 
 (define_insn "*xorsi3_compare0_scratch"
@@ -3082,7 +3371,7 @@
   "TARGET_32BIT"
   "teq%?\\t%0, %1"
   [(set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm, *")]
+   (set_attr "type" "logics_imm,logics_reg")]
 )
 
 ; By splitting (IOR (AND (NOT A) (NOT B)) C) as D = AND (IOR A B) (NOT C), 
@@ -3102,16 +3391,22 @@
   ""
 )
 
-(define_insn "*andsi_iorsi3_notsi"
+(define_insn_and_split "*andsi_iorsi3_notsi"
   [(set (match_operand:SI 0 "s_register_operand" "=&r,&r,&r")
 	(and:SI (ior:SI (match_operand:SI 1 "s_register_operand" "%0,r,r")
 			(match_operand:SI 2 "arm_rhs_operand" "rI,0,rI"))
 		(not:SI (match_operand:SI 3 "arm_rhs_operand" "rI,rI,rI"))))]
   "TARGET_32BIT"
-  "orr%?\\t%0, %1, %2\;bic%?\\t%0, %0, %3"
+  "#"   ; "orr%?\\t%0, %1, %2\;bic%?\\t%0, %0, %3"
+  "&& reload_completed"
+  [(set (match_dup 0) (ior:SI (match_dup 1) (match_dup 2)))
+   (set (match_dup 0) (and:SI (not:SI (match_dup 3)) (match_dup 0)))]
+  ""
   [(set_attr "length" "8")
    (set_attr "ce_count" "2")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "multiple")]
 )
 
 ; ??? Are these four splitters still beneficial when the Thumb-2 bitfield
@@ -3247,7 +3542,9 @@
 		 (const_int 0)))]
   "TARGET_32BIT"
   "bic%?\\t%0, %1, %1, asr #31"
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "logic_shift_reg")]
 )
 
 (define_insn "*smax_m1"
@@ -3256,20 +3553,31 @@
 		 (const_int -1)))]
   "TARGET_32BIT"
   "orr%?\\t%0, %1, %1, asr #31"
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "logic_shift_reg")]
 )
 
-(define_insn "*arm_smax_insn"
+(define_insn_and_split "*arm_smax_insn"
   [(set (match_operand:SI          0 "s_register_operand" "=r,r")
 	(smax:SI (match_operand:SI 1 "s_register_operand"  "%0,?r")
 		 (match_operand:SI 2 "arm_rhs_operand"    "rI,rI")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "@
-   cmp\\t%1, %2\;movlt\\t%0, %2
-   cmp\\t%1, %2\;movge\\t%0, %1\;movlt\\t%0, %2"
+  "#"
+   ; cmp\\t%1, %2\;movlt\\t%0, %2
+   ; cmp\\t%1, %2\;movge\\t%0, %1\;movlt\\t%0, %2"
+  "TARGET_ARM"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 1) (match_dup 2)))
+   (set (match_dup 0)
+        (if_then_else:SI (ge:SI (reg:CC CC_REGNUM) (const_int 0))
+                         (match_dup 1)
+                         (match_dup 2)))]
+  ""
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "sminsi3"
@@ -3296,20 +3604,31 @@
 		 (const_int 0)))]
   "TARGET_32BIT"
   "and%?\\t%0, %1, %1, asr #31"
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "logic_shift_reg")]
 )
 
-(define_insn "*arm_smin_insn"
+(define_insn_and_split "*arm_smin_insn"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r")
 	(smin:SI (match_operand:SI 1 "s_register_operand" "%0,?r")
 		 (match_operand:SI 2 "arm_rhs_operand" "rI,rI")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "@
-   cmp\\t%1, %2\;movge\\t%0, %2
-   cmp\\t%1, %2\;movlt\\t%0, %1\;movge\\t%0, %2"
+  "#"
+    ; cmp\\t%1, %2\;movge\\t%0, %2
+    ; cmp\\t%1, %2\;movlt\\t%0, %1\;movge\\t%0, %2"
+  "TARGET_ARM"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 1) (match_dup 2)))
+   (set (match_dup 0)
+        (if_then_else:SI (lt:SI (reg:CC CC_REGNUM) (const_int 0))
+                         (match_dup 1)
+                         (match_dup 2)))]
+  ""
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple,multiple")]
 )
 
 (define_expand "umaxsi3"
@@ -3322,18 +3641,27 @@
   ""
 )
 
-(define_insn "*arm_umaxsi3"
+(define_insn_and_split "*arm_umaxsi3"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r,r")
 	(umax:SI (match_operand:SI 1 "s_register_operand" "0,r,?r")
 		 (match_operand:SI 2 "arm_rhs_operand" "rI,0,rI")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "@
-   cmp\\t%1, %2\;movcc\\t%0, %2
-   cmp\\t%1, %2\;movcs\\t%0, %1
-   cmp\\t%1, %2\;movcs\\t%0, %1\;movcc\\t%0, %2"
+  "#"
+    ; cmp\\t%1, %2\;movcc\\t%0, %2
+    ; cmp\\t%1, %2\;movcs\\t%0, %1
+    ; cmp\\t%1, %2\;movcs\\t%0, %1\;movcc\\t%0, %2"
+  "TARGET_ARM"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 1) (match_dup 2)))
+   (set (match_dup 0)
+        (if_then_else:SI (geu:SI (reg:CC CC_REGNUM) (const_int 0))
+                         (match_dup 1)
+                         (match_dup 2)))]
+  ""
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,8,12")]
+   (set_attr "length" "8,8,12")
+   (set_attr "type" "store1")]
 )
 
 (define_expand "uminsi3"
@@ -3346,18 +3674,27 @@
   ""
 )
 
-(define_insn "*arm_uminsi3"
+(define_insn_and_split "*arm_uminsi3"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r,r")
 	(umin:SI (match_operand:SI 1 "s_register_operand" "0,r,?r")
 		 (match_operand:SI 2 "arm_rhs_operand" "rI,0,rI")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "@
-   cmp\\t%1, %2\;movcs\\t%0, %2
-   cmp\\t%1, %2\;movcc\\t%0, %1
-   cmp\\t%1, %2\;movcc\\t%0, %1\;movcs\\t%0, %2"
+  "#"
+   ; cmp\\t%1, %2\;movcs\\t%0, %2
+   ; cmp\\t%1, %2\;movcc\\t%0, %1
+   ; cmp\\t%1, %2\;movcc\\t%0, %1\;movcs\\t%0, %2"
+  "TARGET_ARM"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 1) (match_dup 2)))
+   (set (match_dup 0)
+        (if_then_else:SI (ltu:SI (reg:CC CC_REGNUM) (const_int 0))
+                         (match_dup 1)
+                         (match_dup 2)))]
+  ""
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,8,12")]
+   (set_attr "length" "8,8,12")
+   (set_attr "type" "store1")]
 )
 
 (define_insn "*store_minmaxsi"
@@ -3366,7 +3703,7 @@
 	 [(match_operand:SI 1 "s_register_operand" "r")
 	  (match_operand:SI 2 "s_register_operand" "r")]))
    (clobber (reg:CC CC_REGNUM))]
-  "TARGET_32BIT"
+  "TARGET_32BIT && optimize_insn_for_size_p()"
   "*
   operands[3] = gen_rtx_fmt_ee (minmax_code (operands[3]), SImode,
 				operands[1], operands[2]);
@@ -3395,7 +3732,7 @@
 	    (match_operand:SI 3 "arm_rhs_operand" "rI,rI")])
 	  (match_operand:SI 1 "s_register_operand" "0,?r")]))
    (clobber (reg:CC CC_REGNUM))]
-  "TARGET_32BIT && !arm_eliminable_register (operands[1])"
+  "TARGET_32BIT && !arm_eliminable_register (operands[1]) && !arm_restrict_it"
   "*
   {
     enum rtx_code code = GET_CODE (operands[4]);
@@ -3426,7 +3763,57 @@
    (set (attr "length")
 	(if_then_else (eq_attr "is_thumb" "yes")
 		      (const_int 14)
-		      (const_int 12)))]
+		      (const_int 12)))
+   (set_attr "type" "multiple")]
+)
+
+; Reject the frame pointer in operand[1], since reloading this after
+; it has been eliminated can cause carnage.
+(define_insn_and_split "*minmax_arithsi_non_canon"
+  [(set (match_operand:SI 0 "s_register_operand" "=Ts,Ts")
+	(minus:SI
+	 (match_operand:SI 1 "s_register_operand" "0,?Ts")
+	  (match_operator:SI 4 "minmax_operator"
+	   [(match_operand:SI 2 "s_register_operand" "Ts,Ts")
+	    (match_operand:SI 3 "arm_rhs_operand" "TsI,TsI")])))
+   (clobber (reg:CC CC_REGNUM))]
+  "TARGET_32BIT && !arm_eliminable_register (operands[1])
+   && !(arm_restrict_it && CONST_INT_P (operands[3]))"
+  "#"
+  "TARGET_32BIT && !arm_eliminable_register (operands[1]) && reload_completed"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 2) (match_dup 3)))
+
+   (cond_exec (match_op_dup 4 [(reg:CC CC_REGNUM) (const_int 0)])
+              (set (match_dup 0)
+                   (minus:SI (match_dup 1)
+                             (match_dup 2))))
+   (cond_exec (match_op_dup 5 [(reg:CC CC_REGNUM) (const_int 0)])
+              (set (match_dup 0)
+                   (match_dup 6)))]
+  {
+  enum machine_mode mode = SELECT_CC_MODE (GET_CODE (operands[1]),
+                                           operands[2], operands[3]);
+  enum rtx_code rc = minmax_code (operands[4]);
+  operands[4] = gen_rtx_fmt_ee (rc, VOIDmode,
+                                operands[2], operands[3]);
+
+  if (mode == CCFPmode || mode == CCFPEmode)
+    rc = reverse_condition_maybe_unordered (rc);
+  else
+    rc = reverse_condition (rc);
+  operands[5] = gen_rtx_fmt_ee (rc, SImode, operands[2], operands[3]);
+  if (CONST_INT_P (operands[3]))
+    operands[6] = plus_constant (SImode, operands[1], -INTVAL (operands[3]));
+  else
+    operands[6] = gen_rtx_MINUS (SImode, operands[1], operands[3]);
+  }
+  [(set_attr "conds" "clob")
+   (set (attr "length")
+	(if_then_else (eq_attr "is_thumb" "yes")
+		      (const_int 14)
+		      (const_int 12)))
+   (set_attr "type" "multiple")]
 )
 
 (define_code_iterator SAT [smin smax])
@@ -3455,7 +3842,9 @@
     return "usat%?\t%0, %1, %3";
 }
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "sat")])
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "alus_imm")]
+)
 
 (define_insn "*satsi_<SAT:code>_shift"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
@@ -3480,9 +3869,9 @@
     return "usat%?\t%0, %1, %4%S3";
 }
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "sat")
+   (set_attr "predicable_short_it" "no")
    (set_attr "shift" "3")
-   (set_attr "type" "alu_shift")])
+   (set_attr "type" "logic_shift_reg")])
 
 ;; Shift and rotation insns
 
@@ -3547,7 +3936,8 @@
   "TARGET_32BIT"
   "movs\\t%Q0, %Q1, asl #1\;adc\\t%R0, %R1, %R1"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "ashlsi3"
@@ -3572,6 +3962,7 @@
   "TARGET_THUMB1"
   "lsl\\t%0, %1, %2"
   [(set_attr "length" "2")
+   (set_attr "type" "shift_imm,shift_reg")
    (set_attr "conds" "set")])
 
 (define_expand "ashrdi3"
@@ -3629,8 +4020,8 @@
   "TARGET_32BIT"
   "movs\\t%R0, %R1, asr #1\;mov\\t%Q0, %Q1, rrx"
   [(set_attr "conds" "clob")
-   (set_attr "insn" "mov")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "ashrsi3"
@@ -3652,6 +4043,7 @@
   "TARGET_THUMB1"
   "asr\\t%0, %1, %2"
   [(set_attr "length" "2")
+   (set_attr "type" "shift_imm,shift_reg")
    (set_attr "conds" "set")])
 
 (define_expand "lshrdi3"
@@ -3709,8 +4101,8 @@
   "TARGET_32BIT"
   "movs\\t%R0, %R1, lsr #1\;mov\\t%Q0, %Q1, rrx"
   [(set_attr "conds" "clob")
-   (set_attr "insn" "mov")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "lshrsi3"
@@ -3735,6 +4127,7 @@
   "TARGET_THUMB1"
   "lsr\\t%0, %1, %2"
   [(set_attr "length" "2")
+   (set_attr "type" "shift_imm,shift_reg")
    (set_attr "conds" "set")])
 
 (define_expand "rotlsi3"
@@ -3780,51 +4173,52 @@
 		     (match_operand:SI 2 "register_operand" "l")))]
   "TARGET_THUMB1"
   "ror\\t%0, %0, %2"
-  [(set_attr "length" "2")]
+  [(set_attr "type" "shift_reg")
+   (set_attr "length" "2")]
 )
 
 (define_insn "*arm_shiftsi3"
-  [(set (match_operand:SI   0 "s_register_operand" "=r")
+  [(set (match_operand:SI   0 "s_register_operand" "=l,r,r")
 	(match_operator:SI  3 "shift_operator"
-	 [(match_operand:SI 1 "s_register_operand"  "r")
-	  (match_operand:SI 2 "reg_or_int_operand" "rM")]))]
+	 [(match_operand:SI 1 "s_register_operand"  "0,r,r")
+	  (match_operand:SI 2 "reg_or_int_operand" "l,M,r")]))]
   "TARGET_32BIT"
   "* return arm_output_shift(operands, 0);"
   [(set_attr "predicable" "yes")
+   (set_attr "arch" "t2,*,*")
+   (set_attr "predicable_short_it" "yes,no,no")
+   (set_attr "length" "4")
    (set_attr "shift" "1")
-   (set (attr "type") (if_then_else (match_operand 2 "const_int_operand" "")
-		      (const_string "alu_shift")
-		      (const_string "alu_shift_reg")))]
+   (set_attr "type" "alu_shift_reg,alu_shift_imm,alu_shift_reg")]
 )
 
 (define_insn "*shiftsi3_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV (match_operator:SI 3 "shift_operator"
-			  [(match_operand:SI 1 "s_register_operand" "r")
-			   (match_operand:SI 2 "arm_rhs_operand" "rM")])
+			  [(match_operand:SI 1 "s_register_operand" "r,r")
+			   (match_operand:SI 2 "arm_rhs_operand" "M,r")])
 			 (const_int 0)))
-   (set (match_operand:SI 0 "s_register_operand" "=r")
+   (set (match_operand:SI 0 "s_register_operand" "=r,r")
 	(match_op_dup 3 [(match_dup 1) (match_dup 2)]))]
   "TARGET_32BIT"
   "* return arm_output_shift(operands, 1);"
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
-   (set (attr "type") (if_then_else (match_operand 2 "const_int_operand" "")
-		      (const_string "alu_shift")
-		      (const_string "alu_shift_reg")))]
+   (set_attr "type" "alus_shift_imm,alus_shift_reg")]
 )
 
 (define_insn "*shiftsi3_compare0_scratch"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV (match_operator:SI 3 "shift_operator"
-			  [(match_operand:SI 1 "s_register_operand" "r")
-			   (match_operand:SI 2 "arm_rhs_operand" "rM")])
+			  [(match_operand:SI 1 "s_register_operand" "r,r")
+			   (match_operand:SI 2 "arm_rhs_operand" "M,r")])
 			 (const_int 0)))
-   (clobber (match_scratch:SI 0 "=r"))]
+   (clobber (match_scratch:SI 0 "=r,r"))]
   "TARGET_32BIT"
   "* return arm_output_shift(operands, 1);"
   [(set_attr "conds" "set")
-   (set_attr "shift" "1")]
+   (set_attr "shift" "1")
+   (set_attr "type" "shift_imm,shift_reg")]
 )
 
 (define_insn "*not_shiftsi"
@@ -3835,10 +4229,10 @@
   "TARGET_32BIT"
   "mvn%?\\t%0, %1%S3"
   [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
    (set_attr "shift" "1")
-   (set_attr "insn" "mvn")
    (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "type" "mvn_shift,mvn_shift_reg")])
 
 (define_insn "*not_shiftsi_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
@@ -3853,9 +4247,8 @@
   "mvn%.\\t%0, %1%S3"
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
-   (set_attr "insn" "mvn")
    (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "type" "mvn_shift,mvn_shift_reg")])
 
 (define_insn "*not_shiftsi_compare0_scratch"
   [(set (reg:CC_NOOV CC_REGNUM)
@@ -3869,9 +4262,8 @@
   "mvn%.\\t%0, %1%S3"
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
-   (set_attr "insn" "mvn")
    (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "type" "mvn_shift,mvn_shift_reg")])
 
 ;; We don't really have extzv, but defining this using shifts helps
 ;; to reduce register pressure later on.
@@ -4048,6 +4440,7 @@
   [(set_attr "arch" "t2,any")
    (set_attr "length" "2,4")
    (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "yes,no")
    (set_attr "type" "load1")])
 
 (define_insn "unaligned_loadhis"
@@ -4060,6 +4453,7 @@
   [(set_attr "arch" "t2,any")
    (set_attr "length" "2,4")
    (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "yes,no")
    (set_attr "type" "load_byte")])
 
 (define_insn "unaligned_loadhiu"
@@ -4072,6 +4466,7 @@
   [(set_attr "arch" "t2,any")
    (set_attr "length" "2,4")
    (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "yes,no")
    (set_attr "type" "load_byte")])
 
 (define_insn "unaligned_storesi"
@@ -4083,6 +4478,7 @@
   [(set_attr "arch" "t2,any")
    (set_attr "length" "2,4")
    (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "yes,no")
    (set_attr "type" "store1")])
 
 (define_insn "unaligned_storehi"
@@ -4094,8 +4490,67 @@
   [(set_attr "arch" "t2,any")
    (set_attr "length" "2,4")
    (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "yes,no")
    (set_attr "type" "store1")])
 
+;; Unaligned double-word load and store.
+;; Split after reload into two unaligned single-word accesses.
+;; It prevents lower_subreg from splitting some other aligned
+;; double-word accesses too early. Used for internal memcpy.
+
+(define_insn_and_split "unaligned_loaddi"
+  [(set (match_operand:DI 0 "s_register_operand" "=l,r")
+	(unspec:DI [(match_operand:DI 1 "memory_operand" "o,o")]
+		   UNSPEC_UNALIGNED_LOAD))]
+  "unaligned_access && TARGET_32BIT"
+  "#"
+  "&& reload_completed"
+  [(set (match_dup 0) (unspec:SI [(match_dup 1)] UNSPEC_UNALIGNED_LOAD))
+   (set (match_dup 2) (unspec:SI [(match_dup 3)] UNSPEC_UNALIGNED_LOAD))]
+  {
+    operands[2] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    operands[3] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+
+    /* If the first destination register overlaps with the base address,
+       swap the order in which the loads are emitted.  */
+    if (reg_overlap_mentioned_p (operands[0], operands[1]))
+      {
+        rtx tmp = operands[1];
+        operands[1] = operands[3];
+        operands[3] = tmp;
+        tmp = operands[0];
+        operands[0] = operands[2];
+        operands[2] = tmp;
+      }
+  }
+  [(set_attr "arch" "t2,any")
+   (set_attr "length" "4,8")
+   (set_attr "predicable" "yes")
+   (set_attr "type" "load2")])
+
+(define_insn_and_split "unaligned_storedi"
+  [(set (match_operand:DI 0 "memory_operand" "=o,o")
+	(unspec:DI [(match_operand:DI 1 "s_register_operand" "l,r")]
+		   UNSPEC_UNALIGNED_STORE))]
+  "unaligned_access && TARGET_32BIT"
+  "#"
+  "&& reload_completed"
+  [(set (match_dup 0) (unspec:SI [(match_dup 1)] UNSPEC_UNALIGNED_STORE))
+   (set (match_dup 2) (unspec:SI [(match_dup 3)] UNSPEC_UNALIGNED_STORE))]
+  {
+    operands[2] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    operands[3] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+  }
+  [(set_attr "arch" "t2,any")
+   (set_attr "length" "4,8")
+   (set_attr "predicable" "yes")
+   (set_attr "type" "store2")])
+
+
 (define_insn "*extv_reg"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(sign_extract:SI (match_operand:SI 1 "s_register_operand" "r")
@@ -4104,7 +4559,9 @@
   "arm_arch_thumb2"
   "sbfx%?\t%0, %1, %3, %2"
   [(set_attr "length" "4")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "bfm")]
 )
 
 (define_insn "extzv_t2"
@@ -4115,7 +4572,9 @@
   "arm_arch_thumb2"
   "ubfx%?\t%0, %1, %3, %2"
   [(set_attr "length" "4")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "bfm")]
 )
 
 
@@ -4127,7 +4586,8 @@
   "TARGET_IDIV"
   "sdiv%?\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "sdiv")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "sdiv")]
 )
 
 (define_insn "udivsi3"
@@ -4137,7 +4597,8 @@
   "TARGET_IDIV"
   "udiv%?\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "udiv")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "udiv")]
 )
 
 
@@ -4160,14 +4621,27 @@
 
 ;; The constraints here are to prevent a *partial* overlap (where %Q0 == %R1).
 ;; The first alternative allows the common case of a *full* overlap.
-(define_insn "*arm_negdi2"
+(define_insn_and_split "*arm_negdi2"
   [(set (match_operand:DI         0 "s_register_operand" "=r,&r")
 	(neg:DI (match_operand:DI 1 "s_register_operand"  "0,r")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "rsbs\\t%Q0, %Q1, #0\;rsc\\t%R0, %R1, #0"
+  "#"   ; "rsbs\\t%Q0, %Q1, #0\;rsc\\t%R0, %R1, #0"
+  "&& reload_completed"
+  [(parallel [(set (reg:CC CC_REGNUM)
+		   (compare:CC (const_int 0) (match_dup 1)))
+	      (set (match_dup 0) (minus:SI (const_int 0) (match_dup 1)))])
+   (set (match_dup 2) (minus:SI (minus:SI (const_int 0) (match_dup 3))
+                                (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))]
+  {
+    operands[2] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    operands[3] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+  }
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*thumb1_negdi2"
@@ -4176,7 +4650,8 @@
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_THUMB1"
   "mov\\t%R0, #0\;neg\\t%Q0, %Q1\;sbc\\t%R0, %R1"
-  [(set_attr "length" "6")]
+  [(set_attr "length" "6")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "negsi2"
@@ -4187,11 +4662,15 @@
 )
 
 (define_insn "*arm_negsi2"
-  [(set (match_operand:SI         0 "s_register_operand" "=r")
-	(neg:SI (match_operand:SI 1 "s_register_operand" "r")))]
+  [(set (match_operand:SI         0 "s_register_operand" "=l,r")
+	(neg:SI (match_operand:SI 1 "s_register_operand" "l,r")))]
   "TARGET_32BIT"
   "rsb%?\\t%0, %1, #0"
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "yes,no")
+   (set_attr "arch" "t2,*")
+   (set_attr "length" "4")
+   (set_attr "type" "alu_reg")]
 )
 
 (define_insn "*thumb1_negsi2"
@@ -4199,7 +4678,8 @@
 	(neg:SI (match_operand:SI 1 "register_operand" "l")))]
   "TARGET_THUMB1"
   "neg\\t%0, %1"
-  [(set_attr "length" "2")]
+  [(set_attr "length" "2")
+   (set_attr "type" "alu_imm")]
 )
 
 (define_expand "negsf2"
@@ -4233,18 +4713,72 @@
     operands[2] = gen_rtx_REG (CCmode, CC_REGNUM);
 ")
 
-(define_insn "*arm_abssi2"
+(define_insn_and_split "*arm_abssi2"
   [(set (match_operand:SI 0 "s_register_operand" "=r,&r")
 	(abs:SI (match_operand:SI 1 "s_register_operand" "0,r")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "@
-   cmp\\t%0, #0\;rsblt\\t%0, %0, #0
-   eor%?\\t%0, %1, %1, asr #31\;sub%?\\t%0, %0, %1, asr #31"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+   /* if (which_alternative == 0) */
+   if (REGNO(operands[0]) == REGNO(operands[1]))
+     {
+      /* Emit the pattern:
+         cmp\\t%0, #0\;rsblt\\t%0, %0, #0
+         [(set (reg:CC CC_REGNUM)
+               (compare:CC (match_dup 0) (const_int 0)))
+          (cond_exec (lt:CC (reg:CC CC_REGNUM) (const_int 0))
+                     (set (match_dup 0) (minus:SI (const_int 0) (match_dup 1))))]
+      */
+      emit_insn (gen_rtx_SET (VOIDmode,
+                              gen_rtx_REG (CCmode, CC_REGNUM),
+                              gen_rtx_COMPARE (CCmode, operands[0], const0_rtx)));
+      emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                    (gen_rtx_LT (SImode,
+                                                 gen_rtx_REG (CCmode, CC_REGNUM),
+                                                 const0_rtx)),
+                                    (gen_rtx_SET (VOIDmode,
+                                                  operands[0],
+                                                  (gen_rtx_MINUS (SImode,
+                                                                  const0_rtx,
+                                                                  operands[1]))))));
+      DONE;
+     }
+   else
+     {
+      /* Emit the pattern:
+         alt1: eor%?\\t%0, %1, %1, asr #31\;sub%?\\t%0, %0, %1, asr #31
+         [(set (match_dup 0)
+               (xor:SI (match_dup 1)
+                       (ashiftrt:SI (match_dup 1) (const_int 31))))
+          (set (match_dup 0)
+               (minus:SI (match_dup 0)
+                      (ashiftrt:SI (match_dup 1) (const_int 31))))]
+      */
+      emit_insn (gen_rtx_SET (VOIDmode,
+                              operands[0],
+                              gen_rtx_XOR (SImode,
+                                           gen_rtx_ASHIFTRT (SImode,
+                                                             operands[1],
+                                                             GEN_INT (31)),
+                                           operands[1])));
+      emit_insn (gen_rtx_SET (VOIDmode,
+                              operands[0],
+                              gen_rtx_MINUS (SImode,
+                                             operands[0],
+                                             gen_rtx_ASHIFTRT (SImode,
+                                                               operands[1],
+                                                               GEN_INT (31)))));
+      DONE;
+     }
+  }
   [(set_attr "conds" "clob,*")
    (set_attr "shift" "1")
    (set_attr "predicable" "no, yes")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "*thumb1_abssi2"
@@ -4258,21 +4792,65 @@
    (set (match_dup 0) (plus:SI (match_dup 1) (match_dup 2)))
    (set (match_dup 0) (xor:SI (match_dup 0) (match_dup 2)))]
   ""
-  [(set_attr "length" "6")]
+  [(set_attr "length" "6")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*arm_neg_abssi2"
+(define_insn_and_split "*arm_neg_abssi2"
   [(set (match_operand:SI 0 "s_register_operand" "=r,&r")
 	(neg:SI (abs:SI (match_operand:SI 1 "s_register_operand" "0,r"))))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "@
-   cmp\\t%0, #0\;rsbgt\\t%0, %0, #0
-   eor%?\\t%0, %1, %1, asr #31\;rsb%?\\t%0, %0, %1, asr #31"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+   /* if (which_alternative == 0) */
+   if (REGNO (operands[0]) == REGNO (operands[1]))
+     {
+      /* Emit the pattern:
+         cmp\\t%0, #0\;rsbgt\\t%0, %0, #0
+      */
+      emit_insn (gen_rtx_SET (VOIDmode,
+                              gen_rtx_REG (CCmode, CC_REGNUM),
+                              gen_rtx_COMPARE (CCmode, operands[0], const0_rtx)));
+      emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                    gen_rtx_GT (SImode,
+                                                gen_rtx_REG (CCmode, CC_REGNUM),
+                                                const0_rtx),
+                                    gen_rtx_SET (VOIDmode,
+                                                 operands[0],
+                                                 (gen_rtx_MINUS (SImode,
+                                                                 const0_rtx,
+                                                                 operands[1])))));
+     }
+   else
+     {
+      /* Emit the pattern:
+         eor%?\\t%0, %1, %1, asr #31\;rsb%?\\t%0, %0, %1, asr #31
+      */
+      emit_insn (gen_rtx_SET (VOIDmode,
+                              operands[0],
+                              gen_rtx_XOR (SImode,
+                                           gen_rtx_ASHIFTRT (SImode,
+                                                             operands[1],
+                                                             GEN_INT (31)),
+                                           operands[1])));
+      emit_insn (gen_rtx_SET (VOIDmode,
+                              operands[0],
+                              gen_rtx_MINUS (SImode,
+                                             gen_rtx_ASHIFTRT (SImode,
+                                                               operands[1],
+                                                               GEN_INT (31)),
+                                             operands[0])));
+     }
+   DONE;
+  }
   [(set_attr "conds" "clob,*")
    (set_attr "shift" "1")
    (set_attr "predicable" "no, yes")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "*thumb1_neg_abssi2"
@@ -4286,7 +4864,8 @@
    (set (match_dup 0) (minus:SI (match_dup 2) (match_dup 1)))
    (set (match_dup 0) (xor:SI (match_dup 0) (match_dup 2)))]
   ""
-  [(set_attr "length" "6")]
+  [(set_attr "length" "6")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "abssf2"
@@ -4335,8 +4914,8 @@
   }"
   [(set_attr "length" "*,8,8,*")
    (set_attr "predicable" "no,yes,yes,no")
-   (set_attr "neon_type" "neon_int_1,*,*,neon_int_1")
-   (set_attr "arch" "neon_nota8,*,*,neon_onlya8")]
+   (set_attr "type" "neon_move,multiple,multiple,neon_move")
+   (set_attr "arch" "neon_for_64bits,*,*,avoid_neon_for_64bits")]
 )
 
 (define_expand "one_cmplsi2"
@@ -4347,12 +4926,15 @@
 )
 
 (define_insn "*arm_one_cmplsi2"
-  [(set (match_operand:SI         0 "s_register_operand" "=r")
-	(not:SI (match_operand:SI 1 "s_register_operand"  "r")))]
+  [(set (match_operand:SI         0 "s_register_operand" "=l,r")
+	(not:SI (match_operand:SI 1 "s_register_operand"  "l,r")))]
   "TARGET_32BIT"
   "mvn%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "mvn")]
+   (set_attr "predicable_short_it" "yes,no")
+   (set_attr "arch" "t2,*")
+   (set_attr "length" "4")
+   (set_attr "type" "mvn_reg")]
 )
 
 (define_insn "*thumb1_one_cmplsi2"
@@ -4361,7 +4943,7 @@
   "TARGET_THUMB1"
   "mvn\\t%0, %1"
   [(set_attr "length" "2")
-   (set_attr "insn" "mvn")]
+   (set_attr "type" "mvn_reg")]
 )
 
 (define_insn "*notsi_compare0"
@@ -4373,7 +4955,7 @@
   "TARGET_32BIT"
   "mvn%.\\t%0, %1"
   [(set_attr "conds" "set")
-   (set_attr "insn" "mvn")]
+   (set_attr "type" "mvn_reg")]
 )
 
 (define_insn "*notsi_compare0_scratch"
@@ -4384,7 +4966,7 @@
   "TARGET_32BIT"
   "mvn%.\\t%0, %1"
   [(set_attr "conds" "set")
-   (set_attr "insn" "mvn")]
+   (set_attr "type" "mvn_reg")]
 )
 
 ;; Fixed <--> Floating conversion insns
@@ -4504,9 +5086,10 @@
   "TARGET_32BIT <qhs_zextenddi_cond>"
   "#"
   [(set_attr "length" "8,4,8,8")
-   (set_attr "arch" "neon_nota8,*,*,neon_onlya8")
+   (set_attr "arch" "neon_for_64bits,*,*,avoid_neon_for_64bits")
    (set_attr "ce_count" "2")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "type" "multiple,mov_reg,multiple,multiple")]
 )
 
 (define_insn "extend<mode>di2"
@@ -4519,7 +5102,8 @@
    (set_attr "ce_count" "2")
    (set_attr "shift" "1")
    (set_attr "predicable" "yes")
-   (set_attr "arch" "neon_nota8,*,a,t,neon_onlya8")]
+   (set_attr "arch" "neon_for_64bits,*,a,t,avoid_neon_for_64bits")
+   (set_attr "type" "multiple,mov_reg,multiple,multiple,multiple")]
 )
 
 ;; Splits for all extensions to DImode
@@ -4645,7 +5229,7 @@
 			 [(if_then_else (eq_attr "is_arch6" "yes")
 				       (const_int 2) (const_int 4))
 			 (const_int 4)])
-   (set_attr "type" "simple_alu_shift, load_byte")]
+   (set_attr "type" "extend,load_byte")]
 )
 
 (define_insn "*arm_zero_extendhisi2"
@@ -4655,7 +5239,7 @@
   "@
    #
    ldr%(h%)\\t%0, %1"
-  [(set_attr "type" "alu_shift,load_byte")
+  [(set_attr "type" "alu_shift_reg,load_byte")
    (set_attr "predicable" "yes")]
 )
 
@@ -4667,7 +5251,7 @@
    uxth%?\\t%0, %1
    ldr%(h%)\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "simple_alu_shift,load_byte")]
+   (set_attr "type" "extend,load_byte")]
 )
 
 (define_insn "*arm_zero_extendhisi2addsi"
@@ -4676,8 +5260,9 @@
 		 (match_operand:SI 2 "s_register_operand" "r")))]
   "TARGET_INT_SIMD"
   "uxtah%?\\t%0, %2, %1"
-  [(set_attr "type" "alu_shift")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "alu_shift_reg")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_expand "zero_extendqisi2"
@@ -4725,7 +5310,7 @@
    #
    ldrb\\t%0, %1"
   [(set_attr "length" "4,2")
-   (set_attr "type" "alu_shift,load_byte")
+   (set_attr "type" "alu_shift_reg,load_byte")
    (set_attr "pool_range" "*,32")]
 )
 
@@ -4737,7 +5322,7 @@
    uxtb\\t%0, %1
    ldrb\\t%0, %1"
   [(set_attr "length" "2")
-   (set_attr "type" "simple_alu_shift,load_byte")]
+   (set_attr "type" "extend,load_byte")]
 )
 
 (define_insn "*arm_zero_extendqisi2"
@@ -4748,7 +5333,7 @@
    #
    ldr%(b%)\\t%0, %1\\t%@ zero_extendqisi2"
   [(set_attr "length" "8,4")
-   (set_attr "type" "alu_shift,load_byte")
+   (set_attr "type" "alu_shift_reg,load_byte")
    (set_attr "predicable" "yes")]
 )
 
@@ -4759,7 +5344,7 @@
   "@
    uxtb%(%)\\t%0, %1
    ldr%(b%)\\t%0, %1\\t%@ zero_extendqisi2"
-  [(set_attr "type" "simple_alu_shift,load_byte")
+  [(set_attr "type" "extend,load_byte")
    (set_attr "predicable" "yes")]
 )
 
@@ -4770,8 +5355,8 @@
   "TARGET_INT_SIMD"
   "uxtab%?\\t%0, %2, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "xtab")
-   (set_attr "type" "alu_shift")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "alu_shift_reg")]
 )
 
 (define_split
@@ -4822,7 +5407,9 @@
   "TARGET_32BIT"
   "tst%?\\t%0, #255"
   [(set_attr "conds" "set")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "logic_imm")]
 )
 
 (define_expand "extendhisi2"
@@ -4933,7 +5520,7 @@
 			 [(if_then_else (eq_attr "is_arch6" "yes")
 					(const_int 2) (const_int 4))
 			  (const_int 4)])
-   (set_attr "type" "simple_alu_shift,load_byte")
+   (set_attr "type" "extend,load_byte")
    (set_attr "pool_range" "*,1018")]
 )
 
@@ -4992,7 +5579,7 @@
    #
    ldr%(sh%)\\t%0, %1"
   [(set_attr "length" "8,4")
-   (set_attr "type" "alu_shift,load_byte")
+   (set_attr "type" "alu_shift_reg,load_byte")
    (set_attr "predicable" "yes")]
 )
 
@@ -5004,8 +5591,9 @@
   "@
    sxth%?\\t%0, %1
    ldr%(sh%)\\t%0, %1"
-  [(set_attr "type" "simple_alu_shift,load_byte")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "extend,load_byte")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_insn "*arm_extendhisi2addsi"
@@ -5014,6 +5602,7 @@
 		 (match_operand:SI 2 "s_register_operand" "r")))]
   "TARGET_INT_SIMD"
   "sxtah%?\\t%0, %2, %1"
+  [(set_attr "type" "alu_shift_reg")]
 )
 
 (define_expand "extendqihi2"
@@ -5086,7 +5675,7 @@
    #
    ldr%(sb%)\\t%0, %1"
   [(set_attr "length" "8,4")
-   (set_attr "type" "alu_shift,load_byte")
+   (set_attr "type" "alu_shift_reg,load_byte")
    (set_attr "predicable" "yes")]
 )
 
@@ -5098,7 +5687,7 @@
   "@
    sxtb%?\\t%0, %1
    ldr%(sb%)\\t%0, %1"
-  [(set_attr "type" "simple_alu_shift,load_byte")
+  [(set_attr "type" "extend,load_byte")
    (set_attr "predicable" "yes")]
 )
 
@@ -5108,9 +5697,9 @@
 		 (match_operand:SI 2 "s_register_operand" "r")))]
   "TARGET_INT_SIMD"
   "sxtab%?\\t%0, %2, %1"
-  [(set_attr "type" "alu_shift")
-   (set_attr "insn" "xtab")
-   (set_attr "predicable" "yes")]
+  [(set_attr "type" "alu_shift_reg")
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")]
 )
 
 (define_split
@@ -5209,7 +5798,7 @@
 			  (const_int 2)
 			  (if_then_else (eq_attr "is_arch6" "yes")
 					(const_int 4) (const_int 6))])
-   (set_attr "type" "simple_alu_shift,load_byte,load_byte")]
+   (set_attr "type" "extend,load_byte,load_byte")]
 )
 
 (define_expand "extendsfdf2"
@@ -5309,8 +5898,8 @@
 )
 
 (define_insn "*arm_movdi"
-  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r, r, r, r, m")
-	(match_operand:DI 1 "di_operand"              "rDa,Db,Dc,mi,r"))]
+  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r, r, r, q, m")
+	(match_operand:DI 1 "di_operand"              "rDa,Db,Dc,mi,q"))]
   "TARGET_32BIT
    && !(TARGET_HARD_FLOAT && TARGET_VFP)
    && !TARGET_IWMMXT
@@ -5328,7 +5917,7 @@
     }
   "
   [(set_attr "length" "8,12,16,8,8")
-   (set_attr "type" "*,*,*,load2,store2")
+   (set_attr "type" "multiple,multiple,multiple,load2,store2")
    (set_attr "arm_pool_range" "*,*,*,1020,*")
    (set_attr "arm_neg_pool_range" "*,*,*,1004,*")
    (set_attr "thumb2_pool_range" "*,*,*,4094,*")
@@ -5468,8 +6057,7 @@
     }
   }"
   [(set_attr "length" "4,4,6,2,2,6,4,4")
-   (set_attr "type" "*,*,*,load2,store2,load2,store2,*")
-   (set_attr "insn" "*,mov,*,*,*,*,*,mov")
+   (set_attr "type" "multiple,multiple,multiple,load2,store2,load2,store2,multiple")
    (set_attr "pool_range" "*,*,*,*,*,1018,*,*")]
 )
 
@@ -5566,7 +6154,9 @@
   "arm_arch_thumb2"
   "movt%?\t%0, #:upper16:%c2"
   [(set_attr "predicable" "yes")
-   (set_attr "length" "4")]
+   (set_attr "predicable_short_it" "no")
+   (set_attr "length" "4")
+   (set_attr "type" "mov_imm")]
 )
 
 (define_insn "*arm_movsi_insn"
@@ -5583,8 +6173,7 @@
    movw%?\\t%0, %1
    ldr%?\\t%0, %1
    str%?\\t%1, %0"
-  [(set_attr "type" "*,simple_alu_imm,simple_alu_imm,simple_alu_imm,load1,store1")
-   (set_attr "insn" "mov,mov,mvn,mov,*,*")
+  [(set_attr "type" "mov_reg,mov_imm,mvn_imm,mov_imm,load1,store1")
    (set_attr "predicable" "yes")
    (set_attr "pool_range" "*,*,*,*,4096,*")
    (set_attr "neg_pool_range" "*,*,*,*,4084,*")]
@@ -5639,7 +6228,7 @@
    str\\t%1, %0
    mov\\t%0, %1"
   [(set_attr "length" "2,2,4,4,2,2,2,2,2")
-   (set_attr "type" "*,*,*,*,load1,store1,load1,store1,*")
+   (set_attr "type" "mov_reg,mov_imm,multiple,multiple,load1,store1,load1,store1,mov_reg")
    (set_attr "pool_range" "*,*,*,*,*,*,1018,*,*")
    (set_attr "conds" "set,clob,*,*,nocond,nocond,nocond,nocond,nocond")])
 
@@ -5795,7 +6384,8 @@
 				     INTVAL (operands[2]));
   return \"add\\t%0, %|pc\";
   "
-  [(set_attr "length" "2")]
+  [(set_attr "length" "2")
+   (set_attr "type" "alu_reg")]
 )
 
 (define_insn "pic_add_dot_plus_eight"
@@ -5810,7 +6400,8 @@
 				       INTVAL (operands[2]));
     return \"add%?\\t%0, %|pc, %1\";
   "
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "type" "alu_reg")]
 )
 
 (define_insn "tls_load_dot_plus_eight"
@@ -5825,7 +6416,8 @@
 				       INTVAL (operands[2]));
     return \"ldr%?\\t%0, [%|pc, %1]\t\t@ tls_load_dot_plus_eight\";
   "
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "type" "load1")]
 )
 
 ;; PIC references to local variables can generate pic_add_dot_plus_eight
@@ -5886,7 +6478,7 @@
    cmp%?\\t%0, #0
    sub%.\\t%0, %1, #0"
   [(set_attr "conds" "set")
-   (set_attr "type" "simple_alu_imm,simple_alu_imm")]
+   (set_attr "type" "alus_imm,alus_imm")]
 )
 
 ;; Subroutine to store a half word from a register into memory.
@@ -6232,7 +6824,7 @@
       return \"ldrh	%0, %1\";
     }"
   [(set_attr "length" "2,4,2,2,2,2")
-   (set_attr "type" "*,load1,store1,*,*,*")
+   (set_attr "type" "alus_imm,load1,store1,mov_reg,mov_reg,mov_imm")
    (set_attr "conds" "clob,nocond,nocond,nocond,nocond,clob")])
 
 
@@ -6301,16 +6893,15 @@
    str%(h%)\\t%1, %0\\t%@ movhi
    ldr%(h%)\\t%0, %1\\t%@ movhi"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "mov,mvn,mov,*,*")
    (set_attr "pool_range" "*,*,*,*,256")
    (set_attr "neg_pool_range" "*,*,*,*,244")
    (set_attr "arch" "*,*,v6t2,*,*")
    (set_attr_alternative "type"
                          [(if_then_else (match_operand 1 "const_int_operand" "")
-                                        (const_string "simple_alu_imm" )
-                                        (const_string "*"))
-                          (const_string "simple_alu_imm")
-                          (const_string "simple_alu_imm")
+                                        (const_string "mov_imm" )
+                                        (const_string "mov_reg"))
+                          (const_string "mvn_imm")
+                          (const_string "mov_imm")
                           (const_string "store1")
                           (const_string "load1")])]
 )
@@ -6324,8 +6915,7 @@
    mov%?\\t%0, %1\\t%@ movhi
    mvn%?\\t%0, #%B1\\t%@ movhi"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "mov, mov,mvn")
-   (set_attr "type" "simple_alu_imm,*,simple_alu_imm")]
+   (set_attr "type" "mov_imm,mov_reg,mvn_imm")]
 )
 
 (define_expand "thumb_movhi_clobber"
@@ -6448,26 +7038,27 @@
   "
 )
 
-
 (define_insn "*arm_movqi_insn"
-  [(set (match_operand:QI 0 "nonimmediate_operand" "=r,r,r,l,Uu,r,m")
-	(match_operand:QI 1 "general_operand" "r,I,K,Uu,l,m,r"))]
+  [(set (match_operand:QI 0 "nonimmediate_operand" "=r,r,r,l,r,l,Uu,r,m")
+	(match_operand:QI 1 "general_operand" "r,r,I,Py,K,Uu,l,m,r"))]
   "TARGET_32BIT
    && (   register_operand (operands[0], QImode)
        || register_operand (operands[1], QImode))"
   "@
    mov%?\\t%0, %1
    mov%?\\t%0, %1
+   mov%?\\t%0, %1
+   mov%?\\t%0, %1
    mvn%?\\t%0, #%B1
    ldr%(b%)\\t%0, %1
    str%(b%)\\t%1, %0
    ldr%(b%)\\t%0, %1
    str%(b%)\\t%1, %0"
-  [(set_attr "type" "*,simple_alu_imm,simple_alu_imm,load1, store1, load1, store1")
-   (set_attr "insn" "mov,mov,mvn,*,*,*,*")
+  [(set_attr "type" "mov_reg,mov_reg,mov_imm,mov_imm,mvn_imm,load1,store1,load1,store1")
    (set_attr "predicable" "yes")
-   (set_attr "arch" "any,any,any,t2,t2,any,any")
-   (set_attr "length" "4,4,4,2,2,4,4")]
+   (set_attr "predicable_short_it" "yes,yes,yes,no,no,no,no,no,no")
+   (set_attr "arch" "t2,any,any,t2,any,t2,t2,any,any")
+   (set_attr "length" "2,4,4,2,4,2,2,4,4")]
 )
 
 (define_insn "*thumb1_movqi_insn"
@@ -6484,8 +7075,7 @@
    mov\\t%0, %1
    mov\\t%0, %1"
   [(set_attr "length" "2")
-   (set_attr "type" "simple_alu_imm,load1,store1,*,*,simple_alu_imm")
-   (set_attr "insn" "*,*,*,mov,mov,mov")
+   (set_attr "type" "alu_imm,load1,store1,mov_reg,mov_imm,mov_imm")
    (set_attr "pool_range" "*,32,*,*,*,*")
    (set_attr "conds" "clob,nocond,nocond,nocond,nocond,clob")])
 
@@ -6514,7 +7104,7 @@
 (define_insn "*arm32_movhf"
   [(set (match_operand:HF 0 "nonimmediate_operand" "=r,m,r,r")
 	(match_operand:HF 1 "general_operand"	   " m,r,r,F"))]
-  "TARGET_32BIT && !(TARGET_HARD_FLOAT && TARGET_FP16)
+  "TARGET_32BIT && !(TARGET_HARD_FLOAT && TARGET_FP16) && !arm_restrict_it
    && (	  s_register_operand (operands[0], HFmode)
        || s_register_operand (operands[1], HFmode))"
   "*
@@ -6550,8 +7140,7 @@
     }
   "
   [(set_attr "conds" "unconditional")
-   (set_attr "type" "load1,store1,*,*")
-   (set_attr "insn" "*,*,mov,mov")
+   (set_attr "type" "load1,store1,mov_reg,multiple")
    (set_attr "length" "4,4,4,8")
    (set_attr "predicable" "yes")]
 )
@@ -6586,8 +7175,7 @@
     }
   "
   [(set_attr "length" "2")
-   (set_attr "type" "*,load1,store1,*,*")
-   (set_attr "insn" "mov,*,*,mov,mov")
+   (set_attr "type" "mov_reg,load1,store1,mov_reg,mov_reg")
    (set_attr "pool_range" "*,1018,*,*,*")
    (set_attr "conds" "clob,nocond,nocond,nocond,nocond")])
 
@@ -6641,8 +7229,8 @@
    ldr%?\\t%0, %1\\t%@ float
    str%?\\t%1, %0\\t%@ float"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "*,load1,store1")
-   (set_attr "insn" "mov,*,*")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "type" "mov_reg,load1,store1")
    (set_attr "arm_pool_range" "*,4096,*")
    (set_attr "thumb2_pool_range" "*,4094,*")
    (set_attr "arm_neg_pool_range" "*,4084,*")
@@ -6665,9 +7253,8 @@
    mov\\t%0, %1
    mov\\t%0, %1"
   [(set_attr "length" "2")
-   (set_attr "type" "*,load1,store1,load1,store1,*,*")
+   (set_attr "type" "alus_imm,load1,store1,load1,store1,mov_reg,mov_reg")
    (set_attr "pool_range" "*,*,*,1018,*,*,*")
-   (set_attr "insn" "*,*,*,*,*,mov,mov")
    (set_attr "conds" "clob,nocond,nocond,nocond,nocond,nocond,nocond")]
 )
 
@@ -6737,8 +7324,8 @@
 )
 
 (define_insn "*movdf_soft_insn"
-  [(set (match_operand:DF 0 "nonimmediate_soft_df_operand" "=r,r,r,r,m")
-	(match_operand:DF 1 "soft_df_operand" "rDa,Db,Dc,mF,r"))]
+  [(set (match_operand:DF 0 "nonimmediate_soft_df_operand" "=r,r,r,q,m")
+	(match_operand:DF 1 "soft_df_operand" "rDa,Db,Dc,mF,q"))]
   "TARGET_32BIT && TARGET_SOFT_FLOAT
    && (   register_operand (operands[0], DFmode)
        || register_operand (operands[1], DFmode))"
@@ -6754,7 +7341,7 @@
     }
   "
   [(set_attr "length" "8,12,16,8,8")
-   (set_attr "type" "*,*,*,load2,store2")
+   (set_attr "type" "multiple,multiple,multiple,load2,store2")
    (set_attr "arm_pool_range" "*,*,*,1020,*")
    (set_attr "thumb2_pool_range" "*,*,*,1018,*")
    (set_attr "arm_neg_pool_range" "*,*,*,1004,*")
@@ -6798,8 +7385,7 @@
     }
   "
   [(set_attr "length" "4,2,2,6,4,4")
-   (set_attr "type" "*,load2,store2,load2,store2,*")
-   (set_attr "insn" "*,*,*,*,*,mov")
+   (set_attr "type" "multiple,load2,store2,load2,store2,multiple")
    (set_attr "pool_range" "*,*,*,1018,*,*")]
 )
 
@@ -6868,10 +7454,18 @@
    (match_operand:BLK 1 "general_operand" "")
    (match_operand:SI 2 "const_int_operand" "")
    (match_operand:SI 3 "const_int_operand" "")]
-  "TARGET_EITHER"
+  ""
   "
   if (TARGET_32BIT)
     {
+      if (TARGET_LDRD && current_tune->prefer_ldrd_strd
+          && !optimize_function_for_size_p (cfun))
+        {
+          if (gen_movmem_ldrd_strd (operands))
+            DONE;
+          FAIL;
+        }
+
       if (arm_gen_movmemqi (operands))
         DONE;
       FAIL;
@@ -7099,7 +7693,8 @@
 	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
 		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
 		(const_int 6)
-		(const_int 8))))]
+		(const_int 8))))
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "cbranchsi4_scratch"
@@ -7135,7 +7730,8 @@
 	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
 		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
 		(const_int 6)
-		(const_int 8))))]
+		(const_int 8))))
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*negated_cbranchsi4"
@@ -7170,7 +7766,8 @@
 	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
 		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
 		(const_int 6)
-		(const_int 8))))]
+		(const_int 8))))
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*tbit_cbranch"
@@ -7214,7 +7811,8 @@
 	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
 		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
 		(const_int 6)
-		(const_int 8))))]
+		(const_int 8))))
+   (set_attr "type" "multiple")]
 )
   
 (define_insn "*tlobits_cbranch"
@@ -7258,7 +7856,8 @@
 	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
 		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
 		(const_int 6)
-		(const_int 8))))]
+		(const_int 8))))
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*tstsi3_cbranch"
@@ -7295,7 +7894,8 @@
 	        (and (ge (minus (match_dup 2) (pc)) (const_int -2040))
 		     (le (minus (match_dup 2) (pc)) (const_int 2048)))
 		(const_int 6)
-		(const_int 8))))]
+		(const_int 8))))
+   (set_attr "type" "multiple")]
 )
   
 (define_insn "*cbranchne_decr1"
@@ -7398,7 +7998,8 @@
 	   (and (ge (minus (match_dup 4) (pc)) (const_int -2038))
 		(le (minus (match_dup 4) (pc)) (const_int 2048)))
 	   (const_int 8)
-	   (const_int 10)))])]
+	   (const_int 10)))])
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*addsi3_cbranch"
@@ -7479,7 +8080,8 @@
 	   (and (ge (minus (match_dup 5) (pc)) (const_int -2038))
 		(le (minus (match_dup 5) (pc)) (const_int 2048)))
 	   (const_int 8)
-	   (const_int 10)))))]
+	   (const_int 10)))))
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*addsi3_cbranch_scratch"
@@ -7547,7 +8149,8 @@
 	   (and (ge (minus (match_dup 4) (pc)) (const_int -2040))
 		(le (minus (match_dup 4) (pc)) (const_int 2048)))
 	   (const_int 6)
-	   (const_int 8))))]
+	   (const_int 8))))
+   (set_attr "type" "multiple")]
 )
 
 
@@ -7555,46 +8158,48 @@
 
 (define_insn "*arm_cmpsi_insn"
   [(set (reg:CC CC_REGNUM)
-	(compare:CC (match_operand:SI 0 "s_register_operand" "l,r,r,r")
-		    (match_operand:SI 1 "arm_add_operand"    "Py,r,rI,L")))]
+	(compare:CC (match_operand:SI 0 "s_register_operand" "l,r,r,r,r")
+		    (match_operand:SI 1 "arm_add_operand"    "Py,r,r,I,L")))]
   "TARGET_32BIT"
   "@
    cmp%?\\t%0, %1
    cmp%?\\t%0, %1
    cmp%?\\t%0, %1
+   cmp%?\\t%0, %1
    cmn%?\\t%0, #%n1"
   [(set_attr "conds" "set")
-   (set_attr "arch" "t2,t2,any,any")
-   (set_attr "length" "2,2,4,4")
+   (set_attr "arch" "t2,t2,any,any,any")
+   (set_attr "length" "2,2,4,4,4")
    (set_attr "predicable" "yes")
-   (set_attr "type" "*,*,*,simple_alu_imm")]
+   (set_attr "predicable_short_it" "yes,yes,yes,no,no")
+   (set_attr "type" "alus_imm,alus_reg,alus_reg,alus_imm,alus_imm")]
 )
 
 (define_insn "*cmpsi_shiftsi"
   [(set (reg:CC CC_REGNUM)
-	(compare:CC (match_operand:SI   0 "s_register_operand" "r,r")
+	(compare:CC (match_operand:SI   0 "s_register_operand" "r,r,r")
 		    (match_operator:SI  3 "shift_operator"
-		     [(match_operand:SI 1 "s_register_operand" "r,r")
-		      (match_operand:SI 2 "shift_amount_operand" "M,rM")])))]
+		     [(match_operand:SI 1 "s_register_operand" "r,r,r")
+		      (match_operand:SI 2 "shift_amount_operand" "M,r,M")])))]
   "TARGET_32BIT"
-  "cmp%?\\t%0, %1%S3"
+  "cmp\\t%0, %1%S3"
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
-   (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "arch" "32,a,a")
+   (set_attr "type" "alus_shift_imm,alu_shift_reg,alus_shift_imm")])
 
 (define_insn "*cmpsi_shiftsi_swp"
   [(set (reg:CC_SWP CC_REGNUM)
 	(compare:CC_SWP (match_operator:SI 3 "shift_operator"
-			 [(match_operand:SI 1 "s_register_operand" "r,r")
-			  (match_operand:SI 2 "shift_amount_operand" "M,rM")])
-			(match_operand:SI 0 "s_register_operand" "r,r")))]
+			 [(match_operand:SI 1 "s_register_operand" "r,r,r")
+			  (match_operand:SI 2 "shift_amount_operand" "M,r,M")])
+			(match_operand:SI 0 "s_register_operand" "r,r,r")))]
   "TARGET_32BIT"
   "cmp%?\\t%0, %1%S3"
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
-   (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "arch" "32,a,a")
+   (set_attr "type" "alus_shift_imm,alu_shift_reg,alus_shift_imm")])
 
 (define_insn "*arm_cmpsi_negshiftsi_si"
   [(set (reg:CC_Z CC_REGNUM)
@@ -7607,8 +8212,8 @@
   "cmn%?\\t%0, %2%S1"
   [(set_attr "conds" "set")
    (set (attr "type") (if_then_else (match_operand 3 "const_int_operand" "")
-				    (const_string "alu_shift")
-				    (const_string "alu_shift_reg")))
+				    (const_string "alus_shift_imm")
+				    (const_string "alus_shift_reg")))
    (set_attr "predicable" "yes")]
 )
 
@@ -7616,26 +8221,71 @@
 ;; if-conversion can not reduce to a conditional compare, so we do
 ;; that directly.
 
-(define_insn "*arm_cmpdi_insn"
+(define_insn_and_split "*arm_cmpdi_insn"
   [(set (reg:CC_NCV CC_REGNUM)
 	(compare:CC_NCV (match_operand:DI 0 "s_register_operand" "r")
 			(match_operand:DI 1 "arm_di_operand"	   "rDi")))
    (clobber (match_scratch:SI 2 "=r"))]
   "TARGET_32BIT"
-  "cmp\\t%Q0, %Q1\;sbcs\\t%2, %R0, %R1"
+  "#"   ; "cmp\\t%Q0, %Q1\;sbcs\\t%2, %R0, %R1"
+  "&& reload_completed"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 0) (match_dup 1)))
+   (parallel [(set (reg:CC CC_REGNUM)
+                   (compare:CC (match_dup 3) (match_dup 4)))
+              (set (match_dup 2)
+                   (minus:SI (match_dup 5)
+                            (ltu:SI (reg:CC_C CC_REGNUM) (const_int 0))))])]
+  {
+    operands[3] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    if (CONST_INT_P (operands[1]))
+      {
+        operands[4] = GEN_INT (~INTVAL (gen_highpart_mode (SImode,
+                                                           DImode,
+                                                           operands[1])));
+        operands[5] = gen_rtx_PLUS (SImode, operands[3], operands[4]);
+      }
+    else
+      {
+        operands[4] = gen_highpart (SImode, operands[1]);
+        operands[5] = gen_rtx_MINUS (SImode, operands[3], operands[4]);
+      }
+    operands[1] = gen_lowpart (SImode, operands[1]);
+    operands[2] = gen_lowpart (SImode, operands[2]);
+  }
   [(set_attr "conds" "set")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*arm_cmpdi_unsigned"
+(define_insn_and_split "*arm_cmpdi_unsigned"
   [(set (reg:CC_CZ CC_REGNUM)
-	(compare:CC_CZ (match_operand:DI 0 "s_register_operand" "r,r")
-		       (match_operand:DI 1 "arm_di_operand"	"rDi,rDi")))]
+        (compare:CC_CZ (match_operand:DI 0 "s_register_operand" "l,r,r,r")
+                       (match_operand:DI 1 "arm_di_operand"     "Py,r,Di,rDi")))]
+
   "TARGET_32BIT"
-  "cmp\\t%R0, %R1\;it eq\;cmpeq\\t%Q0, %Q1"
+  "#"   ; "cmp\\t%R0, %R1\;it eq\;cmpeq\\t%Q0, %Q1"
+  "&& reload_completed"
+  [(set (reg:CC CC_REGNUM)
+        (compare:CC (match_dup 2) (match_dup 3)))
+   (cond_exec (eq:SI (reg:CC CC_REGNUM) (const_int 0))
+              (set (reg:CC CC_REGNUM)
+                   (compare:CC (match_dup 0) (match_dup 1))))]
+  {
+    operands[2] = gen_highpart (SImode, operands[0]);
+    operands[0] = gen_lowpart (SImode, operands[0]);
+    if (CONST_INT_P (operands[1]))
+      operands[3] = gen_highpart_mode (SImode, DImode, operands[1]);
+    else
+      operands[3] = gen_highpart (SImode, operands[1]);
+    operands[1] = gen_lowpart (SImode, operands[1]);
+  }
   [(set_attr "conds" "set")
-   (set_attr "arch" "a,t2")
-   (set_attr "length" "8,10")]
+   (set_attr "enabled_for_depr_it" "yes,yes,no,*")
+   (set_attr "arch" "t2,t2,t2,a")
+   (set_attr "length" "6,6,10,8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*arm_cmpdi_zero"
@@ -7645,7 +8295,8 @@
    (clobber (match_scratch:SI 1 "=r"))]
   "TARGET_32BIT"
   "orr%.\\t%1, %Q0, %R0"
-  [(set_attr "conds" "set")]
+  [(set_attr "conds" "set")
+   (set_attr "type" "logics_reg")]
 )
 
 (define_insn "*thumb_cmpdi_zero"
@@ -7656,7 +8307,8 @@
   "TARGET_THUMB1"
   "orr\\t%1, %Q0, %R0"
   [(set_attr "conds" "set")
-   (set_attr "length" "2")]
+   (set_attr "length" "2")
+   (set_attr "type" "logics_reg")]
 )
 
 ; This insn allows redundant compares to be removed by cse, nothing should
@@ -7670,7 +8322,8 @@
   "TARGET_32BIT"
   "\\t%@ deleted compare"
   [(set_attr "conds" "set")
-   (set_attr "length" "0")]
+   (set_attr "length" "0")
+   (set_attr "type" "no_insn")]
 )
 
 
@@ -7758,37 +8411,60 @@
    operands[3] = const0_rtx;"
 )
 
-(define_insn "*mov_scc"
+(define_insn_and_split "*mov_scc"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(match_operator:SI 1 "arm_comparison_operator"
 	 [(match_operand 2 "cc_register" "") (const_int 0)]))]
   "TARGET_ARM"
-  "mov%D1\\t%0, #0\;mov%d1\\t%0, #1"
+  "#"   ; "mov%D1\\t%0, #0\;mov%d1\\t%0, #1"
+  "TARGET_ARM"
+  [(set (match_dup 0)
+        (if_then_else:SI (match_dup 1)
+                         (const_int 1)
+                         (const_int 0)))]
+  ""
   [(set_attr "conds" "use")
-   (set_attr "insn" "mov")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*mov_negscc"
+(define_insn_and_split "*mov_negscc"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(neg:SI (match_operator:SI 1 "arm_comparison_operator"
 		 [(match_operand 2 "cc_register" "") (const_int 0)])))]
   "TARGET_ARM"
-  "mov%D1\\t%0, #0\;mvn%d1\\t%0, #0"
+  "#"   ; "mov%D1\\t%0, #0\;mvn%d1\\t%0, #0"
+  "TARGET_ARM"
+  [(set (match_dup 0)
+        (if_then_else:SI (match_dup 1)
+                         (match_dup 3)
+                         (const_int 0)))]
+  {
+    operands[3] = GEN_INT (~0);
+  }
   [(set_attr "conds" "use")
-   (set_attr "insn" "mov")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
-(define_insn "*mov_notscc"
+(define_insn_and_split "*mov_notscc"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(not:SI (match_operator:SI 1 "arm_comparison_operator"
 		 [(match_operand 2 "cc_register" "") (const_int 0)])))]
   "TARGET_ARM"
-  "mvn%D1\\t%0, #0\;mvn%d1\\t%0, #1"
+  "#"   ; "mvn%D1\\t%0, #0\;mvn%d1\\t%0, #1"
+  "TARGET_ARM"
+  [(set (match_dup 0)
+        (if_then_else:SI (match_dup 1)
+                         (match_dup 3)
+                         (match_dup 4)))]
+  {
+    operands[3] = GEN_INT (~1);
+    operands[4] = GEN_INT (~0);
+  }
   [(set_attr "conds" "use")
-   (set_attr "insn" "mov")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "cstoresi4"
@@ -7993,7 +8669,8 @@
   "@
    neg\\t%0, %1\;adc\\t%0, %0, %1
    neg\\t%2, %1\;adc\\t%0, %1, %2"
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*cstoresi_ne0_thumb1_insn"
@@ -8013,7 +8690,8 @@
 			(match_operand:SI 2 "thumb1_cmp_operand" "lI*h,*r"))))]
   "TARGET_THUMB1"
   "cmp\\t%1, %2\;sbc\\t%0, %0, %0"
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "cstoresi_ltu_thumb1"
@@ -8027,7 +8705,8 @@
 	(neg:SI (ltu:SI (match_dup 1) (match_dup 2))))
    (set (match_dup 0) (neg:SI (match_dup 3)))]
   "operands[3] = gen_reg_rtx (SImode);"
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
 )
 
 ;; Used as part of the expansion of thumb les sequence.
@@ -8039,7 +8718,8 @@
 			 (match_operand:SI 4 "thumb1_cmp_operand" "lI"))))]
   "TARGET_THUMB1"
   "cmp\\t%3, %4\;adc\\t%0, %1, %2"
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
 )
 
 
@@ -8069,7 +8749,7 @@
 
 (define_expand "movsfcc"
   [(set (match_operand:SF 0 "s_register_operand" "")
-	(if_then_else:SF (match_operand 1 "expandable_comparison_operator" "")
+	(if_then_else:SF (match_operand 1 "arm_cond_move_operator" "")
 			 (match_operand:SF 2 "s_register_operand" "")
 			 (match_operand:SF 3 "s_register_operand" "")))]
   "TARGET_32BIT && TARGET_HARD_FLOAT"
@@ -8091,7 +8771,7 @@
 
 (define_expand "movdfcc"
   [(set (match_operand:DF 0 "s_register_operand" "")
-	(if_then_else:DF (match_operand 1 "expandable_comparison_operator" "")
+	(if_then_else:DF (match_operand 1 "arm_cond_move_operator" "")
 			 (match_operand:DF 2 "s_register_operand" "")
 			 (match_operand:DF 3 "s_register_operand" "")))]
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
@@ -8110,7 +8790,40 @@
   }"
 )
 
-(define_insn "*movsicc_insn"
+(define_insn "*cmov<mode>"
+    [(set (match_operand:SDF 0 "s_register_operand" "=<F_constraint>")
+	(if_then_else:SDF (match_operator 1 "arm_vsel_comparison_operator"
+			  [(match_operand 2 "cc_register" "") (const_int 0)])
+			  (match_operand:SDF 3 "s_register_operand"
+			                      "<F_constraint>")
+			  (match_operand:SDF 4 "s_register_operand"
+			                      "<F_constraint>")))]
+  "TARGET_HARD_FLOAT && TARGET_FPU_ARMV8 <vfp_double_cond>"
+  "*
+  {
+    enum arm_cond_code code = maybe_get_arm_condition_code (operands[1]);
+    switch (code)
+      {
+      case ARM_GE:
+      case ARM_GT:
+      case ARM_EQ:
+      case ARM_VS:
+        return \"vsel%d1.<V_if_elem>\\t%<V_reg>0, %<V_reg>3, %<V_reg>4\";
+      case ARM_LT:
+      case ARM_LE:
+      case ARM_NE:
+      case ARM_VC:
+        return \"vsel%D1.<V_if_elem>\\t%<V_reg>0, %<V_reg>4, %<V_reg>3\";
+      default:
+        gcc_unreachable ();
+      }
+    return \"\";
+  }"
+  [(set_attr "conds" "use")
+   (set_attr "type" "f_sel<vfp_type>")]
+)
+
+(define_insn_and_split "*movsicc_insn"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r,r,r,r,r,r,r")
 	(if_then_else:SI
 	 (match_operator 3 "arm_comparison_operator"
@@ -8123,26 +8836,60 @@
    mvn%D3\\t%0, #%B2
    mov%d3\\t%0, %1
    mvn%d3\\t%0, #%B1
-   mov%d3\\t%0, %1\;mov%D3\\t%0, %2
-   mov%d3\\t%0, %1\;mvn%D3\\t%0, #%B2
-   mvn%d3\\t%0, #%B1\;mov%D3\\t%0, %2
-   mvn%d3\\t%0, #%B1\;mvn%D3\\t%0, #%B2"
+   #
+   #
+   #
+   #"
+   ; alt4: mov%d3\\t%0, %1\;mov%D3\\t%0, %2
+   ; alt5: mov%d3\\t%0, %1\;mvn%D3\\t%0, #%B2
+   ; alt6: mvn%d3\\t%0, #%B1\;mov%D3\\t%0, %2
+   ; alt7: mvn%d3\\t%0, #%B1\;mvn%D3\\t%0, #%B2"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    enum rtx_code rev_code;
+    enum machine_mode mode;
+    rtx rev_cond;
+
+    emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                  operands[3],
+                                  gen_rtx_SET (VOIDmode,
+                                               operands[0],
+                                               operands[1])));
+
+    rev_code = GET_CODE (operands[3]);
+    mode = GET_MODE (operands[4]);
+    if (mode == CCFPmode || mode == CCFPEmode)
+      rev_code = reverse_condition_maybe_unordered (rev_code);
+    else
+      rev_code = reverse_condition (rev_code);
+
+    rev_cond = gen_rtx_fmt_ee (rev_code,
+                               VOIDmode,
+                               operands[4],
+                               const0_rtx);
+    emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                  rev_cond,
+                                  gen_rtx_SET (VOIDmode,
+                                               operands[0],
+                                               operands[2])));
+    DONE;
+  }
   [(set_attr "length" "4,4,4,4,8,8,8,8")
    (set_attr "conds" "use")
-   (set_attr "insn" "mov,mvn,mov,mvn,mov,mov,mvn,mvn")
    (set_attr_alternative "type"
                          [(if_then_else (match_operand 2 "const_int_operand" "")
-                                        (const_string "simple_alu_imm")
-                                        (const_string "*"))
-                          (const_string "simple_alu_imm")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (const_string "mvn_imm")
                           (if_then_else (match_operand 1 "const_int_operand" "")
-                                        (const_string "simple_alu_imm")
-                                        (const_string "*"))
-                          (const_string "simple_alu_imm")
-                          (const_string "*")
-                          (const_string "*")
-                          (const_string "*")
-                          (const_string "*")])]
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (const_string "mvn_imm")
+                          (const_string "mov_reg")
+                          (const_string "mov_reg")
+                          (const_string "mov_reg")
+                          (const_string "mov_reg")])]
 )
 
 (define_insn "*movsfcc_soft_insn"
@@ -8156,7 +8903,7 @@
    mov%D3\\t%0, %2
    mov%d3\\t%0, %1"
   [(set_attr "conds" "use")
-   (set_attr "insn" "mov")]
+   (set_attr "type" "mov_reg")]
 )
 
 
@@ -8190,7 +8937,8 @@
 		(and (ge (minus (match_dup 0) (pc)) (const_int -2044))
 		     (le (minus (match_dup 0) (pc)) (const_int 2048))))
 	   (const_int 2)
-	   (const_int 4)))]
+	   (const_int 4)))
+   (set_attr "type" "branch")]
 )
 
 (define_insn "*thumb_jump"
@@ -8212,7 +8960,8 @@
 	    (and (ge (minus (match_dup 0) (pc)) (const_int -2044))
 		 (le (minus (match_dup 0) (pc)) (const_int 2048)))
   	    (const_int 2)
-	    (const_int 4)))]
+	    (const_int 4)))
+   (set_attr "type" "branch")]
 )
 
 (define_expand "call"
@@ -8255,7 +9004,7 @@
          (match_operand 1 "" ""))
    (use (match_operand 2 "" ""))
    (clobber (reg:SI LR_REGNUM))]
-  "TARGET_ARM && arm_arch5"
+  "TARGET_ARM && arm_arch5 && !SIBLING_CALL_P (insn)"
   "blx%?\\t%0"
   [(set_attr "type" "call")]
 )
@@ -8265,7 +9014,7 @@
          (match_operand 1 "" ""))
    (use (match_operand 2 "" ""))
    (clobber (reg:SI LR_REGNUM))]
-  "TARGET_ARM && !arm_arch5"
+  "TARGET_ARM && !arm_arch5 && !SIBLING_CALL_P (insn)"
   "*
   return output_call (operands);
   "
@@ -8284,7 +9033,7 @@
 	 (match_operand 1 "" ""))
    (use (match_operand 2 "" ""))
    (clobber (reg:SI LR_REGNUM))]
-  "TARGET_ARM && !arm_arch5"
+  "TARGET_ARM && !arm_arch5 && !SIBLING_CALL_P (insn)"
   "*
   return output_call_mem (operands);
   "
@@ -8297,7 +9046,7 @@
 	 (match_operand 1 "" ""))
    (use (match_operand 2 "" ""))
    (clobber (reg:SI LR_REGNUM))]
-  "TARGET_THUMB1 && arm_arch5"
+  "TARGET_THUMB1 && arm_arch5 && !SIBLING_CALL_P (insn)"
   "blx\\t%0"
   [(set_attr "length" "2")
    (set_attr "type" "call")]
@@ -8308,7 +9057,7 @@
 	 (match_operand 1 "" ""))
    (use (match_operand 2 "" ""))
    (clobber (reg:SI LR_REGNUM))]
-  "TARGET_THUMB1 && !arm_arch5"
+  "TARGET_THUMB1 && !arm_arch5 && !SIBLING_CALL_P (insn)"
   "*
   {
     if (!TARGET_CALLER_INTERWORKING)
@@ -8367,7 +9116,7 @@
 	      (match_operand 2 "" "")))
    (use (match_operand 3 "" ""))
    (clobber (reg:SI LR_REGNUM))]
-  "TARGET_ARM && arm_arch5"
+  "TARGET_ARM && arm_arch5 && !SIBLING_CALL_P (insn)"
   "blx%?\\t%1"
   [(set_attr "type" "call")]
 )
@@ -8378,7 +9127,7 @@
 	      (match_operand 2 "" "")))
    (use (match_operand 3 "" ""))
    (clobber (reg:SI LR_REGNUM))]
-  "TARGET_ARM && !arm_arch5"
+  "TARGET_ARM && !arm_arch5 && !SIBLING_CALL_P (insn)"
   "*
   return output_call (&operands[1]);
   "
@@ -8394,7 +9143,8 @@
 	      (match_operand 2 "" "")))
    (use (match_operand 3 "" ""))
    (clobber (reg:SI LR_REGNUM))]
-  "TARGET_ARM && !arm_arch5 && (!CONSTANT_ADDRESS_P (XEXP (operands[1], 0)))"
+  "TARGET_ARM && !arm_arch5 && (!CONSTANT_ADDRESS_P (XEXP (operands[1], 0)))
+   && !SIBLING_CALL_P (insn)"
   "*
   return output_call_mem (&operands[1]);
   "
@@ -8444,6 +9194,7 @@
    (use (match_operand 2 "" ""))
    (clobber (reg:SI LR_REGNUM))]
   "TARGET_32BIT
+   && !SIBLING_CALL_P (insn)
    && (GET_CODE (operands[0]) == SYMBOL_REF)
    && !arm_is_long_call_p (SYMBOL_REF_DECL (operands[0]))"
   "*
@@ -8460,6 +9211,7 @@
    (use (match_operand 3 "" ""))
    (clobber (reg:SI LR_REGNUM))]
   "TARGET_32BIT
+   && !SIBLING_CALL_P (insn)
    && (GET_CODE (operands[1]) == SYMBOL_REF)
    && !arm_is_long_call_p (SYMBOL_REF_DECL (operands[1]))"
   "*
@@ -8505,6 +9257,10 @@
   "TARGET_32BIT"
   "
   {
+    if (!REG_P (XEXP (operands[0], 0))
+       && (GET_CODE (XEXP (operands[0], 0)) != SYMBOL_REF))
+     XEXP (operands[0], 0) = force_reg (SImode, XEXP (operands[0], 0));
+
     if (operands[2] == NULL_RTX)
       operands[2] = const0_rtx;
   }"
@@ -8519,47 +9275,67 @@
   "TARGET_32BIT"
   "
   {
+    if (!REG_P (XEXP (operands[1], 0)) &&
+       (GET_CODE (XEXP (operands[1],0)) != SYMBOL_REF))
+     XEXP (operands[1], 0) = force_reg (SImode, XEXP (operands[1], 0));
+
     if (operands[3] == NULL_RTX)
       operands[3] = const0_rtx;
   }"
 )
 
 (define_insn "*sibcall_insn"
- [(call (mem:SI (match_operand:SI 0 "" "X"))
+ [(call (mem:SI (match_operand:SI 0 "call_insn_operand" "Cs, US"))
 	(match_operand 1 "" ""))
   (return)
   (use (match_operand 2 "" ""))]
-  "TARGET_32BIT && GET_CODE (operands[0]) == SYMBOL_REF"
+  "TARGET_32BIT && SIBLING_CALL_P (insn)"
   "*
-  return NEED_PLT_RELOC ? \"b%?\\t%a0(PLT)\" : \"b%?\\t%a0\";
+  if (which_alternative == 1)
+    return NEED_PLT_RELOC ? \"b%?\\t%a0(PLT)\" : \"b%?\\t%a0\";
+  else
+    {
+      if (arm_arch5 || arm_arch4t)
+	return \"bx%?\\t%0\\t%@ indirect register sibling call\";
+      else
+	return \"mov%?\\t%|pc, %0\\t%@ indirect register sibling call\";
+    }
   "
   [(set_attr "type" "call")]
 )
 
 (define_insn "*sibcall_value_insn"
  [(set (match_operand 0 "" "")
-       (call (mem:SI (match_operand:SI 1 "" "X"))
+       (call (mem:SI (match_operand:SI 1 "call_insn_operand" "Cs,US"))
 	     (match_operand 2 "" "")))
   (return)
   (use (match_operand 3 "" ""))]
-  "TARGET_32BIT && GET_CODE (operands[1]) == SYMBOL_REF"
+  "TARGET_32BIT && SIBLING_CALL_P (insn)"
   "*
-  return NEED_PLT_RELOC ? \"b%?\\t%a1(PLT)\" : \"b%?\\t%a1\";
+  if (which_alternative == 1)
+   return NEED_PLT_RELOC ? \"b%?\\t%a1(PLT)\" : \"b%?\\t%a1\";
+  else
+    {
+      if (arm_arch5 || arm_arch4t)
+	return \"bx%?\\t%1\";
+      else
+	return \"mov%?\\t%|pc, %1\\t@ indirect sibling call \";
+    }
   "
   [(set_attr "type" "call")]
 )
 
-(define_expand "return"
-  [(return)]
+(define_expand "<return_str>return"
+  [(returns)]
   "(TARGET_ARM || (TARGET_THUMB2
                    && ARM_FUNC_TYPE (arm_current_func_type ()) == ARM_FT_NORMAL
                    && !IS_STACKALIGN (arm_current_func_type ())))
-    && USE_RETURN_INSN (FALSE)"
+    <return_cond_false>"
   "
   {
     if (TARGET_THUMB2)
       {
-        thumb2_expand_return ();
+        thumb2_expand_return (<return_simple_p>);
         DONE;
       }
   }
@@ -8584,13 +9360,13 @@
    (set_attr "predicable" "yes")]
 )
 
-(define_insn "*cond_return"
+(define_insn "*cond_<return_str>return"
   [(set (pc)
         (if_then_else (match_operator 0 "arm_comparison_operator"
 		       [(match_operand 1 "cc_register" "") (const_int 0)])
-                      (return)
+                      (returns)
                       (pc)))]
-  "TARGET_ARM && USE_RETURN_INSN (TRUE)"
+  "TARGET_ARM  <return_cond_true>"
   "*
   {
     if (arm_ccfsm_state == 2)
@@ -8598,20 +9374,21 @@
         arm_ccfsm_state += 2;
         return \"\";
       }
-    return output_return_instruction (operands[0], true, false, false);
+    return output_return_instruction (operands[0], true, false,
+				      <return_simple_p>);
   }"
   [(set_attr "conds" "use")
    (set_attr "length" "12")
    (set_attr "type" "load1")]
 )
 
-(define_insn "*cond_return_inverted"
+(define_insn "*cond_<return_str>return_inverted"
   [(set (pc)
         (if_then_else (match_operator 0 "arm_comparison_operator"
 		       [(match_operand 1 "cc_register" "") (const_int 0)])
                       (pc)
-		      (return)))]
-  "TARGET_ARM && USE_RETURN_INSN (TRUE)"
+		      (returns)))]
+  "TARGET_ARM <return_cond_true>"
   "*
   {
     if (arm_ccfsm_state == 2)
@@ -8619,7 +9396,8 @@
         arm_ccfsm_state += 2;
         return \"\";
       }
-    return output_return_instruction (operands[0], true, true, false);
+    return output_return_instruction (operands[0], true, true,
+				      <return_simple_p>);
   }"
   [(set_attr "conds" "use")
    (set_attr "length" "12")
@@ -8667,7 +9445,8 @@
   "TARGET_ARM"
   "teq\\t%|r0, %|r0\;teq\\t%|pc, %|pc"
   [(set_attr "length" "8")
-   (set_attr "conds" "set")]
+   (set_attr "conds" "set")
+   (set_attr "type" "multiple")]
 )
 
 ;; Call subroutine returning any type.
@@ -8858,7 +9637,8 @@
     return   \"cmp\\t%0, %1\;ldrls\\t%|pc, [%|pc, %0, asl #2]\;b\\t%l3\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "12")]
+   (set_attr "length" "12")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "thumb1_casesi_internal_pic"
@@ -8889,7 +9669,8 @@
               (clobber (reg:SI LR_REGNUM))])]
   "TARGET_THUMB1"
   "* return thumb1_output_casesi(operands);"
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
 )
 
 (define_expand "indirect_jump"
@@ -8915,7 +9696,8 @@
 	(match_operand:SI 0 "s_register_operand" "r"))]
   "TARGET_ARM"
   "mov%?\\t%|pc, %0\\t%@ indirect register jump"
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "type" "branch")]
 )
 
 (define_insn "*load_indirect_jump"
@@ -8936,7 +9718,8 @@
   "TARGET_THUMB1"
   "mov\\tpc, %0"
   [(set_attr "conds" "clob")
-   (set_attr "length" "2")]
+   (set_attr "length" "2")
+   (set_attr "type" "branch")]
 )
 
 
@@ -8955,7 +9738,8 @@
   [(set (attr "length")
 	(if_then_else (eq_attr "is_thumb" "yes")
 		      (const_int 2)
-		      (const_int 4)))]
+		      (const_int 4)))
+   (set_attr "type" "mov_reg")]
 )
 
 
@@ -8991,7 +9775,7 @@
 			  (if_then_else
 			   (match_operand:SI 3 "mult_operator" "")
 			   (const_string "no") (const_string "yes"))])
-   (set_attr "type" "alu_shift,alu_shift,alu_shift,alu_shift_reg")])
+   (set_attr "type" "alu_shift_imm,alu_shift_imm,alu_shift_imm,alu_shift_reg")])
 
 (define_split
   [(set (match_operand:SI 0 "s_register_operand" "")
@@ -9028,7 +9812,7 @@
   [(set_attr "conds" "set")
    (set_attr "shift" "4")
    (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "type" "alus_shift_imm,alus_shift_reg")])
 
 (define_insn "*arith_shiftsi_compare0_scratch"
   [(set (reg:CC_NOOV CC_REGNUM)
@@ -9045,7 +9829,7 @@
   [(set_attr "conds" "set")
    (set_attr "shift" "4")
    (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "type" "alus_shift_imm,alus_shift_reg")])
 
 (define_insn "*sub_shiftsi"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r")
@@ -9058,66 +9842,104 @@
   [(set_attr "predicable" "yes")
    (set_attr "shift" "3")
    (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "type" "alus_shift_imm,alus_shift_reg")])
 
 (define_insn "*sub_shiftsi_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV
-	 (minus:SI (match_operand:SI 1 "s_register_operand" "r,r")
+	 (minus:SI (match_operand:SI 1 "s_register_operand" "r,r,r")
 		   (match_operator:SI 2 "shift_operator"
-		    [(match_operand:SI 3 "s_register_operand" "r,r")
-		     (match_operand:SI 4 "shift_amount_operand" "M,rM")]))
+		    [(match_operand:SI 3 "s_register_operand" "r,r,r")
+		     (match_operand:SI 4 "shift_amount_operand" "M,r,M")]))
 	 (const_int 0)))
-   (set (match_operand:SI 0 "s_register_operand" "=r,r")
+   (set (match_operand:SI 0 "s_register_operand" "=r,r,r")
 	(minus:SI (match_dup 1)
 		  (match_op_dup 2 [(match_dup 3) (match_dup 4)])))]
   "TARGET_32BIT"
   "sub%.\\t%0, %1, %3%S2"
   [(set_attr "conds" "set")
    (set_attr "shift" "3")
-   (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "arch" "32,a,a")
+   (set_attr "type" "alus_shift_imm,alus_shift_reg,alus_shift_imm")])
 
 (define_insn "*sub_shiftsi_compare0_scratch"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV
-	 (minus:SI (match_operand:SI 1 "s_register_operand" "r,r")
+	 (minus:SI (match_operand:SI 1 "s_register_operand" "r,r,r")
 		   (match_operator:SI 2 "shift_operator"
-		    [(match_operand:SI 3 "s_register_operand" "r,r")
-		     (match_operand:SI 4 "shift_amount_operand" "M,rM")]))
+		    [(match_operand:SI 3 "s_register_operand" "r,r,r")
+		     (match_operand:SI 4 "shift_amount_operand" "M,r,M")]))
 	 (const_int 0)))
-   (clobber (match_scratch:SI 0 "=r,r"))]
+   (clobber (match_scratch:SI 0 "=r,r,r"))]
   "TARGET_32BIT"
   "sub%.\\t%0, %1, %3%S2"
   [(set_attr "conds" "set")
    (set_attr "shift" "3")
-   (set_attr "arch" "32,a")
-   (set_attr "type" "alu_shift,alu_shift_reg")])
+   (set_attr "arch" "32,a,a")
+   (set_attr "type" "alus_shift_imm,alus_shift_reg,alus_shift_imm")])
 
 
-(define_insn "*and_scc"
+(define_insn_and_split "*and_scc"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(and:SI (match_operator:SI 1 "arm_comparison_operator"
-		 [(match_operand 3 "cc_register" "") (const_int 0)])
-		(match_operand:SI 2 "s_register_operand" "r")))]
+		 [(match_operand 2 "cc_register" "") (const_int 0)])
+		(match_operand:SI 3 "s_register_operand" "r")))]
   "TARGET_ARM"
-  "mov%D1\\t%0, #0\;and%d1\\t%0, %2, #1"
+  "#"   ; "mov%D1\\t%0, #0\;and%d1\\t%0, %3, #1"
+  "&& reload_completed"
+  [(cond_exec (match_dup 5) (set (match_dup 0) (const_int 0)))
+   (cond_exec (match_dup 4) (set (match_dup 0)
+                                 (and:SI (match_dup 3) (const_int 1))))]
+  {
+    enum machine_mode mode = GET_MODE (operands[2]);
+    enum rtx_code rc = GET_CODE (operands[1]);
+
+    /* Note that operands[4] is the same as operands[1],
+       but with VOIDmode as the result. */
+    operands[4] = gen_rtx_fmt_ee (rc, VOIDmode, operands[2], const0_rtx);
+    if (mode == CCFPmode || mode == CCFPEmode)
+      rc = reverse_condition_maybe_unordered (rc);
+    else
+      rc = reverse_condition (rc);
+    operands[5] = gen_rtx_fmt_ee (rc, VOIDmode, operands[2], const0_rtx);
+  }
   [(set_attr "conds" "use")
-   (set_attr "insn" "mov")
+   (set_attr "type" "multiple")
    (set_attr "length" "8")]
 )
 
-(define_insn "*ior_scc"
+(define_insn_and_split "*ior_scc"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r")
-	(ior:SI (match_operator:SI 2 "arm_comparison_operator"
-		 [(match_operand 3 "cc_register" "") (const_int 0)])
-		(match_operand:SI 1 "s_register_operand" "0,?r")))]
+	(ior:SI (match_operator:SI 1 "arm_comparison_operator"
+		 [(match_operand 2 "cc_register" "") (const_int 0)])
+		(match_operand:SI 3 "s_register_operand" "0,?r")))]
   "TARGET_ARM"
   "@
-   orr%d2\\t%0, %1, #1
-   mov%D2\\t%0, %1\;orr%d2\\t%0, %1, #1"
+   orr%d1\\t%0, %3, #1
+   #"
+  "&& reload_completed
+   && REGNO (operands [0]) != REGNO (operands[3])"
+  ;; && which_alternative == 1
+  ; mov%D1\\t%0, %3\;orr%d1\\t%0, %3, #1
+  [(cond_exec (match_dup 5) (set (match_dup 0) (match_dup 3)))
+   (cond_exec (match_dup 4) (set (match_dup 0)
+                                 (ior:SI (match_dup 3) (const_int 1))))]
+  {
+    enum machine_mode mode = GET_MODE (operands[2]);
+    enum rtx_code rc = GET_CODE (operands[1]);
+
+    /* Note that operands[4] is the same as operands[1],
+       but with VOIDmode as the result. */
+    operands[4] = gen_rtx_fmt_ee (rc, VOIDmode, operands[2], const0_rtx);
+    if (mode == CCFPmode || mode == CCFPEmode)
+      rc = reverse_condition_maybe_unordered (rc);
+    else
+      rc = reverse_condition (rc);
+    operands[5] = gen_rtx_fmt_ee (rc, VOIDmode, operands[2], const0_rtx);
+  }
   [(set_attr "conds" "use")
-   (set_attr "length" "4,8")]
+   (set_attr "length" "4,8")
+   (set_attr "type" "logic_imm,multiple")]
 )
 
 ; A series of splitters for the compare_scc pattern below.  Note that
@@ -9144,6 +9966,16 @@
 	(eq:SI (match_operand:SI 1 "s_register_operand" "")
 	       (const_int 0)))
    (clobber (reg:CC CC_REGNUM))]
+  "arm_arch5 && TARGET_32BIT"
+  [(set (match_dup 0) (clz:SI (match_dup 1)))
+   (set (match_dup 0) (lshiftrt:SI (match_dup 0) (const_int 5)))]
+)
+
+(define_split
+  [(set (match_operand:SI 0 "s_register_operand" "")
+	(eq:SI (match_operand:SI 1 "s_register_operand" "")
+	       (const_int 0)))
+   (clobber (reg:CC CC_REGNUM))]
   "TARGET_32BIT && reload_completed"
   [(parallel
     [(set (reg:CC CC_REGNUM)
@@ -9184,7 +10016,7 @@
 	      (set (match_dup 0) (const_int 1)))])
 
 (define_insn_and_split "*compare_scc"
-  [(set (match_operand:SI 0 "s_register_operand" "=r,r")
+  [(set (match_operand:SI 0 "s_register_operand" "=Ts,Ts")
 	(match_operator:SI 1 "arm_comparison_operator"
 	 [(match_operand:SI 2 "s_register_operand" "r,r")
 	  (match_operand:SI 3 "arm_add_operand" "rI,L")]))
@@ -9209,33 +10041,99 @@
   else
     rc = reverse_condition (rc);
   operands[4] = gen_rtx_fmt_ee (rc, VOIDmode, tmp1, const0_rtx);
-})
+}
+  [(set_attr "type" "multiple")]
+)
 
 ;; Attempt to improve the sequence generated by the compare_scc splitters
 ;; not to use conditional execution.
+
+;; Rd = (eq (reg1) (const_int0))  // ARMv5
+;;	clz Rd, reg1
+;;	lsr Rd, Rd, #5
 (define_peephole2
   [(set (reg:CC CC_REGNUM)
 	(compare:CC (match_operand:SI 1 "register_operand" "")
-		    (match_operand:SI 2 "arm_rhs_operand" "")))
+		    (const_int 0)))
+   (cond_exec (ne (reg:CC CC_REGNUM) (const_int 0))
+	      (set (match_operand:SI 0 "register_operand" "") (const_int 0)))
+   (cond_exec (eq (reg:CC CC_REGNUM) (const_int 0))
+	      (set (match_dup 0) (const_int 1)))]
+  "arm_arch5 && TARGET_32BIT && peep2_regno_dead_p (3, CC_REGNUM)"
+  [(set (match_dup 0) (clz:SI (match_dup 1)))
+   (set (match_dup 0) (lshiftrt:SI (match_dup 0) (const_int 5)))]
+)
+
+;; Rd = (eq (reg1) (const_int0))  // !ARMv5
+;;	negs Rd, reg1
+;;	adc  Rd, Rd, reg1
+(define_peephole2
+  [(set (reg:CC CC_REGNUM)
+	(compare:CC (match_operand:SI 1 "register_operand" "")
+		    (const_int 0)))
    (cond_exec (ne (reg:CC CC_REGNUM) (const_int 0))
 	      (set (match_operand:SI 0 "register_operand" "") (const_int 0)))
    (cond_exec (eq (reg:CC CC_REGNUM) (const_int 0))
 	      (set (match_dup 0) (const_int 1)))
-   (match_scratch:SI 3 "r")]
-  "TARGET_32BIT"
+   (match_scratch:SI 2 "r")]
+  "TARGET_32BIT && peep2_regno_dead_p (3, CC_REGNUM)"
   [(parallel
     [(set (reg:CC CC_REGNUM)
-	  (compare:CC (match_dup 1) (match_dup 2)))
-     (set (match_dup 3) (minus:SI (match_dup 1) (match_dup 2)))])
+	  (compare:CC (const_int 0) (match_dup 1)))
+     (set (match_dup 2) (minus:SI (const_int 0) (match_dup 1)))])
+   (set (match_dup 0)
+	(plus:SI (plus:SI (match_dup 1) (match_dup 2))
+		 (geu:SI (reg:CC CC_REGNUM) (const_int 0))))]
+)
+
+;; Rd = (eq (reg1) (reg2/imm))	// ARMv5 and optimising for speed.
+;;	sub  Rd, Reg1, reg2
+;;	clz  Rd, Rd
+;;	lsr  Rd, Rd, #5
+(define_peephole2
+  [(set (reg:CC CC_REGNUM)
+	(compare:CC (match_operand:SI 1 "register_operand" "")
+		    (match_operand:SI 2 "arm_rhs_operand" "")))
+   (cond_exec (ne (reg:CC CC_REGNUM) (const_int 0))
+	      (set (match_operand:SI 0 "register_operand" "") (const_int 0)))
+   (cond_exec (eq (reg:CC CC_REGNUM) (const_int 0))
+	      (set (match_dup 0) (const_int 1)))]
+  "arm_arch5 && TARGET_32BIT && peep2_regno_dead_p (3, CC_REGNUM)
+  && !(TARGET_THUMB2 && optimize_insn_for_size_p ())"
+  [(set (match_dup 0) (minus:SI (match_dup 1) (match_dup 2)))
+   (set (match_dup 0) (clz:SI (match_dup 0)))
+   (set (match_dup 0) (lshiftrt:SI (match_dup 0) (const_int 5)))]
+)
+
+
+;; Rd = (eq (reg1) (reg2))	// ! ARMv5 or optimising for size.
+;;	sub  T1, Reg1, reg2
+;;	negs Rd, T1
+;;	adc  Rd, Rd, T1
+(define_peephole2
+  [(set (reg:CC CC_REGNUM)
+	(compare:CC (match_operand:SI 1 "register_operand" "")
+		    (match_operand:SI 2 "arm_rhs_operand" "")))
+   (cond_exec (ne (reg:CC CC_REGNUM) (const_int 0))
+	      (set (match_operand:SI 0 "register_operand" "") (const_int 0)))
+   (cond_exec (eq (reg:CC CC_REGNUM) (const_int 0))
+	      (set (match_dup 0) (const_int 1)))
+   (match_scratch:SI 3 "r")]
+  "TARGET_32BIT && peep2_regno_dead_p (3, CC_REGNUM)"
+  [(set (match_dup 3) (match_dup 4))
    (parallel
     [(set (reg:CC CC_REGNUM)
 	  (compare:CC (const_int 0) (match_dup 3)))
      (set (match_dup 0) (minus:SI (const_int 0) (match_dup 3)))])
-   (parallel
-    [(set (match_dup 0)
-	  (plus:SI (plus:SI (match_dup 0) (match_dup 3))
-		   (geu:SI (reg:CC CC_REGNUM) (const_int 0))))
-     (clobber (reg:CC CC_REGNUM))])])
+   (set (match_dup 0)
+	(plus:SI (plus:SI (match_dup 0) (match_dup 3))
+		 (geu:SI (reg:CC CC_REGNUM) (const_int 0))))]
+  "
+  if (CONST_INT_P (operands[2]))
+    operands[4] = plus_constant (SImode, operands[1], -INTVAL (operands[2]));
+  else
+    operands[4] = gen_rtx_MINUS (SImode, operands[1], operands[2]);
+  ")
 
 (define_insn "*cond_move"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r,r")
@@ -9262,7 +10160,7 @@
     return \"\";
   "
   [(set_attr "conds" "use")
-   (set_attr "insn" "mov")
+   (set_attr "type" "mov_reg,mov_reg,multiple")
    (set_attr "length" "4,4,8")]
 )
 
@@ -9289,7 +10187,8 @@
     return \"%i5%d4\\t%0, %1, #1\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "12")]
+   (set_attr "length" "12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*cond_sub"
@@ -9307,7 +10206,8 @@
     return \"sub%d4\\t%0, %1, #1\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*cmp_ite0"
@@ -9371,6 +10271,7 @@
   }"
   [(set_attr "conds" "set")
    (set_attr "arch" "t2,t2,t2,t2,t2,any,any,any,any")
+   (set_attr "type" "multiple")
    (set_attr_alternative "length"
       [(const_int 6)
        (const_int 8)
@@ -9470,7 +10371,8 @@
            (const_int 10))
        (if_then_else (eq_attr "is_thumb" "no")
            (const_int 8)
-           (const_int 10))])]
+           (const_int 10))])
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*cmp_and"
@@ -9551,7 +10453,8 @@
            (const_int 10))
        (if_then_else (eq_attr "is_thumb" "no")
            (const_int 8)
-           (const_int 10))])]
+           (const_int 10))])
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*cmp_ior"
@@ -9632,11 +10535,12 @@
            (const_int 10))
        (if_then_else (eq_attr "is_thumb" "no")
            (const_int 8)
-           (const_int 10))])]
+           (const_int 10))])
+   (set_attr "type" "multiple")]
 )
 
 (define_insn_and_split "*ior_scc_scc"
-  [(set (match_operand:SI 0 "s_register_operand" "=r")
+  [(set (match_operand:SI 0 "s_register_operand" "=Ts")
 	(ior:SI (match_operator:SI 3 "arm_comparison_operator"
 		 [(match_operand:SI 1 "s_register_operand" "r")
 		  (match_operand:SI 2 "arm_add_operand" "rIL")])
@@ -9661,7 +10565,9 @@
 						  DOM_CC_X_OR_Y),
 		    CC_REGNUM);"
   [(set_attr "conds" "clob")
-   (set_attr "length" "16")])
+   (set_attr "length" "16")
+   (set_attr "type" "multiple")]
+)
 
 ; If the above pattern is followed by a CMP insn, then the compare is 
 ; redundant, since we can rework the conditional instruction that follows.
@@ -9674,7 +10580,7 @@
 			  [(match_operand:SI 4 "s_register_operand" "r")
 			   (match_operand:SI 5 "arm_add_operand" "rIL")]))
 		 (const_int 0)))
-   (set (match_operand:SI 7 "s_register_operand" "=r")
+   (set (match_operand:SI 7 "s_register_operand" "=Ts")
 	(ior:SI (match_op_dup 3 [(match_dup 1) (match_dup 2)])
 		(match_op_dup 6 [(match_dup 4) (match_dup 5)])))]
   "TARGET_32BIT"
@@ -9689,10 +10595,12 @@
    (set (match_dup 7) (ne:SI (match_dup 0) (const_int 0)))]
   ""
   [(set_attr "conds" "set")
-   (set_attr "length" "16")])
+   (set_attr "length" "16")
+   (set_attr "type" "multiple")]
+)
 
 (define_insn_and_split "*and_scc_scc"
-  [(set (match_operand:SI 0 "s_register_operand" "=r")
+  [(set (match_operand:SI 0 "s_register_operand" "=Ts")
 	(and:SI (match_operator:SI 3 "arm_comparison_operator"
 		 [(match_operand:SI 1 "s_register_operand" "r")
 		  (match_operand:SI 2 "arm_add_operand" "rIL")])
@@ -9719,7 +10627,9 @@
 						  DOM_CC_X_AND_Y),
 		    CC_REGNUM);"
   [(set_attr "conds" "clob")
-   (set_attr "length" "16")])
+   (set_attr "length" "16")
+   (set_attr "type" "multiple")]
+)
 
 ; If the above pattern is followed by a CMP insn, then the compare is 
 ; redundant, since we can rework the conditional instruction that follows.
@@ -9732,7 +10642,7 @@
 			  [(match_operand:SI 4 "s_register_operand" "r")
 			   (match_operand:SI 5 "arm_add_operand" "rIL")]))
 		 (const_int 0)))
-   (set (match_operand:SI 7 "s_register_operand" "=r")
+   (set (match_operand:SI 7 "s_register_operand" "=Ts")
 	(and:SI (match_op_dup 3 [(match_dup 1) (match_dup 2)])
 		(match_op_dup 6 [(match_dup 4) (match_dup 5)])))]
   "TARGET_32BIT"
@@ -9747,14 +10657,16 @@
    (set (match_dup 7) (ne:SI (match_dup 0) (const_int 0)))]
   ""
   [(set_attr "conds" "set")
-   (set_attr "length" "16")])
+   (set_attr "length" "16")
+   (set_attr "type" "multiple")]
+)
 
 ;; If there is no dominance in the comparison, then we can still save an
 ;; instruction in the AND case, since we can know that the second compare
 ;; need only zero the value if false (if true, then the value is already
 ;; correct).
 (define_insn_and_split "*and_scc_scc_nodom"
-  [(set (match_operand:SI 0 "s_register_operand" "=&r,&r,&r")
+  [(set (match_operand:SI 0 "s_register_operand" "=&Ts,&Ts,&Ts")
 	(and:SI (match_operator:SI 3 "arm_comparison_operator"
 		 [(match_operand:SI 1 "s_register_operand" "r,r,0")
 		  (match_operand:SI 2 "arm_add_operand" "rIL,0,rIL")])
@@ -9781,7 +10693,9 @@
    operands[8] = gen_rtx_COMPARE (GET_MODE (operands[7]), operands[4],
 				  operands[5]);"
   [(set_attr "conds" "clob")
-   (set_attr "length" "20")])
+   (set_attr "length" "20")
+   (set_attr "type" "multiple")]
+)
 
 (define_split
   [(set (reg:CC_NOOV CC_REGNUM)
@@ -9822,26 +10736,117 @@
   "")
 ;; ??? The conditional patterns above need checking for Thumb-2 usefulness
 
-(define_insn "*negscc"
+(define_insn_and_split "*negscc"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(neg:SI (match_operator 3 "arm_comparison_operator"
 		 [(match_operand:SI 1 "s_register_operand" "r")
 		  (match_operand:SI 2 "arm_rhs_operand" "rI")])))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ARM"
-  "*
-  if (GET_CODE (operands[3]) == LT && operands[2] == const0_rtx)
-    return \"mov\\t%0, %1, asr #31\";
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    rtx cc_reg = gen_rtx_REG (CCmode, CC_REGNUM);
+
+    if (GET_CODE (operands[3]) == LT && operands[2] == const0_rtx)
+       {
+         /* Emit mov\\t%0, %1, asr #31 */
+         emit_insn (gen_rtx_SET (VOIDmode,
+                                 operands[0],
+                                 gen_rtx_ASHIFTRT (SImode,
+                                                   operands[1],
+                                                   GEN_INT (31))));
+         DONE;
+       }
+     else if (GET_CODE (operands[3]) == NE)
+       {
+        /* Emit subs\\t%0, %1, %2\;mvnne\\t%0, #0 */
+        if (CONST_INT_P (operands[2]))
+          emit_insn (gen_cmpsi2_addneg (operands[0], operands[1], operands[2],
+                                        GEN_INT (- INTVAL (operands[2]))));
+        else
+          emit_insn (gen_subsi3_compare (operands[0], operands[1], operands[2]));
+
+        emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                      gen_rtx_NE (SImode,
+                                                  cc_reg,
+                                                  const0_rtx),
+                                      gen_rtx_SET (SImode,
+                                                   operands[0],
+                                                   GEN_INT (~0))));
+        DONE;
+      }
+    else
+      {
+        /* Emit: cmp\\t%1, %2\;mov%D3\\t%0, #0\;mvn%d3\\t%0, #0 */
+        emit_insn (gen_rtx_SET (VOIDmode,
+                                cc_reg,
+                                gen_rtx_COMPARE (CCmode, operands[1], operands[2])));
+        enum rtx_code rc = GET_CODE (operands[3]);
+
+        rc = reverse_condition (rc);
+        emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                      gen_rtx_fmt_ee (rc,
+                                                      VOIDmode,
+                                                      cc_reg,
+                                                      const0_rtx),
+                                      gen_rtx_SET (VOIDmode, operands[0], const0_rtx)));
+        rc = GET_CODE (operands[3]);
+        emit_insn (gen_rtx_COND_EXEC (VOIDmode,
+                                      gen_rtx_fmt_ee (rc,
+                                                      VOIDmode,
+                                                      cc_reg,
+                                                      const0_rtx),
+                                      gen_rtx_SET (VOIDmode,
+                                                   operands[0],
+                                                   GEN_INT (~0))));
+        DONE;
+      }
+     FAIL;
+  }
+  [(set_attr "conds" "clob")
+   (set_attr "length" "12")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn_and_split "movcond_addsi"
+  [(set (match_operand:SI 0 "s_register_operand" "=r,l,r")
+	(if_then_else:SI
+	 (match_operator 5 "comparison_operator"
+	  [(plus:SI (match_operand:SI 3 "s_register_operand" "r,r,r")
+	            (match_operand:SI 4 "arm_add_operand" "rIL,rIL,rIL"))
+            (const_int 0)])
+	 (match_operand:SI 1 "arm_rhs_operand" "rI,rPy,r")
+	 (match_operand:SI 2 "arm_rhs_operand" "rI,rPy,r")))
+   (clobber (reg:CC CC_REGNUM))]
+   "TARGET_32BIT"
+   "#"
+   "&& reload_completed"
+  [(set (reg:CC_NOOV CC_REGNUM)
+	(compare:CC_NOOV
+	 (plus:SI (match_dup 3)
+		  (match_dup 4))
+	 (const_int 0)))
+   (set (match_dup 0) (match_dup 1))
+   (cond_exec (match_dup 6)
+	      (set (match_dup 0) (match_dup 2)))]
+  "
+  {
+    enum machine_mode mode = SELECT_CC_MODE (GET_CODE (operands[5]),
+					     operands[3], operands[4]);
+    enum rtx_code rc = GET_CODE (operands[5]);
 
-  if (GET_CODE (operands[3]) == NE)
-    return \"subs\\t%0, %1, %2\;mvnne\\t%0, #0\";
+    operands[6] = gen_rtx_REG (mode, CC_REGNUM);
+    gcc_assert (!(mode == CCFPmode || mode == CCFPEmode));
+    rc = reverse_condition (rc);
 
-  output_asm_insn (\"cmp\\t%1, %2\", operands);
-  output_asm_insn (\"mov%D3\\t%0, #0\", operands);
-  return \"mvn%d3\\t%0, #0\";
+    operands[6] = gen_rtx_fmt_ee (rc, VOIDmode, operands[6], const0_rtx);
+  }
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "12")]
+   (set_attr "enabled_for_depr_it" "no,yes,yes")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "movcond"
@@ -9904,7 +10909,8 @@
   return \"\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,8,12")]
+   (set_attr "length" "8,8,12")
+   (set_attr "type" "multiple")]
 )
 
 ;; ??? The patterns below need checking for Thumb-2 usefulness.
@@ -9922,7 +10928,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_plus_move"
@@ -9944,11 +10951,11 @@
    (set_attr "length" "4,4,8,8")
    (set_attr_alternative "type"
                          [(if_then_else (match_operand 3 "const_int_operand" "")
-                                        (const_string "simple_alu_imm" )
-                                        (const_string "*"))
-                          (const_string "simple_alu_imm")
-                          (const_string "*")
-                          (const_string "*")])]
+                                        (const_string "alu_imm" )
+                                        (const_string "alu_reg"))
+                          (const_string "alu_imm")
+                          (const_string "alu_reg")
+                          (const_string "alu_reg")])]
 )
 
 (define_insn "*ifcompare_move_plus"
@@ -9964,7 +10971,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_move_plus"
@@ -9984,13 +10992,7 @@
    sub%D4\\t%0, %2, #%n3\;mov%d4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,4,8,8")
-   (set_attr_alternative "type"
-                         [(if_then_else (match_operand 3 "const_int_operand" "")
-                                        (const_string "simple_alu_imm" )
-                                        (const_string "*"))
-                          (const_string "simple_alu_imm")
-                          (const_string "*")
-                          (const_string "*")])]
+   (set_attr "type" "alu_reg,alu_imm,multiple,multiple")]
 )
 
 (define_insn "*ifcompare_arith_arith"
@@ -10008,7 +11010,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "12")]
+   (set_attr "length" "12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_arith_arith"
@@ -10024,7 +11027,8 @@
   "TARGET_ARM"
   "%I6%d5\\t%0, %1, %2\;%I7%D5\\t%0, %3, %4"
   [(set_attr "conds" "use")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*ifcompare_arith_move"
@@ -10065,7 +11069,8 @@
   return \"\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_arith_move"
@@ -10082,7 +11087,7 @@
    %I5%d4\\t%0, %2, %3\;mov%D4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,8")
-   (set_attr "type" "*,*")]
+   (set_attr "type" "alu_shift_reg,multiple")]
 )
 
 (define_insn "*ifcompare_move_arith"
@@ -10124,7 +11129,8 @@
   return \"%I7%D6\\t%0, %2, %3\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_move_arith"
@@ -10142,7 +11148,7 @@
    %I5%D4\\t%0, %2, %3\;mov%d4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,8")
-   (set_attr "type" "*,*")]
+   (set_attr "type" "alu_shift_reg,multiple")]
 )
 
 (define_insn "*ifcompare_move_not"
@@ -10158,7 +11164,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_move_not"
@@ -10174,8 +11181,9 @@
    mov%d4\\t%0, %1\;mvn%D4\\t%0, %2
    mvn%d4\\t%0, #%B1\;mvn%D4\\t%0, %2"
   [(set_attr "conds" "use")
-   (set_attr "insn" "mvn")
-   (set_attr "length" "4,8,8")]
+   (set_attr "type" "mvn_reg")
+   (set_attr "length" "4,8,8")
+   (set_attr "type" "mvn_reg,multiple,multiple")]
 )
 
 (define_insn "*ifcompare_not_move"
@@ -10191,7 +11199,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_not_move"
@@ -10207,7 +11216,7 @@
    mov%D4\\t%0, %1\;mvn%d4\\t%0, %2
    mvn%D4\\t%0, #%B1\;mvn%d4\\t%0, %2"
   [(set_attr "conds" "use")
-   (set_attr "insn" "mvn")
+   (set_attr "type" "mvn_reg,multiple,multiple")
    (set_attr "length" "4,8,8")]
 )
 
@@ -10225,7 +11234,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_shift_move"
@@ -10245,10 +11255,7 @@
   [(set_attr "conds" "use")
    (set_attr "shift" "2")
    (set_attr "length" "4,8,8")
-   (set_attr "insn" "mov")
-   (set (attr "type") (if_then_else (match_operand 3 "const_int_operand" "")
-		      (const_string "alu_shift")
-		      (const_string "alu_shift_reg")))]
+   (set_attr "type" "mov_shift_reg,multiple,multiple")]
 )
 
 (define_insn "*ifcompare_move_shift"
@@ -10265,7 +11272,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_move_shift"
@@ -10285,10 +11293,7 @@
   [(set_attr "conds" "use")
    (set_attr "shift" "2")
    (set_attr "length" "4,8,8")
-   (set_attr "insn" "mov")
-   (set (attr "type") (if_then_else (match_operand 3 "const_int_operand" "")
-		      (const_string "alu_shift")
-		      (const_string "alu_shift_reg")))]
+   (set_attr "type" "mov_shift_reg,multiple,multiple")]
 )
 
 (define_insn "*ifcompare_shift_shift"
@@ -10307,7 +11312,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "12")]
+   (set_attr "length" "12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_shift_shift"
@@ -10326,12 +11332,11 @@
   [(set_attr "conds" "use")
    (set_attr "shift" "1")
    (set_attr "length" "8")
-   (set_attr "insn" "mov")
    (set (attr "type") (if_then_else
 		        (and (match_operand 2 "const_int_operand" "")
                              (match_operand 4 "const_int_operand" ""))
-		      (const_string "alu_shift")
-		      (const_string "alu_shift_reg")))]
+		      (const_string "mov_shift")
+		      (const_string "mov_shift_reg")))]
 )
 
 (define_insn "*ifcompare_not_arith"
@@ -10348,7 +11353,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "12")]
+   (set_attr "length" "12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_not_arith"
@@ -10363,7 +11369,7 @@
   "TARGET_ARM"
   "mvn%d5\\t%0, %1\;%I6%D5\\t%0, %2, %3"
   [(set_attr "conds" "use")
-   (set_attr "insn" "mvn")
+   (set_attr "type" "mvn_reg")
    (set_attr "length" "8")]
 )
 
@@ -10381,7 +11387,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "12")]
+   (set_attr "length" "12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_arith_not"
@@ -10396,7 +11403,7 @@
   "TARGET_ARM"
   "mvn%D5\\t%0, %1\;%I6%d5\\t%0, %2, %3"
   [(set_attr "conds" "use")
-   (set_attr "insn" "mvn")
+   (set_attr "type" "multiple")
    (set_attr "length" "8")]
 )
 
@@ -10412,7 +11419,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_neg_move"
@@ -10428,7 +11436,8 @@
    mov%D4\\t%0, %1\;rsb%d4\\t%0, %2, #0
    mvn%D4\\t%0, #%B1\;rsb%d4\\t%0, %2, #0"
   [(set_attr "conds" "use")
-   (set_attr "length" "4,8,8")]
+   (set_attr "length" "4,8,8")
+   (set_attr "type" "logic_shift_imm,multiple,multiple")]
 )
 
 (define_insn "*ifcompare_move_neg"
@@ -10443,7 +11452,8 @@
   "TARGET_ARM"
   "#"
   [(set_attr "conds" "clob")
-   (set_attr "length" "8,12")]
+   (set_attr "length" "8,12")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*if_move_neg"
@@ -10459,7 +11469,8 @@
    mov%d4\\t%0, %1\;rsb%D4\\t%0, %2, #0
    mvn%d4\\t%0, #%B1\;rsb%D4\\t%0, %2, #0"
   [(set_attr "conds" "use")
-   (set_attr "length" "4,8,8")]
+   (set_attr "length" "4,8,8")
+   (set_attr "type" "logic_shift_imm,multiple,multiple")]
 )
 
 (define_insn "*arith_adjacentmem"
@@ -10657,7 +11668,8 @@
   [(unspec_volatile [(const_int 0)] VUNSPEC_THUMB1_INTERWORK)]
   "TARGET_THUMB1"
   "* return thumb1_output_interwork ();"
-  [(set_attr "length" "8")]
+  [(set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 ;; Note - although unspec_volatile's USE all hard registers,
@@ -10844,7 +11856,7 @@
    mvn%D4\\t%0, %2
    mov%d4\\t%0, %1\;mvn%D4\\t%0, %2"
   [(set_attr "conds" "use")
-   (set_attr "insn" "mvn")
+   (set_attr "type" "mvn_reg,multiple")
    (set_attr "length" "4,8")]
 )
 
@@ -10864,7 +11876,8 @@
     return \"mvnne\\t%0, #0\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "8")]
+   (set_attr "length" "8")
+   (set_attr "type" "multiple")]
 )
 
 (define_insn "*not_signextract_onebit"
@@ -10882,7 +11895,8 @@
     return \"movne\\t%0, #0\";
   "
   [(set_attr "conds" "clob")
-   (set_attr "length" "12")]
+   (set_attr "length" "12")
+   (set_attr "type" "multiple")]
 )
 ;; ??? The above patterns need auditing for Thumb-2
 
@@ -10944,7 +11958,8 @@
 		    UNSPEC_PRLG_STK))]
   ""
   ""
-  [(set_attr "length" "0")]
+  [(set_attr "length" "0")
+   (set_attr "type" "block")]
 )
 
 ;; Pop (as used in epilogue RTL)
@@ -11074,6 +12089,7 @@
   assemble_align (32);
   return \"\";
   "
+  [(set_attr "type" "no_insn")]
 )
 
 (define_insn "align_8"
@@ -11083,6 +12099,7 @@
   assemble_align (64);
   return \"\";
   "
+  [(set_attr "type" "no_insn")]
 )
 
 (define_insn "consttable_end"
@@ -11092,6 +12109,7 @@
   making_const_table = FALSE;
   return \"\";
   "
+  [(set_attr "type" "no_insn")]
 )
 
 (define_insn "consttable_1"
@@ -11103,7 +12121,8 @@
   assemble_zeros (3);
   return \"\";
   "
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "no_insn")]
 )
 
 (define_insn "consttable_2"
@@ -11125,7 +12144,8 @@
       }
     return \"\";
   }"
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "no_insn")]
 )
 
 (define_insn "consttable_4"
@@ -11158,7 +12178,8 @@
       }
     return \"\";
   }"
-  [(set_attr "length" "4")]
+  [(set_attr "length" "4")
+   (set_attr "type" "no_insn")]
 )
 
 (define_insn "consttable_8"
@@ -11182,7 +12203,8 @@
       }
     return \"\";
   }"
-  [(set_attr "length" "8")]
+  [(set_attr "length" "8")
+   (set_attr "type" "no_insn")]
 )
 
 (define_insn "consttable_16"
@@ -11206,7 +12228,8 @@
       }
     return \"\";
   }"
-  [(set_attr "length" "16")]
+  [(set_attr "length" "16")
+   (set_attr "type" "no_insn")]
 )
 
 ;; Miscellaneous Thumb patterns
@@ -11234,7 +12257,8 @@
    (use (label_ref (match_operand 1 "" "")))]
   "TARGET_THUMB1"
   "mov\\t%|pc, %0"
-  [(set_attr "length" "2")]
+  [(set_attr "length" "2")
+   (set_attr "type" "no_insn")]
 )
 
 ;; V5 Instructions,
@@ -11245,7 +12269,7 @@
   "TARGET_32BIT && arm_arch5"
   "clz%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "clz")])
+   (set_attr "type" "clz")])
 
 (define_insn "rbitsi2"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
@@ -11253,7 +12277,7 @@
   "TARGET_32BIT && arm_arch_thumb2"
   "rbit%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "insn" "clz")])
+   (set_attr "type" "clz")])
 
 (define_expand "ctzsi2"
  [(set (match_operand:SI           0 "s_register_operand" "")
@@ -11288,13 +12312,15 @@
      (const_int 0)])]
   "TARGET_32BIT"
   ""
+[(set_attr "predicated" "yes")]
 )
 
 (define_insn "force_register_use"
   [(unspec:SI [(match_operand:SI 0 "register_operand" "")] UNSPEC_REGISTER_USE)]
   ""
   "%@ %0 needed"
-  [(set_attr "length" "0")]
+  [(set_attr "length" "0")
+   (set_attr "type" "no_insn")]
 )
 
 
@@ -11342,6 +12368,7 @@
     thumb_set_return_address (operands[0], operands[1]);
     DONE;
   }"
+  [(set_attr "type" "mov_reg")]
 )
 
 
@@ -11352,7 +12379,8 @@
 	(unspec:SI [(const_int 0)] UNSPEC_TLS))]
   "TARGET_HARD_TP"
   "mrc%?\\tp15, 0, %0, c13, c0, 3\\t@ load_tp_hard"
-  [(set_attr "predicable" "yes")]
+  [(set_attr "predicable" "yes")
+   (set_attr "type" "mrs")]
 )
 
 ;; Doesn't clobber R1-R3.  Must use r0 for the first operand.
@@ -11363,7 +12391,8 @@
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_SOFT_TP"
   "bl\\t__aeabi_read_tp\\t@ load_tp_soft"
-  [(set_attr "conds" "clob")]
+  [(set_attr "conds" "clob")
+   (set_attr "type" "branch")]
 )
 
 ;; tls descriptor call
@@ -11382,7 +12411,8 @@
     return "bl\\t%c0(tlscall)";
   }
   [(set_attr "conds" "clob")
-   (set_attr "length" "4")]
+   (set_attr "length" "4")
+   (set_attr "type" "branch")]
 )
 
 ;; For thread pointer builtin
@@ -11407,7 +12437,9 @@
   "arm_arch_thumb2"
   "movt%?\t%0, %L1"
  [(set_attr "predicable" "yes")
-   (set_attr "length" "4")]
+  (set_attr "predicable_short_it" "no")
+  (set_attr "length" "4")
+  (set_attr "type" "mov_imm")]
 )
 
 (define_insn "*arm_rev"
@@ -11419,7 +12451,8 @@
    rev%?\t%0, %1
    rev%?\t%0, %1"
   [(set_attr "arch" "t1,t2,32")
-   (set_attr "length" "2,2,4")]
+   (set_attr "length" "2,2,4")
+   (set_attr "type" "rev")]
 )
 
 (define_expand "arm_legacy_rev"
@@ -11519,7 +12552,8 @@
   revsh%?\t%0, %1
   revsh%?\t%0, %1"
   [(set_attr "arch" "t1,t2,32")
-   (set_attr "length" "2,2,4")]
+   (set_attr "length" "2,2,4")
+   (set_attr "type" "rev")]
 )
 
 (define_insn "*arm_rev16"
@@ -11531,7 +12565,8 @@
    rev16%?\t%0, %1
    rev16%?\t%0, %1"
   [(set_attr "arch" "t1,t2,32")
-   (set_attr "length" "2,2,4")]
+   (set_attr "length" "2,2,4")
+   (set_attr "type" "rev")]
 )
 
 (define_expand "bswaphi2"
@@ -11558,7 +12593,8 @@
                                   false, true))"
   "ldrd%?\t%0, %3, [%1, %2]"
   [(set_attr "type" "load2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb2_ldrd_base"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
@@ -11572,7 +12608,8 @@
                                   operands[1], 0, false, true))"
   "ldrd%?\t%0, %2, [%1]"
   [(set_attr "type" "load2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb2_ldrd_base_neg"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
@@ -11586,7 +12623,8 @@
                                   operands[1], -4, false, true))"
   "ldrd%?\t%0, %2, [%1, #-4]"
   [(set_attr "type" "load2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb2_strd"
   [(set (mem:SI (plus:SI (match_operand:SI 0 "s_register_operand" "rk")
@@ -11603,7 +12641,8 @@
                                   false, false))"
   "strd%?\t%2, %4, [%0, %1]"
   [(set_attr "type" "store2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb2_strd_base"
   [(set (mem:SI (match_operand:SI 0 "s_register_operand" "rk"))
@@ -11617,7 +12656,8 @@
                                   operands[0], 0, false, false))"
   "strd%?\t%1, %2, [%0]"
   [(set_attr "type" "store2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
 
 (define_insn "*thumb2_strd_base_neg"
   [(set (mem:SI (plus:SI (match_operand:SI 0 "s_register_operand" "rk")
@@ -11631,8 +12671,23 @@
                                   operands[0], -4, false, false))"
   "strd%?\t%1, %2, [%0, #-4]"
   [(set_attr "type" "store2")
-   (set_attr "predicable" "yes")])
+   (set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")])
+
+;; ARMv8 CRC32 instructions.
+(define_insn "<crc_variant>"
+  [(set (match_operand:SI 0 "s_register_operand" "=r")
+        (unspec:SI [(match_operand:SI 1 "s_register_operand" "r")
+                    (match_operand:<crc_mode> 2 "s_register_operand" "r")]
+         CRC))]
+  "TARGET_CRC32"
+  "<crc_variant>\\t%0, %1, %2"
+  [(set_attr "type" "crc")
+   (set_attr "conds" "unconditional")]
+)
 
+;; Load the load/store double peephole optimizations.
+(include "ldrdstrd.md")
 
 ;; Load the load/store multiple patterns
 (include "ldmstm.md")
@@ -11667,6 +12722,8 @@
 (include "thumb2.md")
 ;; Neon patterns
 (include "neon.md")
+;; Crypto patterns
+(include "crypto.md")
 ;; Synchronization Primitives
 (include "sync.md")
 ;; Fixed-point patterns
Index: b/src/gcc/config/arm/fmp626.md
===================================================================
--- a/src/gcc/config/arm/fmp626.md
+++ b/src/gcc/config/arm/fmp626.md
@@ -63,12 +63,21 @@
 ;; ALU operations
 (define_insn_reservation "mp626_alu_op" 1
  (and (eq_attr "tune" "fmp626")
-      (eq_attr "type" "alu_reg,simple_alu_imm"))
+      (eq_attr "type" "alu_imm,alus_imm,alu_reg,alus_reg,\
+                       logic_imm,logics_imm,logic_reg,logics_reg,\
+                       adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                       adr,bfm,rev,\
+                       shift_imm,shift_reg,\
+                       mov_imm,mov_reg,mvn_imm,mvn_reg"))
  "fmp626_core")
 
 (define_insn_reservation "mp626_alu_shift_op" 2
  (and (eq_attr "tune" "fmp626")
-      (eq_attr "type" "simple_alu_shift,alu_shift,alu_shift_reg"))
+      (eq_attr "type" "alu_shift_imm,logic_shift_imm,alus_shift_imm,logics_shift_imm,\
+                       alu_shift_reg,logic_shift_reg,alus_shift_reg,logics_shift_reg,\
+                       extend,\
+                       mov_shift,mov_shift_reg,\
+                       mvn_shift,mvn_shift_reg"))
  "fmp626_core")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -77,22 +86,22 @@
 
 (define_insn_reservation "mp626_mult1" 2
  (and (eq_attr "tune" "fmp626")
-      (eq_attr "insn" "smulwy,smlawy,smulxy,smlaxy"))
+      (eq_attr "type" "smulwy,smlawy,smulxy,smlaxy"))
  "fmp626_core")
 
 (define_insn_reservation "mp626_mult2" 2
  (and (eq_attr "tune" "fmp626")
-      (eq_attr "insn" "mul,mla"))
+      (eq_attr "type" "mul,mla"))
  "fmp626_core")
 
 (define_insn_reservation "mp626_mult3" 3
  (and (eq_attr "tune" "fmp626")
-      (eq_attr "insn" "muls,mlas,smull,smlal,umull,umlal,smlalxy,smlawx"))
+      (eq_attr "type" "muls,mlas,smull,smlal,umull,umlal,smlalxy,smlawx"))
  "fmp626_core*2")
 
 (define_insn_reservation "mp626_mult4" 4
  (and (eq_attr "tune" "fmp626")
-      (eq_attr "insn" "smulls,smlals,umulls,umlals"))
+      (eq_attr "type" "smulls,smlals,umulls,umlals"))
  "fmp626_core*3")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Index: b/src/gcc/config/arm/crypto.md
===================================================================
--- /dev/null
+++ b/src/gcc/config/arm/crypto.md
@@ -0,0 +1,86 @@
+;; ARMv8-A crypto patterns.
+;; Copyright (C) 2013-2014 Free Software Foundation, Inc.
+;; Contributed by ARM Ltd.
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published
+;; by the Free Software Foundation; either version 3, or (at your
+;; option) any later version.
+
+;; GCC is distributed in the hope that it will be useful, but WITHOUT
+;; ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+;; or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+;; License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+(define_insn "crypto_<crypto_pattern>"
+  [(set (match_operand:<crypto_mode> 0 "register_operand" "=w")
+        (unspec:<crypto_mode> [(match_operand:<crypto_mode> 1
+                       "register_operand" "w")]
+         CRYPTO_UNARY))]
+  "TARGET_CRYPTO"
+  "<crypto_pattern>.<crypto_size_sfx>\\t%q0, %q1"
+  [(set_attr "type" "<crypto_type>")]
+)
+
+(define_insn "crypto_<crypto_pattern>"
+  [(set (match_operand:<crypto_mode> 0 "register_operand" "=w")
+        (unspec:<crypto_mode> [(match_operand:<crypto_mode> 1 "register_operand" "0")
+                      (match_operand:<crypto_mode> 2 "register_operand" "w")]
+         CRYPTO_BINARY))]
+  "TARGET_CRYPTO"
+  "<crypto_pattern>.<crypto_size_sfx>\\t%q0, %q2"
+  [(set_attr "type" "<crypto_type>")]
+)
+
+(define_insn "crypto_<crypto_pattern>"
+  [(set (match_operand:<crypto_mode> 0 "register_operand" "=w")
+        (unspec:<crypto_mode> [(match_operand:<crypto_mode> 1 "register_operand" "0")
+                      (match_operand:<crypto_mode> 2 "register_operand" "w")
+                      (match_operand:<crypto_mode> 3 "register_operand" "w")]
+         CRYPTO_TERNARY))]
+  "TARGET_CRYPTO"
+  "<crypto_pattern>.<crypto_size_sfx>\\t%q0, %q2, %q3"
+  [(set_attr "type" "<crypto_type>")]
+)
+
+(define_insn "crypto_sha1h"
+  [(set (match_operand:V4SI 0 "register_operand" "=w")
+        (zero_extend:V4SI
+          (unspec:SI [(vec_select:SI
+                        (match_operand:V4SI 1 "register_operand" "w")
+                        (parallel [(match_operand:SI 2 "immediate_operand" "i")]))]
+           UNSPEC_SHA1H)))]
+  "TARGET_CRYPTO"
+  "sha1h.32\\t%q0, %q1"
+  [(set_attr "type" "crypto_sha1_fast")]
+)
+
+(define_insn "crypto_vmullp64"
+  [(set (match_operand:TI 0 "register_operand" "=w")
+        (unspec:TI [(match_operand:DI 1 "register_operand" "w")
+                    (match_operand:DI 2 "register_operand" "w")]
+         UNSPEC_VMULLP64))]
+  "TARGET_CRYPTO"
+  "vmull.p64\\t%q0, %P1, %P2"
+  [(set_attr "type" "neon_mul_d_long")]
+)
+
+(define_insn "crypto_<crypto_pattern>"
+  [(set (match_operand:V4SI 0 "register_operand" "=w")
+        (unspec:<crypto_mode>
+                     [(match_operand:<crypto_mode> 1 "register_operand" "0")
+                      (vec_select:SI
+                        (match_operand:<crypto_mode> 2 "register_operand" "w")
+                        (parallel [(match_operand:SI 4 "immediate_operand" "i")]))
+                      (match_operand:<crypto_mode> 3 "register_operand" "w")]
+         CRYPTO_SELECTING))]
+  "TARGET_CRYPTO"
+  "<crypto_pattern>.<crypto_size_sfx>\\t%q0, %q2, %q3"
+  [(set_attr "type" "<crypto_type>")]
+)
Index: b/src/gcc/config/arm/fa526.md
===================================================================
--- a/src/gcc/config/arm/fa526.md
+++ b/src/gcc/config/arm/fa526.md
@@ -62,12 +62,24 @@
 ;; ALU operations
 (define_insn_reservation "526_alu_op" 1
  (and (eq_attr "tune" "fa526")
-      (eq_attr "type" "alu_reg,simple_alu_imm"))
+      (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                       adr,bfm,rev,\
+                       shift_imm,shift_reg,\
+                       mov_imm,mov_reg,mvn_imm,mvn_reg,\
+                       mrs,multiple,no_insn"))
  "fa526_core")
 
 (define_insn_reservation "526_alu_shift_op" 2
  (and (eq_attr "tune" "fa526")
-      (eq_attr "type" "simple_alu_shift,alu_shift,alu_shift_reg"))
+      (eq_attr "type" "extend,\
+                       alu_shift_imm,alus_shift_imm,\
+                       logic_shift_imm,logics_shift_imm,\
+                       alu_shift_reg,alus_shift_reg,\
+                       logic_shift_reg,logics_shift_reg,\
+                       mov_shift,mov_shift_reg,\
+                       mvn_shift,mvn_shift_reg"))
  "fa526_core")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -76,12 +88,12 @@
 
 (define_insn_reservation "526_mult1" 2
  (and (eq_attr "tune" "fa526")
-      (eq_attr "insn" "smlalxy,smulxy,smlaxy,smlalxy"))
+      (eq_attr "type" "smlalxy,smulxy,smlaxy,smlalxy"))
  "fa526_core")
 
 (define_insn_reservation "526_mult2" 5
  (and (eq_attr "tune" "fa526")
-      (eq_attr "insn" "mul,mla,muls,mlas,umull,umlal,smull,smlal,umulls,\
+      (eq_attr "type" "mul,mla,muls,mlas,umull,umlal,smull,smlal,umulls,\
                        umlals,smulls,smlals,smlawx"))
  "fa526_core*4")
 
Index: b/src/gcc/config/arm/arm-generic.md
===================================================================
--- a/src/gcc/config/arm/arm-generic.md
+++ b/src/gcc/config/arm/arm-generic.md
@@ -114,7 +114,9 @@
 
 (define_insn_reservation "mult" 16
   (and (eq_attr "generic_sched" "yes")
-       (and (eq_attr "ldsched" "no") (eq_attr "type" "mult")))
+       (and (eq_attr "ldsched" "no")
+	    (ior (eq_attr "mul32" "yes")
+		 (eq_attr "mul64" "yes"))))
   "core*16")
 
 (define_insn_reservation "mult_ldsched_strongarm" 3
@@ -122,7 +124,8 @@
        (and (eq_attr "ldsched" "yes") 
 	    (and (eq_attr "tune"
 		  "strongarm,strongarm110,strongarm1100,strongarm1110")
-	         (eq_attr "type" "mult"))))
+		 (ior (eq_attr "mul32" "yes")
+		      (eq_attr "mul64" "yes")))))
   "core*2")
 
 (define_insn_reservation "mult_ldsched" 4
@@ -130,13 +133,17 @@
        (and (eq_attr "ldsched" "yes") 
 	    (and (eq_attr "tune"
 		  "!strongarm,strongarm110,strongarm1100,strongarm1110")
-	         (eq_attr "type" "mult"))))
+	         (ior (eq_attr "mul32" "yes")
+		      (eq_attr "mul64" "yes")))))
   "core*4")
 
 (define_insn_reservation "multi_cycle" 32
   (and (eq_attr "generic_sched" "yes")
        (and (eq_attr "core_cycles" "multi")
-            (eq_attr "type" "!mult,load_byte,load1,load2,load3,load4,store1,store2,store3,store4")))
+            (and (eq_attr "type" "!load_byte,load1,load2,load3,load4,\
+                                  store1,store2,store3,store4")
+		 (not (ior (eq_attr "mul32" "yes")
+			   (eq_attr "mul64" "yes"))))))
   "core*32")
 
 (define_insn_reservation "single_cycle" 1
Index: b/src/gcc/config/arm/vfp11.md
===================================================================
--- a/src/gcc/config/arm/vfp11.md
+++ b/src/gcc/config/arm/vfp11.md
@@ -51,12 +51,13 @@
 
 (define_insn_reservation "vfp_ffarith" 4
  (and (eq_attr "generic_vfp" "yes")
-      (eq_attr "type" "fcpys,ffariths,ffarithd,fcmps,fcmpd"))
+      (eq_attr "type" "fmov,ffariths,ffarithd,fcmps,fcmpd"))
  "fmac")
 
 (define_insn_reservation "vfp_farith" 8
  (and (eq_attr "generic_vfp" "yes")
-      (eq_attr "type" "fadds,faddd,fconsts,fconstd,f_cvt,fmuls,fmacs,ffmas"))
+      (eq_attr "type" "fadds,faddd,fconsts,fconstd,f_cvt,f_cvtf2i,f_cvti2f,\
+                       fmuls,fmacs,ffmas"))
  "fmac")
 
 (define_insn_reservation "vfp_fmul" 9
@@ -66,23 +67,23 @@
 
 (define_insn_reservation "vfp_fdivs" 19
  (and (eq_attr "generic_vfp" "yes")
-      (eq_attr "type" "fdivs"))
+      (eq_attr "type" "fdivs, fsqrts"))
  "ds*15")
 
 (define_insn_reservation "vfp_fdivd" 33
  (and (eq_attr "generic_vfp" "yes")
-      (eq_attr "type" "fdivd"))
+      (eq_attr "type" "fdivd, fsqrtd"))
  "fmac+ds*29")
 
 ;; Moves to/from arm regs also use the load/store pipeline.
 (define_insn_reservation "vfp_fload" 4
  (and (eq_attr "generic_vfp" "yes")
-      (eq_attr "type" "f_loads,f_loadd,r_2_f"))
+      (eq_attr "type" "f_loads,f_loadd,f_mcr,f_mcrr"))
  "vfp_ls")
 
 (define_insn_reservation "vfp_fstore" 4
  (and (eq_attr "generic_vfp" "yes")
-      (eq_attr "type" "f_stores,f_stored,f_2_r"))
+      (eq_attr "type" "f_stores,f_stored,f_mrc,f_mrrc"))
  "vfp_ls")
 
 (define_insn_reservation "vfp_to_cpsr" 4
Index: b/src/gcc/config/arm/neon-docgen.ml
===================================================================
--- a/src/gcc/config/arm/neon-docgen.ml
+++ b/src/gcc/config/arm/neon-docgen.ml
@@ -329,6 +329,85 @@ let gnu_header chan =
   "@c This file is generated automatically using gcc/config/arm/neon-docgen.ml";
   "@c Please do not edit manually."]
 
+let crypto_doc =
+"
+@itemize @bullet
+@item poly128_t vldrq_p128(poly128_t const *)
+@end itemize
+
+@itemize @bullet
+@item void vstrq_p128(poly128_t *, poly128_t)
+@end itemize
+
+@itemize @bullet
+@item uint64x1_t vceq_p64 (poly64x1_t, poly64x1_t)
+@end itemize
+
+@itemize @bullet
+@item uint64x1_t vtst_p64 (poly64x1_t, poly64x1_t)
+@end itemize
+
+@itemize @bullet
+@item uint32_t vsha1h_u32 (uint32_t)
+@*@emph{Form of expected instruction(s):} @code{sha1h.32 @var{q0}, @var{q1}}
+@end itemize
+
+@itemize @bullet
+@item uint32x4_t vsha1cq_u32 (uint32x4_t, uint32_t, uint32x4_t)
+@*@emph{Form of expected instruction(s):} @code{sha1c.32 @var{q0}, @var{q1}, @var{q2}}
+@end itemize
+
+@itemize @bullet
+@item uint32x4_t vsha1pq_u32 (uint32x4_t, uint32_t, uint32x4_t)
+@*@emph{Form of expected instruction(s):} @code{sha1p.32 @var{q0}, @var{q1}, @var{q2}}
+@end itemize
+
+@itemize @bullet
+@item uint32x4_t vsha1mq_u32 (uint32x4_t, uint32_t, uint32x4_t)
+@*@emph{Form of expected instruction(s):} @code{sha1m.32 @var{q0}, @var{q1}, @var{q2}}
+@end itemize
+
+@itemize @bullet
+@item uint32x4_t vsha1su0q_u32 (uint32x4_t, uint32x4_t, uint32x4_t)
+@*@emph{Form of expected instruction(s):} @code{sha1su0.32 @var{q0}, @var{q1}, @var{q2}}
+@end itemize
+
+@itemize @bullet
+@item uint32x4_t vsha1su1q_u32 (uint32x4_t, uint32x4_t)
+@*@emph{Form of expected instruction(s):} @code{sha1su1.32 @var{q0}, @var{q1}, @var{q2}}
+@end itemize
+
+@itemize @bullet
+@item uint32x4_t vsha256hq_u32 (uint32x4_t, uint32x4_t, uint32x4_t)
+@*@emph{Form of expected instruction(s):} @code{sha256h.32 @var{q0}, @var{q1}, @var{q2}}
+@end itemize
+ 
+@itemize @bullet
+@item uint32x4_t vsha256h2q_u32 (uint32x4_t, uint32x4_t, uint32x4_t)
+@*@emph{Form of expected instruction(s):} @code{sha256h2.32 @var{q0}, @var{q1}, @var{q2}}
+@end itemize
+ 
+@itemize @bullet
+@item uint32x4_t vsha256su0q_u32 (uint32x4_t, uint32x4_t)
+@*@emph{Form of expected instruction(s):} @code{sha256su0.32 @var{q0}, @var{q1}}
+@end itemize
+ 
+@itemize @bullet
+@item uint32x4_t vsha256su1q_u32 (uint32x4_t, uint32x4_t, uint32x4_t)
+@*@emph{Form of expected instruction(s):} @code{sha256su1.32 @var{q0}, @var{q1}, @var{q2}}
+@end itemize
+
+@itemize @bullet
+@item poly128_t vmull_p64 (poly64_t a, poly64_t b)
+@*@emph{Form of expected instruction(s):} @code{vmull.p64 @var{q0}, @var{d1}, @var{d2}}
+@end itemize
+
+@itemize @bullet
+@item poly128_t vmull_high_p64 (poly64x2_t a, poly64x2_t b)
+@*@emph{Form of expected instruction(s):} @code{vmull.p64 @var{q0}, @var{d1}, @var{d2}}
+@end itemize
+"
+
 (* Program entry point.  *)
 let _ =
   if Array.length Sys.argv <> 2 then
@@ -339,6 +418,7 @@ let _ =
       let chan = open_out file in
         gnu_header chan;
         List.iter (document_group chan) intrinsic_groups;
+        Printf.fprintf chan "%s\n" crypto_doc;
         close_out chan
     with Sys_error sys ->
       failwith ("Could not create output file " ^ file ^ ": " ^ sys)
Index: b/src/gcc/config/arm/iwmmxt2.md
===================================================================
--- a/src/gcc/config/arm/iwmmxt2.md
+++ b/src/gcc/config/arm/iwmmxt2.md
@@ -24,7 +24,7 @@
   "TARGET_REALLY_IWMMXT"
   "wabs<MMX_char>%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wabs")]
+   (set_attr "type" "wmmx_wabs")]
 )
 
 (define_insn "iwmmxt_wabsdiffb"
@@ -37,7 +37,7 @@
  "TARGET_REALLY_IWMMXT"
  "wabsdiffb%?\\t%0, %1, %2"
  [(set_attr "predicable" "yes")
-  (set_attr "wtype" "wabsdiff")]
+  (set_attr "type" "wmmx_wabsdiff")]
 )
 
 (define_insn "iwmmxt_wabsdiffh"
@@ -50,7 +50,7 @@
   "TARGET_REALLY_IWMMXT"
   "wabsdiffh%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wabsdiff")]
+   (set_attr "type" "wmmx_wabsdiff")]
 )
 
 (define_insn "iwmmxt_wabsdiffw"
@@ -63,7 +63,7 @@
  "TARGET_REALLY_IWMMXT"
  "wabsdiffw%?\\t%0, %1, %2"
  [(set_attr "predicable" "yes")
-  (set_attr "wtype" "wabsdiff")]
+  (set_attr "type" "wmmx_wabsdiff")]
 )
 
 (define_insn "iwmmxt_waddsubhx"
@@ -81,7 +81,7 @@
   "TARGET_REALLY_IWMMXT"
   "waddsubhx%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "waddsubhx")]
+   (set_attr "type" "wmmx_waddsubhx")]
 )
 
 (define_insn "iwmmxt_wsubaddhx"
@@ -99,7 +99,7 @@
   "TARGET_REALLY_IWMMXT"
   "wsubaddhx%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wsubaddhx")]
+   (set_attr "type" "wmmx_wsubaddhx")]
 )
 
 (define_insn "addc<mode>3"
@@ -111,7 +111,7 @@
   "TARGET_REALLY_IWMMXT"
   "wadd<MMX_char>c%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wadd")]
+   (set_attr "type" "wmmx_wadd")]
 )
 
 (define_insn "iwmmxt_avg4"
@@ -143,7 +143,7 @@
   "TARGET_REALLY_IWMMXT"
   "wavg4%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wavg4")]
+   (set_attr "type" "wmmx_wavg4")]
 )
 
 (define_insn "iwmmxt_avg4r"
@@ -175,7 +175,7 @@
   "TARGET_REALLY_IWMMXT"
   "wavg4r%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wavg4")]
+   (set_attr "type" "wmmx_wavg4")]
 )
 
 (define_insn "iwmmxt_wmaddsx"
@@ -194,7 +194,7 @@
  "TARGET_REALLY_IWMMXT"
   "wmaddsx%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-	(set_attr "wtype" "wmadd")]
+	(set_attr "type" "wmmx_wmadd")]
 )
 
 (define_insn "iwmmxt_wmaddux"
@@ -213,7 +213,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmaddux%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmadd")]
+   (set_attr "type" "wmmx_wmadd")]
 )
 
 (define_insn "iwmmxt_wmaddsn"
@@ -232,7 +232,7 @@
  "TARGET_REALLY_IWMMXT"
  "wmaddsn%?\\t%0, %1, %2"
  [(set_attr "predicable" "yes")
-  (set_attr "wtype" "wmadd")]
+  (set_attr "type" "wmmx_wmadd")]
 )
 
 (define_insn "iwmmxt_wmaddun"
@@ -251,7 +251,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmaddun%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmadd")]
+   (set_attr "type" "wmmx_wmadd")]
 )
 
 (define_insn "iwmmxt_wmulwsm"
@@ -265,7 +265,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulwsm%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmulw")]
+   (set_attr "type" "wmmx_wmulw")]
 )
 
 (define_insn "iwmmxt_wmulwum"
@@ -279,7 +279,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulwum%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmulw")]
+   (set_attr "type" "wmmx_wmulw")]
 )
 
 (define_insn "iwmmxt_wmulsmr"
@@ -297,7 +297,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulsmr%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmul")]
+   (set_attr "type" "wmmx_wmul")]
 )
 
 (define_insn "iwmmxt_wmulumr"
@@ -316,7 +316,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulumr%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmul")]
+   (set_attr "type" "wmmx_wmul")]
 )
 
 (define_insn "iwmmxt_wmulwsmr"
@@ -333,7 +333,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulwsmr%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmul")]
+   (set_attr "type" "wmmx_wmul")]
 )
 
 (define_insn "iwmmxt_wmulwumr"
@@ -350,7 +350,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulwumr%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmulw")]
+   (set_attr "type" "wmmx_wmulw")]
 )
 
 (define_insn "iwmmxt_wmulwl"
@@ -361,7 +361,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmulwl%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmulw")]
+   (set_attr "type" "wmmx_wmulw")]
 )
 
 (define_insn "iwmmxt_wqmulm"
@@ -371,7 +371,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmulm%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmulm")]
+   (set_attr "type" "wmmx_wqmulm")]
 )
 
 (define_insn "iwmmxt_wqmulwm"
@@ -381,7 +381,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmulwm%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmulwm")]
+   (set_attr "type" "wmmx_wqmulwm")]
 )
 
 (define_insn "iwmmxt_wqmulmr"
@@ -391,7 +391,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmulmr%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmulm")]
+   (set_attr "type" "wmmx_wqmulm")]
 )
 
 (define_insn "iwmmxt_wqmulwmr"
@@ -401,7 +401,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmulwmr%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmulwm")]
+   (set_attr "type" "wmmx_wqmulwm")]
 )
 
 (define_insn "iwmmxt_waddbhusm"
@@ -417,7 +417,7 @@
   "TARGET_REALLY_IWMMXT"
   "waddbhusm%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "waddbhus")]
+   (set_attr "type" "wmmx_waddbhus")]
 )
 
 (define_insn "iwmmxt_waddbhusl"
@@ -433,7 +433,7 @@
   "TARGET_REALLY_IWMMXT"
   "waddbhusl%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "waddbhus")]
+   (set_attr "type" "wmmx_waddbhus")]
 )
 
 (define_insn "iwmmxt_wqmiabb"
@@ -446,7 +446,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmiabb%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmiaxy")]
+   (set_attr "type" "wmmx_wqmiaxy")]
 )
 
 (define_insn "iwmmxt_wqmiabt"
@@ -459,7 +459,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmiabt%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmiaxy")]
+   (set_attr "type" "wmmx_wqmiaxy")]
 )
 
 (define_insn "iwmmxt_wqmiatb"
@@ -472,7 +472,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmiatb%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmiaxy")]
+   (set_attr "type" "wmmx_wqmiaxy")]
 )
 
 (define_insn "iwmmxt_wqmiatt"
@@ -485,7 +485,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmiatt%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmiaxy")]
+   (set_attr "type" "wmmx_wqmiaxy")]
 )
 
 (define_insn "iwmmxt_wqmiabbn"
@@ -498,7 +498,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmiabbn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmiaxy")]
+   (set_attr "type" "wmmx_wqmiaxy")]
 )
 
 (define_insn "iwmmxt_wqmiabtn"
@@ -511,7 +511,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmiabtn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmiaxy")]
+   (set_attr "type" "wmmx_wqmiaxy")]
 )
 
 (define_insn "iwmmxt_wqmiatbn"
@@ -524,7 +524,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmiatbn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmiaxy")]
+   (set_attr "type" "wmmx_wqmiaxy")]
 )
 
 (define_insn "iwmmxt_wqmiattn"
@@ -537,7 +537,7 @@
   "TARGET_REALLY_IWMMXT"
   "wqmiattn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wqmiaxy")]
+   (set_attr "type" "wmmx_wqmiaxy")]
 )
 
 (define_insn "iwmmxt_wmiabb"
@@ -561,7 +561,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiabb%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiaxy")]
+   (set_attr "type" "wmmx_wmiaxy")]
 )
 
 (define_insn "iwmmxt_wmiabt"
@@ -585,7 +585,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiabt%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiaxy")]
+   (set_attr "type" "wmmx_wmiaxy")]
 )
 
 (define_insn "iwmmxt_wmiatb"
@@ -609,7 +609,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiatb%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiaxy")]
+   (set_attr "type" "wmmx_wmiaxy")]
 )
 
 (define_insn "iwmmxt_wmiatt"
@@ -633,7 +633,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiatt%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiaxy")]
+   (set_attr "type" "wmmx_wmiaxy")]
 )
 
 (define_insn "iwmmxt_wmiabbn"
@@ -657,7 +657,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiabbn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiaxy")]
+   (set_attr "type" "wmmx_wmiaxy")]
 )
 
 (define_insn "iwmmxt_wmiabtn"
@@ -681,7 +681,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiabtn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiaxy")]
+   (set_attr "type" "wmmx_wmiaxy")]
 )
 
 (define_insn "iwmmxt_wmiatbn"
@@ -705,7 +705,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiatbn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiaxy")]
+   (set_attr "type" "wmmx_wmiaxy")]
 )
 
 (define_insn "iwmmxt_wmiattn"
@@ -729,7 +729,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiattn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiaxy")]
+   (set_attr "type" "wmmx_wmiaxy")]
 )
 
 (define_insn "iwmmxt_wmiawbb"
@@ -742,7 +742,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiawbb%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiawxy")]
+   (set_attr "type" "wmmx_wmiawxy")]
 )
 
 (define_insn "iwmmxt_wmiawbt"
@@ -755,7 +755,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiawbt%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiawxy")]
+   (set_attr "type" "wmmx_wmiawxy")]
 )
 
 (define_insn "iwmmxt_wmiawtb"
@@ -768,7 +768,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiawtb%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiawxy")]
+   (set_attr "type" "wmmx_wmiawxy")]
 )
 
 (define_insn "iwmmxt_wmiawtt"
@@ -781,7 +781,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiawtt%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiawxy")]
+   (set_attr "type" "wmmx_wmiawxy")]
 )
 
 (define_insn "iwmmxt_wmiawbbn"
@@ -794,7 +794,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiawbbn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiawxy")]
+   (set_attr "type" "wmmx_wmiawxy")]
 )
 
 (define_insn "iwmmxt_wmiawbtn"
@@ -807,7 +807,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiawbtn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiawxy")]
+   (set_attr "type" "wmmx_wmiawxy")]
 )
 
 (define_insn "iwmmxt_wmiawtbn"
@@ -820,7 +820,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiawtbn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiawxy")]
+   (set_attr "type" "wmmx_wmiawxy")]
 )
 
 (define_insn "iwmmxt_wmiawttn"
@@ -833,7 +833,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmiawttn%?\\t%0, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmiawxy")]
+   (set_attr "type" "wmmx_wmiawxy")]
 )
 
 (define_insn "iwmmxt_wmerge"
@@ -858,7 +858,7 @@
   "TARGET_REALLY_IWMMXT"
   "wmerge%?\\t%0, %1, %2, %3"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "wmerge")]
+   (set_attr "type" "wmmx_wmerge")]
 )
 
 (define_insn "iwmmxt_tandc<mode>3"
@@ -868,7 +868,7 @@
   "TARGET_REALLY_IWMMXT"
   "tandc<MMX_char>%?\\t r15"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "tandc")]
+   (set_attr "type" "wmmx_tandc")]
 )
 
 (define_insn "iwmmxt_torc<mode>3"
@@ -878,7 +878,7 @@
   "TARGET_REALLY_IWMMXT"
   "torc<MMX_char>%?\\t r15"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "torc")]
+   (set_attr "type" "wmmx_torc")]
 )
 
 (define_insn "iwmmxt_torvsc<mode>3"
@@ -888,7 +888,7 @@
   "TARGET_REALLY_IWMMXT"
   "torvsc<MMX_char>%?\\t r15"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "torvsc")]
+   (set_attr "type" "wmmx_torvsc")]
 )
 
 (define_insn "iwmmxt_textrc<mode>3"
@@ -899,5 +899,5 @@
   "TARGET_REALLY_IWMMXT"
   "textrc<MMX_char>%?\\t r15, %0"
   [(set_attr "predicable" "yes")
-   (set_attr "wtype" "textrc")]
+   (set_attr "type" "wmmx_textrc")]
 )
Index: b/src/gcc/config/arm/cortex-a5.md
===================================================================
--- a/src/gcc/config/arm/cortex-a5.md
+++ b/src/gcc/config/arm/cortex-a5.md
@@ -58,12 +58,24 @@
 
 (define_insn_reservation "cortex_a5_alu" 2
   (and (eq_attr "tune" "cortexa5")
-       (eq_attr "type" "alu_reg,simple_alu_imm"))
+       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        adr,bfm,rev,\
+                        shift_imm,shift_reg,\
+                        mov_imm,mov_reg,mvn_imm,mvn_reg,\
+                        mrs,multiple,no_insn"))
   "cortex_a5_ex1")
 
 (define_insn_reservation "cortex_a5_alu_shift" 2
   (and (eq_attr "tune" "cortexa5")
-       (eq_attr "type" "simple_alu_shift,alu_shift,alu_shift_reg"))
+       (eq_attr "type" "extend,\
+                        alu_shift_imm,alus_shift_imm,\
+                        logic_shift_imm,logics_shift_imm,\
+                        alu_shift_reg,alus_shift_reg,\
+                        logic_shift_reg,logics_shift_reg,\
+                        mov_shift,mov_shift_reg,\
+                        mvn_shift,mvn_shift_reg"))
   "cortex_a5_ex1")
 
 ;; Forwarding path for unshifted operands.
@@ -80,7 +92,8 @@
 
 (define_insn_reservation "cortex_a5_mul" 2
   (and (eq_attr "tune" "cortexa5")
-       (eq_attr "type" "mult"))
+       (ior (eq_attr "mul32" "yes")
+	    (eq_attr "mul64" "yes")))
   "cortex_a5_ex1")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -155,7 +168,8 @@
 
 (define_insn_reservation "cortex_a5_fpalu" 4
   (and (eq_attr "tune" "cortexa5")
-       (eq_attr "type" "ffariths, fadds, ffarithd, faddd, fcpys, fmuls, f_cvt,\
+       (eq_attr "type" "ffariths, fadds, ffarithd, faddd, fmov, fmuls,\
+                        f_cvt,f_cvtf2i,f_cvti2f,\
 			fcmps, fcmpd"))
   "cortex_a5_ex1+cortex_a5_fpadd_pipe")
 
@@ -219,14 +233,14 @@
 
 (define_insn_reservation "cortex_a5_fdivs" 14
   (and (eq_attr "tune" "cortexa5")
-       (eq_attr "type" "fdivs"))
+       (eq_attr "type" "fdivs, fsqrts"))
   "cortex_a5_ex1, cortex_a5_fp_div_sqrt * 13")
 
 ;; ??? Similarly for fdivd.
 
 (define_insn_reservation "cortex_a5_fdivd" 29
   (and (eq_attr "tune" "cortexa5")
-       (eq_attr "type" "fdivd"))
+       (eq_attr "type" "fdivd, fsqrtd"))
   "cortex_a5_ex1, cortex_a5_fp_div_sqrt * 28")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -239,12 +253,12 @@
 
 (define_insn_reservation "cortex_a5_r2f" 4
   (and (eq_attr "tune" "cortexa5")
-       (eq_attr "type" "r_2_f"))
+       (eq_attr "type" "f_mcr,f_mcrr"))
   "cortex_a5_ex1")
 
 (define_insn_reservation "cortex_a5_f2r" 2
   (and (eq_attr "tune" "cortexa5")
-       (eq_attr "type" "f_2_r"))
+       (eq_attr "type" "f_mrc,f_mrrc"))
   "cortex_a5_ex1")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Index: b/src/gcc/config/arm/cortex-m4-fpu.md
===================================================================
--- a/src/gcc/config/arm/cortex-m4-fpu.md
+++ b/src/gcc/config/arm/cortex-m4-fpu.md
@@ -26,17 +26,17 @@
 ;; Integer instructions following VDIV or VSQRT complete out-of-order.
 (define_insn_reservation "cortex_m4_fdivs" 15
   (and (eq_attr "tune" "cortexm4")
-       (eq_attr "type" "fdivs"))
+       (eq_attr "type" "fdivs, fsqrts"))
   "cortex_m4_ex_v,cortex_m4_v*13")
 
 (define_insn_reservation "cortex_m4_vmov_1" 1
   (and (eq_attr "tune" "cortexm4")
-       (eq_attr "type" "fcpys,fconsts"))
+       (eq_attr "type" "fmov,fconsts"))
   "cortex_m4_ex_v")
 
 (define_insn_reservation "cortex_m4_vmov_2" 2
   (and (eq_attr "tune" "cortexm4")
-       (eq_attr "type" "f_2_r,r_2_f"))
+       (eq_attr "type" "f_mrc,f_mrrc,f_mcr,f_mcrr"))
   "cortex_m4_ex_v*2")
 
 (define_insn_reservation "cortex_m4_fmuls" 2
@@ -71,7 +71,7 @@
 
 (define_insn_reservation "cortex_m4_f_cvt" 2
   (and (eq_attr "tune" "cortexm4")
-       (eq_attr "type" "f_cvt"))
+       (eq_attr "type" "f_cvt,f_cvtf2i,f_cvti2f"))
   "cortex_m4_ex_v")
 
 (define_insn_reservation "cortex_m4_f_load" 2
Index: b/src/gcc/config/arm/fa606te.md
===================================================================
--- a/src/gcc/config/arm/fa606te.md
+++ b/src/gcc/config/arm/fa606te.md
@@ -62,7 +62,18 @@
 ;; ALU operations
 (define_insn_reservation "606te_alu_op" 1
  (and (eq_attr "tune" "fa606te")
-      (eq_attr "type" "alu_reg,simple_alu_imm,simple_alu_shift,alu_shift,alu_shift_reg"))
+      (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                       adr,bfm,rev,\
+                       shift_imm,shift_reg,extend,\
+                       alu_shift_imm,alus_shift_imm,\
+                       logic_shift_imm,logics_shift_imm,\
+                       alu_shift_reg,alus_shift_reg,\
+                       logic_shift_reg,logics_shift_reg,\
+                       mov_imm,mov_reg,mov_shift,mov_shift_reg,\
+                       mvn_imm,mvn_reg,mvn_shift,mvn_shift_reg,\
+                       mrs,multiple,no_insn"))
  "fa606te_core")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -71,22 +82,22 @@
 
 (define_insn_reservation "606te_mult1" 2
  (and (eq_attr "tune" "fa606te")
-      (eq_attr "insn" "smlalxy"))
+      (eq_attr "type" "smlalxy"))
  "fa606te_core")
 
 (define_insn_reservation "606te_mult2" 3
  (and (eq_attr "tune" "fa606te")
-      (eq_attr "insn" "smlaxy,smulxy,smulwy,smlawy"))
+      (eq_attr "type" "smlaxy,smulxy,smulwy,smlawy"))
  "fa606te_core*2")
 
 (define_insn_reservation "606te_mult3" 4
  (and (eq_attr "tune" "fa606te")
-      (eq_attr "insn" "mul,mla,muls,mlas"))
+      (eq_attr "type" "mul,mla,muls,mlas"))
  "fa606te_core*3")
 
 (define_insn_reservation "606te_mult4" 5
  (and (eq_attr "tune" "fa606te")
-      (eq_attr "insn" "umull,umlal,smull,smlal,umulls,umlals,smulls,smlals"))
+      (eq_attr "type" "umull,umlal,smull,smlal,umulls,umlals,smulls,smlals"))
  "fa606te_core*4")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Index: b/src/gcc/config/arm/cortex-a9.md
===================================================================
--- a/src/gcc/config/arm/cortex-a9.md
+++ b/src/gcc/config/arm/cortex-a9.md
@@ -80,18 +80,24 @@ cortex_a9_p1_e2 + cortex_a9_p0_e1 + cort
 ;; which can go down E2 without any problem.
 (define_insn_reservation "cortex_a9_dp" 2
   (and (eq_attr "tune" "cortexa9")
-         (ior (and (eq_attr "type" "alu_reg,simple_alu_imm")
-                        (eq_attr "neon_type" "none"))
-	      (and (and (eq_attr "type" "alu_shift_reg, simple_alu_shift,alu_shift")
-			(eq_attr "insn" "mov"))
-                 (eq_attr "neon_type" "none"))))
+       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        adr,bfm,rev,\
+                        shift_imm,shift_reg,\
+                        mov_imm,mov_reg,mvn_imm,mvn_reg,\
+                        mov_shift_reg,mov_shift,\
+                        mrs,multiple,no_insn"))
   "cortex_a9_p0_default|cortex_a9_p1_default")
 
 ;; An instruction using the shifter will go down E1.
 (define_insn_reservation "cortex_a9_dp_shift" 3
    (and (eq_attr "tune" "cortexa9")
-	(and (eq_attr "type" "alu_shift_reg, simple_alu_shift,alu_shift")
-	     (not (eq_attr "insn" "mov"))))
+        (eq_attr "type" "alu_shift_imm,alus_shift_imm,\
+                         logic_shift_imm,logics_shift_imm,\
+                         alu_shift_reg,alus_shift_reg,\
+                         logic_shift_reg,logics_shift_reg,\
+                         extend,mvn_shift,mvn_shift_reg"))
    "cortex_a9_p0_shift | cortex_a9_p1_shift")
 
 ;; Loads have a latency of 4 cycles.
@@ -130,29 +136,29 @@ cortex_a9_p1_e2 + cortex_a9_p0_e1 + cort
 ;; We get 16*16 multiply / mac results in 3 cycles.
 (define_insn_reservation "cortex_a9_mult16" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "insn" "smulxy"))
+       (eq_attr "type" "smulxy"))
        "cortex_a9_mult16")
 
 ;; The 16*16 mac is slightly different that it
 ;; reserves M1 and M2 in the same cycle.
 (define_insn_reservation "cortex_a9_mac16" 3
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "insn" "smlaxy"))
+       (eq_attr "type" "smlaxy"))
   "cortex_a9_mac16")
 
 (define_insn_reservation "cortex_a9_multiply" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "insn" "mul,smmul,smmulr"))
+       (eq_attr "type" "mul,smmul,smmulr"))
        "cortex_a9_mult")
 
 (define_insn_reservation "cortex_a9_mac" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "insn" "mla,smmla"))
+       (eq_attr "type" "mla,smmla"))
        "cortex_a9_mac")
 
 (define_insn_reservation "cortex_a9_multiply_long" 5
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "insn" "smull,umull,smulls,umulls,smlal,smlals,umlal,umlals"))
+       (eq_attr "type" "smull,umull,smulls,umulls,smlal,smlals,umlal,umlals"))
        "cortex_a9_mult_long")
 
 ;; An instruction with a result in E2 can be forwarded
@@ -201,7 +207,7 @@ cortex_a9_store3_4, cortex_a9_store1_2,
 ;; Pipelining for VFP instructions.
 ;; Issue happens either along load store unit or the VFP / Neon unit.
 ;; Pipeline   Instruction Classification.
-;; FPS - fcpys, ffariths, ffarithd,r_2_f,f_2_r
+;; FPS - fmov, ffariths, ffarithd,f_mcr,f_mcrr,f_mrc,f_mrrc
 ;; FP_ADD   - fadds, faddd, fcmps (1)
 ;; FPMUL   - fmul{s,d}, fmac{s,d}, ffma{s,d}
 ;; FPDIV - fdiv{s,d}
@@ -214,7 +220,8 @@ cortex_a9_store3_4, cortex_a9_store1_2,
 ;; fmrs, fmrrd, fmstat and fmrx - The data is available after 1 cycle.
 (define_insn_reservation "cortex_a9_fps" 2
  (and (eq_attr "tune" "cortexa9")
-      (eq_attr "type" "fcpys, fconsts, fconstd, ffariths, ffarithd, r_2_f, f_2_r, f_flag"))
+      (eq_attr "type" "fmov, fconsts, fconstd, ffariths, ffarithd,\
+                       f_mcr, f_mcrr, f_mrc, f_mrrc, f_flag"))
  "ca9_issue_vfp_neon + ca9fps")
 
 (define_bypass 1
@@ -226,7 +233,7 @@ cortex_a9_store3_4, cortex_a9_store1_2,
 
 (define_insn_reservation "cortex_a9_fadd" 4
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "type" "fadds, faddd, f_cvt"))
+       (eq_attr "type" "fadds, faddd, f_cvt, f_cvtf2i, f_cvti2f"))
   "ca9fp_add")
 
 (define_insn_reservation "cortex_a9_fcmp" 1
@@ -264,12 +271,12 @@ cortex_a9_store3_4, cortex_a9_store1_2,
 ;; Division pipeline description.
 (define_insn_reservation "cortex_a9_fdivs" 15
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "type" "fdivs"))
+       (eq_attr "type" "fdivs, fsqrts"))
   "ca9fp_ds1 + ca9_issue_vfp_neon, nothing*14")
 
 (define_insn_reservation "cortex_a9_fdivd" 25
   (and (eq_attr "tune" "cortexa9")
-       (eq_attr "type" "fdivd"))
+       (eq_attr "type" "fdivd, fsqrtd"))
   "ca9fp_ds1 + ca9_issue_vfp_neon, nothing*24")
 
 ;; Include Neon pipeline description
Index: b/src/gcc/config/arm/fa626te.md
===================================================================
--- a/src/gcc/config/arm/fa626te.md
+++ b/src/gcc/config/arm/fa626te.md
@@ -68,12 +68,24 @@
 ;; ALU operations
 (define_insn_reservation "626te_alu_op" 1
  (and (eq_attr "tune" "fa626,fa626te")
-      (eq_attr "type" "alu_reg,simple_alu_imm"))
+      (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                       adr,bfm,rev,\
+                       shift_imm,shift_reg,\
+                       mov_imm,mov_reg,mvn_imm,mvn_reg,\
+                       mrs,multiple,no_insn"))
  "fa626te_core")
 
 (define_insn_reservation "626te_alu_shift_op" 2
  (and (eq_attr "tune" "fa626,fa626te")
-      (eq_attr "type" "simple_alu_shift,alu_shift,alu_shift_reg"))
+      (eq_attr "type" "extend,\
+                       alu_shift_imm,alus_shift_imm,\
+                       logic_shift_imm,logics_shift_imm,\
+                       alu_shift_reg,alus_shift_reg,\
+                       logic_shift_reg,logics_shift_reg,\
+                       mov_shift,mov_shift_reg,\
+                       mvn_shift,mvn_shift_reg"))
  "fa626te_core")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
@@ -82,22 +94,22 @@
 
 (define_insn_reservation "626te_mult1" 2
  (and (eq_attr "tune" "fa626,fa626te")
-      (eq_attr "insn" "smulwy,smlawy,smulxy,smlaxy"))
+      (eq_attr "type" "smulwy,smlawy,smulxy,smlaxy"))
  "fa626te_core")
 
 (define_insn_reservation "626te_mult2" 2
  (and (eq_attr "tune" "fa626,fa626te")
-      (eq_attr "insn" "mul,mla"))
+      (eq_attr "type" "mul,mla"))
  "fa626te_core")
 
 (define_insn_reservation "626te_mult3" 3
  (and (eq_attr "tune" "fa626,fa626te")
-      (eq_attr "insn" "muls,mlas,smull,smlal,umull,umlal,smlalxy,smlawx"))
+      (eq_attr "type" "muls,mlas,smull,smlal,umull,umlal,smlalxy,smlawx"))
  "fa626te_core*2")
 
 (define_insn_reservation "626te_mult4" 4
  (and (eq_attr "tune" "fa626,fa626te")
-      (eq_attr "insn" "smulls,smlals,umulls,umlals"))
+      (eq_attr "type" "smulls,smlals,umulls,umlals"))
  "fa626te_core*3")
 
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Index: b/src/gcc/config/arm/neon-gen.ml
===================================================================
--- a/src/gcc/config/arm/neon-gen.ml
+++ b/src/gcc/config/arm/neon-gen.ml
@@ -114,6 +114,7 @@ let rec signed_ctype = function
   | T_uint32x4 -> T_int32x4
   | T_uint64x1 -> T_int64x1
   | T_uint64x2 -> T_int64x2
+  | T_poly64x2 -> T_int64x2
   (* Cast to types defined by mode in arm.c, not random types pulled in from
      the <stdint.h> header in use. This fixes incompatible pointer errors when
      compiling with C++.  *)
@@ -121,9 +122,12 @@ let rec signed_ctype = function
   | T_uint16 | T_int16 -> T_intHI
   | T_uint32 | T_int32 -> T_intSI
   | T_uint64 | T_int64 -> T_intDI
+  | T_float16 -> T_floatHF
   | T_float32 -> T_floatSF
   | T_poly8 -> T_intQI
   | T_poly16 -> T_intHI
+  | T_poly64 -> T_intDI
+  | T_poly128 -> T_intTI
   | T_arrayof (n, elt) -> T_arrayof (n, signed_ctype elt)
   | T_ptrto elt -> T_ptrto (signed_ctype elt)
   | T_const elt -> T_const (signed_ctype elt)
@@ -275,8 +279,8 @@ let rec mode_suffix elttype shape =
     let mode = mode_of_elt elttype shape in
     string_of_mode mode
   with MixedMode (dst, src) ->
-    let dstmode = mode_of_elt dst shape
-    and srcmode = mode_of_elt src shape in
+    let dstmode = mode_of_elt ~argpos:0 dst shape
+    and srcmode = mode_of_elt ~argpos:1 src shape in
     string_of_mode dstmode ^ string_of_mode srcmode
 
 let get_shuffle features =
@@ -291,19 +295,24 @@ let print_feature_test_start features =
     match List.find (fun feature ->
                        match feature with Requires_feature _ -> true
                                         | Requires_arch _ -> true
+                                        | Requires_FP_bit _ -> true
                                         | _ -> false)
                      features with
-      Requires_feature feature -> 
+      Requires_feature feature ->
         Format.printf "#ifdef __ARM_FEATURE_%s@\n" feature
     | Requires_arch arch ->
         Format.printf "#if __ARM_ARCH >= %d@\n" arch
+    | Requires_FP_bit bit ->
+        Format.printf "#if ((__ARM_FP & 0x%X) != 0)@\n"
+                      (1 lsl bit)
     | _ -> assert false
   with Not_found -> assert true
 
 let print_feature_test_end features =
   let feature =
-    List.exists (function Requires_feature x -> true
-                          | Requires_arch x -> true
+    List.exists (function Requires_feature _ -> true
+                          | Requires_arch _ -> true
+                          | Requires_FP_bit _ -> true
                           |  _ -> false) features in
   if feature then Format.printf "#endif@\n"
 
@@ -356,79 +365,96 @@ let print_ops ops =
      abase : "ARM" base name for the type (i.e. int in int8x8_t).
      esize : element size.
      enum : element count.
+     alevel: architecture level at which available.
 *)
 
+type fpulevel = CRYPTO | ALL
+
 let deftypes () =
   let typeinfo = [
     (* Doubleword vector types.  *)
-    "__builtin_neon_qi", "int", 8, 8;
-    "__builtin_neon_hi", "int", 16, 4;
-    "__builtin_neon_si", "int", 32, 2;
-    "__builtin_neon_di", "int", 64, 1;
-    "__builtin_neon_sf", "float", 32, 2;
-    "__builtin_neon_poly8", "poly", 8, 8;
-    "__builtin_neon_poly16", "poly", 16, 4;
-    "__builtin_neon_uqi", "uint", 8, 8;
-    "__builtin_neon_uhi", "uint", 16, 4;
-    "__builtin_neon_usi", "uint", 32, 2;
-    "__builtin_neon_udi", "uint", 64, 1;
+    "__builtin_neon_qi", "int", 8, 8, ALL;
+    "__builtin_neon_hi", "int", 16, 4, ALL;
+    "__builtin_neon_si", "int", 32, 2, ALL;
+    "__builtin_neon_di", "int", 64, 1, ALL;
+    "__builtin_neon_hf", "float", 16, 4, ALL;
+    "__builtin_neon_sf", "float", 32, 2, ALL;
+    "__builtin_neon_poly8", "poly", 8, 8, ALL;
+    "__builtin_neon_poly16", "poly", 16, 4, ALL;
+    "__builtin_neon_poly64", "poly", 64, 1, CRYPTO;
+    "__builtin_neon_uqi", "uint", 8, 8, ALL;
+    "__builtin_neon_uhi", "uint", 16, 4, ALL;
+    "__builtin_neon_usi", "uint", 32, 2, ALL;
+    "__builtin_neon_udi", "uint", 64, 1, ALL;
 
     (* Quadword vector types.  *)
-    "__builtin_neon_qi", "int", 8, 16;
-    "__builtin_neon_hi", "int", 16, 8;
-    "__builtin_neon_si", "int", 32, 4;
-    "__builtin_neon_di", "int", 64, 2;
-    "__builtin_neon_sf", "float", 32, 4;
-    "__builtin_neon_poly8", "poly", 8, 16;
-    "__builtin_neon_poly16", "poly", 16, 8;
-    "__builtin_neon_uqi", "uint", 8, 16;
-    "__builtin_neon_uhi", "uint", 16, 8;
-    "__builtin_neon_usi", "uint", 32, 4;
-    "__builtin_neon_udi", "uint", 64, 2
+    "__builtin_neon_qi", "int", 8, 16, ALL;
+    "__builtin_neon_hi", "int", 16, 8, ALL;
+    "__builtin_neon_si", "int", 32, 4, ALL;
+    "__builtin_neon_di", "int", 64, 2, ALL;
+    "__builtin_neon_sf", "float", 32, 4, ALL;
+    "__builtin_neon_poly8", "poly", 8, 16, ALL;
+    "__builtin_neon_poly16", "poly", 16, 8, ALL;
+    "__builtin_neon_poly64", "poly", 64, 2, CRYPTO;
+    "__builtin_neon_uqi", "uint", 8, 16, ALL;
+    "__builtin_neon_uhi", "uint", 16, 8, ALL;
+    "__builtin_neon_usi", "uint", 32, 4, ALL;
+    "__builtin_neon_udi", "uint", 64, 2, ALL
   ] in
   List.iter
-    (fun (cbase, abase, esize, enum) ->
+    (fun (cbase, abase, esize, enum, fpulevel) ->
       let attr =
         match enum with
           1 -> ""
         | _ -> Printf.sprintf "\t__attribute__ ((__vector_size__ (%d)))"
                               (esize * enum / 8) in
-      Format.printf "typedef %s %s%dx%d_t%s;@\n" cbase abase esize enum attr)
+      if fpulevel == CRYPTO then
+        Format.printf "#ifdef __ARM_FEATURE_CRYPTO\n";
+      Format.printf "typedef %s %s%dx%d_t%s;@\n" cbase abase esize enum attr;
+      if fpulevel == CRYPTO then
+        Format.printf "#endif\n";)
     typeinfo;
   Format.print_newline ();
   (* Extra types not in <stdint.h>.  *)
   Format.printf "typedef float float32_t;\n";
   Format.printf "typedef __builtin_neon_poly8 poly8_t;\n";
-  Format.printf "typedef __builtin_neon_poly16 poly16_t;\n"
-
-(* Output structs containing arrays, for load & store instructions etc.  *)
+  Format.printf "typedef __builtin_neon_poly16 poly16_t;\n";
+  Format.printf "#ifdef __ARM_FEATURE_CRYPTO\n";
+  Format.printf "typedef __builtin_neon_poly64 poly64_t;\n";
+  Format.printf "typedef __builtin_neon_poly128 poly128_t;\n";
+  Format.printf "#endif\n"
+
+(* Output structs containing arrays, for load & store instructions etc.
+   poly128_t is deliberately not included here because it has no array types
+   defined for it.  *)
 
 let arrtypes () =
   let typeinfo = [
-    "int", 8;    "int", 16;
-    "int", 32;   "int", 64;
-    "uint", 8;   "uint", 16;
-    "uint", 32;  "uint", 64;
-    "float", 32; "poly", 8;
-    "poly", 16
+    "int", 8, ALL;    "int", 16, ALL;
+    "int", 32, ALL;   "int", 64, ALL;
+    "uint", 8, ALL;   "uint", 16, ALL;
+    "uint", 32, ALL;  "uint", 64, ALL;
+    "float", 32, ALL; "poly", 8, ALL;
+    "poly", 16, ALL; "poly", 64, CRYPTO
   ] in
-  let writestruct elname elsize regsize arrsize =
+  let writestruct elname elsize regsize arrsize fpulevel =
     let elnum = regsize / elsize in
     let structname =
       Printf.sprintf "%s%dx%dx%d_t" elname elsize elnum arrsize in
     let sfmt = start_function () in
-    Format.printf "typedef struct %s" structname;
+    Format.printf "%stypedef struct %s"
+      (if fpulevel == CRYPTO then "#ifdef __ARM_FEATURE_CRYPTO\n" else "") structname;
     open_braceblock sfmt;
     Format.printf "%s%dx%d_t val[%d];" elname elsize elnum arrsize;
     close_braceblock sfmt;
-    Format.printf " %s;" structname;
+    Format.printf " %s;%s" structname (if fpulevel == CRYPTO then "\n#endif\n" else "");
     end_function sfmt;
   in
     for n = 2 to 4 do
       List.iter
-        (fun (elname, elsize) ->
-          writestruct elname elsize 64 n;
-          writestruct elname elsize 128 n)
+        (fun (elname, elsize, alevel) ->
+          writestruct elname elsize 64 n alevel;
+          writestruct elname elsize 128 n alevel)
         typeinfo
     done
 
@@ -484,6 +510,8 @@ let _ =
   print_ops ops;
   Format.print_newline ();
   print_ops reinterp;
+  print_ops reinterpq;
+  Format.printf "%s" crypto_intrinsics;
   print_lines [
 "#ifdef __cplusplus";
 "}";
Index: b/src/gcc/config/mips/linux-common.h
===================================================================
--- a/src/gcc/config/mips/linux-common.h
+++ b/src/gcc/config/mips/linux-common.h
@@ -44,7 +44,7 @@ along with GCC; see the file COPYING3.
 #undef  LIB_SPEC
 #define LIB_SPEC							\
   LINUX_OR_ANDROID_LD (GNU_USER_TARGET_LIB_SPEC,			\
-		       GNU_USER_TARGET_LIB_SPEC " " ANDROID_LIB_SPEC)
+		    GNU_USER_TARGET_NO_PTHREADS_LIB_SPEC " " ANDROID_LIB_SPEC)
 
 #undef  STARTFILE_SPEC
 #define STARTFILE_SPEC							\
Index: b/src/gcc/tree-vect-slp.c
===================================================================
--- a/src/gcc/tree-vect-slp.c
+++ b/src/gcc/tree-vect-slp.c
@@ -2260,7 +2260,7 @@ vect_slp_analyze_bb_1 (basic_block bb)
     }
 
   /* Cost model: check if the vectorization is worthwhile.  */
-  if (flag_vect_cost_model
+  if (!unlimited_cost_model ()
       && !vect_bb_vectorization_profitable_p (bb_vinfo))
     {
       if (dump_enabled_p ())
Index: b/src/gcc/params.def
===================================================================
--- a/src/gcc/params.def
+++ b/src/gcc/params.def
@@ -544,6 +544,11 @@ DEFPARAM(PARAM_VECT_MAX_VERSION_FOR_ALIA
          "Bound on number of runtime checks inserted by the vectorizer's loop versioning for alias check",
          10, 0, 0)
 
+DEFPARAM(PARAM_VECT_MAX_PEELING_FOR_ALIGNMENT,
+         "vect-max-peeling-for-alignment",
+         "Max number of loop peels to enhancement alignment of data references in a loop",
+         -1, -1, 64)
+
 DEFPARAM(PARAM_MAX_CSELIB_MEMORY_LOCATIONS,
 	 "max-cselib-memory-locations",
 	 "The maximum memory locations recorded by cselib",
Index: b/src/libobjc/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libobjc/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libgfortran/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libgfortran/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libada/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libada/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libffi/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libffi/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libssp/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libssp/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libcpp/configure
===================================================================
--- a/src/libcpp/configure
+++ b/src/libcpp/configure
@@ -7152,9 +7152,7 @@ fi
 case $target in
 	aarch64*-*-* | \
 	alpha*-*-* | \
-	arm*-*-*eabi* | \
-	arm*-*-rtems* | \
-	arm*-*-symbianelf* | \
+	arm*-*-* | \
 	x86_64-*-* | \
 	ia64-*-* | \
 	hppa*64*-*-* | \
Index: b/src/libcpp/configure.ac
===================================================================
--- a/src/libcpp/configure.ac
+++ b/src/libcpp/configure.ac
@@ -184,9 +184,7 @@ m4_changequote(,)
 case $target in
 	aarch64*-*-* | \
 	alpha*-*-* | \
-	arm*-*-*eabi* | \
-	arm*-*-rtems* | \
-	arm*-*-symbianelf* | \
+	arm*-*-* | \
 	x86_64-*-* | \
 	ia64-*-* | \
 	hppa*64*-*-* | \
Index: b/src/libcpp/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libcpp/ChangeLog.linaro
@@ -0,0 +1,75 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-09-05  Yvan Roux  <yvan.roux@linaro.org>
+
+	Backport from trunk r201566.
+	2013-08-07  Richard Earnshaw  <rearnsha@arm.com>
+
+	* configure.ac: Set need_64bit_hwint for all arm targets.
+	* configure: Regenerated.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/libcpp/po/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/libcpp/po/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
Index: b/src/fixincludes/ChangeLog.linaro
===================================================================
--- /dev/null
+++ b/src/fixincludes/ChangeLog.linaro
@@ -0,0 +1,67 @@
+2015-02-12  Michael Collison  <michael.collison@linaro.org>
+
+        GCC Linaro 4.8-2015.02 released.
+
+2014-11-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.11 released.
+
+2014-08-14  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.08 released.
+
+2014-04-07  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.04 released.
+
+2014-03-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.03 released.
+
+2014-02-11  Yvan Roux  <yvan.roux@linaro.org>
+
+	GCC Linaro 4.8-2014.02 released.
+
+2014-01-17  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2014.01 released.
+
+2013-12-21  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.12 released.
+
+2013-11-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.11 released.
+
+2013-10-15  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.10 released.
+
+2013-09-10  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.09 released.
+
+2013-08-14  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.08 released.
+
+2013-07-19  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.07-1 released.
+
+2013-07-05  Christophe Lyon  <christophe.lyon@linaro.org>
+
+	GCC Linaro 4.8-2013.07 released.
+
+2013-06-11  Rob Savoye  <rob.savoye@linaro.org>
+
+	GCC Linaro gcc-linaro-4.8-2013.06 released.
+
+2013-05-14  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.05 released.
+
+2013-04-09  Matthew Gretton-Dann  <matthew.gretton-dann@linaro.org>
+
+	GCC Linaro 4.8-2013.04 released.
